[
  {
    "id": "3fe81706-510a-4542-ae7a-d87b8cb92af6",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "+ η′\nt\nJ\nX\nj=1\nw∗\njµj −\nJ\nX\nj=1\nfjµj\n! + η′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njϵE\nj −\nJ\nX\nj=1\nfjϵE\nj\n! . (A.3)\nEquations (A.2) and (A.3) imply\nJ\nX\nj=1\nw∗\njY I\njt −\nJ\nX\nj=1\nfjY I\njt = (γ′\nt −η′\nt(λ′\nEλE)−1λ′\nEθE)\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nfjZj\n! + η′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nfjY E\nj\n! −η′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njϵE\nj\n+ η′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nfjϵE\nj\n+\nJ\nX\nj=1\nw∗\njξjt −\nJ\nX\nj=1\nfjξjt\n! .",
    "content_hash": "82c487fb67f6848361786723170c364fa13a5f9bab33aa50865a8201f3451558",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d97b18c0-a3d7-4e7e-b419-b00c4a3cbae7",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "OA.5. Implementations of the Optimization Formulations\nTo computationally solve (6), i.e., the Unconstrained design, we propose two methods. The first\nmethod is by enumeration, which takes advantage of the objective function of (6) being separated\nbetween w and v. If we knew which units were to receive treatment and which units were to\nreceive control, then we could decompose (6) into two classical synthetic control problems and\nsolve both of them efficiently. We brute-force enumerate all the possible combinations of the\ntreatment units and control units. Because the two groups of treated and control units can\nbe swapped, we only enumerate combinations such that the cardinality of the treated group is\nsmaller than or equal to the cardinality of the control group. When the cardinality of the treated\ngroup is equal to the cardinality of the control group, we prioritize the treated group to be the\none with the smallest index among the units with positive weights. The second method solves a constrained optimization problem, by converting it into the\ncanonical form of a Quadratic Constraint Quadratic Program (QCQP), which we detail below. The decision variables are wj and vj, ∀j = 1, . . . , J. For simplicity, we write it in a vector form\n˜\nW = (w1, w2, ..., wJ, v1, v2, ..., vJ). Let M be the dimension of the predictors Xj. Let X be an M × J matrix, each column of\nwhich is Xj, which stands for the predictors of unit j.",
    "content_hash": "98e118bef45ccce26cb9cf3a68d554abd44a2eda5d15345a653ecb05daa090eb",
    "location": null,
    "page_start": 1,
    "page_end": 72,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fcec806e-4598-4fa7-8b3b-87c36859f35c",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": ", J,\nvij = 0,\n∀j /∈Jw, i = 1, . . . , J,\nm ≤∥w∥0 ≤m. (9)\nThe parameter ξ > 0 arbitrates potential trade-offs between selecting treated units to fit the\naggregate value of the predictors X and selecting control units to fit the values of the predictors\nfor the treated units. A small value of ξ favors experimental designs with treated units that\nclosely match X. A large value of ξ, on the other hand, favors designs where the values of the\npredictors for the treated units are closely matched by those of their synthetic controls. Let {w∗\nj, v∗\nij}i,j=1,...,J be a solution of the optimization problem in (9). As before, we do\nnot strictly require optimality of {w∗\nj, v∗\nij}i,j=1,...,J, provided {w∗\nj, v∗\nij}i,j=1,...,J is feasible and X −\n11",
    "content_hash": "2600c0a82c2eacc28c58ffe4e0563f7e4909c1d44e0c8f6756d11262d3838bfe",
    "location": null,
    "page_start": 1,
    "page_end": 11,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "30b154bf-4f43-4678-8a3c-b922923c60ff",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "OA.3.2. Proofs of Theorem OA.5 and Theorem OA.6\nProof of Theorem OA.5. For any period t = T0+1, . . . , T we decompose (bτ T\nt −τ T\nt ) as follows,\nbτ T\nt −τ T\nt =\nJ\nX\nj=1\nw∗\njY N\njt −\nJ\nX\nj=1\nv∗\njY N\njt . From (12a), we obtain\nJ\nX\nj=1\nw∗\njY N\njt −\nJ\nX\nj=1\nv∗\njY N\njt = θ′\nt\n\u0010\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nv∗\njZj\n\u0011\n+ λ′\nt\n\u0010\nJ\nX\nj=1\nw∗\njµj −\nJ\nX\nj=1\nv∗\njµj\n\u0011\n+\n\u0010\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt\n\u0011\n. (OA.8)\nSimilarly, using expression (12a), we obtain\nJ\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nv∗\njY E\nj = θE\n\u0010\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nv∗\njZj\n\u0011\n+ λE\n\u0010\nJ\nX\nj=1\nw∗\njµj −\nJ\nX\nj=1\nv∗\njµj\n\u0011\n+\n\u0010\nJ\nX\nj=1\nw∗\njϵE\nj −\nJ\nX\nj=1\nv∗\njϵE\nj\n\u0011\n,\nwhere θE is the (TE × R) matrix with rows equal to the θt’s indexed by E, and ϵE\nj is defined\nanalogously.",
    "content_hash": "743d637d9f878822d1315b01549cfb8df0f2bdbd195b0cff73ebae722fbf1e3a",
    "location": null,
    "page_start": 1,
    "page_end": 68,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "eade0d68-0374-42d9-aa4c-af917cae6c9d",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "wj ≥0, vj ≥0, and wjvj = 0, ∀j = 1, . . . , J. Units with wj > 0 are units that will be assigned to the intervention of interest from T0 + 1 to\nT, and will be used to estimate average outcomes under the intervention. Units with wj = 0\nconstitute an untreated reservoir of potential control units (a “donor pool”). Among units with\nwj = 0, those with vj > 0 will be used to estimate average outcomes under no intervention. The first goal of the experimenter is to choose w1, . . . , wJ such that\nJ\nX\nj=1\nwjY I\njt =\nJ\nX\nj=1\nfjY I\njt,\n(3)\nfor t = T0 +1, . . . , T. If equation (3) holds, a weighted average of outcomes for the units selected\nfor treatment reproduces the average outcome with treatment for the entire population of J units. In practice, however, the choice of w1, . . . , wJ cannot directly rely on matching the population\naverage of Y I\njt, as in equation (3). The quantities Y I\njt are unobserved before time T0 + 1, and\nwill remain unobserved in the experimental periods for the units that are not exposed to the\ntreatment. Instead, we aim to approximate equation (3) using predictors observed at T0 of the\nvalues of Y I\njT0+1, . . . , Y I\njT. Note also that it is not possible to use the weights w1 = f1, . . .",
    "content_hash": "cca204be51cc070d2f566d3ba42bd3937c04f8767dcd17150518d89930d5f91f",
    "location": null,
    "page_start": 1,
    "page_end": 7,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1d71159c-06c9-4bf2-8e27-844092b4f8a8",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "J\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nv∗\njY E\nj = θE\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nv∗\njZj\n! + λE\nJ\nX\nj=1\nw∗\njµj −\nJ\nX\nj=1\nv∗\njµj\n! +\nJ\nX\nj=1\nw∗\njϵE\nj −\nJ\nX\nj=1\nv∗\njϵE\nj\n! . Assumption 3 implies\nJ\nX\nj=1\nw∗\njµj −\nJ\nX\nj=1\nv∗\njµj = −(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njϵE\nj −\nJ\nX\nj=1\nv∗\njϵE\nj\n! . For t ∈B, we have\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt =\nJ\nX\nj=1\nw∗\njY N\njt −\nJ\nX\nj=1\nv∗\njY N\njt\n=\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt −λ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −v∗\nj)ϵE\nj .",
    "content_hash": "7fa0c2032c78c227dc9992e0f94c7a6ef0b69eb3d6e44a76bd442cef5ca641e9",
    "location": null,
    "page_start": 1,
    "page_end": 49,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3c4643ef-9415-4719-b997-3fffa4725a8b",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "m ≤∥w∥0 ≤m. We conclude this section by discussing other possible extensions to the synthetic control\ndesign. First, it is well known that synthetic control estimators may not be unique. Lack\nof uniqueness is typical in settings where the values of the predictors that a synthetic control\nis targeting (i.e., X in equation (6), or Xj for a treated unit in equation (9)) fall inside the\nconvex hull of the values of Xj for the units in the donor pool. To address the potential lack\nof uniqueness, we adapt the penalized estimator of Abadie and L’Hour (2021) to the synthetic\ncontrol designs proposed in this article. The penalized synthetic control estimator of Abadie\nand L’Hour (2021) is unique provided that predictor values for the units in the donor pool are\nin general quadratic position (see Abadie and L’Hour, 2021, for details). Moreover, penalized\nsynthetic controls favor solutions where the synthetic units are composed of units that have\npredictor values, Xj, similar to the target values. Applying the penalized synthetic control of\nAbadie and L’Hour (2021) to the objective function of (6), we obtain\nmin\nw1,...,wJ,\nv1,...,vJ\nX −\nJ\nX\nj=1\nwjXj\n2\n+\nX −\nJ\nX\nj=1\nvjXj\n2\n+ λ1\nJ\nX\nj=1\nwj\nX −Xj\n2\n+ λ2\nJ\nX\nj=1\nvj\nX −Xj\n2\ns.t.",
    "content_hash": "b78e45e37348520f410b8d9c39e1fc2baa9d8f0e09be0e34cdf6486b90f27eee",
    "location": null,
    "page_start": 1,
    "page_end": 14,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7d08959b-7b67-4a0d-a603-496044087221",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Other types of penalization are possible. In particular, Doudchenko and Imbens (2016),\nDoudchenko et al. (2021), and others have proposed synthetic control estimators that use ridge\nor elastic net regularization on the synthetic control weights (e.g., on wj and vj in design (6)). The\nsynthetic control designs proposed in this article can be modified to incorporate regularization\non the weights. Finally, Abadie and L’Hour (2021), Arkhangelsky et al. (2021), and Ben-Michael, Feller\nand Rothstein (2021) have proposed bias-correction techniques for synthetic control methods. Section OA.1 in the Online Appendix provides details on how to apply bias correction techniques\nin a synthetic control design. 3. Formal Results\nWe introduce an extension of the linear factor model commonly employed in the synthetic control\nliterature and use it to analyze the properties of estimators based on synthetic control designs. Assumption 1 Potential outcomes follow a linear factor model,\nY N\njt = δt + θ′\ntZj + λ′\ntµj + ϵjt,\n(12a)\nY I\njt = υt + γ′\ntZj + η′\ntµj + ξjt,\n(12b)\nwhere Zj is a (R × 1) vector of observed covariates, θt and γt are (R × 1) vectors of unknown\nparameters, µj is a (F × 1) vector of unobserved covariates, λt and ηt are (F × 1) vectors of\nunknown parameters, and ϵjt and ξjt are unobserved random shocks.",
    "content_hash": "75130183aed9875644850150629c41abfb714799635f2b6bd64a01c0cc376d4b",
    "location": null,
    "page_start": 1,
    "page_end": 15,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bf051542-142c-4a90-aea5-8f687d023aa3",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": ", T} be the synthetic control estimator in the experimental periods. Similarly, for each t ∈B in the blank periods, let\nbut =\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt. Such but for t ∈B are placebo treatment effects estimated for the blank periods. We study the\nproperties of a test based on combinations from the set {but : t ∈B ∪{T0 + 1, . . . , T}}. We define Π as the set of all (T −T0)-combinations of B∪{T0+1, . . . , T}. That is, for each π ∈\nΠ, π is a subset of indices from the blank periods and the experimental periods B∪{T0+1, . . . , T},\nsuch that |π| = T −T0. The cardinality of Π is |Π| = (T −TE)!/((T −T0)!(T0 −TE)!). For each\ncomponents of θt to λt, which can change the value of ζ. Poincar´e’s separation theorem implies that ζ cannot\nincrease as a result of this shift. Moreover, moving predictors from Zj to µj cannot decrease the values of λ and\nη. Overall, the value of the bias bound in (15) cannot decrease and will typically increase by moving predictors\nfrom Zj to µj. This is not necessarily true for the bound in (16), because a shift of components from Zj to µj\ndecreases the value of R. 19",
    "content_hash": "c7b6460e5d05d178bd7c68d3d32bfec41797d22e6a51aaa2df7839ee28a9154b",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8f0af79d-e35e-41c0-afe9-83a75b8f6725",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "π ∈Π, let π(i) be the ith smallest value in π, and\nbeπ = (buπ(1), buπ(2), ..., buπ(T−T0)). In addition, let be = (buT0+1, . . . , buT) = (bτT0+1, . . . , bτT). This is a vector of treatment effect\nestimates from the experimental periods. For any (T −T0)-dimensional vector e = (e1, . . . , eT−T0),\nwe adopt the test statistic,\nS(e) =\n1\nT −T0\nT−T0\nX\nt=1\n|et| . (18)\nOther choices of test statistics are possible, such as those based on an Lp-norm of e (Cher-\nnozhukov, W¨uthrich and Zhu, 2021) and one-sided versions of the resulting test statistics (i.e.,\nwith the positive or the negative parts of et replacing |et| in equation (18)). The p-value of a permutation test on (18) is\nbp = 1\n|Π|\nX\nπ∈Π\n1{S(beπ) ≥S(be)}\n(19)\nTheorem 2 below shows that if λt are exchangeable random variables for t ∈B ∪{T0+1, . . . , T},\nthen a test of the null hypothesis in (17) based on the p-value in (19) is exact. Theorem 2 Suppose that Assumptions 1, 2(ii), and 3(i) hold.",
    "content_hash": "6d21f69aecb731969d0b557214bf9cd8ab23c777cced6292c948d32dbba899fd",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "af8fa7c7-1208-40a8-8570-862f551cfc73",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "≤Pr\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n≤q1−α+ϵB\n! + δB(ϵB) + δT (ϵT )\n≤Pr\nJ\nX\nj=1\nw∗\njϵj∗−\nJ\nX\nj=1\nv∗\njϵj∗\n≤q1−α+ϵB\n! + ϵT + δB(ϵB) + δT (ϵT )\n≤1 −α + ϵB + ϵT + δB(ϵB) + δT (ϵT ). where the second inequality is because the probability increases if we increase from bq1−α to\nq1−α+ϵB; the third inequality is due to Condition (A.8); the last inequality is due to the definition\nof q1−α+ϵB in (A.9). A.3.2. Proof of Theorem 3\nIn this section, we use Lemma A.1 to prove Theorem 3. Instead of proving exactly Theorem 3,\nwe prove Theorem A.2 below with all the constants provided. Then, setting zB = (T0 −TE)−1\n2\nand zE = T\n−1\n2\nE\nwe prove Theorem 3. Theorem A.2 Assume that Assumptions 1– 3 hold. Assume there exists a constant κ < ∞,\nsuch that for all j = 1, . . . , J, t = 1, . . . , T, ϵjt are continuously distributed with the probability\ndensity function upper bounded by κ. Assume that for t = T0 +1, . . . , T, and j = 1, . . . , J, ξjt has\nthe same distribution as ϵjt.",
    "content_hash": "e2b9652695e5eea8943fc0217c46b743bb780d69a3921c790d50e9aa68fe756b",
    "location": null,
    "page_start": 1,
    "page_end": 46,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "628649f8-5ed0-4f25-bc54-6029dc1a8ee0",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "5.2.2. Performance with Nonlinearities\nWe now examine the behavior of estimators based on synthetic control designs under deviations\nfrom the linear model in (12a) and (12b). We consider a nonlinear data generating process,\nY N\njt = δt + exp (θ′\ntZj) + exp (λ′\ntµj) + ϵjt,\n(23a)\nY I\njt = υt + exp (γ′\ntZj) + exp (η′\ntµj) + ξjt. (23b)\nThe motivation to study a nonlinear model is that nonlinearities may induce interpolation biases,\naffecting the relative performance of the different designs. All parameter values are the same\nas in the simulation setup of section 5.1, except for the values of θt, γt, λt, and ηt, which are\nchosen to be random vectors of i.i.d. Uniform (0, 3) random variables, instead of Uniform (0, 10),\nto control the magnitude of the exponential components in the nonlinear design. Table 3 reports the results for τt. In comparison to the results in Table 2, we now see that the\nUnit-level and Penalized designs can easily match and in some cases improve the performance\nof the Unconstrained design. By fitting each treated unit with a unit-specific synthetic control,\nthe Unit-level design can ameliorate interpolation biases induced by the aggregation of Xj.",
    "content_hash": "f17b3630802ddaf67a172b0814b19d8d9baf17502c69111a7966e7e196614adb",
    "location": null,
    "page_start": 1,
    "page_end": 30,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d3de22f1-5bfc-4a87-8cfb-f78242ccb286",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "≥Pr\nJ\nX\nj=1\nw∗\njϵj∗−\nJ\nX\nj=1\nv∗\njϵj∗\n≤q1−α−ϵB\n! −ϵT −δB(ϵB) −δT (ϵT )\n≥1 −α −ϵB −ϵT −δB(ϵB) −δT (ϵT ). where the second inequality is because the probability decreases if we decrease from bq1−α to\nq1−α−ϵB; the third inequality is due to Condition (A.8); the last inequality is due to the definition\nof q1−α−ϵB in (A.9). Part 2: Consider the event\nE2 =\n\u001a\nbq1−α ≤q1−α+ϵB\n\u001b\n. We wish to show that event E1 happens conditional on event CB. We use Condition (A.8) to\nshow that conditional on event CB,\n1\nT0 −TE\nX\nt∈B\n1\nn\nJ\nX\nj=1\nw∗\njYjt−\nJ\nX\nj=1\nv∗\njYjt\n≤q1−α+ϵB\no\n≥Pr\nJ\nX\nj=1\nw∗\njϵj∗−\nJ\nX\nj=1\nv∗\njϵj∗\n≤q1−α+ϵB\n! −ϵB\n≥1 −α + ϵB −ϵB\n=1 −α,\nwhere the first inequality is due to Condition (A.7); the second inequality is due to the definition\nof q1−α+ϵB in (A.9);\nDue to (20), since bq1−α is the smallest value satisfying this condition, we have that event E2 =\n{bq1−α ≤q1−α+ϵB} happens conditional on event CB.",
    "content_hash": "2312469457f6e6a075c8a8e921f7a078812230c989fa043c0bdc270c06f3e6fd",
    "location": null,
    "page_start": 1,
    "page_end": 45,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7918cf29-ad7f-4737-8e50-1f8f9ec32d59",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "OA.2. Approximate Validity when λt are not Exchangeable\nRecall that in Theorem 2 we have shown that when λt are exchangeable for t ∈B∪{T0+1, . . . , T}\nthe p-value in (19) is exact. In this section, we discuss the case when λt are not necessarily\nexchangeable. We show below in Theorem OA.1 that the p-value in (19) is approximately valid\nwhen TE is large. Theorem OA.1 Assume that Assumptions 1 – 3 hold. Assume there exists a constant κ < ∞,\nsuch that for j = 1, . . . , J, t = 1, . . . , T, ϵjt are continuously distributed with (a version of)\nthe probability density function upper bounded by κ. Then, under the null hypothesis (17), the\np-values of equation (19) are approximately valid. In particular, there is an event C, such that\nconditional on C, for any α ∈(0, 1], we have\nα −2z2 −1\n|Π| ≤Pr(bp ≤α) ≤α + 2z2,\nand the event C happens with probability at least\nPr(C) ≥1 −2J exp\n−\nz2\n1ζ2\n8σ2λ\n4F 2TE\n! −z1\nz2\n4e\np\n2J(min{T −T0, T0 −TE})3 κ,\nwhere z1, z2 are arbitrary positive constants.",
    "content_hash": "3a2feeb4f9fab15758fd4249892bb930b3a170bbabb21946f56dca3e11ecd8d8",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3efb1261-c7ef-4745-a886-a34b21bb21d0",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "estimator,\n1\nm\nJ\nX\nj=1\nDjYjt −\n1\nJ −m\nJ\nX\nj=1\n(1 −Dj)Yjt. 3. STR: Divide the sample in m strata, such that each stratum has at least two units. In\neach stratum, one unit is assigned to treatment at random. The composition of the strata\nis chosen to minimize the maximal within-strata discrepancy in the covariates, Zj, and\npre-experimental outcomes (all normalized to have unit variance). Let Bjk be a binary\nvariable that equals one if and only if unit j belongs to cluster k. Let Jk be the number of\nunits in stratum k. m\nX\nk=1\nJk\nJ\n\u0012\nJ\nX\nj=1\nBjkDjYjt −\n1\nJk −1\nk\nX\nj=1\nBjk(1 −Dj)Yjt\n\u0013\n,\nwhere Jk represents the number of units within the k-th block. 4. REG: Randomized assignment of m units to treatment followed by regression adjustment\non the covariates, Zj. Ordinary least-squares adjustment on all pre-treatment outcomes\n33",
    "content_hash": "d3c1ad98472243ea5079fcf96fc5f025bdbf89e38f1bb04c1562247fe982014d",
    "location": null,
    "page_start": 1,
    "page_end": 33,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "17121873-9ed0-4ac4-a037-3c85fdf43c69",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "is unfeasible as the number of pre-treatment outcomes exceeds the number of units in the\nsample. 5. 1-NN and 5-NN : Randomized assignment of m units to treatment followed by 1-nearest\nneighbor and 5-nearest neighbor matching, respectively, on all pre-experimental outcomes\nand covariates. In both cases, predictors are rescaled to have unit variance. Results are reported in Table 5. Across all values of m, the synthetic control design outperforms\nrandomized assignment, including variants that incorporate pre-stratification, post-stratification,\nor regression adjustment. Taken together with the findings in Table 1, these results underscore\nthe potential of synthetic controls as a more effective design strategy in experiments involving\naggregate units and a limited number of treated units. 6. Conclusions\nExperimental design methods have largely been concerned with settings where a large number\nof experimental units are randomly assigned to a treatment arm, and a similarly large number of\nexperimental units are assigned to a control arm. This focus on large samples and randomization\nhas proven to be enormously useful in various classes of problems but becomes inadequate when\ntreating more than a few units is unfeasible, as is often the case in experimental studies with\nlarge aggregate units (e.g., markets). In that case, randomized designs may produce estimators\nthat are substantially biased (post-randomization) relative to the average treatment effect or\nto the average treatment effect on the treated. Large biases can be expected when the unit or\nunits assigned to treatment fail to approximate average outcomes under treatment for the entire\npopulation or when the units in the control arm fail to approximate the outcomes that treated\nunits would experience without treatment.",
    "content_hash": "ff8faad74f28b15ce17029f88730a3f867ba878965bc6de2b14a31209180e550",
    "location": null,
    "page_start": 1,
    "page_end": 34,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "01fbe401-9501-4e0f-a8c6-104d6bee0bfc",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "and v = (v1, . . . , vJ) is\nmin\nw1,...,wJ,\nv1,...,vJ\nX −\nJ\nX\nj=1\nwjXj\n2\n+\nX −\nJ\nX\nj=1\nvjXj\n2\ns.t. J\nX\nj=1\nwj = 1,\nJ\nX\nj=1\nvj = 1,\nwj, vj ≥0,\n∀j = 1, . . . , J,\nwjvj = 0,\n∀j = 1, . . . , J,\nm ≤∥w∥0 ≤m. (6)\nThe first term of the objective function in (6) measures the discrepancies between the population\naverage of the features in Xj (f-weighted) and the averages of the features for units assigned to\nthe treatment group (w-weighted). The second term is analogous but with the second average\ntaken over the units assigned to no intervention (v-weighted). The first four constraints require\nthat the weights in w, as well as the weights in v, are non-negative and sum to one. They\nalso require that any unit selected for treatment cannot be utilized as a control unit — so,\nif wj > 0, then vj = 0. The last constraint allows a minimum and maximum number of\nunits assigned to treatment. This restriction is of practical importance in a variety of contexts,\nespecially when experimentation is costly and the experimenter is restricted in the number of\nunits that may receive the treatment. We say that the design is Unconstrained if m = 1 and\nm = J −1; otherwise, we say the design is Constrained.",
    "content_hash": "f76ecc5f4bfb69c56f58b8cb6b9f963ec4989b1686e08b4495238551e07a7dc5",
    "location": null,
    "page_start": 1,
    "page_end": 9,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "39876e26-d3c8-4ed9-9861-60b3bf9038ec",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "“Distributional Synthetic Controls.” Econometrica, 91(3): 1105–1117. Jones, Nick, and Sam Barrows. 2019. “Synthetic Control and Alternatives to A/B Testing at Uber.”\nPresented at PyData Amsterdam 2019, https://youtu.be/j5DoJV5S2Ao. 36",
    "content_hash": "306655d4d8c2b119048fe35e234bb157e16a9173b2b4dded3a9609bd15e81aff",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5dae1091-0dae-468c-982e-a854fff1ffe6",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "“Walmart Condensed Sales Data.” https://www.kaggle.com/ds/3471234. Rigollet, Philippe, and Jan-Christian H¨utter. 2019. “High Dimensional Statistics.” http://\nwww-math.mit.edu/~rigollet/PDFs/RigNotes17.pdf. Thorlund, Kristian, Louis Dron, Jay JH Park, and Edward J Mills. 2020. “Synthetic and\nexternal controls in clinical trials–a primer for researchers.” Clinical epidemiology, 457–467. Vives-i-Bastida,\nJaume. 2022. “Predictor Selection for Synthetic Controls.” arXiv e-prints,\n2203.11576. Vovk, Vladimir, Alex Gammerman, and Glenn Shafer. 2005. Algorithmic Learning in a Random\nWorld. Springer Science & Business Media. 37",
    "content_hash": "5c7015e3bddffb23d279aa9071e895b13649d9dc62eef8f4ca79fcacbf7ce771",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d6832227-0994-4255-b293-764e2a2ed952",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Synthetic Controls for Experimental Design\nAlberto Abadie\nJinglong Zhao\nMIT\nBoston University\nApril 2025\nAbstract\nThis article studies experimental design in settings where the experimental units are\nlarge aggregate entities (e.g., markets), and only one or a small number of units can\nbe exposed to the treatment. In such settings, randomization of the treatment may\nresult in treated and control groups with substantially different baseline characteris-\ntics, inducing biases. We propose a variety of experimental non-randomized synthetic\ncontrol designs (Abadie, Diamond and Hainmueller, 2010, Abadie and Gardeazabal,\n2003) that select the units to be treated, as well as the untreated units to be used as a\ncontrol group. Average potential outcomes with treatment are estimated as weighted\naverages of observed outcomes for treated units, and average potential outcomes\nwithout treatment as weighted averages of observed outcomes for control units. We\nanalyze the properties of estimators based on synthetic control designs and propose\nnew inferential techniques. We show that in experimental settings with aggregate\nunits, synthetic control designs can substantially reduce estimation biases in compar-\nison to randomization of the treatment. 1. Introduction\nConsider the problem of a ride-sharing company choosing between two compensation plans for\ndrivers (Doudchenko et al., n.d.; Jones and Barrows, 2019). The company can either keep the\ncurrent compensation plan or adopt a new one with higher incentives. In order to estimate the\neffect of a change in compensation plans on profits, the company’s data science unit designs\nan experimental evaluation where the new plan is deployed at a small scale, say, in one of the\nlocal markets (cities) in the country.",
    "content_hash": "e35386ac460b4911190e7593b3efd0385a8c953a347d5eb9c7870b01c5169759",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "Synthetic Controls for Experimental Design",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f0d894be-ad0b-46ad-9eee-702c0881ab61",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "In this setting, a randomized control trial — or A/B test,\nwhere drivers in a local market are randomized into the new plan (active treatment arm) or\nthe status quo (control treatment arm) — is problematic. On the one hand, such an experiment\nAlberto Abadie, Department of Economics, MIT, abadie@mit.edu. Jinglong Zhao, Questrom School of Busi-\nness, Boston University, jinglong@bu.edu. The authors are grateful to Victor Chernozhukov, Guido Imbens,\nRahul Mazumder, Jaume Vives-i-Bastida, Yinchu Zhu and seminar participants at Amazon.com, Brown, Har-\nvard, Princeton, Stanford, the Online Causal Inference Seminar, and USC for helpful comments and discussions. The replication codes can be found at Github. Abadie gratefully acknowledges NSF funding (SES-1756692). arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
    "content_hash": "740f43828b48a70431e65b9d489234301a8e84c67d91f9796a9ed9b40ac73889",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "April 2025",
      "heading_level": 2
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ba6c8f2a-86d6-4c21-8e90-8508ae6fafde",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Appendix\nA. Proofs\nA.1. Proof of Theorem 1\nProof of Theorem 1. For any period t = T0 + 1, . . . , T we decompose (bτt −τt) as follows,\nbτt −τt =\nJ\nX\nj=1\nw∗\njY I\njt −\nJ\nX\nj=1\nv∗\njY N\njt\n! −\nJ\nX\nj=1\nfjY I\njt −\nJ\nX\nj=1\nfjY N\njt\n! =\nJ\nX\nj=1\nw∗\njY I\njt −\nJ\nX\nj=1\nfjY I\njt\n! −\nJ\nX\nj=1\nv∗\njY N\njt −\nJ\nX\nj=1\nfjY N\njt\n! . (A.1)\nThe first term in (A.1) measures the difference between the synthetic treatment outcome and the\naggregated treatment outcomes. The second term measures the difference between the synthetic\ncontrol outcome and the aggregate control outcomes. We bound these two terms separately. From (12b), we obtain\nJ\nX\nj=1\nw∗\njY I\njt −\nJ\nX\nj=1\nfjY I\njt = γ′\nt\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nfjZj\n! + η′\nt\nJ\nX\nj=1\nw∗\njµj −\nJ\nX\nj=1\nfjµj\n! +\nJ\nX\nj=1\nw∗\njξjt −\nJ\nX\nj=1\nfjξjt\n!",
    "content_hash": "8de94fddf55c7ef1b7f717bbb1428a78e45c155a1e2333be356482e9f0a07928",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "19d83b34-3be1-4d88-a8af-daf4b7c7b068",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "OA.14\n\nTable OA.1: Average Treatment Effects on the Treated (Averages over 1000 Simulations)\nτ T\nt\nbτt\nMAE T\nRMSE T\nt = 26\nt = 27\nt = 28\nt = 29\nt = 30\nt = 26\nt = 27\nt = 28\nt = 29\nt = 30\nUnconstrained\n-13.58\n-10.98\n-8.34\n-5.00\n-2.46\n-13.57\n-10.97\n-8.37\n-5.06\n-2.52\n1.01\n1.18\nConstrained\nm = 1\n-13.80\n-11.09\n-8.37\n-4.82\n-2.50\n-13.61\n-10.97\n-8.39\n-4.86\n-2.41\n3.02\n3.57\nm = 2\n-13.53\n-10.89\n-8.42\n-4.93\n-2.40\n-13.58\n-10.90\n-8.43\n-5.01\n-2.40\n1.80\n2.13\nm = 3\n-13.39\n-10.92\n-8.35\n-4.97\n-2.54\n-13.56\n-11.00\n-8.38\n-5.05\n-2.52\n1.39\n1.64\nm = 4\n-13.54\n-11.05\n-8.42\n-4.95\n-2.52\n-13.59\n-11.06\n-8.40\n-4.99\n-2.50\n1.22\n1.44\nm = 5\n-13.57\n-11.03\n-8.42\n-5.02\n-2.45\n-13.57\n-11.01\n-8.37\n-5.02\n-2.48\n1.11\n1.30\nm = 6\n-13.61\n-11.06\n-8.36\n-4.99\n-2.46\n-13.51\n-10.95\n-8.29\n-5.01\n-2.47\n1.03\n1.22\nm = 7\n-13.58\n-10.97\n-8.34\n-5.00\n-2.46\n-13.57\n-10.96\n-8.37\n-5.06\n-2.52\n1.01\n1.18\nWeakly-targeted\nβ = 0.01\n-13.59\n-10.99\n-8.31\n-5.00\n-2.51\n-13.58\n-10.95\n-8.38\n-4.99\n-2.53\n1.31\n1.55\nβ = 0.1\n-13.59\n-10.98\n-8.37\n-5.01\n-2.51\n-13.57\n-11.00\n-8.34\n-4.98\n-2.52\n1.07\n1.26\nβ = 1\n-13.55\n-10.99\n-8.35\n-5.01\n-2.48\n-13.56\n-10.98\n-8.32\n-4.93\n-2.44\n0.99\n1.16\nβ = 10\n-13.45\n-10.96\n-8.35\n-5.06\n-2.50\n-13.57\n-10.98\n-8.38\n-5.01\n-2.51\n0.99\n1.15\nβ = 100\n-13.49\n-10.90\n-8.33\n-5.01\n-2.48\n-13.60\n-10.98\n-8.39\n-5.07\n-2.52\n1.00\n1.16\nUnit-level\nξ = 0.01\n-13.60\n-10.98\n-8.35\n-4.98\n-2.51\n-13.60\n-10.95\n-8.39\n-5.04\n-2.53\n1.09\n1.29\nξ = 0.1\n-13.56\n-10.99\n-8.38\n-4.95\n-2.50\n-13.58\n-10.97\n-8.35\n-4.97\n-2.47\n1.08\n1.28\nξ = 1\n-13.57\n-11.02\n-8.31\n-4.91\n-2.48\n-13.57\n-10.99\n-8.39\n-4.99\n-2.49\n1.40\n1.66\nξ = 10\n-13.78\n-10.98\n-8.37\n-4.87\n-2.52\n-13.60\n-10.93\n-8.45\n-5.05\n-2.52\n1.96\n2.33\nξ = 100\n-13.81\n-10.90\n-8.39\n-4.80\n-2.55\n-13.61\n-10.86\n-8.48\n-5.02\n-2.54\n2.35\n2.78\nPenalized\nλ = 0.01\n-13.59\n-10.99\n-8.35\n-5.00\n-2.47\n-13.59\n-10.98\n-8.35\n-5.05\n-2.48\n1.05\n1.23\nλ = 0.1\n-13.57\n-10.98\n-8.36\n-4.91\n-2.42\n-13.64\n-11.03\n-8.43\n-5.03\n-2.50\n1.36\n1.61\nλ = 1\n-13.69\n-11.01\n-8.33\n-4.80\n-2.48\n-13.67\n-10.96\n-8.41\n-4.87\n-2.45\n2.19\n2.59\nλ = 10\n-13.88\n-11.06\n-8.23\n-4.73\n-2.40\n-13.68\n-11.04\n-8.37\n-4.79\n-2.45\n3.81\n4.50\nλ = 100\n-13.90\n-11.10\n-8.27\n-4.75\n-2.46\n-13.64\n-10.94\n-8.42\n-4.86\n-2.50\n4.24\n5.00\nOA.15",
    "content_hash": "258f94f128b4f8349ae45cebbc8f052c6ec1d7c83027e7d243838240a28c7efe",
    "location": null,
    "page_start": 1,
    "page_end": 66,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a1812557-1dd8-4979-addc-c956fbf2a793",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "OA.3.1.2. Performance with Nonlinearities\nWe now study the behavior of the estimators based on synthetic control designs under deviations\nfrom the linear model in (12a) and (12b). We consider a nonlinear data generating process as\ndefined in (23a) and (23b). Table OA.2 reports the results for τ T\nt . In comparison to the results in Tables OA.1, we\nnow see that the Unit-level and Penalized designs can easily match and in some cases improve\nthe performance of the Unconstrained design, especially for the estimation of τ T\nt . By fitting\neach treated unit with a unit-specific synthetic control, the Unit-level design can ameliorate\ninterpolation biases induced by the aggregation of Xj. Like in Table OA.1, the Weakly targeted\ndesign easily outperforms the unconstrained estimator for large values of β.",
    "content_hash": "cf40ecc1a1856065fa3919bfa523e1a72b47f165b2ec5e423de7b5b983bd697a",
    "location": null,
    "page_start": 1,
    "page_end": 65,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "979a4f5b-6784-43b4-a283-bf24e19853b6",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "−\nPTp\nk=1 |Πk|\n√\nk3\n|Π|\n· z1\nz2\n· 4e\n√\n2Jκ,\nwhich finishes the proof. OA.3. Estimating the Average Effect of Treatment on the Treated Units\nIn Section 3, we have shown formal results of the bias bounds in estimating the average treatment\neffect. In this section, we present similar results for estimating the average effect of treatment\non the treated units. Similar to Assumption 3, we begin with the assumption of perfect fit. OA.11",
    "content_hash": "81e394c043caff1455b9f13991731dd50a48686769e4dc1c523db8898582baf6",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1f2707c7-f549-432f-9335-7b7f2848a59a",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "We obtain\nPr\n\u0012\nX\nt∈π\\π0\n|ut| −\nX\nt∈π0\\π\n|ut|\n≤2kz1\n\u0013\n≤4e\n√\n2Jk3z1κ. Next, due to Markov inequality, for any constant z2 > 0, we have\nPr\nTp\nX\nk=1\nX\nπ∈Πk\n1\n\u001a\nX\nt∈π\\π0\n|ut| −\nX\nt∈π0\\π\n|ut|\n≤2kz1\n\u001b\n≥|Π|z2\n! ≤\n1\n|Π|z2\nTp\nX\nk=1\nX\nπ∈Πk\nE\n\"\n1\n\u001a\nX\nt∈π\\π0\n|ut| −\nX\nt∈π0\\π\n|ut|\n≤2kz1\n\u001b#\n≤\nPTp\nk=1 |Πk|4e\n√\n2Jk3z1κ\n|Π|z2\n. To conclude step two, define the event\nC2 =\n( Tp\nX\nk=1\nX\nπ∈Πk\n1\n\u001a\nX\nt∈π\\π0\n|ut| −\nX\nt∈π0\\π\n|ut|\n≤2kz1\n\u001b\n< |Π|z2\n)\n. (OA.6)\nOA.8",
    "content_hash": "8f08cd4c724135e73d33218ca510c366a377cb3d0d3eabc04fdd9f69ceede490",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f35a784b-05d0-49bc-8356-ed1033587320",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "In those instances, random samples from Π can be\nused to approximate the p-value in equation (19). The inferential technique proposed in this article is related to, but distinct from, the permu-\ntation methods in Abadie, Diamond and Hainmueller (2010), Chernozhukov, W¨uthrich and Zhu\n(2019), Chernozhukov, W¨uthrich and Zhu (2021), Firpo and Possebom (2018), Lei and Cand`es\n(2021), and others. Inferential methods that reassign treatment across units (e.g., Abadie, Dia-\nmond and Hainmueller, 2010) are not appropriate for the designs of Section 2, which explicitly\nselect treated and control units to satisfy an optimality criterion. Similar to Chernozhukov, W¨uthrich and Zhu (2021), our method is based on rearrangements\nof estimated treatment effects across time periods. But unlike Chernozhukov, W¨uthrich and Zhu\n(2021), which proposes permutations over all periods, including the pre-intervention periods, our\ninferential method permutes only over the blank periods and post-intervention periods, which\nare not used to estimate the weights in the synthetic control design.",
    "content_hash": "f0ca361399fe570043217ff69e6338ef75eb2918e0fd41879d536454100f3632",
    "location": null,
    "page_start": 1,
    "page_end": 21,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "28638708-d4b1-4bac-87ae-00e09534cfad",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "≥1 −\nJ\nX\nj=1\nPr\nX\ns∈E\nλ\n2F\nTEζ |ϵjs| > z1\n2\n! ≥1 −2J exp\n−\nz2\n1ζ2\n8σ2λ\n4F 2TE\n! ,\nwhere the second inequality follows from union bound, and the third inequality is the Chernoff\nbound for sub-Gaussian random variables. (Step two.) Define ˜z1 = 2z1 > 0, and Tp = min{T −T0, T0−TE}. For each k ∈{0, 1, 2, ..., Tp},\nwe define the following sets of permutations. First, define Π0 = {π0}, where π0 is defined as the\nset of post-intervention indices π0 = {T0 + 1, . . . , T}. Then, for any k ∈{1, 2, . . . , Tp}, define\nΠk =\n\u001a\nπ ∈Π\n|π \\ π0| = k\n\u001b\nto be the set of (T −T0)-combinations with exactly k many indices from the blank periods. Using\nthe above definitions, we can decompose Π into\nΠ =\nTp\n[\nk=0\nΠk. OA.7",
    "content_hash": "bb97047bba7e72641cb40734e09e425dd3aad8a5820f94db57814394ad5a186d",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d2ad5c66-3216-4599-9962-f69f32d08cbb",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "To prove 2, note that for any v ≥0,\nfaX(v) = 1\n|a|fX(v/a) ≤1\n|a|ΛX. Lemma OA.4 Recall that ut is defined as (OA.4) and (OA.5), for the blank periods and the\nexperimental periods, respectively. Under the null hypothesis (17), the probability density of ut\ncan be bounded by\nΛut ≤1\n2\n√\neJκ. OA.4",
    "content_hash": "c0b909a592cab217a9d7e2fa0b9222083fd84a87e4c4b55c1d1a7ff0ba1bdb41",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8e039578-bb34-4a32-8403-311c4cb8fbdc",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "OA.2.2. Lemmas for the Proof of Theorem OA.1\nFor each continuously distributed random variable X with a density fX, define ΛX to be the\nsmallest upper bound on the probability density fX. Lemma OA.2 (Corollary 2, Bobkov and Chistyakov (2014)) Let X1, X2, . . . , Xn be in-\ndependent and continuously distributed random variables with densities fX1, fX2, . . . , fXn. For\nany k ∈{1, 2, . . . , n}, let ΛXk be the smallest upper bound on the probability density fXk. For\nany a1, a2, . . . , an, let X = a1X1+a2X2+. . .+anXn. Suppose for any k ∈{1, 2, . . . , n}, ΛXk ≤κ;\nand if Pn\nk=1 a2\nk = 1,\nΛX ≤√eκ. Lemma OA.3 Let X be a continuously distributed random variable with a density fX. Let ΛX\nbe the smallest upper bound on the probability density fX. 1. The random variable |X| has a density f|X| bounded by Λ|X| ≤2ΛX;\n2. For any constant a ̸= 0, the random variable aX has a density faX bounded by ΛaX ≤\nΛX/|a|. Proof of Lemma OA.3. To prove 1, note that for any v ≥0,\nf|X|(v) = fX(v) + fX(−v) ≤2ΛX.",
    "content_hash": "0391d6dfe890fa7ecc0c4a16681c93628e87cb4a981d168cfa9a7019f77e9795",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "81a17492-1a7f-4a0e-9d8b-1d7352fee183",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "To address these challenges, we propose using the synthetic control method (Abadie, Dia-\nmond and Hainmueller, 2010, Abadie and Gardeazabal, 2003) as an experimental design to select\ntreated units in non-randomized experiments, as well as the untreated units to serve as a compar-\nison group. We adopt the name synthetic control designs to refer to the resulting experimental\n1A randomized evaluation across many markets is a potential solution to the problem of experimental interfer-\nence between drivers. In practice, however, large-scale market-level randomized evaluations are often unfeasible. In the context of the ridesharing company example, large-scale market-level randomized evaluations (i) could be\nprohibitively expensive, (ii) could still raise substantial equity concerns, (iii) could negatively affect morale for\nthe large number of drivers in the treated cities if the program is rolled back after experimentation, and (iv) in\nsome cases, the number of cities where the company operates could be too small for effective randomization. 2",
    "content_hash": "513118336e6ce0c8c129d0e4a434d20acc07a9d70a1953089bee9a85f489141a",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1ceef88a-f2d5-4c1d-9f8c-9622349c2fe8",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "raises equity concerns, as drivers in the same local market but in different treatment arms obtain\ndifferent compensations for the same jobs. On the other hand, if drivers in the active treatment\narm respond to higher incentives by working longer hours, they will effectively steal business\nfrom drivers in the control arm of the experiment, resulting in biased experimental estimates.1\nOne possible approach to this problem is to assign an entire local market to treatment, and\nuse the rest of the local markets, which remain under the current compensation plan during\nthe experimental periods, as potential comparison units. In this setting, using randomization to\nassign the active treatment allows ex-ante (i.e., pre-randomization) unbiased estimation of the\neffect of the active treatment. However, ex-post (i.e., post-randomization) biases can be large if,\nat baseline, the treated unit differs from the untreated units in the values of the features that\naffect the outcomes of interest. We document the magnitude and practical relevance of these\nbiases in Sections 4 and 5. As in the ride-sharing example where there is only one treated local market, large biases may\narise more generally in randomized studies when either the treatment arm or the control arm\ncontains a small number of units, so randomized treatment assignment may not produce treated\nand control groups that are similar in their features. In those cases, the fact that estimation\nbiases would have averaged out over alternative treatment assignments is of little comfort to a\nresearcher who, in practice, is limited to one assignment only.",
    "content_hash": "1b2cc598c9b220856a6e8b3abf9c434b0e70caa9d04fb5334bff211a63cf315e",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "360dff38-df68-4e23-bb24-e1e9a07cbd54",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "designs.2,3\nIn our framework, the choice of the treated unit (or treated units, if multiple treated units\nare desired) aims to accomplish two goals. First, the treated units should be representative of an\naggregate of interest, such as a national market, so that the estimated effect reflects the aggregate\nimpact of the treatment. Second, the treated units should not be idiosyncratic in the sense that\nthe untreated units cannot closely approximate their features. Otherwise, the reliability of the\nestimate of the effect on the treated unit may be questionable. We show how to achieve these\ntwo objectives, whenever they are possible to achieve, using synthetic control methods. While we are aware of the extensive use of synthetic control methods for experimental design\nin data science units, especially in the technology industry,4 the academic literature on this sub-\nject is at a nascent stage. There are, however, a few publicly available studies that are connected\nto this article. Aside from the present article, to our knowledge, Doudchenko et al. (n.d.) and\nDoudchenko et al. (2021) are the only other publicly available studies on the topic of experi-\nmental design with synthetic controls. The focus of Doudchenko et al. (n.d.) is on statistical\npower, which they calculate by simulating the estimated effects of placebo interventions using\nhistorical (pre-experimental) data. That is, the selection of treated units is based on a measure\nof statistical power implied by the distribution of the placebo estimates for each unit. As a\nresult, estimates based on the procedure in Doudchenko et al.",
    "content_hash": "b6634ecc1b5c6cbceb23cc3e7851d635beb970a4e66b0717174a43615f6886aa",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c266670e-f304-4664-9e59-1dd42e91fe7c",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "(n.d.) target the effect of the\ntreatment for the unit or units that are most closely tracked in the placebo distribution. In\nthe same spirit, the target parameter in Doudchenko et al. (2021) is the treatment effect for a\nweighted average of treated units that can be closely matched in their pre-treatment outcomes\nby a weighted average of untreated units. In the present article, we aim to take a different\nperspective on the problem of unit selection in experiments with synthetic controls; one that\ntakes into account the extent to which different sets of treated and control units approximate an\n2While we leave the “experimental” qualifier implicit in “synthetic control design”, it should be noted that the\nsynthetic control designs proposed in this article differ from observational synthetic control designs (e.g., Abadie,\nDiamond and Hainmueller, 2010, Abadie and Gardeazabal, 2003, Doudchenko and Imbens, 2016), for which the\nidentity of the treated unit(s) is taken as given. 3See, e.g., Abadie (2021), Amjad, Shah and Shen (2018), Arkhangelsky et al. (2021), Doudchenko and Imbens\n(2016) for background material on synthetic controls and related methods. 4See, in particular, Jones and Barrows (2019), which also provides the basis for the ride-sharing example\nabove. 3",
    "content_hash": "7cbf48666781d498bd3a8deb23235a86124e6e054c4b78911d10cdcd19a7bcba",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fd519cb9-e649-4aea-90cf-e3c9a0bd4fa9",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Their work primarily focuses on the analysis of experimental\ndata, but not on the design of experiments. Bottmer et al. (2021) is also related to the present\narticle in the sense that it studies synthetic control estimation in an experimental setting. Their\narticle, however, considers only the case when the treatment is randomized, and is not concerned\nwith issues of experimental design. The research designs in this article are also related to ex-ante synthetic control designs for\nobservational studies (see Abadie, 2021, Kasy and Lehner, 2023, and Chen, 2023, the latter\nfor an online version of the problem) in that the synthetic weights are computed and can be\npre-registered before post-intervention outcomes are realized. However, our methods differ sig-\nnificantly in one key aspect: they confront the challenge that experimenters face when selecting\n5Consistent with the majority of literature on synthetic controls, our focus is primarily on average treatment\neffects. For an analysis of distributional effects using synthetic controls, see Gunsilius (2023). 4",
    "content_hash": "b626c0f93c4d6605ed7f303a78ef87579f429897dd7f8ee32746741cb32c8956",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1ae376e9-7c73-4fef-81c6-ea2af8380521",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "aggregate causal effect of interest chosen by the analyst, such as the average treatment effect for\nthe relevant population.5 The inferential methods in the present article also differ from those in\nthe related literature. In particular, Doudchenko et al. (2021) proposes a permutation procedure\nfor inference that requires that potential outcomes without the treatment are independent and\nidentically distributed (i.i.d.) in time. In contrast, the inferential procedure proposed in the\npresent article allows for time series dependence and non-stationarity in outcomes, which are\npervasive features of time-series data. Another important difference between the present article\nand Doudchenko et al. (n.d.) and Doudchenko et al. (2021) is that Doudchenko et al. (n.d.) and\nDoudchenko et al. (2021) make use of pre-treatment outcomes only to select treated and control\nunits, while our method allows the use of other observed features of the units. A related literature applies synthetic control methods and nearest-neighbor matching methods\nto select experimental sites in multi-site designs (Egami and Lee, 2024, Montiel Olea et al., 2024). In contrast, we examine settings where treatment occurs at the aggregate (i.e., site) level: each\nsite receives only treatment or only control, precluding the estimation of site-level treatment\neffects. Agarwal, Shah and Shen (2021) proposes synthetic interventions, a framework related to\nsynthetic controls, and applies it to estimate treatment effect heterogeneity in an experimental\nsetting with multiple treatments.",
    "content_hash": "dda00aaeb3ec8e13369b775910bf593180a306f538c569973f0904c369e32db9",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d231b217-702e-40b3-9ea1-c3f7a43c080e",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Section 3 details the formal properties of estimators\nbased on synthetic control designs and proposes inferential methods. In Section 4, we report\nthe findings from an empirical validation of synthetic control designs using sales data from a\nsample of Walmart stores. Section 5 discusses the results of simulation studies. Finally, Section\n6 provides concluding remarks. The Appendix contains proofs and supplemental materials. 2. Synthetic Control Designs\nWe consider a setting with T time periods and J units, which may represent J local markets as\nin the ride-sharing example in the previous section. Let T0 be the number of pre-experimental\nperiods, with 1 ≤T0 < T. At the end of period T0, a researcher designs an experiment to conduct\nduring periods T0+1, T0+2, . . . , T. Using the information available at T0, the experimenter aims\n5",
    "content_hash": "972cdad988cd8fdbe0291bc43a526b7a628c7b5423110cf0befdd81404fedc6c",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cad14824-58cb-42fa-9d5e-7eaf1b5a5881",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "specific units for exposure to the intervention of interest. In a wider context, our methods are\nrooted in the broader framework of experimental non-randomized designs (see, e.g., Armstrong\nand Koles´ar, 2018, Kasy, 2016, Thorlund et al., 2020). Yet, they diverge by addressing a distinct\nchallenge: estimating synthetic control-based aggregate counterfactuals in experimental settings\nwhere only a limited number of aggregate units can be treated. An alternative approach to control post-randomization bias involves stratifying units based\non covariate values prior to randomization of treatment within each stratum. Stratification can\nsignificantly reduce post-randomization biases if units have similar covariate values within strata. However, traditional stratification methods do not adapt to the setting considered in this article,\nwhich features a limited number of large aggregate entities as units of analysis and a single unit\nor a handful chosen for treatment. Because every stratum in stratified designs must have at least\none unit randomized into treatment, the number of strata cannot exceed the desired number of\ntreated units in the experiment. In the case of only one treated unit, we would be limited to\na single stratum. This may lead to significant variation in units’ characteristics within strata,\nreducing the appeal of stratification procedures. Additionally, a stratified design with a small\nnumber of strata or treated units may result in the selection of a set of treated units that is not\ntruly representative of the target population. The rest of the article is organized as follows: Section 2 presents and discusses the synthetic\ncontrol designs proposed in this article.",
    "content_hash": "ff73fbc4b93301ef36de52fda4132cc35aef07079c5492ea3399d2441eabe4e4",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d77b02a2-e9ba-425a-8dd5-a851c21e14a0",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Without loss of generality, and because\nit is often the case in applications, we can assume that the weights fj sum to one,\nJ\nX\nj=1\nfj = 1. When units are equally weighted, we set fj = 1/J for j = 1, . . . , J. We use the notation f for a\nvector that collects the values of fj for all the units, i.e., f = (f1, . . . , fJ). At time T0, in order to estimate the treatment effect τt for t = T0+1, . . . , T, the experimenter\nchooses w = (w1, . . . , wJ) and v = (v1, . . . , vJ), such that\nJ\nX\nj=1\nwj = 1,\nJ\nX\nj=1\nvj = 1,\n(2)\n6",
    "content_hash": "e7f21794020ba9790e1db609f05645ad197ace2c1f8d484c894dc6bcb643974d",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e03a184a-0992-4fa6-bc99-6dd0963cdffb",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "to select the set of units that will receive the treatment (intervention) during the experimental\nperiods. To define causal parameters, we formally adopt a potential outcomes framework. For any\nj ∈{1, . . . , J} and any t ∈{T0 + 1, . . . , T}, Y I\njt is the potential outcome for unit j at time t\nwhen unit j is exposed to treatment starting at T0 + 1. Similarly, for any j ∈{1, . . . , J} and\nany t ∈{1, . . . , T}, Y N\njt is the potential outcome for unit j at time t under no treatment. In the\nridesharing example, Y I\njt and Y N\njt could measure net revenue divided by market size under the\nactive and the control treatment, respectively. Unit-level treatment effects are defined as\nY I\njt −Y N\njt ,\nfor j = 1, . . . , J and t = T0 + 1, . . . , T. They represent the effect of switching unit j to the active\ntreatment at time T0+1 on the outcome of unit j at time t > T0. We aim to estimate the average\ntreatment effect\nτt =\nJ\nX\nj=1\nfj · (Y I\njt −Y N\njt ),\n(1)\nfor t = T0 + 1, . . . , T. In this expression, f1, . . . , fJ are known positive weights that define the\naverage of interest. In the ride-sharing example from the previous section, fj may represent the\nsize of local market j as a share of the national market.",
    "content_hash": "2523b1c1b9972c4c1c65304c1785ffdde91adac5939e3cdd9cfdf639f13d1243",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6c189bb6-2b8b-4a18-879c-cf504aca5494",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": ", wJ = fJ,\nbecause it would leave no units in the donor pool, making the set of units with vj > 0 empty\nand violating equation (2). The second goal of the experimenter is to choose v1, . . . , vJ such that\nJ\nX\nj=1\nvjY N\njt =\nJ\nX\nj=1\nfjY N\njt ,\n(4)\nor, alternatively,\nJ\nX\nj=1\nvjY N\njt =\nJ\nX\nj=1\nwjY N\njt . (5)\nIf equations (4) or (5) hold, a weighted average of outcomes for the units in the donor pool\nreproduces the average outcome without treatment for the entire population of J units (equation\n(4)), or for the units selected for treatment (equation (5)). Like in the previous case with treated\noutcomes, it is not feasible to directly choose v1, . . . , vJ so that equation (4) or (5) is satisfied. 7",
    "content_hash": "9d11a59d543f6e0a3b63aba2b21278a41056dc2a2149e7754e9613cc2f2bb06a",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7ee582d1-0105-40af-b79f-d4ad528d4e17",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": ", wJ) and v = (v1, . . . , vJ) to match the pre-intervention values of\npredictors of the potential outcomes Y N\njt and Y I\njt for t > T0. Let Xj be a column vector of pre-intervention features of unit j. We view the features in\nXj as predictors of the values of Y N\njt and Y I\njt in the experimental periods, in a sense that will be\nmade precise in Section 3. We use the notation\nX =\nJ\nX\nj=1\nfjXj. That is, X is the vector of population values for the predictors in Xj. For any real vector x,\n∥x∥is the Euclidean norm of x, and ∥x∥0 is the number of non-zero coordinates of x. Let m\nand m be positive integers such that 1 ≤m ≤m ≤J −1. A simple selector of w = (w1, . . . , wJ)\n8",
    "content_hash": "b2733e879c6e36dc2afd41b535ce2675e890681c420a5a6d4ca0499a48591e66",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "852eba02-cae4-4f49-beb4-6cdb59ff5495",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Instead, we propose a variety of methods to approximate either (4) or (5) based on predictors of\nY N\njT0+1, . . . , Y N\njT . For the treated units, we define Yjt = Y N\njt if t = 1, . . . , T0, and Yjt = Y I\njt if t = T0 + 1, . . . , T. For the untreated units, we define Yjt = Y N\njt , for all t = 1, . . . , T. That is, Yjt is the outcome\nobserved for unit j = 1, . . . , J at time t = 1, . . . , T. We say that\nJ\nX\nj=1\nwjYjt\nand\nJ\nX\nj=1\nvjYjt\nare the synthetic treated and synthetic control outcomes, respectively. The difference between\nthese two quantities is\nτt(w, v) =\nJ\nX\nj=1\nwjYjt −\nJ\nX\nj=1\nvjYjt,\nfor t = T0 + 1, . . . , T. Suppose that equations (3) and (4) hold. Then, τt(w, v) is equal to the\naverage treatment effect, τt. If equation (5) holds instead, then τt(w, v) is equal to the average\neffect of the treatment on the treated (w-weighted),\nτ T\nt =\nJ\nX\nj=1\nwj · (Y I\njt −Y N\njt )\n(Doudchenko et al., 2021). We choose w = (w1, . . .",
    "content_hash": "12bf223af7f8971526f108f1aa2a00dbccd704a0c018126cda4d9144641457ff",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0e1bf5b1-3cdb-47fd-8160-55d8c489a9f8",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "The last constraint in (6) is not the\nonly conceivable restriction to the size or cost of the experiment. An explicit upper bound on\nthe cost of an experiment would be given by c′d ≤B, where the j-th coordinate of c is equal to\nthe cost of assigning unit j to treatment, d is a J-dimensional vector with ones at coordinates\nwhere wj > 0, and zeros otherwise, and B is the experimenter’s budget. Let w∗= (w∗\n1, . . . , w∗\nJ) and v∗= (v∗\n1, . . . , v∗\nJ) be a solution to the optimization problem in\n(6). In practice, we do not require optimality of (w∗, v∗), as long as (w∗, v∗) is feasible and\nsatisfies X −PJ\nj=1 w∗\njXj ≈0 and X −PJ\nj=1 v∗\njXj ≈0, where 0 is a vector of zeros of the same\n9",
    "content_hash": "ef88c5f5fc51b9c7182c093420c1daf3ce0ca8fbcfc2159abb3338235478af40",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c01e41b7-6cab-4ee6-824a-27be03a897a0",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "J\nX\nj=1\nwj = 1,\nJ\nX\nj=1\nvj = 1,\nwj, vj ≥0,\n∀j = 1, . . . , J,\nwjvj = 0,\n∀j = 1, . . . , J,\nm ≤∥w∥0 ≤m. (11)\nHere, λ1 and λ2 are positive constants that penalize discrepancies between the target values of\nthe predictor X and the values of the predictors for the units that contribute to their synthetic\ncounterparts.6\n6See Abadie and L’Hour (2021) for details on penalized synthetic control estimators. The synthetic control\n14",
    "content_hash": "d6c8f685315bba2306c102c86336e1d57489b1059a60c996a114bbf31f4b249e",
    "location": null,
    "page_start": 9,
    "page_end": 14,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "78715292-4300-45c5-9bb6-012c4c67f5c4",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "(8)\nThe parameter β > 0 reflects the trade-off between selecting treated units to fit the aggregate\nvalue of the predictors X, and selecting control units to fit the aggregate value of the treated\nunits. A small value of β favors designs with treated units that closely match X. A large\nvalue of β, on the other hand, favors designs with aggregate treated and aggregate control units\nthat closely match each other. While it is possible to use data-driven selectors of β, the rule\nof thumb β = 1 provides a natural choice that equally weights the two terms in the objective\nfunction in (8). For this formulation of the synthetic control design, the treatment assignment\n10",
    "content_hash": "1884faf24f6191997f25d423ec48c1f83c5aa38ef9bfa66d3cdc95f0bc897fa6",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "85ae1d34-de33-4a54-aaf6-ee59e40b0557",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "dimension as Xj. Suppose that units with w∗\nj > 0 are assigned to treatment in the experiment,\nand units with w∗\nj = 0 are kept untreated. A synthetic control estimator of τt is bτt = τt(w∗, v∗),\ni.e.,\nbτt =\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt. (7)\nThis estimator is based on approximations to equations (3) and (4) that rely on Xj, the observed\npredictors of the potential outcomes, Y N\njt , and Y I\njt. Note that for every solution to (6) with\nm ≤∥v∥0 ≤m, there exists another solution that swaps the roles of the treated and the\nuntreated in the experiment without altering the value of the objective function. In what follows, we take the weight selector in (6) as a starting point for synthetic control\ndesigns and modify it in several ways. A second formulation of the synthetic control design is\nbased on equations (3) and (5),\nmin\nw1,...,wJ,\nv1,...,vJ\nX −\nJ\nX\nj=1\nwjXj\n2\n+ β\nJ\nX\nj=1\nwjXj −\nJ\nX\nj=1\nvjXj\n2\ns.t. J\nX\nj=1\nwj = 1,\nJ\nX\nj=1\nvj = 1,\nwj, vj ≥0,\n∀j = 1, . . . , J,\nwjvj = 0,\n∀j = 1, . . . , J,\nm ≤∥w∥0 ≤m.",
    "content_hash": "ad6a533fdc1195a73b29b5679cbd4d7d55f5aa64daa3ae5f5eaf6a3b0b133a47",
    "location": null,
    "page_start": 10,
    "page_end": 14,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "77af665f-b7ff-419b-b355-b34e28f6a739",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "and estimation procedures follow the same steps as those used in the previous formulation. Large\nvalues for β produce estimators that target the w-weighted average effect of the treatment on the\ntreated, τ T\nt of Doudchenko et al. (2021). Small values of β prioritize estimation of the average\ntreatment effect, τt. In our third formulation of the synthetic control design, the experimenter selects a synthetic\ntreated unit to match the average values of the characteristics in the population. However,\nunlike the design in (8), the experimenter chooses multiple synthetic controls, one for each unit\nthat contributes to the synthetic treated unit. For any J-dimensional vector of non-negative\ncoordinates, w = (w1, . . . , wJ), let Jw be the set of the indices with non-zero coordinates,\nJw = {j : wj > 0}. Our next version of the synthetic control design is:\nmin\nwj,∀j=1,2,..,J,\nvij,∀i,j=1,2,...,J\nX −\nJ\nX\nj=1\nwjXj\n2\n+ ξ\nJ\nX\nj=1\nwj\nXj −\nJ\nX\ni=1\nvijXi\n2\ns.t. J\nX\nj=1\nwj = 1,\nwj ≥0,\n∀j = 1, . . . , J,\nJ\nX\ni=1\nvij = 1,\n∀j ∈Jw,\nvij = 0,\n∀i ∈Jw, j = 1, . . . , J,\nvij ≥0,\n∀j ∈Jw, i = 1, . . .",
    "content_hash": "a3b1f71d23870f05ce4bba9d186aa3054565369b9aa105222491e8f200dabfbc",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "78b7ce67-1407-460c-8447-6cbc680a5f15",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "(a)\n(b)\nFigure 1: Clustering in a synthetic control design\nNote: Panels (a) and (b) plot the values of the predictors in Xj, which is bivariate in this simple example. Units\nassigned to treatment are drawn in red. In panel (a), we treat the entire sample as a single cluster. In panel (b),\nwe divide the sample into three clusters and assign one unit in each cluster to the treatment.\nPJ\nj=1 w∗\njXj ≈0 and Xj −PJ\nj=1 v∗\nijXj ≈0 for all j such that w∗\nj > 0. Assign units with w∗\nj > 0\nto treatment in the experiment, and keep units with w∗\nj = 0 untreated. Let\nv∗\nj =\nJ\nX\ni=1\nw∗\ni v∗\nij.\n(10)\nThen,\nbτt =\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n=\nJ\nX\nj=1\nw∗\nj\nYjt −\nJ\nX\ni=1\nv∗\nijYit\n!\n.\nOur next adjustment to the synthetic control design is motivated by settings where experimen-\ntal units may be naturally divided into clusters with similar values in the predictors, X1, . . . , XJ.\nFor example, weather patterns, which may be highly dependent across cities in the same region\n12",
    "content_hash": "361fe4c99f6ba7c9cd7df93069dd6058ecf17a601deb9a1c806e95c9f67049d1",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7ceb5105-5a7f-4eb6-944b-19f27a70432b",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "(e.g., Northeast, Midwest, etc., in the US), may influence the seasonality of the demand for\nride-sharing services. In those cases, it is natural to treat each cluster (each region, in our ex-\nample) as a distinct experimental design to ameliorate interpolation biases. Figure 1 illustrates\nthis point. Panels (a) and (b) depict identical samples in the space of the predictors. In this\nsimple example, we have two predictors only, and their values for each unit are represented by\nthe coordinates of the dots in the figure. Red dots represent units assigned to treatment. All\nother units are plotted as black dots. Panel (a) visualizes the result of treating the entire sample\nas one cluster. Three units are assigned to treatment. They closely reproduce the value of X,\nbut they all fall in the same central cluster, far away from observations in other clusters. In panel\n(b), assignment to treatment takes into account the clustered nature of the data, and one unit\nis treated per cluster. This provides a better approximation of the distribution of the predictor\nvalues for the entire sample, ameliorating concerns of interpolation biases. Suppose we divide the set of J available units into K clusters. Let Ik be the set of indices\nfor the units in cluster k. The cluster mean is\nXk =\nX\nj∈Ik\nfjXj\n. X\nj∈Ik\nfj,\nfor each cluster k = 1, . . . , K. For each index i = 1, . . . , J, let k(i) be the cluster to which unit i\nbelongs, i.e., i ∈Ik(i).",
    "content_hash": "50e71586a8534bd63f4bf5646fd049189e8e5996714864c7f116a8266e79d8d9",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "dbd5eb03-76c1-4d93-b2d1-03aa8562e714",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "A clustered version of the synthetic control design in (9) is given by:\nmin\nwj,∀j=1,2,..,J,\nvij,∀i,j=1,2,...,J\nK\nX\nk=1\nX\nj∈Ik\nfj\n!(\r\r\r\r\r Xk −\nX\nj∈Ik\nwjXj\n2\n+ ξ\nX\nj∈Ik\nwj\nXj −\nX\ni,j∈Ik\nvijXi\n2)\ns.t. X\nj∈Ik\nwj = 1,\n∀k = 1, . . . , K,\nwj ≥0,\n∀j = 1, . . . , J,\nJ\nX\ni=1\nvij = 1,\n∀j ∈Jw\nvij ≥0,\n∀j ∈Jw, i = 1, . . . , J,\nvij = 0,\n∀j /∈Jw, i = 1, . . . , J,\nvij = 0,\n∀i ∈Jw, j = 1, . . . , J,\nvij = 0,\n∀i, j, such that k(i) ̸= k(j),\n13",
    "content_hash": "bc3a369f70051aae46d484be0122709f12f5ee166a4f4984576d46407fa33f2c",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "33f16425-061c-4995-aef9-dd683aa11e80",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Equation (12a) is the linear factor model for potential outcomes under no treatment, a benchmark\ncommonly used in the literature to analyze the properties of synthetic control estimators (see,\ne.g., Abadie, Diamond and Hainmueller, 2010, Ferman, 2021). Equation (12b) extends the linear\nfactor structure to potential outcomes under treatment. The reason for this extension is that,\nin contrast to synthetic control estimation with observational data, synthetic control designs\nrequire the choice of a treatment group in addition to the choice of a comparison group. design in (11) is a penalized version of (6). Section OA.1 in the Online Appendix discusses how to apply the\nAbadie and L’Hour penalty to the other synthetic designs proposed in this article. 15",
    "content_hash": "6f53be7ad0b4d9abd3db1ddaba3ec90f16823d6cea5a589f8a7403c0719dd9ca",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e7be3f74-cc4d-4b76-966e-c2e5c8969faf",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "We employ the covariates in Zj as well as pre-experimental values of the outcome variable\nYjt to construct the vectors of predictors, Xj. In particular, let E ⊆{1, . . . , T0}, let TE = |E|,\nand let Y E\nj be the (TE × 1) vector of TE pre-experimental outcomes for unit j and time indices\nin E. We define\nXj =\n\u0012 Y E\nj\nZj\n\u0013\n,\nfor j = 1, . . . , J. That is, the vector of predictors Xj collects the covariates in Zj and the\npre-experimental outcome values Yjt for the fitting periods in E. In practice, the values in Xj\nare often scaled to make them independent of units of measurement or to reflect the relative\nimportance of each of the predictors (see, e.g., Abadie, 2021). The next assumption gathers regularity conditions on model primitives. Assumption 2\n(i) F ≤TE. Moreover, let λE be the (TE × F) matrix with rows equal to the λt’s indexed by E. Let ζE be the smallest eigenvalue of λ′\nEλE. Then, ζ = ζE/TE > 0. (ii) For each j = 1, . . . , J, ϵj1, . . . , ϵjT is a sequence of i.i.d. sub-Gaussian random variables\nwith mean zero and variance proxy σ2. For any j = 1, . . . , J, ξjT0+1, . . . , ξjT is a sequence of\ni.i.d.",
    "content_hash": "3d13aa556eb3dd6e245738cb69aa17deeec13c6814244adfc5639284c146cfcb",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d4c18087-0a56-40f9-8824-e422f7be2f57",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "sub-Gaussian random variables with mean zero, variance proxy σ2, and independent\nof ϵj1, . . . , ϵjT. Assumption 2(i) is similar to conditions in Abadie, Diamond and Hainmueller (2010). As-\nsumption 2(ii) is similar to conditions in Abadie, Diamond and Hainmueller (2010), Doudchenko\nand Imbens (2016), Chernozhukov, W¨uthrich and Zhu (2021), and Arkhangelsky et al. (2021). Sub-Gaussianity is not strictly necessary, but it simplifies the form of our results. It can be relaxed\nby assuming bounded finite-order moments (instead of bounding the entire moment generating\nfunction). At the same time, sub-Gaussianity is a relatively mild assumption. It holds for any\nGaussian distribution, as well as any distribution with a bounded support. Distributions with\nheavy tails, such as the Cauchy distribution, are not sub-Gaussian. Notably, Assumption 2(ii)\nallows for dependence of ϵjt and ξjt across units. 16",
    "content_hash": "39f9b8a13f2ba9744b582543fe331f6714f3c321ec18e08890ae1fa2684b3b2f",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "646caba8-39c3-45ff-9d2e-b0db3aa9772e",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Assumption 4 There exists a positive constant d > 0, such that with probability one, (i)\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nfjZj\n2\n2 ≤Rd2,\nJ\nX\nj=1\nv∗\njZj −\nJ\nX\nj=1\nfjZj\n2\n2 ≤Rd2,\nand (ii)\nJ\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nfjY E\nj\n2\n2 ≤TEd2,\nJ\nX\nj=1\nv∗\njY E\nj −\nJ\nX\nj=1\nfjY E\nj\n2\n2 ≤TEd2. Let λt,f be the f-th coordinate of λt, and\nλ = max\nt=1,...,T\nf=1,...,F\n|λtf|. We define ηtf, θtr, γtr, η, θ and γ analogously, so |ηtf| ≤η for t = T0 + 1, . . . , T, f = 1, . . . , F,\n|θtr| ≤θ for t = 1, . . . , T, r = 1, . . . , R, and |γtr| ≤γ, for t = T0 + 1, . . . , T, r = 1, . . . , R. Next\ntheorem extends results on the bias of synthetic control estimators (see, e.g., Abadie, Diamond\nand Hainmueller, 2010, Vives-i-Bastida, 2022) to the experimental set-up of Section 2. 17",
    "content_hash": "092ef0ffeb3d8b32edbb553ffdf78c9f79da645c9587a4ec2a9022544b099c50",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1f5d186f-a930-4f3e-8eb5-6994f6b3967d",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Unless otherwise noted, all probability statements are over the joint distribution of ϵjt and\nξjt and conditional on the values of the other components on the right-hand sides of equations\n(12a) and (12b). The next assumption pertains to the quality of the synthetic control fit. For\nconcreteness, we focus on the base design in (6), and choose where w∗= (w∗\n1, . . . , w∗\nJ) and\nv∗= (v∗\n1, . . . , v∗\nJ) so that the synthetic treated and synthetic control units reproduce the average\nvalues of Xj. Assumption 3 With probability one, (i)\nJ\nX\nj=1\nw∗\njZj =\nJ\nX\nj=1\nv∗\njZj =\nJ\nX\nj=1\nfjZj,\nand (ii)\nJ\nX\nj=1\nw∗\njY E\nj =\nJ\nX\nj=1\nv∗\njY E\nj =\nJ\nX\nj=1\nfjY E\nj . Assumption 3 implies that the synthetic treated and control units defined by w∗and v∗pro-\nvide a perfect fit for X. Assumption 3 is a strong restriction, which may only hold approximately\nin practice. The next assumption relaxes the perfect fit condition in Assumption 3.",
    "content_hash": "06b2ea262ce6e32e8c7d5ba90f6dbba7d264008d11efea14c62eea6336d40e22",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c34f0fe5-7aba-4f9e-93e3-11393c03ee17",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Intuitively, the bias of the synthetic control estimator\nis small when a good fit in pre-experimental outcomes (Assumption 3) is obtained by implicitly\nfitting the values of the latent variables, µj. Overfitting happens when pre-experimental outcomes\nare instead fitted out of the variability in the individual transitory shocks, ϵjt. A small number\nof fitting periods TE combined with large variability in ϵjt increases the risk of overfitting and, as\na result, increases the bias bound. Similarly, for any fixed value of TE, the bias bound increases\nwith J, reflecting the increased risk of over-fitting caused by increased variability in ϵjt over larger\ndonor pools. Finally, the number of unobserved factors F enters the bound (15) linearly, which\nhighlights the importance of including the observed predictors Zj — other than pre-experimental\noutcomes — in the vector of fitting variables Xj. Under the factor model in equations (12a) and\n(12b), observed predictors not included in Zj are shifted to µj, increasing F and the magnitude\nof the bound.7\n7Shifting predictors from Zj to µj changes the bias bound (15) in a more complex manner than what might\nbe inferred from a cursory look at the bias formula. First, moving predictors from Zj to µj also means shifting\n18",
    "content_hash": "c49d6ae86aca1678fa441d39ce48c9b285758609aa24418ac988ccf00667cd07",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "13dd190b-7090-479a-a288-ca419e04ee8e",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Theorem 1 If Assumptions 1 – 3 hold, then for any t ≥T0 + 1,\n|E [bτt −τt] | ≤λ(η + λ)F\nζ\np\n2 log (2J) σ\n√TE\n. (15)\nIf Assumptions 1, 2, and 4 hold, then for any t ≥T0 + 1,\n|E [bτt −τt] | ≤\n\u0010\n(γ + θ)R + λ(η + λ)F\nζ\n(1 + θR)\n\u0011\nd + λ(η + λ)F\nζ\np\n2 log (2J) σ\n√TE\n. (16)\nNote that, while the factor model in equations (12a) and (12b) leave the sign and scale of\nλt and ηt free (e.g., multiplying λt and dividing µt by the same non-zero constant does not\nchange the value of λ′\ntµj), the value of the bound in Theorem 1 is invariant to changes in the\nsign or the scale of λt and ηt. Moreover, the bound in (16) does not depend on the scale of Zj,\nbecause changing the scale of Zj leaves the product θd unchanged. The scale of Yjt does affect\nthe bound in (16) because the treatment effect τt is measured in the same units as Yjt. The\nresults in Theorem 1 do not depend on the specific formulation of the synthetic control design\n(e.g., Constrained vs. Unconstrained). The bias bounds (15) and (16) depend on the ratio between the scale of ϵjt, represented by\nσ, and the number of fitting periods TE.",
    "content_hash": "e2d8fd652cdb45f1988607c8f3604fdc50d081c44d5ece6d81b5281e080ea7b2",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "72a56408-3c6d-437f-8e35-e258c8844a71",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "We next turn our attention to inference. We utilize a set of blank periods, B ⊆{1, . . . , T0}\\E,\nwhich comprise pre-experimental periods whose outcomes Yjt have not been used to calculate w∗\nor v∗. Because pre-experimental periods that are not in E or B are not used in our procedure,\nwithout loss of generality, we consider B = {1, . . . , T0} \\ E. We, therefore, assume that the\nnumber of elements of B is TB = |B| = T0 −TE. We aim to test the null hypothesis:\nFor t = T0 + 1, . . . , T, and j = 1, . . . , J,\nY I\njt = δt + θ′\ntZj + λ′\ntµj + ξjt,\n(17)\nwhere ξjt has the same distribution as ϵjt. Under the null hypothesis in (17), the distribution of Y I\njt is the same as the distribution of\nY N\njt , for t = T0 + 1, . . . , T, and j = 1, . . . , J. But the realized values of Y I\njt and Y N\njt may differ. Recall from (7) that, for t ∈{T0 + 1, . . . , T}, a synthetic control estimator is defined as\nbτt =\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt. Let but = bτt, ∀t ∈{T0 + 1, . . .",
    "content_hash": "e200223e0686a6d4731d5a0ceb4214215f99cb717cc9ae00ef85dd3341d781b9",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6569d09f-1a5b-4a47-b446-8eca30ac22fe",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Assume that {λt}t∈B∪{T0+1,...,T} is\na sequence of exchangeable random variables independent of {ϵjt}t∈B∪{T0+1,...,T} and {ξjt}t∈{T0+1,...,T}. Then under the null hypothesis (17), we have\nα −1\n|Π| ≤Pr(bp ≤α) ≤α,\nfor any α ∈[0, 1], where Pr(bp ≤α) is taken over the distribution of {ξjt, ϵjt, λt}. Note that, under the assumptions of Theorem 2, the potential outcome series Y N\njt is allowed\nto be non-stationary through the term δt+θ′\ntZj in equation (12a). This is in contrast to a related\nresult in Doudchenko et al. (2021), which requires that the potential outcomes Y N\njt are i.i.d. over\ntime. 20",
    "content_hash": "00b8e2e189d5296d3d9c021f107c30809d5c36eafe410b51528c151626cfe310",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a38cc10d-0257-4d90-9cb7-1a603ad2de9f",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Relative to Chernozhukov,\nW¨uthrich and Zhu (2021), the generative models of equations (12a) and (12b), which allow for\nunobserved factors, and the finite sample nature of the results require a novel testing procedure\nthat, similar to split conformal prediction methods (Lei et al., 2018, Vovk, Gammerman and\nShafer, 2005), takes advantage of the availability of blank periods. 21",
    "content_hash": "835a3f524c8928d10941cf50b8c0021d0986112889ccfdae634cd02b4ff19416",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "17b598e8-97ac-432d-bb47-2d8be0505d51",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "The assumptions in Theorem 2 build upon those in Theorem 1. Although these assump-\ntions are simple and sufficient for the result of the theorem, they can be substantially re-\nlaxed. Under exchangeability of λt, if Assumption 2(ii) is violated, the result for Theorem 2\nholds if for each j = 1, . . . , J, {ϵjt}t∈B∪{T0+1,...,T} and {ξjt}t∈{T0+1,...,T} are sequences of ex-\nchangeable random variables. Second, if Assumption 3(i) is violated, the result for Theorem 2\nholds if {(θt, λt)}t∈B∪{T0+1,...,T} is a sequence of exchangeable random variables independent of\n{ϵjt}t∈B∪{T0+1,...,T} and {ξjt}t∈{T0+1,...,T}. In the above two cases under exchangeability of λt, we\nstill have exact p-value. Finally, exchangeability of λt is a strong restriction. Theorem OA.1 in\nthe Online Appendix relaxes this restriction by showing that for fixed λt (i.e., without resorting\nto exchangeability of λt), the p-value in (19) is still approximately valid for large TE. In some settings, the number of possible combinations, |Π|, could be very large, making exact\ncalculation of bp computationally expensive.",
    "content_hash": "53201f046d7fb9a23673dafe590a0077c3b1c91295331581b488b457705aaa92",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "380ac247-a8d9-44fe-a7b1-217fed620453",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Then the confidence interval defined in (21) approximately achieves\npoint-wise coverage, i.e., for any α ∈(0, 1) and any t ∈{T0 + 1, ..., T}, as (T0 −TE), TE →+∞,\nPr\n\u0010\nτt ∈bC1−α(Y1t, Y2t, ..., YJt)\n\u0011\n−(1 −α)\n= O\n\u0010log (T0 −TE)/(T0 −TE)\n\u00011/2 +\nlog TE/TE\n\u00011/2\u0011\n−→0. 4. Empirical Illustration Using Walmart Data\nIn this section, we illustrate the applicability of the methods in this article using store-level data\nfrom Walmart (Prakash, 2023). The dataset is a balanced panel of weekly sales for J = 45\nWalmart stores and T = 143 weeks, spanning the period from the week of February 5, 2010, to\nthe week of October 26, 2012. We estimate the effect of a placebo intervention and show that,\nin the presence of a good pre-intervention fit, the methods of Section 3 produce point estimates\nthat are close to zero and a test result that does not reject the null hypothesis in (17) for the\nplacebo intervention. 22",
    "content_hash": "374fc319df6f5aed330331210941958521f8fde57352e54928d4fc7ebfce7c11",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2f1e8a99-7889-4550-8f11-45a81935c2ab",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Then the confidence interval defined in (21) approximately achieves\npoint-wise coverage, i.e., for any α ∈(0, 1) and any t ∈{T0 + 1, ..., T},\nPr\n\u0010\nτt ∈bC1−α(Y1t, Y2t, ..., YJt)\n\u0011\n−(1 −α)\n≤\ns\n1\n2(T0 −TE) log\n\u0010 2\nzB\n\u0011\n+κ\nv\nu\nu\nt8eJσ2λ\n2η2F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n+2κ\nv\nu\nu\nt8eJσ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n+zB+3zE. where zB and zE are arbitrary positive constants. Proof of Theorem A.2. We outline the proof of Theorem A.2 as follows. We first define four\nevents. We then check Conditions (A.7) under the first two events and (A.8) under the last two\nevents. Finally, we apply Lemma A.1 and conclude the proof. Step 1: We define the following four events. First, in the blank periods and conditioning on the\nweights (w∗, v∗) that we obtain from the fitting periods,\n46",
    "content_hash": "1d017fdd8cb335159fb470634a0c3cad06b40d8fc222b2917341cafa07deeb8a",
    "location": null,
    "page_start": 22,
    "page_end": 46,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9d5c2bbd-73d7-4b5b-9eca-31744ad76f7d",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Confidence intervals for τt can be constructed using split conformal inference methods. For\nany α ∈(0, 1), let\nbq1−α = inf\nz∈R\n(\n1\nT0 −TE\nX\nt∈B\n1\n\u001a\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤z\n\u001b\n≥1 −α\n)\n(20)\nbe the empirical (1−α)-quantile on the absolute values of placebo treatment effects in the blank\nperiods, and\nbC1−α(Y1t, Y2t, ..., YJt) =\n\u0014\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −bq1−α,\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt + bq1−α\n\u0015\n. (21)\nWe next show that the confidence interval defined in (21) approximately achieves correct point-\nwise coverage in large samples if treatment does not change the distribution of the idiosyncratic\nnoises. Theorem 3 Assume that Assumptions 1– 3 hold. Assume there exists a constant κ < ∞, such\nthat for all j = 1, . . . , J, t = 1, . . . , T, ϵjt are continuously distributed with the probability density\nfunction upper bounded by κ. Assume that for t = T0 + 1, . . . , T, and j = 1, . . . , J, ξjt has the\nsame distribution as ϵjt.",
    "content_hash": "f7ea79a3d6e206585030080ad2f09d5a11e6e51b3cfa2b135236b34ca6a3cbbf",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "94727dba-e191-4fe6-8730-99b6b45636e7",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Figure 2: Synthetic Treatment Unit and Synthetic Control Unit, m = 2\nNote: The black solid line represents the synthetic treated outcome. The black dashed line represents the synthetic\ncontrol outcome. The blue dashed lines are individual stores’ sales. We consider the design of a fictitious experiment across stores taking place on July 20, 2012\n(week 129 in the data). Out of the T0 = 128 pre-experimental weeks, we take the first TE = 100\nweeks as the fitting period, and the last (T0 −TE) = 28 weeks as the blank period. The number\nof weeks in the experimental period is T −T0 = 15. The outcomes {Yjt}j=1,...,J,t=1,...,T are weekly\nsales (units of revenue are undisclosed in the data). We use uniform weights fj = 1/J for\nj = 1, ..., J, to average sales across all stores. For the purpose of estimating the synthetic treated\nand synthetic control weights, we normalize each of the 100 pre-experimental outcomes to have\na unit variance. We compute synthetic treated and control units that apply the Constrained formulation in\n(6) with m = 2. We adopt m = 2 because using only one store for the synthetic treated fails to\nproduce a good fit between the resulting synthetic treated and synthetic control units during the\nfitting period. Increasing to m = 3 brings only marginal improvements in fit. Figures 2 and 3\nreport results for m = 2. Results for m = 1 and m = 3 appear in the Online Appendix.",
    "content_hash": "37a3a165164bc891e177161ce50be7a88f2fd58a96c56f19bca7eec7eb829479",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5168cf1a-2fcd-4ada-9ba2-62f1bcf74606",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Figure 2 reports the time series of weekly sales for the synthetic treated unit (black solid\nline), the synthetic control unit (black dashed line), and for each individual store in the dataset\n23",
    "content_hash": "0183943b68fd8260c189764358ed2ce1bb4b4be29b063de271ec4e32bd6cbbe0",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "aa1e98c4-5f12-43cc-a844-8aea386942ac",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Figure 3: Treatment Effect Estimate, when m = 2.\nNote: This figure reports the difference between the synthetic treated and synthetic control outcomes of Figure 2.\nFor the experimental periods, this is the treatment effect estimate. The shaded region indicates the 95% confidence\ninterval for each of the experimental periods.\n(blue dashed lines). Weekly sales for the synthetic treated and the synthetic control units closely\nfollow each other during the fitting period. The gap between the two synthetic units remains\nsmall after the fitting period, indicating good out-of-sample predictive power in the absence of\nintervention.\nFigure 3 reports the difference in weekly sales between the synthetic treated and the synthetic\ncontrol units. The p-value of equation (19), calculated over the residuals of Figure 3, is equal to\n0.933, which results in a failure to reject the null hypothesis (17). Confidence intervals based on\nequation (21) cover zero for all t in the experimental period.\nTable 1 compares the performance of the synthetic control design to those of straight ran-\ndomization followed by difference-in-means, randomization after stratification on pre-intervention\noutcomes followed by difference-in-means, and 1- and 5-nearest neighbor adjustment after ran-\ndomization. In particular, Table 1 reports out-of-sample root mean square error (RMSE) over\nthe post-intervention period, normalized by the post-intervention outcome mean (see Section\n5.2.4 for a precise definition of the estimators and RMSE performance metric). For each of the\n24",
    "content_hash": "a60289194dad45b9029b27bf9b659d20366cb0bc1745a0690e736e19e52de87f",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7ac458ae-6552-4791-b384-93d9b0724ba1",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Simulation Study\nThis section presents simulation results that showcase the behavior of estimators based on syn-\nthetic control designs. We consider a setting with J = 15 units, R = 7 observable covariates,\nand F = 11 unobservable covariates. We simulate data for a total of T = 30 periods, comprising\nT0 = 25 pre-experimental periods and T −T0 = 5 experimental or post-intervention periods. We compute weights during the first TE = 20 periods and leave periods t = 21, . . . , 25 as blank\nperiods. We set the weights fj in expression (1) to be fj = 1/J, for all j = 1, ..., J. For our baseline simulation design, we use the factor model in Assumption 1 to generate\npotential outcomes. For t = 1, . . . , T, we generate the series δt and υt as small-to-large re-\narrangements of T i.i.d. Uniform (0, 20) random variables. For j = 1, . . . , J, we set both Zj\nand µj to be random vectors of i.i.d. Uniform (0, 1) random variables. For t = 1, . . . , T, we\nset θt, γt, λt, and ηt to be random vectors of i.i.d. Uniform (0, 10) random variables. Finally,\n25",
    "content_hash": "d517f547cac3fb4a3b0b1dee24aa3f9150ad2f1356d80ffbde272a7b3eb7092c",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "09099481-4398-437c-bcb5-beec241128cf",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Table 1: Out-of-Sample Normalized Root Mean Square Error\nSC\nRND\nSTR\n1-NN\n5-NN\nm = 1\n0.052\n0.452\n0.452\n0.096\n0.082\nm = 2\n0.018\n0.312\n0.299\n0.070\n0.063\nm = 3\n0.019\n0.254\n0.173\n0.059\n0.053\nm = 4\n0.027\n0.223\n0.181\n0.052\n0.048\nm = 5\n0.012\n0.202\n0.164\n0.047\n0.043\nNote: Root mean square error divided by the average outcome in the experimen-\ntal periods. m stands for the maximum number of treated units. SC: Constrained\nformulation of the synthetic control design. RND: Randomized treatment assign-\nment followed by the difference-in-means estimator. STR: Stratified randomization,\nfollowed by difference in means in each stratum. 1-NN: Randomized treatment assign-\nment followed by 1-nearest neighbor matching, using all pre-experimental outcomes. 5-NN: Randomized treatment assignment followed by 5-nearest neighbor matching,\nusing all pre-experimental outcomes. three randomization-based estimators, the reported RMSE is the average over 1000 randomized\ntreatment assignments. The synthetic control design dominates all other alternatives, even when\nit uses only the outcomes in the fitting periods to construct the synthetic treated and synthetic\ncontrol units, whereas stratification and nearest-neighbor adjustment utilize all pre-intervention\noutcomes. 5.",
    "content_hash": "416a1993e660cb16fc8f4928d4bea97897f797f58789c4d8936a6bf236975801",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a3f749a2-4466-44d0-8e82-2fac4aac3663",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "The inferential procedure of Section 3 produces p-value equal to 0.004 for the null\nhypothesis of no treatment effect in (17). 5.2. Performance Across Many Simulations\nThis section compares the performance of the different varieties of the synthetic control designs\nover 1000 simulations that independently generate the model primitives (i.e., the factor loadings,\ncovariates, and error terms) of Assumption 1. The data generating process is the same as in\nSection 5.1. We consider five varieties of the synthetic control design:\n1. Unconstrained design: This is the design in (6) without a cardinality constraint, so m = 1\nand m = J −1 = 14. 2. Constrained design: Same as the design in (6), but with m = 1 and m = 1, . . . , 7. 3. Weakly-targeted design: This is the design in (8). We vary β from 0.01 to 100. 4. Unit-level design: This is the design in (9), which fits a different synthetic control to each\n26",
    "content_hash": "8b6f8e6929b85c3cd325de58bfe1fb5c2fe01063475c4fd22e2402e3f0c24a61",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9fd23d7f-b87d-4edd-af18-ad0aae2bf163",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "for j = 1, . . . , J, and any t = 1, . . . , T, we set ϵjt and ξjt to be i.i.d. Normal (0, σ2) random\nvariables, with σ2 = 1. We present additional simulation results of alternative values of the noise\nparameter σ2 in Section OA.7 in the Online Appendix. 5.1. Results for a Single Simulation\nUsing the data generating process described above, we draw a single sample and conduct the\nsynthetic control design in (6), with parameters m = 1 and m = 14, i.e., no constraint on the\nnumber of treated units. We report the results in Figures 4 and 5. In Figure 4, each blue\ndashed line represents an outcome trajectory Yjt, for t = 1, . . . , T and j = 1, . . . J. The solid\nblack line represents the trajectory of the synthetic treated unit PJ\nj=1 w∗\njYjt, for t = 1, . . . , T. The black dashed line represents the trajectory of the synthetic control unit PJ\nj=1 v∗\njYjt, for\nt = 1, . . . , T. The synthetic treated and synthetic control units closely track each other in the\npre-experimental periods. They diverge during the experimental periods, when a treatment effect\nemerges as a result of the differences in the parameters of the data-generating processes for Y N\njt\nand Y I\njt. Figure 5 reports the difference between the synthetic treated and the synthetic control\noutcomes.",
    "content_hash": "86da9920998d4560ec423c94e7c4408ce1082e7f18d0598956d9e9234466f3b7",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ad278a49-51d4-4090-8160-dfd47492f4cb",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Figure 4: Synthetic Treatment Unit and Synthetic Control Unit, σ2 = 1\nNote: The black solid line represents the synthetic treated outcome (w∗-weighted).\nThe black dashed line\nrepresents the synthetic control outcome (v∗-weighted).\nunit assigned to treatment. We vary ξ from 0.01 to 100.\n5. Penalized design: This is the design in (11), with λ = λ1 = λ2. We vary λ from 0.01 to\n100.\nThe Constrained design imposes sparsity in the synthetic treatment weights through a hard\ncardinality constraint specified by the integer m. The Weakly-targeted design targets the average\ntreatment effect for small values of β and a weighted average effect for the treated for large values\nof β. For the Unit-level design, large values of ξ generate sparsity in the synthetic treated weights.\nA sufficiently large value of ξ produces a Unit-level design where the only single treated unit can\nbe closely fitted by a convex combination of the other units. For large values of λ, the Penalized\ndesign behaves like a one-to-one matching design, assigning all the weight to one treated and one\ncontrol unit.\nFor the Unit-level design, synthetic control weights are aggregated as in (10). For the Uncon-\nstrained and Penalized designs, the synthetic treated and synthetic control weights can always be\nswapped without changing the objective values for their respective designs. For the Constrained\ndesign, the weights can be swapped when ∥v∗∥0 ≤m. When it is possible to swap synthetic\n27",
    "content_hash": "9fa5cc9ebc39453a4b4e8b807380e967f1500b21831e3b294702e08f95109c49",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cce134e2-1bf2-459e-9261-c0c891e7a051",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Figure 4: Synthetic Treatment Unit and Synthetic Control Unit, σ2 = 1\nNote: The black solid line represents the synthetic treated outcome (w∗-weighted).\nThe black dashed line\nrepresents the synthetic control outcome (v∗-weighted).\nunit assigned to treatment. We vary ξ from 0.01 to 100.\n5. Penalized design: This is the design in (11), with λ = λ1 = λ2. We vary λ from 0.01 to\n100.\nThe Constrained design imposes sparsity in the synthetic treatment weights through a hard\ncardinality constraint specified by the integer m. The Weakly-targeted design targets the average\ntreatment effect for small values of β and a weighted average effect for the treated for large values\nof β. For the Unit-level design, large values of ξ generate sparsity in the synthetic treated weights.\nA sufficiently large value of ξ produces a Unit-level design where the only single treated unit can\nbe closely fitted by a convex combination of the other units. For large values of λ, the Penalized\ndesign behaves like a one-to-one matching design, assigning all the weight to one treated and one\ncontrol unit.\nFor the Unit-level design, synthetic control weights are aggregated as in (10). For the Uncon-\nstrained and Penalized designs, the synthetic treated and synthetic control weights can always be\nswapped without changing the objective values for their respective designs. For the Constrained\ndesign, the weights can be swapped when ∥v∗∥0 ≤m. When it is possible to swap synthetic\n27\n\nFigure 5: Treatment Effect Estimate, when σ2 = 1.\nNote: This figure reports the difference between the synthetic treated and synthetic control outcomes of Figure 4.\nFor the experimental periods, this is the treatment effect estimate.\ntreated and synthetic control weights, we choose the treated units so that the number of units\nwith positive weights in w∗is smaller than the number of units with positive weights in v∗. When\n∥w∗∥0 = ∥v∗∥0, we determine whether to swap using a specific rule described in Section OA.4 of\nthe Online Appendix.\n5.2.1.\nAverage Treatment Effects\nThe first panel of Table 2 reports average treatment effects, τt, over 1000 simulations. The second\npanel reports estimates of the average treatment effects, mean absolute error, root mean square\nerror, and p-value, all averaged over 1000 simulations, as well as rejection rates. Mean absolute\nerror (MAE) and root mean square error are defined as\nMAE =\n1\nT −T0\nT\nX\nt=T0+1\n|bτt −τt|,\nRMSE =\nv\nu\nu\nt\n1\nT −T0\nT\nX\nt=T0+1\n(bτt −τt)2,\n(22)\nand the p-value is defined as in (19). Because the treatment effect is not equal to zero in the\nsimulation of Table 2, smaller p-values and larger rejection rates reflect better performance of\nthe testing procedure for a particular design.\n28",
    "content_hash": "73afb2f22501699c63d49965e404856124630edccf1bc60e99f1888ebc1bf0fd",
    "location": null,
    "page_start": 27,
    "page_end": 28,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b6d6e651-1bf2-4641-99a3-6abfb3332979",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Table 2: Average Treatment Effects (Averages over 1000 Simulations)\nτt\nt = 26\nt = 27\nt = 28\nt = 29\nt = 30\n-13.58\n-10.99\n-8.35\n-5.00\n-2.50\nbτt\nMAE\nRMSE\nbp\nbp < 0.05\nt = 26\nt = 27\nt = 28\nt = 29\nt = 30\nUnconstrained\n-13.57\n-10.97\n-8.37\n-5.06\n-2.52\n0.83\n0.97\n0.014\n0.946\nConstrained\nm = 1\n-13.61\n-10.97\n-8.39\n-4.86\n-2.41\n2.93\n3.45\n0.057\n0.668\nm = 2\n-13.58\n-10.90\n-8.43\n-5.01\n-2.40\n1.69\n2.00\n0.028\n0.854\nm = 3\n-13.56\n-11.00\n-8.38\n-5.05\n-2.52\n1.26\n1.49\n0.019\n0.916\nm = 4\n-13.59\n-11.06\n-8.40\n-4.99\n-2.50\n1.06\n1.25\n0.016\n0.935\nm = 5\n-13.57\n-11.01\n-8.37\n-5.02\n-2.48\n0.93\n1.09\n0.015\n0.933\nm = 6\n-13.51\n-10.95\n-8.29\n-5.01\n-2.47\n0.87\n1.02\n0.015\n0.942\nm = 7\n-13.57\n-10.96\n-8.37\n-5.06\n-2.52\n0.83\n0.97\n0.014\n0.946\nWeakly-targeted\nβ = 0.01\n-13.58\n-10.95\n-8.38\n-4.99\n-2.53\n1.18\n1.38\n0.018\n0.920\nβ = 0.1\n-13.57\n-11.00\n-8.34\n-4.98\n-2.52\n0.93\n1.08\n0.014\n0.949\nβ = 1\n-13.56\n-10.98\n-8.32\n-4.93\n-2.44\n0.86\n1.01\n0.013\n0.951\nβ = 10\n-13.57\n-10.98\n-8.38\n-5.01\n-2.51\n0.94\n1.10\n0.013\n0.955\nβ = 100\n-13.60\n-10.98\n-8.39\n-5.07\n-2.52\n1.01\n1.18\n0.013\n0.951\nUnit-level\nξ = 0.01\n-13.60\n-10.95\n-8.39\n-5.04\n-2.53\n0.95\n1.13\n0.014\n0.938\nξ = 0.1\n-13.58\n-10.97\n-8.35\n-4.97\n-2.47\n0.91\n1.07\n0.015\n0.942\nξ = 1\n-13.57\n-10.99\n-8.39\n-4.99\n-2.49\n1.34\n1.58\n0.020\n0.899\nξ = 10\n-13.60\n-10.93\n-8.45\n-5.05\n-2.52\n2.16\n2.57\n0.030\n0.829\nξ = 100\n-13.61\n-10.86\n-8.48\n-5.02\n-2.54\n2.76\n3.27\n0.040\n0.770\nPenalized\nλ = 0.01\n-13.59\n-10.98\n-8.35\n-5.05\n-2.48\n0.88\n1.02\n0.014\n0.950\nλ = 0.1\n-13.64\n-11.03\n-8.43\n-5.03\n-2.50\n1.21\n1.43\n0.019\n0.904\nλ = 1\n-13.67\n-10.96\n-8.41\n-4.87\n-2.45\n2.08\n2.46\n0.037\n0.791\nλ = 10\n-13.68\n-11.04\n-8.37\n-4.79\n-2.45\n3.72\n4.40\n0.091\n0.542\nλ = 100\n-13.64\n-10.94\n-8.42\n-4.86\n-2.50\n4.17\n4.93\n0.111\n0.490\nNote: Unless otherwise noted, all designs use m = 1 and m = 14.",
    "content_hash": "3e9c80b08b383b5931d7270986700547a3ce0aba7780209c54f5faad7a185edc",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0a653879-130c-4222-b6b5-e5b4cec9bb65",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "In Table 2, the Unconstrained design has a strong relative performance. The performance of\nthe Constrained design improves for larger m, and is virtually identical to the performance of the\nUnconstrained design when m = 7. The performance of Weakly-targeted and Unit-level designs\nis best when β and ξ take intermediate values. The Penalized design yields results similar to\nthose of the Unconstrained design for small values of the penalization parameter λ. 29",
    "content_hash": "9bb53609a94d9480dd516969bf9d8c6d70611439433616dbb3ba30004378cb46",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "87ab55a7-a2aa-4fe4-8ad7-73c29e46c94b",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "The Penalized design selects synthetic treated and control units close to X in the space of the\npredictors, which can reduce interpolation biases at the potential cost of lower precision for large\nvalues of λ (in which case, the Penalized design employs a small number of units in the synthetic\ntreated and synthetic control). 5.2.3. Test size\nIn this section, we generate the model primitives under the null hypothesis (17). That is, we\nemploy a data generating process such that the values of the common factors and the distributions\nof the idiosyncratic error variables are unaffected by the intervention. We report the simulation results in Table 4, which organizes information in the same way\nas in Table 2. Because the data are generated from the same distribution under treatment and\nunder no treatment, the average treatment effects in Table 4 are close to zero. The same is\ntrue for the averages of bτt for all designs. Under the null hypothesis (17), the p-value should\napproximately follow a uniform distribution between zero and one. The results in Table 4 show\n30",
    "content_hash": "b2dc2e6de614ba658c90130db534e8a05b675190086530ec8941596475f220d6",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d6b1b67b-66e9-44a6-ab5a-58bfb9bde824",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "good behavior of our testing procedure under the null hypothesis: average p-values and rejection\nrates are close to 0.5 and 0.05, respectively. 5.2.4. Comparison to Randomized Treatment Assignment\nRandomized treatment assignment produces ex-ante (pre-randomization) unbiased estimation of\nthe average treatment effect. As we show below, however, ex-post (post-randomization) biases\ncan be large, especially when only a small number of units are treated. In this section, we adopt the same set-up as for Table 2. We consider randomized treatment\n31",
    "content_hash": "e011a9081d8fbdba5add452d041940fd6937b7f6e72976bc5151b772475b68c7",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "15027465-82dc-4e1b-8707-6008bb706642",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Table 3: Average Treatment Effects, Nonlinear Model (Averages over 1000 Simulations)\nτt\nt = 26\nt = 27\nt = 28\nt = 29\nt = 30\n-13.18\n-10.72\n-7.96\n-5.47\n-2.43\nbτt\nMAE\nRMSE\nbp\nbp < 0.05\nt = 26\nt = 27\nt = 28\nt = 29\nt = 30\nUnconstrained\n-13.44\n-10.92\n-8.18\n-5.85\n-2.78\n1.99\n2.54\n0.059\n0.741\nConstrained\nm = 1\n-15.70\n-13.18\n-10.50\n-7.76\n-4.78\n3.51\n4.27\n0.061\n0.717\nm = 2\n-14.27\n-11.86\n-8.90\n-6.44\n-3.34\n2.64\n3.29\n0.061\n0.725\nm = 3\n-13.69\n-11.38\n-8.38\n-5.95\n-2.97\n2.23\n2.83\n0.058\n0.745\nm = 4\n-13.58\n-11.09\n-8.23\n-5.89\n-2.75\n2.10\n2.67\n0.058\n0.754\nm = 5\n-13.37\n-10.97\n-8.14\n-5.79\n-2.88\n2.05\n2.61\n0.060\n0.747\nm = 6\n-13.54\n-11.03\n-8.31\n-5.86\n-2.86\n2.00\n2.56\n0.060\n0.738\nm = 7\n-13.49\n-10.94\n-8.17\n-5.86\n-2.78\n1.98\n2.53\n0.058\n0.743\nWeakly-targeted\nβ = 0.01\n-11.66\n-9.02\n-6.37\n-3.87\n-1.00\n2.59\n3.24\n0.116\n0.604\nβ = 0.1\n-12.08\n-9.60\n-6.87\n-4.31\n-1.47\n2.15\n2.74\n0.083\n0.680\nβ = 1\n-12.51\n-10.13\n-7.35\n-4.81\n-1.91\n1.97\n2.51\n0.057\n0.761\nβ = 10\n-13.03\n-10.51\n-7.81\n-5.25\n-2.32\n2.19\n2.76\n0.031\n0.854\nβ = 100\n-13.28\n-10.72\n-8.00\n-5.43\n-2.59\n2.45\n3.11\n0.024\n0.886\nUnit-level\nξ = 0.01\n-11.76\n-9.15\n-6.51\n-3.91\n-1.15\n2.57\n3.22\n0.118\n0.593\nξ = 0.1\n-13.11\n-10.59\n-7.82\n-5.15\n-2.29\n2.06\n2.64\n0.060\n0.754\nξ = 1\n-13.74\n-11.12\n-8.42\n-5.75\n-2.84\n2.37\n3.02\n0.029\n0.850\nξ = 10\n-13.74\n-11.20\n-8.55\n-5.89\n-3.09\n3.02\n3.77\n0.028\n0.866\nξ = 100\n-13.79\n-11.16\n-8.54\n-5.90\n-3.08\n3.20\n4.00\n0.029\n0.863\nPenalized\nλ = 0.01\n-13.40\n-10.93\n-8.32\n-5.82\n-2.82\n1.97\n2.53\n0.055\n0.759\nλ = 0.1\n-13.33\n-10.79\n-8.13\n-5.56\n-2.65\n2.07\n2.65\n0.045\n0.779\nλ = 1\n-13.32\n-10.84\n-8.15\n-5.39\n-2.60\n3.08\n3.84\n0.056\n0.738\nλ = 10\n-13.39\n-10.82\n-7.95\n-5.34\n-2.58\n3.85\n4.80\n0.103\n0.595\nλ = 100\n-13.35\n-10.82\n-8.00\n-5.29\n-2.57\n4.10\n5.11\n0.117\n0.562\nNote: Unless otherwise noted, all designs use m = 1 and m = 14.",
    "content_hash": "f098ba1240ff84e7c333f6512692675aea50dcd8b075c5b69a24f9468b749c4b",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fbaf4cb7-f708-4d9f-99ea-4f8fa0993930",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "assignment with m treated units. Dj is a treatment indicator that equals one if unit j is ran-\ndomized into the treated group and zero otherwise. We study the performance of the following\nestimation strategies:\n1. SC: Constrained formulation of the synthetic control design. The results reproduce those\nof Table 2. 2. RND: Randomized assignment of m units to treatment followed by the difference in means\n32",
    "content_hash": "13a7eaf78f1a9e61f5168b99768376c2a633ac47cbf0a38c95e97e091df85ab2",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2de6bde5-221c-4391-a9b3-40b648c64c9e",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Table 4: Average Treatment Effects Under the Null Hypothesis (17) (Averages over 1000 Simu-\nlations)\nτt\nt = 26\nt = 27\nt = 28\nt = 29\nt = 30\n-0.01\n0.00\n0.00\n0.01\n-0.01\nbτt\nMAE\nRMSE\nbp\nbp < 0.05\nt = 26\nt = 27\nt = 28\nt = 29\nt = 30\nUnconstrained\n-0.01\n0.00\n-0.03\n-0.04\n-0.07\n0.97\n1.13\n0.495\n0.060\nConstrained\nm = 1\n0.20\n0.14\n0.02\n-0.08\n0.05\n3.00\n3.55\n0.495\n0.056\nm = 2\n-0.02\n-0.01\n-0.02\n-0.09\n-0.03\n1.80\n2.13\n0.497\n0.038\nm = 3\n-0.09\n-0.07\n-0.02\n-0.05\n-0.02\n1.37\n1.62\n0.505\n0.048\nm = 4\n-0.02\n-0.02\n0.00\n-0.01\n-0.01\n1.19\n1.41\n0.494\n0.054\nm = 5\n0.01\n-0.02\n0.03\n0.00\n-0.05\n1.07\n1.25\n0.496\n0.057\nm = 6\n0.07\n0.06\n0.10\n-0.01\n-0.03\n0.99\n1.17\n0.484\n0.054\nm = 7\n-0.01\n0.00\n-0.02\n-0.04\n-0.07\n0.96\n1.13\n0.495\n0.059\nWeakly-targeted\nβ = 0.01\n0.01\n0.03\n-0.06\n0.00\n-0.03\n1.27\n1.50\n0.503\n0.042\nβ = 0.1\n0.02\n-0.01\n0.01\n0.04\n-0.03\n1.03\n1.21\n0.498\n0.055\nβ = 1\n0.00\n0.01\n0.03\n0.10\n0.04\n0.95\n1.11\n0.501\n0.044\nβ = 10\n-0.07\n-0.01\n-0.03\n0.04\n0.00\n0.94\n1.09\n0.485\n0.061\nβ = 100\n-0.09\n-0.08\n-0.06\n-0.05\n-0.04\n0.95\n1.11\n0.493\n0.051\nUnit-level\nξ = 0.01\n0.00\n0.03\n-0.04\n-0.04\n-0.02\n1.05\n1.25\n0.511\n0.053\nξ = 0.1\n0.00\n0.00\n0.03\n0.02\n0.02\n1.05\n1.24\n0.500\n0.049\nξ = 1\n0.01\n0.02\n-0.06\n-0.05\n-0.03\n1.38\n1.63\n0.499\n0.046\nξ = 10\n0.18\n0.00\n-0.02\n-0.15\n-0.02\n1.97\n2.33\n0.496\n0.038\nξ = 100\n0.19\n-0.03\n-0.02\n-0.18\n-0.03\n2.34\n2.77\n0.502\n0.053\nPenalized\nλ = 0.01\n0.00\n0.00\n0.02\n-0.04\n-0.01\n1.01\n1.18\n0.494\n0.051\nλ = 0.1\n-0.07\n-0.05\n-0.07\n-0.11\n-0.10\n1.32\n1.56\n0.505\n0.041\nλ = 1\n0.02\n0.07\n-0.07\n-0.07\n0.01\n2.17\n2.57\n0.495\n0.045\nλ = 10\n0.16\n0.03\n-0.11\n-0.08\n-0.08\n3.79\n4.48\n0.514\n0.045\nλ = 100\n0.22\n0.15\n-0.14\n-0.14\n-0.08\n4.22\n5.00\n0.515\n0.041\nNote: Unless otherwise noted, all designs use m = 1 and m = 14.",
    "content_hash": "b4945cdb3220ee7833c954f3db081bb8e2e9b4747e7dc29fa26f9aa6ff00180e",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "123bbc71-d87f-40c6-b60c-9daec3c3d4cc",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Table 5: RMSE for Different Experimental Designs and Estimators (Averages over 1000 Simu-\nlations)\nSC\nRND\nSTR\nREG\n1-NN\n5-NN\nm = 1\n3.45\n6.35\n6.35\n8.14\n5.28\n4.40\nm = 2\n2.00\n4.70\n3.53\n6.00\n3.69\n3.20\nm = 3\n1.49\n3.91\n2.75\n5.11\n3.02\n2.66\nm = 4\n1.25\n3.49\n2.44\n4.49\n2.67\n2.40\nm = 5\n1.09\n3.22\n2.07\n4.12\n2.38\n2.28\nm = 6\n1.02\n3.04\n1.95\n3.87\n2.24\n2.23\nm = 7\n0.97\n3.01\n1.85\n3.90\n2.18\n2.32\nNote: SC: Constrained formulation of the synthetic control design. RND: Random-\nized treatment assignment followed by the difference-in-means estimator. STR: Strat-\nified randomization, followed by difference in means in each stratum. REG: Random-\nized treatment assignment followed by regression adjustment. 1-NN: Randomized\ntreatment assignment followed by 1-nearest neighbor matching. 5-NN: Randomized\ntreatment assignment followed by 5-nearest neighbor matching. SC uses outcomes\nin the fitting periods and covariates as predictors. STR, 1-NN, and 5-NN use all\npre-intervention outcomes and covariates. REG adjusts for the covariates only.",
    "content_hash": "e239e04417c58dcd8f535ccaf7222642d86ba072935ed49e50e407005f4a46a4",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "34bfe6d9-b76b-4d81-8ff3-0dfcce2ca43f",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "In this article, we have proposed synthetic control techniques, widely used in observational\nstudies, to design experiments when the treatment can only be applied to a small number of\nexperimental units. The synthetic control design optimizes jointly over the identities of the units\nassigned to the treatment and the control arms and over the weights that determine the relative\ncontribution of those units to reproduce the counterfactuals of interest. We propose various\n34",
    "content_hash": "f90d98ef12bffc62e58e059b0f36b90fac81a77cfd6e12a9a954dab95fb5dfc7",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1d319a53-154f-405b-8952-f7aca63ebd7e",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "designs to estimate average treatment effects, analyze the properties of such designs and the\nresulting estimators, and devise inferential methods to test a null hypothesis of no treatment\neffects and construct confidence intervals. In addition, we report results from an application to\nretail sales data and simulation results that demonstrate the applicability and computational\nfeasibility of the methods proposed in this article. We show that synthetic control design can\nsubstantially outperform randomized designs in experimental settings with a small number of\ntreated units. Corporate researchers, policymakers, and academic investigators are often confronted with\nsettings where interventions at the micro-unit level (e.g., customers, workers, or families) are\nunfeasible, impractical, or ineffective (see, e.g., Duflo, Glennerster and Kremer, 2007, Jones and\nBarrows, 2019). Consequently, there is broad scope for experimental design methods targeting\nlarge aggregate entities (such as regional markets, school districts, or states), a setting where\nsynthetic control designs offer a powerful tool for data-driven evaluation of treatment effects. References\nAbadie, Alberto. 2021. “Using Synthetic Controls: Feasibility, Data Requirements, and Methodolog-\nical Aspects.” Journal of Economic Literature, 59(2): 391–425. Abadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2010. “Synthetic Control Methods for\nComparative Case Studies: Estimating the Effect of California’s Tobacco Control Program.” Journal\nof the American Statistical Association, 105(490): 493–505. Abadie, Alberto, and Javier Gardeazabal. 2003.",
    "content_hash": "e8178ba86ca7c87d85189cfc853228e218f957fc8a58457909b1955bb812dbfe",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3c30b79b-974d-4987-872a-c2d15cc6c33a",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "“The Economic Costs of Conflict: A Case Study\nof the Basque County.” American Economic Review, 93(1): 113–132. Abadie, Alberto, and J´er´emy L’Hour. 2021. “A Penalized Synthetic Control Estimator for Disag-\ngregated Data.” Journal of the American Statistical Association, 116(536): 1817–1834. Agarwal, Anish, Devavrat Shah, and Dennis Shen. 2021. “Synthetic Interventions.” arXiv e-\nprints, 2006.07691. Amjad, Muhammad, Devavrat Shah, and Dennis Shen. 2018. “Robust Synthetic Control.”\nJournal of Machine Learning Research, 19(22): 1–51. Arkhangelsky, Dmitry, Susan Athey, David A. Hirshberg, Guido W. Imbens, and Stefan\nWager. 2021. “Synthetic Difference-in-Differences.” American Economic Review, 111(12): 4088–4118. Armstrong, Timothy B., and Michal Koles´ar. 2018. “Finite-Sample Optimal Estimation and\nInference on Average Treatment Effects Under Unconfoundedness.” arXiv e-prints, 1712.04594v2. 35",
    "content_hash": "76531a92339f735c9f85794ba615eafa6339ed86e7ed819f70d400399d96d0d6",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "624ff535-cab3-4417-ad3e-b8d94719000d",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Ben-Michael, Eli, Avi Feller, and Jesse Rothstein. 2021. “The Augmented Synthetic Control\nMethod.” Journal of the American Statistical Association, 116(536): 1789–1803. Bobkov, SG, and GP Chistyakov. 2014. “Bounds on the Maximum of the Density for Sums of\nIndependent Random Variables.” Journal of Mathematical Sciences, 199(2): 100–106. Bottmer, Lea, Guido Imbens, Jann Spiess, and Merrill Warnick. 2021. “A Design-Based\nPerspective on Synthetic Control Methods.” arXiv e-prints, 2101.09398. Chen, Jiafeng. 2023. “Synthetic Control as Online Linear Regression.” Econometrica, 91(2): 465–491. Chernozhukov, Victor, Kaspar W¨uthrich, and Yinchu Zhu. 2019. “Distributional Conformal\nPrediction.” arXiv e-prints, 1909.07889. Chernozhukov, Victor, Kaspar W¨uthrich, and Yinchu Zhu. 2021. “An Exact and Robust Con-\nformal Inference Method for Counterfactual and Synthetic Controls.” Journal of the American Sta-\ntistical Association, 116(536): 1849–1864. Doudchenko, Nick, Khashayar Khosravi, Jean Pouget-Abadie, Sebastien Lahaie, Miles Lu-\nbin, Vahab Mirrokni, Jann Spiess, et al. 2021. “Synthetic Design: An Optimization Approach to\nExperimental Design with Synthetic Controls.” Advances in Neural Information Processing Systems,\n34.",
    "content_hash": "755b24516d6e746c687a933422c2558882fae454e61a0b22652a3a4022cbb758",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d47328f4-7ab8-4306-ba3e-4c082c04c273",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Doudchenko, Nikolay, and Guido W Imbens. 2016. “Balancing, Regression, Difference-in-\nDifferences and Synthetic Control Methods: A Synthesis.” National Bureau of Economic Research. Doudchenko, Nikolay, David Gilinson, Sean Taylor, and Nils Wernerfelt. n.d. “Designing Ex-\nperiments with Synthetic Controls.” https://mackinstitute.wharton.upenn.edu/wp-content/\nuploads/2020/03/Wernerfelt-Nils-Doudchenko-Nick-Gilinson-David-and-Taylor-Sean_\nDesigning-Experiments-with-Synthetic-Controls.pdf. Duflo, Esther, Rachel Glennerster, and Michael Kremer. 2007. “Using Randomization in De-\nvelopment Economics Research: A Toolkit.” Handbook of development economics, 4: 3895–3962. Egami, Naoki, and Diana Da In Lee. 2024. “Designing Multi-Site Studies for External Validity: Site\nSelection via Synthetic Purposive Sampling.” https://naokiegami.com/paper/sps.pdf. Ferman, Bruno. 2021. “On the Properties of the Synthetic Control Estimator with Many Periods and\nMany Controls.” Journal of the American Statistical Association, 116(536): 1764–1772. Firpo, Sergio, and Vitor Possebom. 2018. “Synthetic Control Method: Inference, Sensitivity Anal-\nysis and Confidence Sets.” Journal of Causal Inference, 6(2): 20160026. Gunsilius, F. F. 2023.",
    "content_hash": "b54ee0a31132708e9506ab0322a70e798cc715674dbd71d69dc1f465ea8af2f9",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e0c217db-8eaf-4504-89ae-f6efb11e6bb2",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Kasy, Max, and Lukas Lehner. 2023. “Employing the Unemployed of Marienthal: Evaluation\nof a Guaranteed Job Program.” https://maxkasy.github.io/home/files/papers/Jobguarantee_\nmarienthal.pdf. Kasy, Maximilian. 2016. “Why Experimenters Might Not Always Want to Randomize, and What\nThey Could Do Instead.” Political Analysis, 24(3): 324–338. Lei, Jing, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. 2018. “Distribution-Free Predictive Inference for Regression.” Journal of the American Statistical\nAssociation, 113(523): 1094–1111. Lei, Lihua, and Emmanuel J Cand`es. 2021. “Conformal Inference of Counterfactuals and Indi-\nvidual Treatment Effects.” Journal of the Royal Statistical Society Series B: Statistical Methodology,\n83(5): 911–938. Montiel Olea, Jos´e Luis, Brenda Prallon, Chen Qiu, J¨org Stoye, and Yiwei Sun. 2024. “Ex-\nternally Valid Selection of Experimental Sites via the k-Median Problem.” arXiv e-prints, 2101.09398. Oliveira, Roberto I, Paulo Orenstein, Thiago Ramos, and Jo˜ao Vitor Romano. 2022. “Split\nconformal prediction for dependent data.” arXiv e-prints, 2203.15885. Prakash, Sourav. 2023.",
    "content_hash": "c078e811bd949d77924e18af211ae22ac791fcb93c7e9b84e83d522dc6fb3323",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0e19feb4-e233-42d0-ab62-98b28b9c2f43",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "(A.2)\nSimilarly, using expression (12a), we obtain\nJ\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nfjY E\nj = θE\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nfjZj\n! + λE\nJ\nX\nj=1\nw∗\njµj −\nJ\nX\nj=1\nfjµj\n! +\nJ\nX\nj=1\nw∗\njϵE\nj −\nJ\nX\nj=1\nfjϵE\nj\n! ,\nwhere θE is the (TE × R) matrix with rows equal to the θt’s indexed by E, and ϵE\nj is defined\nanalogously. Pre-multiplying by η′\nt(λ′\nEλE)−1λ′\nE yields\nη′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nfjY E\nj\n! = η′\nt(λ′\nEλE)−1λ′\nEθE\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nfjZj\n! 38",
    "content_hash": "046024078ad83bb0313e1e595e220745c1731cf0692a5ec83e489c439abde086",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3b4c05eb-d672-43e5-a4bb-202885ab6c8e",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "(A.4)\nIf Assumption 3 holds, (A.4) becomes\nJ\nX\nj=1\nw∗\njY I\njt −\nJ\nX\nj=1\nfjY I\njt = −η′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njϵE\nj\n+ η′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nfjϵE\nj\n+\nJ\nX\nj=1\nw∗\njξjt −\nJ\nX\nj=1\nfjξjt\n! . (A.5)\nOnly the first term on the right-hand side of (A.5) has a non-zero mean (because the weights,\nw∗\nj, depend on the error terms ϵE\nj ). Therefore,\nE\n\"\nJ\nX\nj=1\nw∗\njY I\njt −\nJ\nX\nj=1\nfjY I\njt\n#\f\f\f\f\f =\nE\n\"\nη′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njϵE\nj\n#\f\f\f\f\f . Using the same line of reasoning for the second term on the right-hand side of (A.1), we obtain\nE\n\"\nJ\nX\nj=1\nv∗\njY N\njt −\nJ\nX\nj=1\nfjY N\njt\n#\f\f\f\f\f =\nE\n\"\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nv∗\njϵE\nj\n#\f\f\f\f\f . 39",
    "content_hash": "62568f6f1d9fefacfd9c5ac5bd066a54aecbf339b7584b4dff2444c5d6b5ade3",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "58184ec8-0214-4c2f-a768-649ad2e9c4f9",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "An analogous argument yields\nE\nh\nJ\nX\nj=1\nv∗\njY N\njt −\nJ\nX\nj=1\nfjY N\njt\ni\f\f\f\f\f ≤λ\n2F\nζ\np\n2 log (2J) σ\n√TE\n,\nwhich completes the proof of the theorem. Suppose now Assumption 4 holds (but Assumption 3 does not). To obtain a bound on the bias,\nwe bound the first two terms in (A.4). Recall that\nη′\nt(λ′\nEλE)−1λs\n≤ληF\nTEζ . 40",
    "content_hash": "c6dd048ca89240e92981019c88119b0cadbd422c51a27164d578e5bc5e03ae81",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a748cb16-179c-470a-adbb-6b87fe2500ae",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "For any t ≥T0 + 1 and s ∈E, under Assumption 2 (i), we apply Cauchy-Schwarz inequality and\nthe eigenvalue bound on the Rayleigh quotient to obtain\nη′\nt(λ′\nEλE)−1λs\n\u00012 ≤\nη′\nt(λ′\nEλE)−1ηt\n\u0001 λ′\ns(λ′\nEλE)−1λs\n\u0001\n≤\n\u0012η2F\nTEζ\n\u0013\nλ\n2F\nTEζ\n! . Similarly,\nλ′\nt(λ′\nEλE)−1λs\n\u00012 ≤\nλ\n2F\nTEζ\n!2\n. (A.6)\nLet\nϵE\njt = η′\nt(λ′\nEλE)−1λ′\nEϵE\nj =\nX\ns∈E\nη′\nt(λ′\nEλE)−1λsϵjs. Because ϵE\njt is a linear combination of independent sub-Gaussians with variance proxy σ2, it\nfollows that ϵE\njt is sub-Gaussian with variance proxy (η λF/ζ)2σ2/TE. Let S = {w ∈RJ :\nPJ\nj=1 wj = 1}. Theorem 1.16 from Rigollet and H¨utter (2019) implies\nE\nh\nJ\nX\nj=1\nw∗\njY I\njt −\nJ\nX\nj=1\nfjY I\njt\ni\n=\nE\nh\nJ\nX\nj=1\nw∗\njϵE\njt\ni\f\f\f\f\f ≤E\n\"\nmax\nw∈S\nJ\nX\nj=1\nwjϵE\njt\n#\n≤η λF\nζ\np\n2 log (2J) σ\n√TE\n.",
    "content_hash": "ecea481d864050116015dae4a1aa62cf57dfcefb5eace8e9a56eadbd59f26e1a",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "46e8cad8-f655-49c4-a682-7a230b744ed8",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "for t ∈B ∪{T0 + 1, . . . , T}. For t ∈{T0 + 1, . . . , T}, but are the post-intervention estimates of\nthe treatment effects; and for t ∈B, but are the placebo treatment effects estimated for the blank\nperiods. Let\nut =\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt\nfor t ∈B, and\nut =\nJ\nX\nj=1\nw∗\njξjt −\nJ\nX\nj=1\nv∗\njϵjt\nfor t ∈{T0+1, . . . , T}. The null hypothesis (17) and the assumptions of Theorem OA.6 imply that\n{ut}t∈B∪{T0+1,...,T} is a sequence of exchangeable random variables. Additionally, Assumption 1\nand the null hypothesis (17) imply\nbut =\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt = θ′\nt\nJ\nX\nj=1\n(w∗\nj −v∗\nj)Zj + λ′\nt\nJ\nX\nj=1\n(w∗\nj −v∗\nj)µj + ut,\nfor t ∈B ∪{T0 + 1, . . . , T}. The result of the theorem follows now from Theorem D.1 in\nChernozhukov, W¨uthrich and Zhu (2021). OA.4.",
    "content_hash": "320d2cae52061da9adb4dfcd079a0e843c54b16f1e4652d31b1699534a1ea65c",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "53f22931-b9af-493d-8931-2bda7b57af61",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Therefore, the absolute value of each element in vector (γ′\nt −η′\nt(λ′\nEλE)−1λ′\nEθE) is bounded by\nγ + θληF\nζ\n. Cauchy–Schwarz inequality and Assumption 4 imply\n(γ′\nt −η′\nt(λ′\nEλE)−1λ′\nEθE)\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nfjZj\n! ≤\n\u0010\nγ + θληF\nζ\n\u0011√\nR\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nfjZj\n2\n≤\n\u0010\nγ + θληF\nζ\n\u0011\nRd,\nand\nη′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nfjY E\nj\n! ≤ληF\nζ\nd. Combining the last two displayed equations with (A.4), we have\nE\n\"\nJ\nX\nj=1\nw∗\njY I\njt −\nJ\nX\nj=1\nfjY I\njt\n#\f\f\f\f\f ≤\n\u0010\nγR + ληF\nζ\n(1 + θR)\n\u0011\nd + ληF\nζ\np\n2 log (2J) σ\n√TE\n. An analogous derivation produces\nE\n\"\nJ\nX\nj=1\nv∗\njY N\njt −\nJ\nX\nj=1\nfjY N\njt\n#\f\f\f\f\f ≤\n\u0010\nθR + λ\n2F\nζ (1 + θR)\n\u0011\nd + λ\n2F\nζ\np\n2 log (2J) σ\n√TE\n,\nwhich finishes the proof of the theorem. A.2. Proof of Theorem 2\nProof of Theorem 2.",
    "content_hash": "ac5d98c33d1e37445d8317b7ec6a010cf198fb173cf891be04644cf381ec5ac7",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a158d486-5b90-4807-bf5f-3e2c7a210f02",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Recall that\nbut =\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt,\nfor t ∈B ∪{T0 + 1, . . . , T}. For t ∈{T0 + 1, . . . , T}, but are the post-intervention estimates of\nthe treatment effects; and for t ∈B, but are the placebo treatment effects estimated for the blank\nperiods. Let\nut =\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt\n41",
    "content_hash": "fc823a406a3a0471200afc209a9498b9a24fa6484b33b048d3087b6a0caae323",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b77dd084-490f-4182-8a8f-4df9cd4c0c1b",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Lemma A.1 Assume there exist parameters ϵB and ϵT , as well as events CB and CT , such that\nthe following two conditions hold:\n1. There exists a high probability event CB such that conditional on this event, for any weights\n(w∗, v∗) and any q ∈R,\n1\nT0 −TE\nX\nt∈B\n1\nn\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤q\no\n−PE,q\n≤ϵB. (A.7)\n2. Recall that τt = PJ\nj=1 fj(Y I\njt −Y N\njt ). There exists a high probability event CT such that\nconditional on this event, for any weights (w∗, v∗), any q ∈R, and any t ∈{T0 + 1, ..., T},\nPr\n\u0010\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n≤q\n\u0011\n−PE,q\n≤ϵT . (A.8)\n42",
    "content_hash": "24ba5d71cc3666f348af50e93bb87998e67c4b06f5af48124ad6685ac5bf0b36",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4091e8e5-6cd1-4656-8a43-d9d1b95765cc",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "for t ∈B, and\nut =\nJ\nX\nj=1\nw∗\njξjt −\nJ\nX\nj=1\nv∗\njϵjt\nfor t ∈{T0 + 1, . . . , T}. The null hypothesis (17) and the assumptions of Theorem 2 imply that\n{ut}t∈B∪{T0+1,...,T} is a sequence of exchangeable random variables. Additionally, Assumption 1\nand the null hypothesis (17) imply\nbut =\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt = θ′\nt\nJ\nX\nj=1\n(w∗\nj −v∗\nj)Zj + λ′\nt\nJ\nX\nj=1\n(w∗\nj −v∗\nj)µj + ut,\nfor t ∈B ∪{T0 + 1, . . . , T}. The result of the theorem then follows from Theorem D.1 in\nChernozhukov, W¨uthrich and Zhu (2021). A.3. Proof of Theorem 3\nA.3.1. A Technical Lemma\nWe first define the following quantity and present a technical lemma. Let ϵ∗= (ϵ1∗, ϵ2∗, ..., ϵJ∗)\nbe an i.i.d. copy of (ϵ1t, ϵ2t, ..., ϵJt) the idiosyncratic noises. Using the definition of ϵ∗and\nconditioning on the weights (w∗, v∗), we define, for any q ∈R,\nPE,q = Pr\nJ\nX\nj=1\nw∗\njϵj∗−\nJ\nX\nj=1\nv∗\njϵj∗\n≤q\n! .",
    "content_hash": "bdc775eacd40493a6c005343b2fc9fc1cc3c8be5a61961a631f0b68b44ae8e93",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9abee8a9-c096-46f0-a7ab-efdba5ef7483",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "We aim to show that event E1 occurs given event CB. For any positive integer k ∈N, we can use\nCondition (A.7) to show that conditional on event CB,\n1\nT0 −TE\nX\nt∈B\n1\nn\nJ\nX\nj=1\nw∗\njYjt−\nJ\nX\nj=1\nv∗\njYjt\n≤q1−α−ϵB −1\nk\no\n≤Pr\nJ\nX\nj=1\nw∗\njϵj∗−\nJ\nX\nj=1\nv∗\njϵj∗\n≤q1−α−ϵB −1\nk\n! + ϵB\n<1 −α −ϵB + ϵB\n=1 −α\n≤\n1\nT0 −TE\nX\nt∈B\n1\nn\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤bq1−α\no\n,\n43",
    "content_hash": "d31d9ced3dd862e17118a3014b7fda8e907ae45439a43d4a9e0f8b58a4980fe5",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1d95222e-36ac-49a4-8d69-4080a5778047",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Assume that the joint event CB ∩CT happens with probability at least 1 −δB(ϵB) −δT (ϵT ), where\nwe use δB(ϵB) and δT (ϵT ) to stand for two quantities that each depends on ϵB and ϵT , respectively. In addition, assume that\nPJ\nj=1 w∗\njϵj∗−PJ\nj=1 v∗\njϵj∗\nhas a continuous distribution. Then, for any\nα ∈(0, 1) and any t ∈{T0 + 1, ..., T},\nPr\n\u0010\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n≤bq1−α\n\u0011\n−(1 −α)\n≤ϵB + ϵT + δB(ϵB) + δT (ϵT ). Note that Lemma A.1 does not require Assumptions 1–3. But for Conditions (A.7) and (A.8)\nto hold, we will apply Assumptions 1–3. To prove Lemma A.1, we borrow the proof techniques\nfrom Oliveira et al. (2022). We first define the following quantile on the probability distribution\n(instead of the empirical distribution),\nq1−α = inf\nz∈R\n(\nPr\n\u0012\nJ\nX\nj=1\nw∗\njϵj∗−\nJ\nX\nj=1\nv∗\njϵj∗\n≤z\n\u0013\n≥1 −α\n)\n(A.9)\nIntuitively, bq1−α as defined in (20) approximates q1−α as defined in (A.9). Proof of Lemma A.1. This proof proceeds in two parts. Part 1: Consider the event\nE1 =\n\u001a\nbq1−α ≥q1−α−ϵB\n\u001b\n.",
    "content_hash": "45777a7c191feffbfc86140a6affff90b060ad6c9330cca28104303e04d8ff6d",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "064d6854-7713-46f2-a5fc-1aa97942f327",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "where the first inequality is due to Condition (A.7); the second inequality is due to the infimum\npart of (A.9) (because q1−α−ϵB −1\nk < q1−α−ϵB which is the infimum value such that the probability\nin (A.9) is greater or equal to 1 −α); the last inequality is due to the definition of bq1−α in (20). The above inequality suggests that for any k ∈N, the event\nE(≤)\nk\n=\n(\n1\nT0 −TE\nX\nt∈B\n1\nn\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤q1−α−ϵB −1\nk\no\n≤\n1\nT0 −TE\nX\nt∈B\n1\nn\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤bq1−α\no)\nhappens conditional on event CB. Since the left hand side of the inequality inside event E(≤)\nk\n,\nwhich is\n1\nT0−TE\nP\nt∈B 1\nn\f\f\f PJ\nj=1 w∗\njYjt −PJ\nj=1 v∗\njYjt\n≤q1−α−ϵB −1\nk\no\n, is increasing in k, so the\nprobability E(≤)\nk\ndecreases in k.",
    "content_hash": "1cd8d8813c350fc020e5e3a35e8d63daeeae2d70637986cc400bfb14380ea48b",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2925a352-13a8-41fe-b5e5-710b59fd3cfa",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "−δB(ϵB) −δT (ϵT )\n≥Pr\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n≤q1−α−ϵB\n! −δB(ϵB) −δT (ϵT )\n44",
    "content_hash": "e325265c708e4f845915d717aa6bf938f969bc1f02396a28aeaf1ca33f109ee8",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "869edb56-3edb-40a4-b804-8d4ba3ff685f",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Given that the lower bound of Pr(E(≤)\nk\n) exists, the limit of\nlimk→+∞Pr(E(≤)\nk\n) exists, i.e.,\n1 −δB ≤lim\nk→+∞Pr(E(≤)\nk\n) = Pr(E(≤)\n∞),\nwhere we use Pr(E(≤)\n∞) to stand for the limiting event\nE(≤)\n∞\n=\n(\n1\nT0 −TE\nX\nt∈B\n1\nn\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤q1−α−ϵB\no\n≤\n1\nT0 −TE\nX\nt∈B\n1\nn\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤bq1−α\no)\n. This means that, event E1 = {bq1−α ≥q1−α−ϵB} happens conditional on event CB. Due to the\nassumption of Lemma A.1, event CT ∩E1 happens with probability at least 1 −δB(ϵB) −δT (ϵT ). Next we have, for any t ∈{T0 + 1, ..., T} in the experimental periods,\nPr\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n≤bq1−α\n! ≥Pr\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n≤bq1−α\n\\\n(CT ∩E1)\n!",
    "content_hash": "148982c65b40210c29e700234084618f73eb35a4f7c0bde50646f054689a2f71",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "125d309d-b676-4c74-bac9-058ea1e42411",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Due to the assumption of Lemma A.1, event\nCT ∩E2 happens with probability at least 1 −δB(ϵB) −δT (ϵT ). Then, for any t ∈{T0 + 1, ..., T} in the experimental periods,\nPr\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n≤bq1−α\n! ≤Pr\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n≤bq1−α\n\\\n(CT ∩E2)\n! + δB(ϵB) + δT (ϵT )\n45",
    "content_hash": "d8f90f613b7467bcd37282f41e98a712fac161046e9f095ac2f28e7d76f38050",
    "location": null,
    "page_start": 45,
    "page_end": 45,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1f9ae5b7-24af-4ae5-9bef-542f2edc2a15",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Note that,\nmax\nt∈B\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −v∗\nj)ϵE\nj\n≤max\nt∈B\nJ\nX\nj=1\n|w∗\nj −v∗\nj|\nX\ns∈E\n|λ′\nt(λ′\nEλE)−1λs||ϵjs|\n≤\nJ\nX\nj=1\n|w∗\nj −v∗\nj|\nX\ns∈E\nλ\n2F\nTEζ |ϵjs|,\nwhere the second inequality is due to (A.6), and because |w∗\nj −v∗\nj| ≥0 and |ϵjs| ≥0. Therefore,\nPr(E2) ≥1 −Pr\nJ\nX\nj=1\n|w∗\nj −v∗\nj|\n2\nX\ns∈E\nλ\n2F\nTEζ |ϵjs| >\nv\nu\nu\nt2σ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011! ≥1 −\nJ\nX\nj=1\nPr\nX\ns∈E\nλ\n2F\nTEζ |ϵjs| >\nv\nu\nu\nt2σ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011! ≥1 −zE,\nwhere the second inequality follows from union bound, and the third inequality is the Chernoff\nbound for sub-Gaussian random variables. 47",
    "content_hash": "5d41d58cd44a7ff3dba00592bab4f1b5104ad57041410e62b293f3e16374a369",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ecf58090-d9d3-49d2-8ee5-51202536014b",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "E1 =\n(\n1\nT0 −TE\nX\nt∈B\n1\nn\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤q\no\n−\n1\nT0 −TE\nX\nt∈B\nPr\n\u0010\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤q\n\u0011\f\f\f\f\f ≤\ns\n1\n2(T0 −TE) log\n\u0010 2\nzB\n\u0011)\n. Due to Hoeffding inequality for bounded random variables (conditioning on the weights (w∗, v∗),\nthese indicators are independent), we have that event E1 happens with probability 1 −zB. Second, in the blank periods,\nE2 =\n\n\n∀t ∈B,\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −v∗\nj)ϵE\nj\n≤\nv\nu\nu\nt8σ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n\n\n\n=\n\n\nmax\nt∈B\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −v∗\nj)ϵE\nj\n≤\nv\nu\nu\nt8σ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n\n\n.",
    "content_hash": "e28a8e9b8c6df5348b8f3eb3b6c04db3390d9558f09f2537dc023b05f4e30eb7",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "651235a7-614d-43b7-a298-e64dc9566712",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Third, in the experimental periods,\nE3 =\n\n\n∀t ∈{T0 + 1, . . . , T},\nη′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −fj)ϵE\nj\n≤\nv\nu\nu\nt8σ2λ\n2η2F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n\n\n\n=\n\n\n\nmax\nt∈{T0+1,...,T}\nη′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −fj)ϵE\nj\n≤\nv\nu\nu\nt8σ2λ\n2η2F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n\n\n. Note that,\nmax\nt∈{T0+1,...,T}\nη′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −fj)ϵE\nj\n≤\nmax\nt∈{T0+1,...,T}\nJ\nX\nj=1\n|w∗\nj −fj|\nX\ns∈E\n|η′\nt(λ′\nEλE)−1λs||ϵjs|\n≤\nJ\nX\nj=1\n|w∗\nj −fj|\nX\ns∈E\nληF\nTEζ |ϵjs|,\nwhere the second inequality is due to (A.6), and because |w∗\nj −fj| ≥0 and |ϵjs| ≥0.",
    "content_hash": "e68d527eed4dee2d93a8c038f7114ffa56982d9a02938284ec044f8a5d80ebf9",
    "location": null,
    "page_start": 48,
    "page_end": 48,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3cd3830d-8bb9-4eb9-8991-34b518afbc1c",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Therefore,\nPr(E3) ≥1 −Pr\nJ\nX\nj=1\n|w∗\nj −fj|\n2\nX\ns∈E\nληF\nTEζ |ϵjs| >\nv\nu\nu\nt2σ2λ\n2η2F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011! ≥1 −\nJ\nX\nj=1\nPr\nX\ns∈E\nληF\nTEζ |ϵjs| >\nv\nu\nu\nt2σ2λ\n2η2F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011! ≥1 −zE,\nwhere the second inequality follows from union bound, and the third inequality is the Chernoff\nbound for sub-Gaussian random variables. Fourth, in the experimental periods,\nE4 =\n\n\n∀t ∈{T0 + 1, . . . , T},\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(v∗\nj −fj)ϵE\nj\n≤\nv\nu\nu\nt8σ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n\n\n. Similar to the event E3, we can show that Pr(E4) ≥1 −zE. Step 2: Now we check Conditions (A.7) and (A.8). We first check Condition (A.7). In the\nstatement of Condition (A.7), let CB = E1 ∩E2. Note that\n48",
    "content_hash": "b62d0c44c94abbab357b4b8e30046e6bda73ff8a0f04ccbb0c9108b736add26e",
    "location": null,
    "page_start": 48,
    "page_end": 48,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c1f32b60-a1d8-4e52-8e2d-8b9c014d657c",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Conditional on event E2, we have for any t ∈B in the blank periods,\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n=\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt −λ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −v∗\nj)ϵE\nj\n≤\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt\n+\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −v∗\nj)ϵE\nj\n≤\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt\n+\nv\nu\nu\nt8σ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\nFrom the above inequality, for any t ∈B, due to Lemma OA.3-1 and Lemma OA.4, the probability\ndensity of\nPJ\nj=1 w∗\njϵjt −PJ\nj=1 v∗\njϵjt\nis upper bounded by κ\n√\neJ, where e ≈2.718 is the base of\nthe natural logarithm.",
    "content_hash": "8c54c4ca2dd725f114ab4b0651ad6873b7e60face61548c2d781f2556f3a1f48",
    "location": null,
    "page_start": 49,
    "page_end": 49,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "57e8429a-a9fb-4340-a5b9-711d67cfff91",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "This implies that, conditional on event E2, for any t ∈B and any q ∈R,\nPr\n\u0012\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤q\n\u0013\n−Pr\n\u0012\nJ\nX\nj=1\nw∗\njϵj∗−\nJ\nX\nj=1\nv∗\njϵj∗\n≤q\n\u0013\n=\nPr\n\u0012\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤q\n\u0013\n−Pr\n\u0012\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt\n≤q\n\u0013\n≤κ\nv\nu\nu\nt8eJσ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n. 49",
    "content_hash": "0a248bec2f41f21a38b962dc008b1065f2f32dbad1bbf1055d90adf3a674bae0",
    "location": null,
    "page_start": 49,
    "page_end": 49,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b2eb6cce-2b08-4096-8491-e02a8f416cd0",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Assumption 3 implies\nJ\nX\nj=1\nw∗\njµj −\nJ\nX\nj=1\nv∗\njµj = −(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njϵE\nj −\nJ\nX\nj=1\nv∗\njϵE\nj\n! . Under the null hypothesis (17), it follows that\nbut =\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n= −λ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −v∗\nj)ϵE\nj + ut,\nOA.6",
    "content_hash": "a289786b152a1c79fef31cf42f9ab641a75fd2459c516ef80423f26963ab5587",
    "location": null,
    "page_start": 49,
    "page_end": 57,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f2911d26-00f9-4d8c-9290-0b0a982f4aca",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "This means that, conditional on event E2, for any t ∈B and any q ∈R,\n1\nT0 −TE\nX\nt∈B\nPr\n\u0010\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤q\n\u0011\n−Pr\n\u0012\nJ\nX\nj=1\nw∗\njϵj∗−\nJ\nX\nj=1\nv∗\njϵj∗\n≤q\n\u0013\n≤κ\nv\nu\nu\nt8eJσ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n. To conclude checking Condition (A.7), we see that conditional on event CB = E1 ∩E2, for any\nweights (w∗, v∗) and q ∈R,\n1\nT0 −TE\nX\nt∈B\n1\nn\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt\n≤q\no\n−Pr\n\u0012\nJ\nX\nj=1\nw∗\njϵj∗−\nJ\nX\nj=1\nv∗\njϵj∗\n≤q\n\u0013\n≤\ns\n1\n2(T0 −TE) log\n\u0010 2\nzB\n\u0011\n+ κ\nv\nu\nu\nt8eJσ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n. We then move on to check Condition (A.8). In the statement of Condition (A.8), let CT = E3∩E4.",
    "content_hash": "ccfbb6a478fe87ba9a12542aa61ea00399c55b1a60b656b69d8578ac2031d554",
    "location": null,
    "page_start": 50,
    "page_end": 50,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2bebea7c-65be-4f39-9159-5a1e1035ef5e",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "For any t ∈{T0 + 1, ..., T} in the experimental periods, we have\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n=\nJ\nX\nj=1\n(w∗\nj −fj)Y I\njt −\nJ\nX\nj=1\n(v∗\nj −fj)Y N\njt\n=\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt −η′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −fj)ϵE\nj + λ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(v∗\nj −fj)ϵE\nj . where the third equality is using Assumption 3 and using the assumption that ξjt has the same\ndistribution as ϵjt for t = T0 + 1, . . . , T, and j = 1, . . . , J. Conditional on event E3 ∩E4, we have for any t ∈{T0 + 1, ..., T} in the experimental periods,\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n≤\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt\n+\nv\nu\nu\nt8σ2λ\n2η2F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n+\nv\nu\nu\nt8σ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n. 50",
    "content_hash": "55441eb686bb0d03448010ef28ea028ce221c8802b171b00dda199abd9d089dd",
    "location": null,
    "page_start": 50,
    "page_end": 50,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "34015bdb-619a-4409-a41b-90ae87487a0c",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Following the same argument, we see that conditional on event CT = E3 ∩E4, for any t ∈\n{T0 + 1, ..., T} and any q ∈R,\nPr\n\u0010\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n≤q\n\u0011\n−Pr\n\u0012\nJ\nX\nj=1\nw∗\njϵj∗−\nJ\nX\nj=1\nv∗\njϵj∗\n≤q\n\u0013\n≤κ\nv\nu\nu\nt8eJσ2λ\n2η2F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n+ κ\nv\nu\nu\nt8eJσ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n. Step 3: Now we apply Lemma A.1. Note that, the joint event CB ∩CT = E1 ∩E2 ∩E3 ∩E4\nhappens with probability at least 1 −zB −3zE. Due to Lemma A.1,\nPr\n\u0010\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt −τt\n≤bq1−α\n\u0011\n−(1 −α)\n≤\ns\n1\n2(T0 −TE) log\n\u0010 2\nzB\n\u0011\n+κ\nv\nu\nu\nt8eJσ2λ\n2η2F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n+2κ\nv\nu\nu\nt8eJσ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n+zB+3zE.",
    "content_hash": "25c06feb1f0c383373fd470825304a289e2139220eb273e1f93b0f349b3f826a",
    "location": null,
    "page_start": 51,
    "page_end": 51,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3abfc4bd-68b8-4619-80fc-3b48f48474b1",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Because\nPJ\nj=1 w∗\njYjt −PJ\nj=1 v∗\njYjt −τt\n≤bq1−α is equivalent to τt ∈bC1−α(Y1t, Y2t, ..., YJt), this\nimplies\nPr\n\u0010\nτt ∈bC1−α(Y1t, Y2t, ..., YJt)\n\u0011\n−(1 −α)\n≤\ns\n1\n2(T0 −TE) log\n\u0010 2\nzB\n\u0011\n+κ\nv\nu\nu\nt8eJσ2λ\n2η2F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n+2κ\nv\nu\nu\nt8eJσ2λ\n4F 2\nζ2TE\nlog\n\u00102J\nzE\n\u0011\n+zB+3zE. 51",
    "content_hash": "61d46f09606bebbf30838c7d225fa0daa0ec3e92ce3a8c4a7d1acc0eb5273ae1",
    "location": null,
    "page_start": 51,
    "page_end": 51,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "313609ac-a26f-4d65-940c-8f4e3fd6fd8a",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Online Appendix\nSynthetic Controls for Experimental Design\nAlberto Abadie and Jinglong Zhao\nApril 24, 2025\nOA.1. Designs Based on Penalized and Bias-corrected Synthetic Control Methods\nConsider the design problem in (9),\nX −\nJ\nX\nj=1\nwjXj\n2\n|\n{z\n}\n(a)\n+ξ\nJ\nX\nj=1\nwj\nXj −\nJ\nX\ni=1\nvijXi\n2\n|\n{z\n}\n(b)\n. (OA.1)\nTo apply the penalized synthetic control method of Abadie and L’Hour (2021) to this design, we\nreplace the term (a) in (OA.1) with\nX −\nJ\nX\nj=1\nwjXj\n2\n+ λ1\nJ\nX\nj=1\nwj∥X −Xj∥2,\n(OA.2)\nand the terms (b) with\nXj −\nJ\nX\ni=1\nvijXi\n2\n+ λ2\nJ\nX\ni=1\nvij∥Xj −Xi∥2. (OA.3)\nHere, λ1 and λ2 are positive constants that penalize discrepancies between the target values of\nthe predictors (X in (OA.2) and Xj in (OA.3)) and the values of the predictors for the units\nthat contribute to their synthetic counterparts. All designs of Section 2 depend on terms akin to (a) and (b) in (OA.1). These terms can be\nadapted as in (OA.2) and (OA.3) to implement the penalized synthetic control design of Abadie\nand L’Hour (2021).",
    "content_hash": "b37d153ff125ea3b15770388eb7d7eb53b0bd2be9a5774e9d16ec25975f5a6eb",
    "location": null,
    "page_start": 52,
    "page_end": 52,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "653b3493-af56-44f4-8149-f95c42f7a50e",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "For all the designs in Section 2, the bias-corrected estimator of Abadie and L’Hour (2021) is\nbτ BC\nt\n=\nJ\nX\nj=1\nw∗\nj(Yjt −bµ0t(Xj)) −\nJ\nX\nj=1\nv∗\nj(Yjt −bµ0t(Xj)),\nwhere t ≥T0+1 and the terms bµ0t(Xj) are the fitted values of a regression of untreated outcomes,\nY N\njt , on unit’s characteristics, Xj. To avoid over-fitting biases, bµ0t(Xj) can be cross-fitted for\nthe untreated. OA.1",
    "content_hash": "c30d3bfc33e3c361fe051efa8f80ee3400164ef3e498ddc9e34591a9ab034ad0",
    "location": null,
    "page_start": 52,
    "page_end": 52,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a7ab3e38-04b9-4654-97a8-6ad527c94b66",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "A limitation of the result in Theorem OA.1 is that there are values of the parameters of the\ndata generating for which the result of the theorem provides a tight bound on test size only for\nlarge values of TE. We prove Theorem OA.1 next. OA.2.1. Definitions\nFirst, define Tp = min{T −T0, T0 −TE}. Next, recall that\nbut =\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt,\nfor t ∈B ∪{T0 + 1, . . . , T}. For t ∈{T0 + 1, . . . , T}, but are the post-intervention estimates of\nthe treatment effects; and for t ∈B, but are the placebo treatment effects estimated for the blank\nperiods. OA.2",
    "content_hash": "3e61c6b2dcbd02b949eaa52b908d655739f6cec12152ba13e2f16ff0361d1269",
    "location": null,
    "page_start": 53,
    "page_end": 53,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f324c5cc-9246-4e16-9351-92d0d0c590dd",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "In step three, we show that, conditional on C1 and C2, the ordering of S(eπ) and S(e)\nwill be the same as the ordering of S(beπ) and S(be) for most π ∈Π, which implies that bF(S(be))\nand ˜F(S(e)) are also close to each other. In step four, we conclude the proof by linking bF(S(be))\nto the estimated p-value, and ˜F(S(e)) to the nominal level α. OA.3",
    "content_hash": "9a6630ada6dc3aade22be9fdeb519648d8d1b0d10214ba587f28eb0a1d1aa4ba",
    "location": null,
    "page_start": 54,
    "page_end": 54,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "63ae979c-2071-4b83-9167-a175f29cf058",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Let\nut =\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt\n(OA.4)\nfor t ∈B, and\nut =\nJ\nX\nj=1\nw∗\njξjt −\nJ\nX\nj=1\nv∗\njϵjt\n(OA.5)\nfor t ∈{T0 + 1, . . . , T}. For each π ∈Π, similar to our definition of beπ, define the (T −T0)-\ndimensional vector\neπ = (uπ(1), uπ(2), ..., uπ(T−T0)). In addition, let e = (u1, . . . , uT−T0) = (τT0+1, . . . , τT). It is useful to observe that, under the\nnull hypothesis in (17), the random variables ut for t ∈B ∪{T0 + 1, . . . , T} are independent and\nidentically distributed. Next, define the following two functions. Let\nbF(x) = 1\n|Π|\nX\nπ∈Π\n1 {S(beπ) < x} ,\nand\n˜F(x) = 1\n|Π|\nX\nπ∈Π\n1 {S(eπ) < x} . The proof of Theorem OA.1 proceeds in four steps. In step one, we define a high probability\nevent, C1, such that ut and but are close to each other under C1. In step two, we define a high\nprobability event, C2, such that many components of {S(eπ)}π∈Π are well-separated from S(e)\nunder C2.",
    "content_hash": "d91f3819874b83fe715eef9baa91de3e305bce228b4636150118b4b00ce9b9e6",
    "location": null,
    "page_start": 54,
    "page_end": 54,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ef84097a-d9e6-4145-8542-2b69d81c92a1",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Proof of Lemma OA.4. This proof consists of two steps. In Step 1, we prove a version of the\nlemma after conditioning on (w∗, v∗). In Step 2, we apply the law of total probability to obtain\na bound on the unconditional density of ut. Step 1. We condition on (w∗, v∗) and write ut|(w∗, v∗) to indicate that we are conditional on\n(w∗, v∗). Fix any t ∈B ∪{T0 + 1, . . . , T}. Using Lemma OA.2 (Bobkov and Chistyakov, 2014, Corol-\nlary 2), let there be J variables ϵjt for any j ∈{1, 2, . . . , J}. For any j ∈{1, 2, . . . , J}, define\naj =\nw∗\nj −v∗\nj\nqPJ\nj=1(w∗\nj −v∗\nj )2 such that PJ\nj=1 a2\nj = 1. Using aj, we can write ut as ut =\nqPJ\nj=1(w∗\nj −v∗\nj)2 ·\nPJ\nj=1 ajϵjt.",
    "content_hash": "91287f06fa8dbefdcd4137dad2ead5d8edcffaa1e7289aaf6338b5083195c93f",
    "location": null,
    "page_start": 56,
    "page_end": 56,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9445bfe1-a92b-43d3-8590-f76610616f83",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "When J is even,\nΛut|(w∗,v∗) ≤\n1\nqPJ\nj=1(w∗\nj −v∗\nj)2\n· ΛPJ\nj=1 ajϵjt\n≤\n1\nqPJ\nj=1(w∗\nj −v∗\nj)2\n· √eκ\n≤\n1\nqPJ\nj=1( 2\nJ )2\n· √eκ\n=\n√\nJ\n2\n√eκ,\nwhere the first inequality is due to Lemma OA.3 Part 2; the second inequality is due to\nLemma OA.2; the third inequality is due to convexity and Jensen’s inequality, and the worst\ncase is taken when w∗\nj = 2/J for one half of total units and v∗\nj = 2/J for the other half. When\nJ is odd,\nΛut|(w∗,v∗) ≤\n1\nqPJ\nj=1(w∗\nj −v∗\nj)2\n· ΛPJ\nj=1 ajϵjt\n≤\n1\nqPJ\nj=1(w∗\nj −v∗\nj)2\n· √eκ\n≤\n1\nq\nJ+1\n2 (\n2\nJ+1)2 + J−1\n2 (\n2\nJ−1)2\n· √eκ\n=\nr\nJ2 −1\nJ\n·\n√eκ\n2 ,\nOA.5",
    "content_hash": "aab548d77c20b3f13322a710548460b19e3d58a16cd978b0beff3d48de2cf9b8",
    "location": null,
    "page_start": 56,
    "page_end": 56,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fa2f82c5-60a3-4a0b-b40e-ecb7b1de407e",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "≤\n√\nJ\n2\n√eκ,\nwhere the first inequality is due to Lemma OA.3 Part 2; the second inequality is due to\nLemma OA.2; the third inequality is due to convexity and Jensen’s inequality, and the worst\ncase is taken when w∗\nj = 2/(J + 1) for (J + 1)/2 of total units and v∗\nj = 2/(J −1) for the other\n(J −1)/2 of total units. Step 2. Using the law of total probability, we show\nfut(u) =\nZ\n(w∗,v∗)\nf\nu|(w∗, v∗)\n\u0001\ndP(w∗, v∗)\n≤\nZ\n(w∗,v∗)\n√\nJ\n2\n√eκ dP(w∗, v∗)\n=\n√\nJ\n2\n√eκ,\nwhere we use P(w∗, v∗) to stand for the joint distribution of (w∗, v∗). OA.2.3. Proof of Theorem OA.1\nProof of Theorem OA.1. (Step one.) Note that\nJ\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nv∗\njY E\nj = θE\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nv∗\njZj\n! + λE\nJ\nX\nj=1\nw∗\njµj −\nJ\nX\nj=1\nv∗\njµj\n! +\nJ\nX\nj=1\nw∗\njϵE\nj −\nJ\nX\nj=1\nv∗\njϵE\nj\n! .",
    "content_hash": "6211e9709fdd7c6675b2cdfe3c96a99fe90a316fc16a23da885028a57212f475",
    "location": null,
    "page_start": 57,
    "page_end": 57,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6a87b664-1778-4864-a9f8-92100bfbf5c8",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "for t ∈B ∪{T0 + 1, . . . , T}. We next define an event\nC1 =\n(\n∀t ∈B ∪{T0 + 1, . . . , T},\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −v∗\nj)ϵE\nj\n≤z1\n)\n=\n(\nmax\nt∈B∪{T0+1,...,T}\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −v∗\nj)ϵE\nj\n≤z1\n)\n. Note that,\nmax\nt∈B∪{T0+1,...,T}\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\n(w∗\nj −v∗\nj)ϵE\nj\n≤\nmax\nt∈B∪{T0+1,...,T}\nJ\nX\nj=1\n|w∗\nj −v∗\nj|\nX\ns∈E\n|λ′\nt(λ′\nEλE)−1λs||ϵjs|\n≤\nJ\nX\nj=1\n|w∗\nj −v∗\nj|\nX\ns∈E\nλ\n2F\nTEζ |ϵjs|,\nwhere the second inequality is due to (A.6), and because |w∗\nj −v∗\nj| ≥0 and |ϵjs| ≥0. Therefore,\nPr(C1) ≥1 −Pr\nJ\nX\nj=1\n|w∗\nj −v∗\nj|\n2\nX\ns∈E\nλ\n2F\nTEζ |ϵjs| > z1\n2\n!",
    "content_hash": "4140524607bda62849c299ac8335e13f9c3ec7a337daf196fd68c7708652f76d",
    "location": null,
    "page_start": 58,
    "page_end": 58,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "52fe891d-ce98-4ac0-b379-259f3d888586",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Then, for any k ∈{1, 2, ..., Tp} and π ∈Πk, we focus on the following indicator\n1\n\u001a\nX\nt∈π\\π0\n|ut| −\nX\nt∈π0\\π\n|ut|\n≤2kz1\n\u001b\n. The above indicator involves 2k instances of |ut|’s. Intuitively, it is obtained by canceling out\ncommon terms in S(eπ) and S(e). Below we focus on the properties of the sum of such indicators. First, focus on the probability\ndensity of\nP\nt∈π\\π0 |ut| −P\nt∈π0\\π |ut|\n. We have\nΛ|\nP\nt∈π\\π0 |ut|−P\nt∈π0\\π |ut|| ≤2ΛP\nt∈π\\π0 |ut|−P\nt∈π0\\π |ut|\n≤2\n√\n2kΛP\nt∈π\\π0\n1\n√\n2k |ut|−P\nt∈π0\\π\n1\n√\n2k |ut|\n≤2\n√\n2k√eΛ|ut|\n≤2\n√\n2k√e\n√\neJκ\n= 2\n√\n2Jkeκ,\nwhere the first inequality is due to Lemma OA.3-1; the second inequality is due to Lemma OA.3-2;\nthe third inequality is due to Lemma OA.2; the last inequality is due to Lemma OA.4 and OA.3-1.",
    "content_hash": "c61f48a2fdadf4efde9654617c4e554b6c329d33b3f6823629ecb95f140a1add",
    "location": null,
    "page_start": 59,
    "page_end": 59,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f348662d-801b-42a0-8e66-8023a4456b2f",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "The probability that event C2 happens is at least\nPr(C2) ≥1 −\nPTp\nk=1 |Πk|\n√\nk3\n|Π|\n4e\n√\n2Jz1κ\nz2\n. (Step three.)\nConditional on event C2, fewer than |Π|z2 of the absolute value terms in\n(OA.6) are such that\nP\nt∈π\\π0 |ut| −P\nt∈π0\\π |ut|\n≤2kz1. For all the others,\nP\nt∈π\\π0 |ut| −\nP\nt∈π0\\π |ut|\n> 2kz1. Conditional on event C1, we know that |but −ut| ≤z1 for any t ∈B ∪{T0 + 1, . . . , T}. So we\nhave that P\nt∈π\\π0 |ut| −P\nt∈π0\\π |ut| > 2kz1 implies\nS(beπ) −S(be) =\n1\nT −T0\nX\nt∈π\n|but| −\n1\nT −T0\nX\nt∈π0\n|but|\n=\n1\nT −T0\n\nX\nt∈π\\π0\n|but| −\nX\nt∈π0\\π\n|but|\n\n\n≥\n1\nT −T0\n\nX\nt∈π\\π0\n(|ut| −z1) −\nX\nt∈π0\\π\n(|ut| + z1)\n\n\n>\n1\nT −T0\n(2kz1 −2kz1)\n= 0,\nwhere the first equality is due to definition S(eπ) =\n1\nT−T0\nP\nt∈π |ut|.",
    "content_hash": "d15e681e03ce84356ea612b5fd9da8763b1c0244e82d7934ae4ce09ba732b929",
    "location": null,
    "page_start": 60,
    "page_end": 60,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1ac2a915-6892-4d46-b7fb-6165ce78afdd",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Similarly, C1 and P\nt∈π\\π0 |ut|−\nP\nt∈π0\\π |ut| < −2kz1 imply\nS(beπ) −S(be) =\n1\nT −T0\n\nX\nt∈π\\π0\n|but| −\nX\nt∈π0\\π\n|but|\n\n\n≤\n1\nT −T0\n\nX\nt∈π\\π0\n(|ut| + z1) −\nX\nt∈π0\\π\n(|ut| −z1)\n\n\n<\n1\nT −T0\n(−2kz1 + 2kz1)\n= 0. Combining both cases, we know that conditional on C1 and when\nP\nt∈π\\π0 |ut| −P\nt∈π0\\π |ut|\n>\n2kz1, the ordering of S(eπ) and S(e) is the same as the ordering of S(beπ) and S(be). As a\nOA.9",
    "content_hash": "5afc59299cbef61f65d8e044a18df79170ba0b384faa67b77cc9b483c71e264d",
    "location": null,
    "page_start": 60,
    "page_end": 60,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "51b9cbdf-f2b9-4e6e-93b0-c5a058668403",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "result, for those π such that\nP\nt∈π\\π0 |ut| −P\nt∈π0\\π |ut|\n> 2kz1, we have 1 {S(beπ) ≥S(be)} =\n1 {S(eπ) ≥S(e)}. There are at most |Π|z2 many π’s that contribute to the following summation,\nTp\nX\nk=1\nX\nπ∈Πk\n\u0012\n1 {S(beπ) ≥S(be)} −1 {S(eπ) ≥S(e)}\n\u0013\f\f\f\f < |Π|z2. Note that S(beπ0) = S(be) and S(eπ0) = S(e), so 1 {S(beπ) > S(be)} = 1 {S(eπ0) > S(e)} is always\ntrue. Combining π0 we have\nX\nπ∈Π\n\u0012\n1 {S(beπ) ≥S(be)} −1 {S(eπ) ≥S(e)}\n\u0013\n=\nTp\nX\nk=0\nX\nπ∈Πk\n\u0012\n1 {S(beπ) ≥S(be)} −1 {S(eπ) ≥S(e)}\n\u0013\n<|Π|z2. We conclude step three using the following block of inequalities.",
    "content_hash": "7e738cf6a786d98e6572791ce89b557b2e8403bd841fe032e34d27bfaab55fb7",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bf52fe6c-36eb-46b7-b3f7-33baeb5473ac",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "For any α ∈(0, 1],\nPr\n\u0010\n1 −bF(S(be)) ≤α\n\u0011\n−Pr\n\u0010\n1 −˜F(S(e)) ≤α\n\u0011\n=\nPr\n\u0012\n1 −1\n|Π|\nX\nπ∈Π\n1{S(beπ) < S(be)} ≤α\n\u0013\n−Pr\n\u0012\n1 −1\n|Π|\nX\nπ∈Π\n1{S(eπ) < S(e)} ≤α\n\u0013\n=\nPr\n\u0012 X\nπ∈Π\n1{S(beπ) ≥S(be)} ≤α|Π|\n\u0013\n−Pr\n\u0012 X\nπ∈Π\n1{S(eπ) ≥S(e)} ≤α|Π|\n\u0013\n=\nE\n\u0014\n1\n\u001a X\nπ∈Π\n1{S(beπ) ≥S(be)} ≤α|Π|\n\u001b\u0015\n−E\n\u0014\n1\n\u001a X\nπ∈Π\n1{S(eπ) ≥S(e)} ≤α|Π|\n\u001b\u0015\n≤E\n1\n\u001a X\nπ∈Π\n1{S(beπ) ≥S(be)} ≤α|Π|\n\u001b\n−1\n\u001a X\nπ∈Π\n1{S(eπ) ≥S(e)} ≤α|Π|\n\u001b\n≤Pr\n\u0012\f\f\f\fα|Π| −\nX\nπ∈Π\n1{S(eπ) ≥S(e)}\n≤\nX\nπ∈Π\n\u0012\n1 {S(beπ) ≥S(be)} −1 {S(eπ) ≥S(e)}\n\u0013\n\u0013\n,\nwhere the second inequality is due to the following: |1{a ≤c}−1{b ≤c}| ≤1{|c−b| ≤|a−b|}.",
    "content_hash": "12e4cd1d1d402cb2bb78df7b231038415b73238da7236e7b40413f4cb82cac5b",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6ad28d64-ac41-4876-9f04-94af815cf6d9",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Conditional on events C1 and C2, we obtain\nPr\n\u0012\n1 −bF(S(be)) ≤α\n\u0013\n−Pr\n\u0012\n1 −˜F(S(e)) ≤α\n\u0013\n≤Pr\n\u0012\f\f\f\fα|Π| −\nX\nπ∈Π\n1{S(eπ) ≥S(e)}\n< |Π|z2\n\u0013\nOA.10",
    "content_hash": "cf34ac796c20fda312cb6014cf9e749c050503002837889ed12f3900c04f5b72",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "68c8ba70-8b68-42f8-91f7-3b0c8c871fb2",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "≤2|Π|z2\n|Π|\n= 2z2,\n(OA.7)\nwhere the last inequality is because P\nπ∈Π 1{S(eπ) ≥S(e)} is a discrete uniform distribution\nover {1, 2, ..., |Π|}, and that there are at most 2|Π|z2 many integers centered around α|Π|. (Step four.) Note that, for any α ∈(0, 1],\nα −1\n|Π| ≤Pr\n\u0010\n1 −˜F(S(e)) ≤α\n\u0011\n≤α. So conditional on events C1 and C2, (OA.7) implies\nPr\n\u0010\n1 −bF(S(be)) ≤α\n\u0011\n≤Pr\n\u0010\n1 −˜F(S(e)) ≤α\n\u0011\n+ 2z2 ≤α + 2z2\nand\nPr\n\u0010\n1 −bF(S(be)) ≤α\n\u0011\n≥Pr\n\u0010\n1 −˜F(S(e)) ≤α\n\u0011\n−2z2 ≥α −2z2 −1\n|Π|. Combining both parts, conditional on C = C1 ∩C2, we have\nα −2z2 −1\n|Π| ≤Pr(bp ≤α) = Pr\n\u0010\n1 −bF(S(be)) ≤α\n\u0011\n≤α + 2z2,\nand C happens with probability at least\nPr(C1 ∩C2) ≥(1 −Pr(C1)) + (1 −Pr(C2)) −1\n≥1 −2J exp\n−\nz2\n1ζ2\n8σ2λ\n4F 2TE\n!",
    "content_hash": "1be389ec0da662641c02912bab16977bcf229c4c6fe97af577533e1795fabf1f",
    "location": null,
    "page_start": 62,
    "page_end": 62,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ad56677b-53aa-4ad6-b5c0-7ac7eecd4bef",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Assumption 5 With probability one, (i)\nJ\nX\nj=1\nw∗\njZj =\nJ\nX\nj=1\nv∗\njZj,\nand (ii)\nJ\nX\nj=1\nw∗\njY E\nj =\nJ\nX\nj=1\nv∗\njY E\nj . In practice, Assumption 3 may only hold approximately. The next assumption accommodates\nsettings with imperfect fit. Assumption 6 There exists a positive constant d > 0, such that with probability one,\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nv∗\njZj\n2\n2 ≤Rd2,\nJ\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nv∗\njY E\nj\n2\n2 ≤TEd2. Using the above assumptions, we are able to provide the following bias bounds. Theorem OA.5 If Assumptions 1, 2, and 5 hold, then for any t ≥T0 + 1,\n|E\n\u0002\nbτ T\nt −τ T\nt\n\u0003\n| ≤λ\n2F\nζ 2\np\n2 log (2J) σ\n√TE\n. If Assumptions 1, 2, and 6 hold, then for any t ≥T0 + 1,\n|E [bτt −τt] | ≤\n\u0010\nθR + λ\n2F\nζ (1 + θR)\n\u0011\nd + λ\n2F\nζ 2\np\n2 log (2J) σ\n√TE\n. We provide the following result on inference. Theorem OA.6 Suppose that Assumptions 1 and 5(i) hold.",
    "content_hash": "3fbf7a5e19b63931d92c8edf49510aec879575931314cbdefe6f54274cf75e13",
    "location": null,
    "page_start": 63,
    "page_end": 63,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a55c8983-5737-42e0-8d23-b6e8647c7c13",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Assume that {λt}t∈B∪{T0+1,...,T} is a\nsequence of exchangeable random variables independent of {ϵjt}t∈B∪{T0+1,...,T} and {ξjt}t∈{T0+1,...,T}. Assume also that for each j = 1, . . . , J, {ϵjt}t∈B∪{T0+1,...,T} and {ξjt}t∈{T0+1,...,T} are two sequences\nof exchangeable random variables, respectively. Then under the null hypothesis (17), we have\nα −1\n|Π| ≤Pr(bp ≤α) ≤α,\nfor any α ∈[0, 1], where Pr(bp ≤α) is taken over the distributions of {ξjt, ϵjt, λt}. OA.12",
    "content_hash": "6fb3c6995017127a958d4101b9d8c62f53cecdda5a085e6e72d97def82fd9389",
    "location": null,
    "page_start": 63,
    "page_end": 63,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "169869fc-824a-43f4-be9b-f83d5f4879e4",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "If Assumption 1 holds but 5(i) is violated, the same result holds if {(θt, λt)}t∈B∪{T0+1,...,T} is a\nsequence of exchangeable random variables independent of {ϵjt}t∈B∪{T0+1,...,T} and {ξjt}t∈{T0+1,...,T},\nand if for each j = 1, . . . , J, {ϵjt}t∈B∪{T0+1,...,T} and {ξjt}t∈{T0+1,...,T} are two sequences of ex-\nchangeable random variables, respectively. Here, Pr(bp ≤α) is taken over the distributions of\n{ξjt, ϵjt, λt, θt}. OA.3.1. Simulation Results for Average Treatment Effects on the Treated\nIn this section, we estimate the average treatment effects on the treated units by conducting\nsimulations following the simulation setup as in Section 5.2. Recall that we compare the perfor-\nmance of different synthetic control designs over 1000 simulations that independently generate\nthe model primitives (i.e., the factor loadings, covariates, and error terms) of Assumption 1. The\ndata generating process for each one of the 1000 simulations is the same as in Section 5.1. The\nfive varieties of the synthetic control design are described in Section 5.2.",
    "content_hash": "4b831a7f27e61ed393bfd6fb8601ee9b3c13d1fc5a57cfffd5d713287abfadcd",
    "location": null,
    "page_start": 64,
    "page_end": 64,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5a4e7931-e0c1-4a05-b0e4-d01d43c5938d",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "We report the average treatment effects on the treated units in Table OA.1 as well as the\naverage treatment effects on the treated units under the nonlinear model in Table OA.2. OA.3.1.1. Average Treatment Effects on the Treated\nThe first five columns in Table OA.1 in the Online Appendix report averages of τ T\nt , the average\neffect of treatment on the treated units. These quantities depend on the weights for the treated\nunits, which are different across different formulations of the synthetic control design. The next\nfive columns report averages of bτt. They are the same as in Table 2, yet we use them as estimators\nfor τ T\nt in Table OA.1. The last two columns of Table OA.1 report averages across simulations of\nthe mean absolute error and the root mean square error, defined as in (22) but with τ T\nt replacing\nτt. The results in Table OA.1 are qualitatively similar to those for τt in Table 2, with one\nnotable exception. As expected, for intermediate and large values of β, the Weakly-targeted\ndesign outperforms the other designs when the goal is to estimate τ T\nt . This is because in the\nWeakly-targeted design the synthetic control weights are targeted to τ T\nt (and more so as β becomes\nlarge). OA.13",
    "content_hash": "a3ffbaa356dce3296280a33bcd8bb07a75945f0a64db0da936ed2b7991e7e9f7",
    "location": null,
    "page_start": 64,
    "page_end": 64,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9f7244b5-ca0b-4fa9-bf96-b21347284cbe",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Table OA.2: Average Treatment Effects on the Treated, Nonlinear Model (Averages over 1000 Simulations)\nτ T\nt\nbτt\nMAE T\nRMSE T\nt = 26\nt = 27\nt = 28\nt = 29\nt = 30\nt = 26\nt = 27\nt = 28\nt = 29\nt = 30\nUnconstrained\n-13.30\n-10.65\n-7.98\n-5.47\n-2.41\n-13.44\n-10.92\n-8.18\n-5.85\n-2.78\n2.31\n3.01\nConstrained\nm = 1\n-13.41\n-10.81\n-8.01\n-5.25\n-2.61\n-15.70\n-13.18\n-10.50\n-7.76\n-4.78\n3.53\n4.28\nm = 2\n-13.19\n-10.88\n-7.93\n-5.42\n-2.47\n-14.27\n-11.86\n-8.90\n-6.44\n-3.34\n2.86\n3.61\nm = 3\n-13.17\n-10.82\n-8.07\n-5.54\n-2.44\n-13.69\n-11.38\n-8.38\n-5.95\n-2.97\n2.52\n3.25\nm = 4\n-13.29\n-10.82\n-8.03\n-5.59\n-2.31\n-13.58\n-11.09\n-8.23\n-5.89\n-2.75\n2.44\n3.16\nm = 5\n-13.23\n-10.77\n-7.88\n-5.58\n-2.30\n-13.37\n-10.97\n-8.14\n-5.79\n-2.88\n2.39\n3.10\nm = 6\n-13.36\n-10.71\n-7.99\n-5.50\n-2.36\n-13.54\n-11.03\n-8.31\n-5.86\n-2.86\n2.32\n3.02\nm = 7\n-13.31\n-10.68\n-7.97\n-5.46\n-2.39\n-13.49\n-10.94\n-8.17\n-5.86\n-2.78\n2.30\n3.01\nWeakly-targeted\nβ = 0.01\n-13.14\n-10.67\n-7.97\n-5.47\n-2.45\n-11.66\n-9.02\n-6.37\n-3.87\n-1.00\n2.68\n3.36\nβ = 0.1\n-13.20\n-10.66\n-7.92\n-5.47\n-2.44\n-12.08\n-9.60\n-6.87\n-4.31\n-1.47\n2.31\n2.94\nβ = 1\n-13.20\n-10.71\n-7.91\n-5.47\n-2.46\n-12.51\n-10.13\n-7.35\n-4.81\n-1.91\n1.91\n2.43\nβ = 10\n-13.27\n-10.67\n-7.96\n-5.45\n-2.49\n-13.03\n-10.51\n-7.81\n-5.25\n-2.32\n1.33\n1.66\nβ = 100\n-13.29\n-10.77\n-8.04\n-5.47\n-2.53\n-13.28\n-10.72\n-8.00\n-5.43\n-2.59\n1.14\n1.40\nUnit-level\nξ = 0.01\n-13.20\n-10.76\n-7.98\n-5.44\n-2.48\n-11.76\n-9.15\n-6.51\n-3.91\n-1.15\n2.68\n3.35\nξ = 0.1\n-13.35\n-10.81\n-8.07\n-5.53\n-2.55\n-13.11\n-10.59\n-7.82\n-5.15\n-2.29\n1.98\n2.51\nξ = 1\n-13.40\n-10.77\n-8.05\n-5.47\n-2.56\n-13.74\n-11.12\n-8.42\n-5.75\n-2.84\n1.47\n1.81\nξ = 10\n-13.35\n-10.66\n-7.99\n-5.38\n-2.71\n-13.74\n-11.20\n-8.55\n-5.89\n-3.09\n1.49\n1.80\nξ = 100\n-13.39\n-10.63\n-7.95\n-5.37\n-2.69\n-13.79\n-11.16\n-8.54\n-5.90\n-3.08\n1.56\n1.89\nPenalized\nλ = 0.01\n-13.32\n-10.69\n-8.07\n-5.50\n-2.41\n-13.40\n-10.93\n-8.32\n-5.82\n-2.82\n2.25\n2.93\nλ = 0.1\n-13.17\n-10.72\n-8.09\n-5.59\n-2.55\n-13.33\n-10.79\n-8.13\n-5.56\n-2.65\n1.87\n2.39\nλ = 1\n-13.40\n-10.80\n-7.98\n-5.35\n-2.66\n-13.32\n-10.84\n-8.15\n-5.39\n-2.60\n1.97\n2.44\nλ = 10\n-13.41\n-10.89\n-8.00\n-5.32\n-2.64\n-13.39\n-10.82\n-7.95\n-5.34\n-2.58\n2.79\n3.46\nλ = 100\n-13.42\n-10.87\n-8.02\n-5.26\n-2.63\n-13.35\n-10.82\n-8.00\n-5.29\n-2.57\n3.06\n3.80\nOA.16",
    "content_hash": "5ec7abfa9bd545937f5de4bb87544b05006b6b7f1c213e8b4690d80d2af742e0",
    "location": null,
    "page_start": 67,
    "page_end": 67,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "710cd487-3458-4c5b-a2e5-c2886025b158",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Pre-multiplying by λ′\nt(λ′\nEλE)−1λ′\nE yields\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nv∗\njY E\nj\n! =λ′\nt(λ′\nEλE)−1λ′\nEθE\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nv∗\njZj\n! +λ′\nt\nJ\nX\nj=1\nw∗\njµj −\nJ\nX\nj=1\nv∗\njµj\n! +λ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njϵE\nj −\nJ\nX\nj=1\nv∗\njϵE\nj\n! . (OA.9)\nEquations (OA.8) and (OA.9) imply\nJ\nX\nj=1\nw∗\njY N\njt −\nJ\nX\nj=1\nv∗\njY N\njt = (θ′\nt −λ′\nt(λ′\nEλE)−1λ′\nEθE)\n\u0010\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nv∗\njZj\n\u0011\nOA.17",
    "content_hash": "63fcf3bd350a08df3033195bd93502ee4e103a71624f607dbf3ec15f1df1f96c",
    "location": null,
    "page_start": 68,
    "page_end": 68,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1af00081-093e-46a5-8ff6-cae290f4d6f9",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "+ λ′\nt(λ′\nEλE)−1λ′\nE\n\u0010\nJ\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nv∗\njY E\nj\n\u0011\n−λ′\nt(λ′\nEλE)−1λ′\nE\n\u0010\nJ\nX\nj=1\nw∗\njϵE\nj −\nJ\nX\nj=1\nv∗\njϵE\nj\n\u0011\n+\n\u0010\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt\n\u0011\n. (OA.10)\nIf Assumption 3 holds, (OA.10) becomes\nJ\nX\nj=1\nw∗\njY N\njt −\nJ\nX\nj=1\nv∗\njY N\njt = −λ′\nt(λ′\nEλE)−1λ′\nE\n\u0010\nJ\nX\nj=1\nw∗\njϵE\nj −\nJ\nX\nj=1\nv∗\njϵE\nj\n\u0011\n+\n\u0010\nJ\nX\nj=1\nw∗\njϵjt −\nJ\nX\nj=1\nv∗\njϵjt\n\u0011\n. (OA.11)\nOnly the first term on the right-hand side of (OA.11) has a non-zero mean (because the weights\nw∗\nj and v∗\nj, depend on the error terms ϵE\nj ).",
    "content_hash": "2d6cb3400f36a2c851b54cef8668d567648bdc744a7a9b9cc30653c49a21f939",
    "location": null,
    "page_start": 69,
    "page_end": 69,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a1410f71-382d-4e79-8fe1-7df69377dd73",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Theorem 1.16 from Rigollet and H¨utter (2019) implies\nE\nh\nJ\nX\nj=1\nw∗\njY N\njt −\nJ\nX\nj=1\nv∗\njY N\njt\ni\f\f\f\f\f ≤\nE\nh\nJ\nX\nj=1\nw∗\njϵE\njt\ni\f\f\f\f\f +\nE\nh\nJ\nX\nj=1\nv∗\njϵE\njt\ni\nOA.18",
    "content_hash": "2366e90bfc613287808ce1fac122c1a53d9d2ef81ca1601ff9220c1b057df0fb",
    "location": null,
    "page_start": 69,
    "page_end": 69,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a14460ee-8f3c-4b16-ba8d-426a20248ffa",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Therefore,\nE\n\"\nJ\nX\nj=1\nw∗\njY N\njt −\nJ\nX\nj=1\nv∗\njY N\njt\n#\f\f\f\f\f =\nE\n\"\nλ′\nt(λ′\nEλE)−1λ′\nE\n\u0010\nJ\nX\nj=1\nw∗\njϵE\nj −\nJ\nX\nj=1\nv∗\njϵE\nj\n\u0011#\n≤\nE\n\"\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njϵE\nj\n#\f\f\f\f\f +\nE\n\"\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nv∗\njϵE\nj\n#\f\f\f\f\f . For any t ≥T0 + 1 and s ∈E, under Assumption 2 (i), we apply Cauchy-Schwarz inequality\nand the eigenvalue bound on the Rayleigh quotient to obtain\nλ′\nt(λ′\nEλE)−1λs\n\u00012 ≤\nλ\n2F\nTEζ\n!2\n. Let\nϵE\njt = λ′\nt(λ′\nEλE)−1λ′\nEϵE\nj =\nX\ns∈E\nλ′\nt(λ′\nEλE)−1λsϵjs. Because ϵE\njt is a linear combination of independent sub-Gaussians with variance proxy σ2, we\nknow ϵE\njt is sub-Gaussian with variance proxy (λ\n2F/ζ)2σ2/TE. Let S = {w ∈RJ : PJ\nj=1 wj = 1}\nbe the unit simplex.",
    "content_hash": "117c8b7f5068f2a894a9539db0fcedba460a1d08933894355c0e755b5f284e10",
    "location": null,
    "page_start": 69,
    "page_end": 69,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "73ca34c1-8dff-4e1b-a9a3-dedf0794c466",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "≤E\n\"\nmax\nw∈S\nJ\nX\nj=1\nwjϵE\njt\n#\n+ E\n\"\nmax\nv∈S\nJ\nX\nj=1\nvjϵE\njt\n#\n≤λ\n2F\nζ 2\np\n2 log (2J) σ\n√TE\n,\nwhich finishes the proof of the theorem. Suppose now Assumption 6 holds (but Assumption 5 does not). To obtain a bound on the\nbias we need to bound the first two terms in (OA.10). Recall that\nλ′\nt(λ′\nEλE)−1λs ≤λ\n2F\nTEζ . Therefore, the absolute value of each element in vector (θ′\nt −λ′\nt(λ′\nEλE)−1λ′\nEθE) is bounded by\nθ\n\u0010\n1 + λ\n2F\nζ\n\u0011\n. Cauchy–Schwarz inequality and Assumption 6 imply\n(θ′\nt −λ′\nt(λ′\nEλE)−1λ′\nEθE)\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nv∗\njZj\n! ≤θ\n\u0010\n1 + λ\n2F\nζ\n\u0011√\nR\nJ\nX\nj=1\nw∗\njZj −\nJ\nX\nj=1\nv∗\njZj\n2\n≤θ\n\u0010\n1 + λ\n2F\nζ\n\u0011\nRd,\nand\nλ′\nt(λ′\nEλE)−1λ′\nE\nJ\nX\nj=1\nw∗\njY E\nj −\nJ\nX\nj=1\nv∗\njY E\nj\n! ≤λ\n2F\nζ d.",
    "content_hash": "b776fb3b441b446580ba8dcbd5beda81b172592ad70ef43f90525835c4b7302c",
    "location": null,
    "page_start": 70,
    "page_end": 70,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3d3145fe-1d25-4184-a783-299b0b4a77ff",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Combining the last two displayed equations with (OA.10), we have\nE\n\"\nJ\nX\nj=1\nw∗\njY I\njt −\nJ\nX\nj=1\nfjY I\njt\n#\f\f\f\f\f ≤\n\u0010\nθR + λ\n2F\nζ (1 + θR)\n\u0011\nd + λ\n2F\nζ 2\np\n2 log (2J) σ\n√TE\n,\nwhich finishes the proof of the theorem. Proof of Theorem OA.6. Recall that\nbut =\nJ\nX\nj=1\nw∗\njYjt −\nJ\nX\nj=1\nv∗\njYjt,\nOA.19",
    "content_hash": "6fc4b7916cc55572bbbb98425ff370f1627035b77f16e83a40d5a088c97de853",
    "location": null,
    "page_start": 70,
    "page_end": 70,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9ea3a803-4b16-4b4c-8d5b-a1a1438183df",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Swapping Treated and Control Weights\nRecall that when it is possible to swap synthetic treated and synthetic control weights, we choose\nthe treated units so that the number of units with positive weights in w∗is smaller than the\nnumber of units with positive weights in v∗. When ∥w∗∥0 = ∥v∗∥0, we determine whether or\nnot to swap using the following rule. For the Unconstrained design, we choose the treated group\nto be the one with the smallest index among the units with positive weights. We use the same\nprocedure based on the lowest index for Constrained with m = 7 (highest value) and Penalized\nwith λ = 0.01 (lowest value). Then, starting from m = 7 and for smaller values of m, we\nassign to the treated group the set of weights that is most similar to the weights obtained for\n∥w∗∥0 ≤m + 1 (in terms of what units obtain positive weights). In those cases where the two\nsets of swappable weights for ∥w∗∥0 ≤m are equally similar to the synthetic treated weights for\n∥w∗∥0 ≤m + 1, we select the set of weights with the smallest index. We follow the analogous\nprocedure for λ > 0.01, starting from smaller values of λ. OA.20",
    "content_hash": "94c1260100c13802351f0327878c1c838b98f47afbc5181ffc1f4a9d90c58da9",
    "location": null,
    "page_start": 71,
    "page_end": 71,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "33ff22f8-4200-4705-965c-60d05f1df8d6",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Define P 0 = {P 0\nk,l}k,l=1,...,2J ∈R2J×2J, such that P 0 has only two diagonal blocks, while the\ntwo off-diagonal blocks are zero. Define for any k, l = 1, . . . , 2J,\nP 0\nk,l =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM\nX\ni=1\nXi,kXi,l,\nk, l = 1, . . . , J;\nM\nX\ni=1\nXi,(k−J)Xi,(l−J),\nk, l = J + 1, . . . , 2J;\n0,\notherwise. Define q0 ∈R2J, such that for any k = 1, . . . , 2J\nq0\nk =\n\n\n\n\n\n\n\n\n\n\n\n\n\n−2\nM\nX\ni=1\nXi,k · (\nJ\nX\nj=1\nfjXi,j),\nk = 1, . . . , J;\n−2\nM\nX\ni=1\nXi,k−J · (\nJ\nX\nj=1\nfjXi,j),\nk = J + 1, . . . , 2J. OA.21",
    "content_hash": "be00a9a65de8d5d77ce6883e4508351a856f4a58977acbc48811085a13727b8e",
    "location": null,
    "page_start": 72,
    "page_end": 72,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f4ff4ad6-21a8-4ddb-bf43-2b7423404721",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Further define e1 = (1, 1, ..., 1, 0, 0, ..., 0)′ whose first J elements are 1 and last J elements 0;\nand e2 = (0, 0, ..., 0, 1, 1, ..., 1)′ whose first J elements are 0 and last J elements 1. Finally, define P 1 = {P 1\nk,l}k,l=1,...,2J ∈R2J×2J such that P 1 only has non-zero values in the\ntwo off-diagonal blocks, i.e., for any k, l = 1, . . . , 2J,\nP 1\nk,l =\n\n\n\n\n\n\n\n\n\n1,\nk = l + J;\n1,\nk = l −J;\n0,\notherwise. Using the above notations we re-write the (non-convex) QCQP as follows,\nmin\n˜\nW ′P 0 ˜\nW + q0′ ˜\nW\n(OA.12)\ns.t. e′\n1 ˜\nW = 1,\ne′\n2 ˜\nW = 1,\n˜\nW ′P 1 ˜\nW = 0,\n˜\nW ≥0. The first computational method (enumeration) solves two synthetic control problems in each\niteration. The synthetic control problems can be efficiently solved. We implement the synthetic\ncontrol problem using the “lsei” function from “limSolve” package in R 4.0.2. For the second\ncomputational method (quadratic programming), the problem (OA.12) is implemented using\nGurobi 9.0.2 in R 4.0.2.",
    "content_hash": "9bf9621e5770edb66a8d3b6da239ebba3305b3991c52c7e4cc0c4296c5f30cec",
    "location": null,
    "page_start": 73,
    "page_end": 73,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e3c686d1-c343-4a79-a676-278253e0887e",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Since the QCQP is non-convex, the computation leads to some numerical\nerrors up to 0.001 in finding the treated and control weights. So we round the treated and control\nweights to the nearest 2-digits in the implementation of the QCQP. Moreover, for all the weights\nthat are less than or equal to 0.01, we trim the weights to zero. This is because smaller weights\nsuffer from greater impacts of numerical errors, and that numerical errors could cause zero weights\nto be non-zero, thus having a non-negligible impact on the swapping rule. To conclude, we compare the treated and control weights calculated from both methods. Both methods yield the same treated and control weights up to some negligible rounding error,\nwhile the first method takes longer computational time. OA.22",
    "content_hash": "458638c8ce25f28c2d52b5b6104d5dce656a08fbf34cd66a654442d0081aea62",
    "location": null,
    "page_start": 73,
    "page_end": 73,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ce6938e5-1b19-4ab3-8bc1-42bd4420258d",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "The other designs are computationally implemented using either one of the above two meth-\nods. The Constrained design is implemented using the enumeration method. In cases when the\ncardinality constraint m is small, this brute force enumeration is very efficient. The Weakly-targeted\ndesign is implemented using the quadratic programming method. In the QCQP formulation, the\nobjective function has both a different quadratic term P 0 and a different linear term q0. The\nUnit-Level design is implemented using the enumeration method. The Penalized design is im-\nplemented using the quadratic programming method. In the QCQP formulation, the objective\nfunction has the same quadratic term P 0 and a different linear term q0.\nOA.23\n\nOA.6.\nAdditional Illustrations Using Walmart Data\nIn this section, we present results for m = 1 and m = 3. Using only one treated unit (m = 1) fails\nto produce a good fit between the treated and synthetic control unit in the fitting periods. For\nthe case of m = 1, Figures OA.1 and OA.2 reveal a substantial gap with a clear seasonal trend\nbetween the two synthetic units. Figures OA.3 and OA.4 report results for m = 3. Increasing\nm from m = 2 to m = 3 results in a minor improvement in fit, and leaves estimation results\nsubstantively unchanged.\nOA.24",
    "content_hash": "65c3f1ba983e7a2c515e0e97aa9e1ace7222e5f01ce5d7d448ea7130a8bf4b53",
    "location": null,
    "page_start": 74,
    "page_end": 75,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4c81e850-6d84-471d-bc82-d206841e29be",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "OA.6.\nAdditional Illustrations Using Walmart Data\nIn this section, we present results for m = 1 and m = 3. Using only one treated unit (m = 1) fails\nto produce a good fit between the treated and synthetic control unit in the fitting periods. For\nthe case of m = 1, Figures OA.1 and OA.2 reveal a substantial gap with a clear seasonal trend\nbetween the two synthetic units. Figures OA.3 and OA.4 report results for m = 3. Increasing\nm from m = 2 to m = 3 results in a minor improvement in fit, and leaves estimation results\nsubstantively unchanged.\nOA.24\n\nFigure OA.1: Synthetic Treated Unit and Synthetic Control Unit, m = 1\nNote: The black solid line represents the synthetic treated outcome. The black dashed line represents the synthetic\ncontrol outcome. The blue dashed lines are individual stores’ sales.\nFigure OA.2: Placebo Treatment Effects, m = 1\nNote: This figure reports the difference between the synthetic treated and synthetic control outcomes of Fig-\nure OA.1.\nOA.25\n\nFigure OA.3: Synthetic Treatment Unit and Synthetic Control Unit, when m = 3.\nNote: The black solid line represents the synthetic treated outcome. The black dashed line represents the synthetic\ncontrol outcome. The blue dashed lines are individual stores’ sales.\nFigure OA.4: Treatment Effect Estimate, when m = 3.\nNote: This figure reports the difference between the synthetic treated and synthetic control outcomes of Fig-\nure OA.3. For the experimental periods, this is the treatment effect estimate.\nOA.26",
    "content_hash": "070953e1660f2702cf6b2e09368b93d0cfd3e6f26005aec449da7bf4154ed835",
    "location": null,
    "page_start": 75,
    "page_end": 77,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ff7357d8-f482-43ea-a138-b7a9e8284a83",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Figure OA.3: Synthetic Treatment Unit and Synthetic Control Unit, when m = 3.\nNote: The black solid line represents the synthetic treated outcome. The black dashed line represents the synthetic\ncontrol outcome. The blue dashed lines are individual stores’ sales.\nFigure OA.4: Treatment Effect Estimate, when m = 3.\nNote: This figure reports the difference between the synthetic treated and synthetic control outcomes of Fig-\nure OA.3. For the experimental periods, this is the treatment effect estimate.\nOA.26\n\nOA.7.\nAdditional Simulation Results\nIn Section 5.1 the idiosyncratic shocks are i.i.d. Normal with variance σ2 = 1. Figures OA.5\nand OA.6 report results for σ2 = 5 and σ2 = 10, respectively. Figures OA.7 and OA.8 report\ndifferences between the outcomes for the synthetic treated and the synthetic control units for the\nsame values for σ2. As the value of σ2 increases, the quality of the post-treatment estimation\nand inference deteriorates, and the p-value for the null hypotheses of in (17) increases. The\ndeterioration in pre-treatment fit in Figures OA.5 and OA.6 provides a diagnosis of the accuracy\nof the respective estimates.\nOA.27\n\nFigure OA.5: Synthetic Treatment Unit and Synthetic Control Unit, when σ2 = 5.\nFigure OA.6: Synthetic Treatment Unit and Synthetic Control Unit, when σ2 = 10.\nOA.28",
    "content_hash": "c506059406074a35652a2f313a9ff35fe4b45957ada179f98a09284c82e25525",
    "location": null,
    "page_start": 77,
    "page_end": 79,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9ba1a9f7-d52d-478e-ba0d-fd924c3d9167",
    "source_id": "1e210ea3-49a0-4944-8fd1-fd8579325123",
    "content": "Figure OA.5: Synthetic Treatment Unit and Synthetic Control Unit, when σ2 = 5.\nFigure OA.6: Synthetic Treatment Unit and Synthetic Control Unit, when σ2 = 10.\nOA.28\n\nFigure OA.7: Treatment Effect Estimate, when σ2 = 5.\nFigure OA.8: Treatment Effect Estimate, when σ2 = 10.\nOA.29",
    "content_hash": "be325fedcdeaa292eb61c8554a26f4f3187e7588236346dd44db980bc87e499d",
    "location": null,
    "page_start": 79,
    "page_end": 80,
    "metadata": {
      "section": "arXiv:2108.02196v5  [stat.ME]  23 Apr 2025",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9a48d78c-616f-406e-bfad-e364a983a944",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": ",\n(A.1)\nwhere ∥M∥P,q ⩽n1/q∥F∥P,q and K(q, c) > 0 is a constant depending only on q and c. A.6. Proof of Lemma 2.1\nProof. Since J exists and Jββ is invertible, (2.8) has the unique solution µ0 given in\n(2.10), and so we have by (2.6) that E[ψ(W; θ0, η0)] = 0 for η0 given in (2.9). Moreover,\n∂η′EP ψ(W; θ0, η0) =\n\u0010\n[Jθβ −µ0Jββ], E[∂β′ℓ(W; θ0, β0)] ⊗Idθ×dθ\n\u0011\n= 0,\nwhere Idθ×dθ is the dθ × dθ identity matrix and ⊗is the Kronecker product. Hence, the\nasserted claim holds by the remark after Definition 2.1. ■\nA.7. Proof of Lemma 2.2\nThe proof follows similarly to that of Lemma 2.1, except that now we have to verify (2.4)\nintead of (2.3). To do so, take any β ∈B such that ∥β −β0∥∗\nq ⩽λN/rN and any dθ × dβ\nmatrix µ. Denote η = (β′, vec(µ)′)′.",
    "content_hash": "21d4fcc69092ac9ac836e7c37d22ef94062a094409c6655ada2d6d3cdbcc6b15",
    "location": null,
    "page_start": 1,
    "page_end": 52,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "04f578fb-95e9-47bc-9402-75e0e5b8881a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n19\nwhich corresponds to the estimator θ0 described in the Introduction in (1.5). It is important to note that the concentrating-out approach described here gives a\nNeyman orthogonal score without requiring that ℓ(W; θ, β) is the log-likelihood function. Except for the technical conditions needed to ensure the existence of derivatives and\ntheir interchangeability, the only condition that is required is that θ0 and β0 solve the\noptimization problem (2.5). If ℓ(W; θ, β) is the log-likelihood function, however, it follows\nfrom Newey (1994), p. 1359, that the concentrating-out approach actually yields the\nefficient score. An alternative, but closely related, approach to derive the efficient score\nin the likelihood setting would be to apply Neyman’s construction described above for\na one-dimensional least favorable parametric sub-model; see Severini and Wong (1992)\nand Chap. 25 of van der Vaart (1998). Remark 2.5. (Generating Orthogonal Scores by Varying B) When we calculate the\n“concentrated-out” nonparametric part βθ, we can use some other set of functions Υ\ninstead of B on the right-hand side of (2.20):\nβθ = arg max\nβ∈Υ EP [ℓ(W; θ, β)]. By replacing B by Υ we can generate a different Neyman orthogonal score. Of course,\nthis replacement may also change the true value θ0 of the parameter of interest, which\nis an important consideration for the selection of Υ. For example, consider the partially\nlinear model and assume that X has two components, X1 and X2.",
    "content_hash": "d642298de2e844c6b45e3dcb68b8af280d3308efdf52e07ee681839e99d3dee5",
    "location": null,
    "page_start": 1,
    "page_end": 19,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9cbc77c8-c88a-4e52-bf6d-608080510745",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "52\nCCDDHNR\nsuitably measurable functions f : W →R, equipped with a measurable envelope F : W →\nR. Lemma 6.2. (Maximal Inequality, Chernozhukov et al. (2014)) Work with the\nsetup above. Suppose that F ⩾supf∈F |f| is a measurable envelope for F with ∥F∥P,q <\n∞for some q ⩾2. Let M = maxi⩽n F(Wi) and σ2 > 0 be any positive constant such\nthat supf∈F ∥f∥2\nP,2 ⩽σ2 ⩽∥F∥2\nP,2. Suppose that there exist constants a ⩾e and v ⩾1\nsuch that\nlog sup\nQ\nN(ϵ∥F∥Q,2, F, ∥· ∥Q,2) ⩽v log(a/ϵ), 0 < ϵ ⩽1. Then\nEP [∥Gn∥F] ⩽K\ns\nvσ2 log\n\u0012a∥F∥P,2\nσ\n\u0013\n+ v∥M∥P,2\n√n\nlog\n\u0012a∥F∥P,2\nσ\n\u0013! ,\nwhere K is an absolute constant. Moreover, for every t ⩾1, with probability > 1 −t−q/2,\n∥Gn∥F ⩽(1+α)EP [∥Gn∥F]+K(q)\nh\n(σ+n−1/2∥M∥P,q)\n√\nt+α−1n−1/2∥M∥P,2t\ni\n, ∀α > 0,\nwhere K(q) > 0 is a constant depending only on q. In particular, setting a ⩾n and\nt = log n, with probability > 1 −c(log n)−1,\n∥Gn∥F ⩽K(q, c)\nσ\ns\nv log\n\u0012a∥F∥P,2\nσ\n\u0013\n+ v∥M∥P,q\n√n\nlog\n\u0012a∥F∥P,2\nσ\n\u0013!",
    "content_hash": "3ef97bb649726735ee80e933ca94b95031a95995e960d66302f9f3c70d391faf",
    "location": null,
    "page_start": 1,
    "page_end": 52,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a7fc39d6-9193-4ccd-98cc-7c95fe331a0a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Double/Debiased Machine Learning for Treatment\nand Structural Parameters\nVictor Chernozhukov†, Denis Chetverikov‡, Mert Demirer†\nEsther Duflo†, Christian Hansen§, Whitney Newey†, James Robins⋆\n†Massachusetts Institute of Technology, 50 Memorial Drive,\nCambridge, MA, 02139, USA\nE-mail: vchern@mit.edu, mdemirer@mit.edu, duflo@mit.edu, wnewey@mit.edu\n†University of California Los Angeles, 315 Portola Plaza,\nLos Angeles, CA 90095\nE-mail: chetverikov@econ.ucla.edu\n§University of Chicago, 5807 S. Woodlawn Ave., Chicago, IL 60637\nE-mail: chansen1@chicagobooth.edu\n⋆Harvard University, 677 Huntington Avenue Boston, Massachusetts 02115\nE-mail: robins@hsph.harvard.edu\nReceived: June 2014\nSummary\nWe revisit the classic semiparametric problem of inference on a low\ndimensional parameter θ0 in the presence of high-dimensional nuisance parameters\nη0. We depart from the classical setting by allowing for η0 to be so high-dimensional\nthat the traditional assumptions, such as Donsker properties, that limit complexity\nof the parameter space for this object break down. To estimate η0, we consider the\nuse of statistical or machine learning (ML) methods which are particularly well-suited\nto estimation in modern, very high-dimensional cases. ML methods perform well by\nemploying regularization to reduce variance and trading off regularization bias with\noverfitting in practice.",
    "content_hash": "fee3778bab344fc8d12dc16fb2a9d1cdd49e5ae5f0b9025d62b4ba84c7d7581b",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "Double/Debiased Machine Learning for Treatment",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a96801b7-59ce-4252-b051-3ec713959b64",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Further details about\nthe methods are provided in the main text. 6.2. The effect of 401(k) Eligibility and Participation on Net Financial Assets\nThe key problem in determining the effect of 401(k) eligibility is that working for a\nfirm that offers access to a 401(k) plan is not randomly assigned. To overcome the lack\nof random assignment, we follow the strategy developed in Poterba et al. (1994a) and\nPoterba et al. (1994b). In these papers, the authors use data from the 1991 Survey of\nIncome and Program Participation and argue that eligibility for enrolling in a 401(k)\nplan in this data can be taken as exogenous after conditioning on a few observables of\nwhich the most important for their argument is income. The basic idea of their argument\nis that, at least around the time 401(k) initially became available, people were unlikely\nto be basing their employment decisions on whether an employer offered a 401(k) but\nwould instead focus on income and other aspects of the job. Following this argument,\nwhether one is eligible for a 401(k) may then be taken as exogenous after appropriately\nconditioning on income and other control variables related to job choice. A key component of the argument underlying the exogeneity of 401(k) eligibility is\nthat eligibility may only be taken as exogenous after conditioning on income and other\nvariables related to job choice that may correlate with whether a firm offers a 401(k). Poterba et al. (1994a) and Poterba et al. (1994b) and many subsequent papers adopt\nthis argument but control only linearly for a small number of terms.",
    "content_hash": "0363eee185ccb9b62e548e07b265d93a6fcae9f65eaac3d33d3a1f614fb7cb36",
    "location": null,
    "page_start": 1,
    "page_end": 41,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3a3d4663-b850-420c-962d-18cbb7b3ccbe",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "But ∥fk(0)∥= 0 since\nEPN [ψ(W; θ0, η0) | (Wi)i∈Ic\nk] = EPN [ψ(W; θ0, η0)]. In addition, on the event EN, by the Neyman λN near-orthogonality condition imposed\nin Assumption 3.1,\n∥f ′\nk(0)∥=\n∂ηEPN ψ(W; θ0, η0)[bη0,k −η0]\n⩽λN. Moreover, on the event EN,\n∥f ′′\nk (˜r)∥⩽\nsup\nr∈(0,1)\n∥f ′′\nk (r)∥⩽λ′\nN\nby the definition λ′\nN in Assumption 3.2. Hence,\nI4,k = √n∥fk(1)∥= OPN (√n(λN + λ′\nN)). Combining the bounds on I3,k and I4,k with (A.16) and using the fact that n−1 =\nO(N −1) gives (A.15). Step 4. To establish (A.7), note that\nEPN\nh\r\r\r 1\n√\nN\nN\nX\ni=1\nψ(Wi; θ0, η0)\n2i\n= EPN\nh\n∥ψ(W; θ0, η0)∥2i\n⩽c2\n1\nby Assumption 3.2. Combining this with Markov’s inequality gives (A.7).",
    "content_hash": "3aec11d926918031df8eaf0c36e3b33a03067c60f300b36cebc573ae148a8c9b",
    "location": null,
    "page_start": 1,
    "page_end": 58,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "759dadf4-f0c7-4688-9266-89378926f2e1",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "However, both regularization bias and overfitting in estimating\nη0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML\nestimators of η0 into estimating equations for θ0. This bias results in the naive estimator\nfailing to be N −1/2 consistent, where N is the sample size. We show that the impact\nof regularization bias and overfitting on estimation of the parameter of interest θ0 can\nbe removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal\nmoments/scores that have reduced sensitivity with respect to nuisance parameters to\nestimate θ0, and (2) making use of cross-fitting which provides an efficient form of\ndata-splitting. We call the resulting set of methods double or debiased ML (DML). We\nverify that DML delivers point estimators that concentrate in a N −1/2-neighborhood\nof the true parameter values and are approximately unbiased and normally distributed,\nwhich allows construction of valid confidence statements. The generic statistical theory\nof DML is elementary and simultaneously relies on only weak theoretical requirements\nwhich will admit the use of a broad array of modern ML methods for estimating the\nnuisance parameters such as random forests, lasso, ridge, deep neural nets, boosted\ntrees, and various hybrids and ensembles of these methods.",
    "content_hash": "05b5b5fc6a700ede5f3807c03bdfb0158fced9201645525ee24e66862aae05f5",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "and Structural Parameters",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bba3ba85-2635-427f-9117-6b777e965f03",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "We illustrate the general\ntheory by applying it to provide theoretical properties of DML applied to learn the main\nregression parameter in a partially linear regression model, DML applied to learn the\ncoefficient on an endogenous variable in a partially linear instrumental variables model,\nDML applied to learn the average treatment effect and the average treatment effect\non the treated under unconfoundedness, and DML applied to learn the local average\ntreatment effect in an instrumental variables setting. In addition to these theoretical\napplications, we also illustrate the use of DML in three empirical examples. arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
    "content_hash": "d7523bd37f83b24d782ff0848cfb311088335b8d9be0eb0bd46d0d65b6fb612f",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "and Structural Parameters",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c7de4ebe-9c14-4e9d-9151-e08e0fced4d0",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "arXiv, 2013. Belloni, A., V. Chernozhukov, and L. Wang (2011). Square-root-lasso: Pivotal recovery\nof sparse signals via conic programming. Biometrika 98, 791–806. arXiv, 2010. Belloni, A., V. Chernozhukov, and L. Wang (2014). Pivotal estimation via square-root\nlasso in nonparametric regression. Annals of Statistics 42, 757–788. arXiv, 2011. Belloni, A., V. Chernozhukov, and Y. Wei (2016). Post-selection inference for generalized\nlinear models with many controls. Journal of Business and Economic Statistics 34,\n606–619. arXiv, 2013. Bera, A., G. Montes-Rojas, and W. Sosa-Escudero (2010). General specification testing\nwith locally misspecified models. Econometric Theory 26, 1838–1845. Bickel, P. and Y. Ritov (1988). Estimating integrated squared density derivatives. Sankhya A-50, 381–393. Bickel, P. J. (1982). On adaptive estimation. Annals of Statistics 10, 647–671. Bickel, P. J., C. A. J. Klaassen, Y. Ritov, and J. A. Wellner (1998). Efficient and Adaptive\nEstimation for Semiparametric Models. Springer. Bickel, P. J., Y. Ritov, and A.",
    "content_hash": "4dfd062337ed9b705a8fe3ba63eec2393046ed4a6e298c23b5330c4eadb543db",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8985d52b-eeed-4c67-aa8d-202ff25b5099",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "For\nsimplicity, assume that N/K is an integer. Assumption 5.1. (Regularity Conditions for ATE and ATTE Estimation) For all prob-\nability laws P ∈P for the triple (Y, D, X) the following conditions hold: (a) equations\n(5.1)-(5.2) hold, with D ∈{0, 1}, (b) ∥Y ∥P,q ⩽C, (c) PP (ε ⩽m0(X) ⩽1 −ε) = 1,\n(d) ∥U∥P,2 ⩾c, (e) ∥EP [U 2 | X]∥P,∞⩽C, and (f) given a random subset I of [N] of\nsize n = N/K, the nuisance parameter estimator bη0 = bη0((Wi)i∈Ic) obeys the following\nconditions: with P-probability no less than 1 −∆N:\n∥bη0 −η0∥P,q ⩽C,\n∥bη0 −η0∥P,2 ⩽δN,\n∥bm0 −1/2∥P,∞⩽1/2 −ε,\nand\n(i) for the score ψ in (5.3), where bη0 = (bg0, bm0) and the target parameter is ATE,\n∥bm0 −m0∥P,2 × ∥bg0 −g0∥P,2 ⩽δNN −1/2,\n(ii) for the score ψ in (5.4), where bη0 = (bg0, bm0, bp0) and the target parameter is ATTE,\n∥bm0 −m0∥P,2 × ∥bg0 −g0∥P,2 ⩽δNN −1/2. Remark 5.1. The only non-primitive condition here is the assumption on the rate of\nestimating the nuisance parameters.",
    "content_hash": "f212bf72000f69e445d602d44ea43d527b28aab6fb013c044998bb8b2efad0bf",
    "location": null,
    "page_start": 1,
    "page_end": 36,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fb64832e-4a23-45d7-9733-b04cbd120926",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "48\nCCDDHNR\nBilias, Y. (2000). Sequential testing of duration data: The case of the Pennsylvania\n‘reemployment bonus’ experiment. Journal of Applied Econometrics 15, 575–594. Bilias, Y. and R. Koenker (2002). Quantile regression for duration data: A reappraisal of\nthe pennsylvania reemployment bonus experiments. In B. Fitzenberger, R. Koenker,\nand J. A. Machado (Eds.), Studies in Empirical Economics: Economic Applications of\nQuantile Regression, pp. 199–220. Physica-Verlag Heidelberg. B¨uhlmann, P. and S. van de Geer (2011). Statistics for High-Dimensional Data. Springer\nSeries in Statistics. Chamberlain, G. (1987). Asymptotic efficiency in estimation with conditional moment\nrestrictions. Journal of Econometrics 34, 305–334. Chamberlain, G. (1992). Efficiency bounds for semiparametric regression. Economet-\nrica 60, 567–596. Chen, X., O. Linton, and I. van Keilegom (2003). Estimation of semiparametric models\nwhen the criterion function is not smooth. Econometrica 71, 1591–1608. Chen, X. and H. White (1999). Improved rates and asymptotic normality for nonpara-\nmetric neural network estimators. IEEE Transactions on Information Theory 45,\n682–691. Chernozhukov, V., D. Chetverikov, and K. Kato (2014).",
    "content_hash": "842a014b9be850b566a713be28cb860acc3de4d4b2cf46e5156d3f4d62a4f209",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a61720a9-60c6-4477-bb61-9d5a3b7b08a1",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Journal of Econometrics 174, 1–23. Ferguson, T. (1967). Mathematical Statistics: A Decision Theoretic Approach. Academic\nPress. Fr¨olich, M. (2007). Nonparametric IV estimation of local average treatment effects with\ncovariates. Journal of Econometrics 139, 35–75. Gautier, E. and A. Tsybakov (2014). High-dimensional instrumental variables regression\nand confidence sets. arXiv:1105.2454. arXiv, 2011. Hahn, J. (1998). On the role of the propensity score in efficient semiparametric estimation\nof average treatment effects. Econometrica 66, 315–331. Hansen, L. (1982). Large sample properties of generalized method of moments estimators. Econometrica 50, 1029–1054. Hasminskii, R. and I. Ibragimov (1978). On the nonparametric estimation of functionals. Proceedings of the 2nd Prague Symposium on Asymptotic Statistics, 41–51.",
    "content_hash": "7aa66761ff8afccd56ed09f2107bab1e4701a42f143468eaba13557760881ba6",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4df7f7fd-5831-474b-aa9c-ea9ede934b50",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "The asymptotic variance of semiparametric estimators. Economet-\nrica 62, 1349–1382. Newey,\nW. K.,\nF. Hsieh,\nand\nJ. Robins\n(1998). Undersmoothing\nand\nbias\ncorrected\nfunctional\nestimation. Working\npaper,\nMIT\nEconomics\nDept.,\nhttp://economics.mit.edu/files/11219. Newey, W. K., F. Hsieh, and J. M. Robins (2004). Twicing kernels and a small bias\nproperty of semiparametric estimators. Econometrica 72, 947–962. Neyman, J. (1959). Optimal asymptotic tests of composite statistical hypotheses. In\nU. Grenander (Ed.), Probability and Statistics, pp. 416—-444. New York, John Wiley. Neyman, J. (1979). c(α) tests and their use. Sankhya, 1–21. Poterba, J. M., S. F. Venti, and D. A. Wise (1994a). 401(k) plans and tax-deferred\nsavings. In D. Wise (Ed.), Studies in the Economics of Aging, pp. 105–142. Chicago:\nUniversity of Chicago Press.",
    "content_hash": "254ff3b6adde7b76dc5af0b528475bf33e0a35eb0d0d4643bdeae586cc779e55",
    "location": null,
    "page_start": 1,
    "page_end": 49,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f340bf4d-66bc-44bd-8a71-7b37a339a00e",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "For “Reg. Tree,” we fit a single CART tree to estimate each nuisance function with penalty param-\neter chosen by 10-fold cross-validation. The results in the “Random Forest” column are\n12There are six treatment groups in the experiments. Following Bilias (2000). we merge the groups 4\nand 6.",
    "content_hash": "2b40adbba56138d080bff20d6930c07ae6169c639c583087f5903edd0c83b38f",
    "location": null,
    "page_start": 1,
    "page_end": 39,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "67c3f64f-44a2-47a6-afd2-4432600eb461",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n39\nvalidity:\nlim\nN→∞sup\nP ∈P\nPP\n\u0010\nθ0 ∈[˜θ0 ± Φ−1(1 −α/2)bσ/\n√\nN]\n\u0011\n−(1 −α)\n= 0. 6. EMPIRICAL EXAMPLES\nTo illustrate the methods developed in the preceding sections, we consider three em-\npirical examples. The first example reexamines the Pennsylvania Reemployment Bonus\nexperiment which used a randomized control trial to investigate the incentive effect of\nunemployment insurance. In the second, we use the DML method to estimate the effect of\n401(k) eligibility, the treatment variable, and 401(k) participation, a self-selected decision\nto receive the treatment that we instrument for with assignment to the treatment state,\non accumulated assets. In this example, the treatment variable is not randomly assigned\nand we aim to eliminate the potential biases due to the lack of random assignment by\nflexibly controlling for a rich set of variables. In the third, we revisit Acemoglu et al. (2001) IV estimation of the effects of institutions on economic growth by estimating a\npartially linear IV model. 6.1. The effect of Unemployment Insurance Bonus on Unemployment Duration\nIn this example, we re-analyze the Pennsylvania Reemployment Bonus experiment which\nwas conducted by the US Department of Labor in the 1980s to test the incentive effects\nof alternative compensation schemes for unemployment insurance (UI). This experiment\nhas been previously studied by Bilias (2000) and Bilias and Koenker (2002).",
    "content_hash": "efb403737bbc152d4bd2cadc4c6062416e175b9bd3cfe5688dc623b44cb53368",
    "location": null,
    "page_start": 1,
    "page_end": 39,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5a5d4552-1784-4ed6-9d80-bd69e9a0e5b1",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Berkeley Division of Biostatistics Working Paper Series. van der Laan, M. J., E. C. Polley, and A. E. Hubbard (2007). Super learner. Statisti-\ncal Applications in Genetics and Molecular Biology 6. Retrieved 24 Feb. 2017, from\ndoi:10.2202/1544-6115.1309. van der Laan, M. J. and S. Rose (2011). Targeted Learning: Causal Inference for Obser-\nvational and Experimental Data. Springer. van der Vaart, A. W. (1991). On differentiable functionals. Annals of Statistics 19,\n178–204. van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press. Wager, S. and G. Walther (2016). Adaptive concentration of regression trees, with ap-\nplication to random forests. arXiv:1503.06388. arXiv, 2015. Wooldridge, J. (1991). Specification testing and quasi-maximum-likelihood estimation. Journal of Econometrics 48, 29–55.",
    "content_hash": "1695f90d8b4b40e323eed8b26f66f20d3dffe39b5ba2372c275a98bc34933a69",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c48ef131-66ff-4716-80d3-ee7eeb05ba7a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Such comparisons also extend to\napproximately sparse models. 4.2. Inference in Partially Linear IV Models\nHere we extend the partially linear regression model studied in Section 4.1 to allow for\ninstrumental variable (IV) identification. Specifically, we consider the model\nY = Dθ0 + g0(X) + U,\nEP [U | X, Z] = 0,\n(4.5)\nZ = m0(X) + V,\nEP [V | X] = 0,\n(4.6)\nwhere Z is the instrumental variable. As before, the parameter of interest is θ and its true\nvalue is θ0. If Z = D, the model (4.5)-(4.6) coincides with (4.1)-(4.2) but is otherwise\ndifferent. To estimate θ0 and to perform inference on it, we will use the score\nψ(W; θ, η) := (Y −Dθ −g(X))(Z −m(X)),\nη = (g, m),\n(4.7)\nwhere W = (Y, D, X, Z) and g and m are P-square-integrable functions mapping the\nsupport of X to R.",
    "content_hash": "e5a192f7dd5dbf17e93a01c93cba8b27e4c9eaf89f992513a013baff2e6bfa55",
    "location": null,
    "page_start": 1,
    "page_end": 33,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "24950b53-e586-4668-a1eb-130a065f612e",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "For convenience, we also denote\n∂ηEP ψ(W; θ0, η0)[η −η0] := D0[η −η0],\nη ∈T. (2.2)\nNote that ψ(W; θ0, η0 +r(η−η0)) here is well-defined because for all r ∈[0, 1) and η ∈T,\nη0 + r(η −η0) = (1 −r)η0 + rη ∈T\nsince T is a convex set. In addition, let TN ⊂T be a nuisance realization set such that\nthe estimators bη0 of η0 specified below take values in this set with high probability. In\npractice, we typically assume that TN is a properly shrinking neighborhood of η0. Note\nthat TN −η0 is the nuisance deviation set, which contains deviations of bη0 from η0, bη0−η0,\nwith high probability. The Neyman orthogonality condition requires that the derivative\nin (2.2) vanishes for all η ∈TN. Definition 2.1. (Neyman orthogonality) The score ψ = (ψ1, . . .",
    "content_hash": "431c5d53a9790a00efa33161470b0f1495b34413aa699c7ebdcb87d58825a731",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "71ea16de-4a68-4151-b5d6-6498e97ecef9",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "24\nCCDDHNR\nthe estimators:\n˜θ0 = 1\nK\nK\nX\nk=1\nˇθ0,k. (3.3)\nThis approach generalizes the 50-50 cross-fitting method mentioned in the Introduc-\ntion. We now define a variation of this basic cross-fitting approach that may behave\nbetter in small samples. Definition 3.2. (DML2) 1) Take a K-fold random partition (Ik)K\nk=1 of observation\nindices [N] = {1, ..., N} such that the size of each fold Ik is n = N/K. Also, for each\nk ∈[K] = {1, . . . , K}, define Ic\nk := {1, ..., N} \\ Ik. 2) For each k ∈[K], construct a ML\nestimator\nbη0,k = bη0((Wi)i∈Ic\nk)\nof η0, where bη0,k is a random element in T, and where randomness depends only on the\nsubset of data indexed by Ic\nk. 3) Construct the estimator ˜θ0 as the solution to the following\nequation:\n1\nK\nK\nX\nk=1\nEn,k[ψ(W; ˜θ0, bη0,k)] = 0,\n(3.4)\nwhere ψ is the Neyman orthogonal score, and En,k is the empirical expectation over the\nk-th fold of the data; that is, En,k[ψ(W)] = n−1 P\ni∈Ik ψ(Wi).",
    "content_hash": "8ae0e7d32335769c4262b4c3b3cc4e1e1027b36be06c97901d84253967188a0e",
    "location": null,
    "page_start": 1,
    "page_end": 24,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "79a2fd51-83bf-47e1-a8dc-ac6817a1bd9f",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "16\nCCDDHNR\noptimization problem:\nmax\nβ∈B EP [ℓ(W; θ, β)]. Under mild regularity conditions, βθ satisfies\n∂βEP [ℓ(W; θ, βθ)] = 0,\nfor all θ ∈Θ. (2.16)\nDifferentiating (2.16) with respect to θ and interchanging the order of differentiation\ngives\n0 = ∂θ∂βEP\nh\nℓ(W; θ, βθ)\ni\n= ∂β∂θEP\nh\nℓ(W; θ, βθ)\ni\n= ∂βEP\nh\n∂θℓ(W; θ, βθ) + [∂θβθ]\n′∂βℓ(W; θ, βθ)\ni\n= ∂βEP\nh\nψ(W; θ, β, ∂θβθ)\ni\nβ=βθ\n,\nwhere we denoted\nψ(W; θ, β, ∂θβθ) := ∂θℓ(W; θ, β) + [∂θβθ]′ ∂βℓ(W; θ, β). This vector of functions is a score with nuisance parameters η = (β′, vec(∂θβθ))′. As\nbefore, additional nuisance parameters, ∂θβθ in this case, are introduced when the or-\nthogonal score is formed. Evaluating these equations at θ0 and β0, it follows from\nthe previous equation that ψ(W; θ, β, ∂θβθ) is orthogonal with respect to β and from\nEP [∂βℓ(W; θ0, β0)] = 0 that we have orthogonality with respect to ∂θβθ.",
    "content_hash": "f4829a4924297279ec4b4009a23cd8ccd0f97c3635c76fd8216970b28f22f8bc",
    "location": null,
    "page_start": 1,
    "page_end": 16,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6d866a97-d997-4bdd-8e70-f376c8b3dbd9",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "The following lemma shows that the score ψ in (2.18) satisfies the Neyman orthogonality\ncondition. Lemma 2.3. (Neyman Orthogonal Scores for GMM Settings) If (2.17) holds, Gγ\nexists, and Ωis invertible, the score ψ in (2.18) is Neyman orthogonal at (θ0, η0) with\nrespect to the nuisance realization set TN = T. As in the quasi-likelihood case, we can also consider near-orthogonal scores. Specifically,\nnote that one of the orthogonality conditions that the score ψ in (2.18) has to satisfy is\nthat µ0Gβ = 0, which can be rewritten as\nA′Ω−1/2(I −L(L′L)−1L′)L = 0,\nwhere L = Ω−1/2Gβ\nHere, the part A′Ω−1/2L(L′L)−1L′ can be expressed as γ0L′, where γ0 = A′Ω−1/2L(L′L)−1\nsolves the optimization problem\nmin ∥γ∥o such that ∥A′Ω−1/2L −γL′L∥∞= 0,\nfor a suitably chosen norm ∥· ∥o. When L′L is close to being singular, this problem can\nbe relaxed:\nmin ∥γ∥o such that ∥A′Ω−1/2L −γL′L∥∞⩽rN. (2.19)\nThis relaxation leads to Neyman near-orthogonal scores:\nLemma 2.4.",
    "content_hash": "61a17dd1cb4b6174b82ec48c2cfe5db50d41e06ac8f5733b169b7498d3c8be98",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b217d05b-d001-4f11-8066-c194f0850195",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "This completes the proof of the lemma. ■\nA.12. Proof of Theorem 3.1 (DML2 case)\nTo start with, note that (3.11) follows immediately from the assumptions. Hence, it\nsuffices to show that (3.10) holds uniformly over P ∈PN. Fix any sequence {PN}N⩾1 such that PN ∈PN for all N ⩾1. Since this sequence is\nchosen arbitrarily, to show that (3.10) holds uniformly over P ∈PN, it suffices to show\nthat\n√\nNσ−1(˜θ0 −θ0) =\n1\n√\nN\nN\nX\ni=1\n¯ψ(Wi) + OPN (ρN) ; N(0, Id). (A.4)\nTo do so, we proceed in 5 steps. Step 1 shows the main argument, and Steps 2–5 present\nauxiliary calculations. In the proof, it will be convenient to denote by EN the event\nthat bη0,k ∈TN for all k ∈[K]. Note that by Assumption 3.2 and the union bound,\nPPN (EN) ⩾1 −K∆n = 1 −o(1) since ∆n = o(1). Step 1. Denote\nbJ0 := 1\nK\nK\nX\nk=1\nEn,k[ψa(W; bη0,k)],\nRN,1 := bJ0 −J0,\nRN,2 := 1\nK\nK\nX\nk=1\nEn,k[ψ(W; θ0, bη0,k)] −1\nN\nN\nX\ni=1\nψ(Wi; θ0, η0).",
    "content_hash": "24ecf1a41535ea23378a1eed059fcfb63dc577048b8421135adb62bf5162fabc",
    "location": null,
    "page_start": 1,
    "page_end": 55,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "68498e2a-94b9-448c-9771-26e9549b0e9c",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(Partially Linear Regression) As a lead example, consider the fol-\nlowing partially linear regression (PLR) model as in Robinson (1988):\nY = Dθ0 + g0(X) + U,\nE[U | X, D] = 0,\n(1.1)\nD = m0(X) + V,\nE[V | X] = 0,\n(1.2)\nwhere Y is the outcome variable, D is the policy/treatment variable of interest, vector\nX = (X1, ..., Xp)\nconsists of other controls, and U and V are disturbances.1 The first equation is the\nmain equation, and θ0 is the main regression coefficient that we would like to infer. If\nD is exogenous conditional on controls X, θ0 has the interpretation of the treatment\neffect (TE) parameter or “lift” parameter in business applications. The second equation\nkeeps track of confounding, namely the dependence of the treatment variable on controls. This equation is not of interest per se but is important for characterizing and remov-\ning regularization bias. The confounding factors X affect the policy variable D via the\nfunction m0(X) and the outcome variable via the function g0(X). In many applications,\nthe dimension p of vector X is large relative to N. To capture the feature that p is not\nvanishingly small relative to the sample size, modern analyses then model p as increasing\nwith the sample size, which causes traditional assumptions that limit the complexity of\nthe parameter space for the nuisance parameters η0 = (m0, g0) to fail. ■\nRegularization Bias.",
    "content_hash": "da36d4e644d4c3cc916057ac3fd7843e827e033b91f1d8aa2efedbc42f3f6bb4",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "02072b77-f6c9-4085-9b02-aca8e2a8cb33",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "2\nCCDDHNR\n1. INTRODUCTION AND MOTIVATION\n1.1. Motivation\nWe develop a series of simple results for obtaining root-N consistent estimation, where N\nis the sample size, and valid inferential statements about a low-dimensional parameter of\ninterest, θ0, in the presence of a high-dimensional or “highly complex” nuisance parame-\nter, η0. The parameter of interest will typically be a causal parameter or treatment effect\nparameter, and we consider settings in which the nuisance parameter will be estimated\nusing machine learning (ML) methods such as random forests, lasso or post-lasso, neu-\nral nets, boosted regression trees, and various hybrids and ensembles of these methods. These ML methods are able to handle many covariates and provide natural estimators\nof nuisance parameters when these parameters are highly complex. Here, highly complex\nformally means that the entropy of the parameter space for the nuisance parameter is\nincreasing with the sample size in a way that moves us outside of the traditional frame-\nwork considered in the classical semi-parametric literature where the complexity of the\nnuisance parameter space is taken to be sufficiently small. Offering a general and simple\nprocedure for estimating and doing inference on θ0 that is formally valid in these highly\ncomplex settings is the main contribution of this paper. Example 1.1.",
    "content_hash": "febcdb03e32a48be1e9a7ab782ea6e1e701c39e3421ce0615f661a8d91035377",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a1944967-5ac2-422c-961c-19e600a2ff78",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "A naive approach to estimation of θ0 using ML methods would\nbe, for example, to construct a sophisticated ML estimator Dbθ0 + bg0(X) for learning the\n1We consider the case where D is a scalar for simplicity. Extension to the case where D is a vector\nof fixed, finite dimension is accomplished by introducing an equation like (1.2) for each element of the\nvector.",
    "content_hash": "8c1013bb56e7cab755127e7b081da909acc1b345c3768c47c740ccc1a279c42d",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f58fbbf4-2ac5-44b7-88ae-ed86dec85c56",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "To heuristically illustrate the impact of the bias in learning g0, we can decompose the\nscaled estimation error in bθ0 as\n√n(bθ0 −θ0) =\n\u0010 1\nn\nX\ni∈I\nD2\ni\n\u0011−1 1\n√n\nX\ni∈I\nDiUi\n|\n{z\n}\n:=a\n+\n\u0010 1\nn\nX\ni∈I\nD2\ni\n\u0011−1 1\n√n\nX\ni∈I\nDi(g0(Xi) −bg0(Xi))\n|\n{z\n}\n:=b\n. The first term is well-behaved under mild conditions, obeying a ; N(0, ¯Σ) for some ¯Σ. Term b is the regularization bias term, which is not centered and diverges in general. Indeed, we have\nb = (EDi\n2)−1 1\n√n\nX\ni∈I\nm0(Xi)(g0(Xi) −bg0(Xi)) + oP (1)\nto the first order. Heuristically, b is the sum of n terms that do not have mean zero,\nm0(Xi)(g0(Xi) −bg0(Xi)), divided by √n. These terms have non-zero mean because,\nin high dimensional or otherwise highly complex settings, we must employ regularized\nestimators - such as lasso, ridge, boosting, or penalized neural nets - for informative\nlearning to be feasible. The regularization in these estimators keeps the variance of the\nestimator from exploding but also necessarily induces substantive biases in the estimator\nbg0 of g0.",
    "content_hash": "c96970ddd7f1cafbdc2af710ff9f8851a13cf25948c38dc4825c19bce954eea4",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fbc1fe5d-8e81-44a3-8977-fc19802504df",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Specifically, the rate of convergence of (the bias of) bg0 to g0 in the root mean\nsquared error sense will typically be n−φg with φg < 1/2. Hence, we expect b to be of\nstochastic order √nn−φg →∞since Di is centered at m0(Xi) ̸= 0, which then implies\n(1.4). Overcoming Regularization Biases using Orthogonalization. Now consider a\nsecond construction that employs an “orthogonalized” formulation obtained by directly\npartialling out the effect of X from D to obtain the orthogonalized regressor V = D −\nm0(X). Specifically, we obtain bV = D −bm0(X), where bm0 is an ML estimator of m0\n2For instance, we could use lasso if we believe g0 is well-approximated by a sparse linear combination of\nprespecified functions of X. In other settings, we could, for example, use iterative methods that alternate\nbetween random forests, for estimating g0, and least squares, for estimating θ0.",
    "content_hash": "d618a1c41e06cf51ceee44d0a9a4edaeaff4ac80d809cd97f7c445111e85f2df",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "67b5607d-1425-493a-a2d0-26a05bd341e4",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n3\nregression function Dθ0 +g0(X).2 Suppose, for the sake of clarity, that we randomly split\nthe sample into two parts: a main part of size n, with observation numbers indexed by\ni ∈I, and an auxiliary part of size N −n, with observations indexed by i ∈Ic. For\nsimplicity, we take n = N/2 for the moment and turn to more general cases which cover\nunequal split-sizes, using more than one split, and achieving the same efficiency as if the\nfull sample were used for estimating θ0 in the formal development in Section 3. Suppose\nbg0 is obtained using the auxiliary sample and that, given this bg0, the final estimate of θ0\nis obtained using the main sample:\nbθ0 =\n\u0010 1\nn\nX\ni∈I\nD2\ni\n\u0011−1 1\nn\nX\ni∈I\nDi(Yi −bg0(Xi)). (1.3)\nThe estimator bθ0 will generally have a slower than 1/√n rate of convergence, namely,\n|√n(bθ0 −θ0)| →P ∞. (1.4)\nAs detailed below, the driving force behind this “inferior” behavior is the bias in learning\ng0. Figure 1 provides a numerical illustration of this phenomenon for a naive ML estimator\nbased on a random forest in a simple computational experiment.",
    "content_hash": "982b3f43e1e4fdcc482e29f7382bc4c79da00bc943ef00abd7608afdded24309",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5ad9e49c-be8c-4243-a85e-e02300c43dbb",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "The simulated distribution of the\ncentered estimator, ˇθ0 −θ0, (given by the blue histogram) illustrates that the estimator is approximately\nunbiased, concentrates around θ0, and is well-approximated by the normal approximation obtained in\nSection 3 (shown by the red curve). obtained using the auxiliary sample of observations. We are now solving an auxiliary\nprediction problem to estimate the conditional mean of D given X, so we are doing\n“double prediction” or “double machine learning”. After partialling the effect of X out from D and obtaining a preliminary estimate of g0\nfrom the auxiliary sample as before, we may formulate the following “debiased” machine\nlearning estimator for θ0 using the main sample of observations:\nˇθ0 =\n\u0010 1\nn\nX\ni∈I\nbViDi\n\u0011−1 1\nn\nX\ni∈I\nbVi(Yi −bg0(Xi)).3\n(1.5)\nBy approximately orthogonalizing D with respect to X and approximately removing the\ndirect effect of confounding by subtracting an estimate of g0, ˇθ0 removes the effect of\nregularization bias that contaminates (1.3).",
    "content_hash": "affd87b9fafb63e4477d27b111dfd5d9189e5b728a21253785147999c6e1360a",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "078301c0-1edb-44f2-89d0-76d57a408f65",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "The formulation of ˇθ0 also provides direct\nlinks to both the classical econometric literature, as the estimator can clearly be inter-\npreted as a linear instrumental variable (IV) estimator, and to the more recent literature\non debiased lasso in the context where g0 is taken to be well-approximated by a sparse\n3In Section 4, we also consider another debiased estimator, based on the partialling-out approach of\nRobinson (1988):\nˇθ0 =\n\u0010 1\nn\nX\ni∈I\nbVi bVi\n\u0011−1 1\nn\nX\ni∈I\nbVi(Yi −bℓ0(Xi)),\nℓ0(X) = E[Y |X].",
    "content_hash": "2b91c3d1310d9059137e8ca1be22790b4e7c8310542df0a17d934bbacc8d330b",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "60cbd000-d4b0-4c8e-9ee4-be5e2e464786",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "4\nCCDDHNR\n-0.2\n-0.15\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNon-Orthogonal, n = 500, p = 20\nSimulation\nN(0,'s)\n-0.2\n-0.15\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nOrthogonal, n = 500, p = 20\nSimulation\nN(0,'s)\nFigure 1. Left Panel: Behavior of a conventional (non-orthogonal) ML estimator, bθ0, in the partially\nlinear model in a simple simulation experiment where we learn g0 using a random forest. The g0 in this\nexperiment is a very smooth function of a small number of variables, so the experiment is seemingly\nfavorable to the use of random forests a priori. The histogram shows the simulated distribution of the\ncentered estimator, bθ0 −θ0. The estimator is badly biased, shifted much to the right relative to the\ntrue value θ0. The distribution of the estimator (approximated by the blue histogram) is substantively\ndifferent from a normal approximation (shown by the red curve) derived under the assumption that the\nbias is negligible. Right Panel: Behavior of the orthogonal, DML estimator, ˇθ0, in the partially linear\nmodel in a simple experiment where we learn nuisance functions using random forests. Note that the\nsimulated data are exactly the same as those underlying left panel.",
    "content_hash": "f3794570be5c9e895589a5b731198f988f20241c3f5aaf7991b99f77f9124450",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c49814db-6aec-41a5-876d-eb0c967b7a2a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Indeed, this term is upper-bounded by √nn−(φm+φg),\nwhere n−φm and n−φg are respectively the rates of convergence of bm0 to m0 and bg0 to\ng0; and this upper bound can clearly vanish even though both m0 and g0 are estimated\nat relatively slow rates. Verifying that ˇθ0 has good properties then requires that the\nremainder term, c∗, is sufficiently well-behaved. Sample-splitting will play a key role in\nallowing us to guarantee that c∗= oP (1) under weak conditions as outlined below and\ndiscussed in detail in Section 3. The Role of Sample Splitting in Removing Bias Induced by Overfitting. Our analysis makes use of sample-splitting which plays a key role in establishing that\nremainder terms, like c∗, vanish in probability. In the partially linear model, we have\nthat the remainder c∗contains terms like\n1\n√n\nX\ni∈I\nVi(bg0(Xi) −g0(Xi))\n(1.6)\nthat involve 1/√n normalized sums of products of structural unobservables from model\n(1.1)-(1.2) with estimation errors in learning the nuisance functions g0 and m0 and need\nto be shown to vanish in probability. The use of sample splitting allows simple and tight\ncontrol of such terms. To see this, assume that observations are independent and recall\nthat bg0 is estimated using only observations in the auxiliary sample.",
    "content_hash": "f9697df14a2a114a108d3f309096cc2b7ba25de56e86276f1bac3eb85f33d74b",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d43cc769-2ad3-4631-8e34-e571d910952e",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Then, conditioning\non the auxiliary sample and recalling that E[Vi|Xi] = 0, it is easy to verify that term\n(1.6) has mean zero and variance of order\n1\nn\nX\ni∈I\n(bg0(Xi) −g0(Xi))2 →P 0. Thus, the term (1.6) vanishes in probability by Chebyshev’s inequality. While sample splitting allows us to deal with remainder terms such as c∗, its direct\n4Each of these works differs in terms of detail but can be viewed through the lens of either “debiasing” or\n“orthogonalization” to alleviate the impact of regularization bias on subsequent estimation and inference.",
    "content_hash": "77136dd714602487228af5e15cf0d14accbce263d7f5977e9ef221540a249f5d",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "18b3a277-b1df-469c-a002-8ca33b77628b",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n5\nlinear combination of prespecified functions of X; see, e.g., Belloni et al. (2013); Zhang\nand Zhang (2014); Javanmard and Montanari (2014b); van de Geer et al. (2014); Belloni\net al. (2014); and Belloni et al. (2014).4\nTo illustrate the benefits of the auxiliary prediction step and estimating θ0 with ˇθ0,\nwe sketch the properties of ˇθ0 here. We can decompose the scaled estimation error of ˇθ0\ninto three components:\n√n(ˇθ0 −θ0) = a∗+ b∗+ c∗. The leading term, a∗, will satisfy\na∗= (EV 2)−1 1\n√n\nX\ni∈I\nViUi ; N(0, Σ)\nunder mild conditions. The second term, b∗, captures the impact of regularization bias\nin estimating g0 and m0. Specifically, we will have\nb∗= (EV 2)−1 1\n√n\nX\ni∈I\n( bm0(Xi) −m0(Xi))(bg0(Xi) −g0(Xi)),\nwhich now depends on the product of the estimation errors in bm0 and bg0. Because this\nterm depends only on the product of the estimation errors, it can vanish under a broad\nrange of data-generating processes.",
    "content_hash": "fc8e7f4b08fb9d6411892ef4c346ad1c8662a3e2dfc9fa15d7034ee8f5bd5920",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cb94ad50-c034-444e-b912-7b729a5e56c3",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "As an artificial but illustrative example of the problems that may result from overfit-\nting, let bg0(Xi) = g0(Xi) + (Yi −g0(Xi))/N 1/2−ϵ for any i in the sample used to form\nestimator bg0, and note that the second term provides a simple model that captures over-\nfitting of the outcome variable within the estimation sample. This estimator is excellent\nin terms of rates: If the Ui’s and Di’s are bounded, bg0 converges uniformly to g0 at\nthe nearly parametric rate N −1/2+ϵ. Despite this fast rate of convergence, term c∗now\nexplodes if we do not use sample splitting. For example, suppose that the full sample is\nused to estimate both bg0 and ˇθ0. A simple calculation then reveals that term c∗becomes\n1\n√\nN\nN\nX\ni=1\nVi(bg0(Xi) −g0(Xi)) ∝N ϵ →∞. This bias due to overfitting is illustrated in the left panel of Figure 2. The histogram\nin the figure gives a simulated distribution for the studentized ˇθ resulting from using the\nfull sample and the contrived estimator bg(Xi) given above. We can see that the histogram\nis shifted markedly to the left demonstrating substantial bias resulting from overfitting. The right panel of Figure 2 also illustrates that this bias is completely removed by\nsample splitting.",
    "content_hash": "b0f6b0b616cb9362bc512207a746488831b1c77c6400bad38aa3d653e4a92020",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ff755f09-d76e-4e82-aa8f-e08efc1bdbbf",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "The results the right panel of Figure 2 make use of the two-fold cross-\nfitting procedure discussed above using the estimator ˇθ and the contrived estimator\nbg(Xi) exactly as in the left panel. The difference is that bg(Xi) is formed in one half of\nthe sample and then ˇθ is estimated using the other half of the sample. This procedure\nis then repeated swapping the roles of the two samples and the results are averaged. We\ncan see that the substantial bias from the full sample estimator has been removed and\nthat the spread of the histogram corresponding to the cross-fit estimator is roughly the\nsame as that of the full sample estimator clearly illustrating the bias-reduction property\nand efficiency of the cross-fitting procedure. A less contrived example that highlights the improvements brought by sample-splitting\nis the sparse high-dimensional instrumental variable (IV) model analyzed in Belloni et al. (2012). Specifically, they consider the IV model\nY = Dθ0 + ϵ\nwhere E[ϵ|D] ̸= 0 but instruments Z exist such that E[D|Z] is not a constant and",
    "content_hash": "b726abd4cbd67eb61e4813b64baf6db7db95590cd82c439f2771eb5eb3fd8a51",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bf45029c-8973-4270-b3f4-c6c924c4b4dd",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "6\nCCDDHNR\napplication does have the drawback that the estimator of the parameter of interest only\nmakes use of the main sample which may result in a substantial loss of efficiency as we\nare only making use of a subset of the available data. However, we can flip the role of the\nmain and auxiliary samples to obtain a second version of the estimator of the parameter\nof interest. By averaging the two resulting estimators, we may regain full efficiency. Indeed, the two estimators will be approximately independent, so simply averaging them\noffers an efficient procedure. We call this sample splitting procedure where we swap the\nroles of main and auxiliary samples to obtain multiple estimates and then average the\nresults cross-fitting. We formally define this procedure and discuss a K-fold version of\ncross-fitting in Section 3. Without sample splitting, terms such as (1.6) may not vanish and can lead to poor\nperformance of estimators of θ0. The difficulty arises because model errors, such as Vi,\nand estimation errors, such as bg0(Xi) −g0(Xi), are generally related because the data\nfor observation i is used in forming the estimator bg0. The association may then lead to\npoor performance of an estimator of θ0 that makes use of bg0 as a plug-in estimator for\ng0 even when this estimator converges at a very favorable rate, say N −1/2+ϵ.",
    "content_hash": "69ec0147a7571dcf30c276a1fbd06cff363b015b62993a7ab38400c508c78436",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5e99d8db-aaa2-46af-bb03-69a2b4782ddf",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "We note that this example provides a\nprototypical example where Neyman orthogonality holds and ML methods can usefully\nbe adopted to aid in learning structural parameters of interest. We also note that the\nweaker conditions required when using sample sample-splitting would also carry over to\nsparsity-based estimators in the partially linear model cited above. We discuss this in\nmore detail in Section 4. While we find substantial appeal in using sample-splitting, one may also use empirical\nprocess methods to verify that biases introduced due to overfitting are negligible. For\nexample, consider the problematic term in the partially linear model described previously,\n1\n√n\nP\ni∈I Vi(bg0(Xi) −g0(Xi)). This term is clearly bounded by\nsup\ng∈GN\n1\n√n\nX\ni∈I\nVi(g(Xi) −g0(Xi))\n,\n(1.7)\nwhere GN is the smallest class of functions that contains estimators of g0, bg, with high\nprobability. In conventional semiparametric statistical and econometric analysis, the com-\nplexity of GN is controlled by invoking Donsker conditions which allow verification that\nterms such as (1.7) vanish asymptotically. Importantly, Donsker conditions require that\nGN has bounded complexity, specifically a bounded entropy integral. Because of the latter\nproperty, Donsker conditions are inappropriate in settings using ML methods where the\ndimension of X is modeled as increasing with the sample size and estimators necessarily",
    "content_hash": "186ca7ef4cb7f9a652128efe837cf331ebe89326e621c18971a440543150ee02",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3a7dfbd1-3b69-4376-a670-6d6ad9853956",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n7\nFigure 2. This figure illustrates how the bias resulting from overfitting in the estimation of nuisance\nfunctions can cause the main estimator ˇθ0 to be biased and how sample splitting completely eliminates\nthis problem. Left Panel: The histogram shows the finite-sample distribution of ˇθ0 in the partially\nlinear model where nuisance parameters are estimated with overfitting using the full sample, i.e. without\nsample splitting. The finite-sample distribution is clearly shifted to the left of the true parameter value\ndemonstrating the substantial bias. Right Panel: The histogram shows the finite-sample distribution\nof ˇθ0 in the partially linear model where nuisance parameters are estimated with overfitting using the\ncross-fitting sample-splitting estimator. Here, we see that the use of sample-splitting has completely\neliminated the bias induced by overfitting. E[ϵ|Z] = 0. Within this model, Belloni et al. (2012) focus on the problem of estimating the\noptimal instrument, η0(Z) = E[D|Z] using lasso-type methods. If η0(Z) is approximately\nsparse in the sense that only s terms of the dictionary of series transformations B(Z) =\n(B1(Z), . . . , Bp(Z)) are needed to approximate the function accurately, Belloni et al. (2012) require that s2 ≪n to establish their asymptotic results when sample splitting is\nnot used but show that these results continue to hold under the much weaker requirement\nthat s ≪n if one employs sample splitting.",
    "content_hash": "648eb0e86d66d5576dc6f5f57333fd46a57a60347610551ba74a806338c85f10",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a8092591-17dd-42c2-b263-aec23d460e96",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "For example, in the partially linear model above, the score function is φ(W; θ, g) =\n(Y −θD −g(X))D. It is easy to see that this score function φ is sensitive to biased\nestimation of g. Specifically, the Gateaux derivative operator with respect to g does not\nvanish:\n∂gEφ(W; θ0, g0)[g −g0] ̸= 0.5\nThe proofs of the general results in Section 3 show that this term’s vanishing is a key to\nestablishing good behavior of an estimator for θ0. By contrast the orthogonalized or double/debiased ML estimator ˇθ0 given in (1.5)\nsolves\n1\nn\nX\ni∈I\nψ(W; ˇθ0, bη0) = 0,\nwhere bη0 is the estimator of the nuisance parameter η0 and ψ is an orthogonalized or\ndebiased “score” function that satisfies the property that the Gateaux derivative operator\nwith respect to η vanishes when evaluated at the true parameter values:\n∂ηEψ(W; θ0, η0)[η −η0] = 0. (1.8)\nWe refer to property (1.8) as “Neyman orthogonality” and to ψ as the Neyman orthogonal\nscore function due to the fundamental contributions in Neyman (1959) and Neyman\n(1979), where this notion was introduced.",
    "content_hash": "e1638f879108a9b74e1fff4680412d469660de252f810741a0f646c35cad5331",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7c57b650-bf3b-46e5-b471-d1b1ef1eb2fd",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Intuitively, the Neyman orthogonality condition\nmeans that the moment conditions used to identify θ0 are locally insensitive to the value\nof the nuisance parameter which allows one to plug-in noisy estimates of these parameters\nwithout strongly violating the moment condition. In the partially linear model (1.1)-(1.2),\nthe estimator ˇθ0 uses the score function ψ(W; θ, η) = (Y −Dα−g(X))(D −m(X)), with\n5See Section 2 for the definition of the Gateaux derivative operator.",
    "content_hash": "fa705666eb13da297da936f704c89bd22f16e11e6f30d2447c579332ce42addd",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "21ebed90-4460-4b83-8be1-b75fe70b004a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "8\nCCDDHNR\nlive in highly complex spaces. For example, Donsker conditions rule out even the simplest\nlinear parametric model with high-dimensional regressors with parameter space given by\nthe Euclidean ball with the unit radius:\nGN = {x 7→g(x) = x′θ;\nθ ∈RpN : ∥θ∥⩽1}. The entropy of this model, as measured by the logarithm of the covering number, grows\nat the rate pN. Without invoking Donsker conditions, one may still show that terms\nsuch as (1.7) vanish as long as GN’s entropy does not increase with N too rapidly. A\nfairly general treatment is given in Belloni et al. (2017) who provide a set of conditions\nunder which terms like c∗can vanish making use of the full sample. However, these\nconditions on the growth of entropy could result in unnecessarily strong restrictions on\nmodel complexity, such as very strict requirements on sparsity in the context of lasso\nestimation as demonstrated in IV example mentioned above. Sample splitting allows one\nto obtain good results under very weak conditions. Neyman Orthogonality and Moment Conditions. Now we turn to a generaliza-\ntion of the orthogonalization principle above. The first “conventional” estimator bθ0 given\nin (1.3) can be viewed as a solution to estimating equations\n1\nn\nX\ni∈I\nφ(W; bθ0, bg0) = 0,\nwhere φ is a known “score” function and bg0 is the estimator of the nuisance parameter\ng0.",
    "content_hash": "7d30e765deec8a2ba224ac37e006c18b37f1aa2205359a007cb8116da7aa5542",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "30ad93ef-dbe7-485e-ac97-917b11b29dca",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(2003),\nNewey et al. (2004), van der Laan and Rose (2011), and Ai and Chen (2012). Neyman\northogonality (1.8), introduced by Neyman (1959), plays a key role in optimal testing\ntheory and adaptive estimation, semiparametric learning theory and econometrics, and,\nmore recently, targeted learning theory. For example, Andrews (1994a), Newey (1994) and\nvan der Vaart (1998) provide a general set of results on estimation of a low-dimensional\nparameter θ0 in the presence of nuisance parameters η0. Andrews (1994a) uses Ney-\nman orthogonality (1.8) and Donsker conditions to demonstrate the key equicontinuity\ncondition\n1\n√n\nX\ni∈I\n\u0010\nψ(Wi; θ0, bη) −\nZ\nψ(w; θ0, bη)dP(w) −ψ(Wi; θ0, η0)\n\u0011\n→P 0,\nwhich reduces to (1.6) in the partially linear regression model. Newey (1994) gives condi-\ntions on estimating equations and nuisance function estimators so that nuisance function\nestimators do not affect the limiting distribution of parameters of interest, providing a\nsemiparametric version of Neyman orthogonality. van der Vaart (1998) discusses use of\nsemiparametrically efficient scores to define estimators that solve estimating equations\nsetting averages of efficient scores to zero.",
    "content_hash": "008518362e6fea9b235d2b9ebbc2464af6d3aa9ae0245f5a521e512bec61e51a",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e65b2140-a5c1-466a-9ef8-86c2a965bf19",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "He also uses efficient scores to define k-step\nestimators, where a preliminary estimator is used to estimate the efficient score and then\nupdating is done to further improve estimation; see also comments below on the use of\nsample-splitting. There is also a related targeted maximum likelihood learning approach, introduced in\nScharfstein et al. (1999) in the context of treatments effects analysis and substantially\ngeneralized by van der Laan and Rubin (2006). van der Laan and Rubin (2006) use max-\nimum likelihood in a least favorable direction and then perform “one-step” or “k-step”\nupdates using the estimated scores in an effort to better estimate the target parameter.6\nThis procedure is like the least favorable direction approach in semiparametrics; see, for\n6Targeted minimum loss estimation, which shares similar properties, is also discussed in e.g. van der\nLaan and Rose (2011) and van der Laan (2015).",
    "content_hash": "27bd239b453e2a21fc67643ca7642c1e044c1374825715910a8a1cda6ffa5e4e",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0c511c16-4ce7-4a43-8a8b-bfe1b3d42be8",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n9\nthe nuisance parameter being η = (m, g). It is easy to see that these score functions ψ are\nnot sensitive to biased estimation of η0 in the sense that (1.8) holds. The proofs of the\ngeneral results in Section 3 show that this property and sample splitting are two generic\nkeys that allow establishing good behavior of an estimator for θ0. 1.2. Literature Overview\nOur paper builds upon two important bodies of research within the semiparametric liter-\nature. The first is the literature on obtaining\n√\nN-consistent and asymptotically normal\nestimates of low-dimensional objects in the presence of high-dimensional or nonparamet-\nric nuisance functions. The second is the literature on the use of sample-splitting to relax\nentropy conditions. We provide links to each of these literatures in turn. The problem we study is obviously related to the classical semiparametric estimation\nframework which focuses on obtaining\n√\nN-consistent and asymptotically normal esti-\nmates for low-dimensional components with nuisance parameters estimated by conven-\ntional nonparametric estimators such as kernels or series. See, for example, the work by\nLevit (1975), Ibragimov and Hasminskii (1981), Bickel (1982), Robinson (1988), Newey\n(1990), van der Vaart (1991), Andrews (1994a), Newey (1994), Newey et al. (1998),\nRobins and Rotnitzky (1995), Linton (1996), Bickel et al. (1998), Chen et al.",
    "content_hash": "4a15d1854fc86c985897f6248aaae77039122983ee5c5b17b7fef24c2e219592",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "91590f31-fdfa-4fd5-a11c-e7dc11b07bee",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Importantly, its use\nhere is not simply as a device to make proofs elementary (which it does), but as a prac-\ntical method to allow us to overcome the overfitting/high-complexity phenomena that\ncommonly arise in data analysis based on highly adaptive ML methods. Our treatment\nbuilds upon the sample-splitting ideas employed in Belloni et al. (2010) and Belloni et al. (2012) who considered sample-splitting in a high-dimensional sparse optimal IV model to\nweaken the sparsity condition mentioned in the previous paragraph to s ≪n. This work\nin turn was inspired by Angrist and Krueger (1995). We also build on Ayyagari (2010)\nand Robins et al. (2013), where ML methods and sample splitting were used in the esti-\nmation of a partially linear model of the effects of pollution while controlling for several\ncovariates. We use the term “cross-fitting” to characterize our recommended procedure,\npartly borrowing the jargon from Fan et al. (2012) which employed a slightly different\nform of sample-splitting to estimate the scale parameter in a high-dimensional sparse\nregression. Of course, the use of sample-splitting to relax entropy conditions has a long",
    "content_hash": "2581700ec2d791442f5c69b0940bf93254da5597bc08473e4759456d4ca602d2",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4dab771c-7cdc-4d53-ba48-b70fd5d4cc2e",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "10\nCCDDHNR\nexample, Severini and Wong (1992). The introduction of the likelihood introduces major\nbenefits such as allowing simple and natural imposition of constraints inherent in the\ndata, such as support restrictions when the outcome is binary or censored, and permit-\nting the use of likelihood cross-validation to choose the nuisance parameter estimator. This data adaptive choice of the nuisance parameter has been dubbed the “super learner”\nby van der Laan et al. (2007). In subsequent work, van der Laan and Rose (2011) empha-\nsize the use of ML methods to estimate the nuisance parameters for use with the super\nlearner. Much of this work, including recent work such as Luedtke and van der Laan\n(2016), Toth and van der Laan (2016), and Zheng et al. (2016), focuses on formal results\nunder a Donsker condition, though the use of sample splitting to relax these conditions\nhas also been advocated in the targeted maximum likelihood setting as discussed below. The Donsker condition is a powerful classical condition that allows rich structures for\nfixed function classes G, but it is unfortunately unsuitable for high-dimensional settings. Examples of function classes where a Donsker condition holds include functions of a\nsingle variable that have total variation bounded by 1 and functions x 7→f(x) that have\nr > dim(x)/2 uniformly bounded derivatives. As a further example, functions composed\nfrom function classes with VC dimensions bounded by p through a fixed number of\nalgebraic and monotone transforms are Donsker.",
    "content_hash": "5bcc8f28aff5281ad169db87c9a8da73b96eb214ce12145b0fc7f5a1c0234606",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ae1ff894-bf69-4ec2-b213-0ea66100e9a5",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "However, this property will no longer\nhold if we let dim(x) grow to infinity with the sample size as this increase in dimension\nwould require that the VC dimension also increases with n. More generally, Donsker\nconditions are easily violated once dimensions get large. A major point of departure\nof the present work from the classical literature on semiparametric estimation is its\nexplicit focus on high-complexity/entropy cases. One way to analyze the problem of\nestimation in high-entropy cases is to see to what degree equicontinuity results continue\nto hold while allowing moderate growth of the complexity/entropy of GN. Examples of\npapers taking this approach in an approximately sparse settings are Belloni et al. (2017),\nBelloni et al. (2014), Belloni et al. (2016), Chernozhukov et al. (2015b), Javanmard\nand Montanari (2014a), van de Geer et al. (2014), and Zhang and Zhang (2014). In\nall of these examples, entropy growth must be limited in what may be very restrictive\nways. The entropy conditions rule out the contrived overfitting example mentioned above,\nwhich does approximate realistic examples, and may otherwise place severe restrictions\non the model. For example, in Belloni et al. (2010) and Belloni et al. (2012), the optimal\ninstrument needs to be sparse of order s ≪√n. A key device that we use to avoid strong entropy conditions is cross-fitting via sample\nsplitting. Cross-fitting is a practical, efficient form of data splitting.",
    "content_hash": "961cd1be89f193db66303a1c5ff5ca8f7877dd9bdb56bae26563b6c9377813cf",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "30b41dba-4467-496a-bb90-00d39c372774",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n11\nhistory in semiparametric estimation problems. For example, Bickel (1982) considered\nestimating nuisance functions using a vanishing fraction of the sample, and these results\nwere extended to sample splitting into two equal halves and discretization of the param-\neter space by Schick (1986). Similarly, van der Vaart (1998) uses 2-way sample splitting\nand discretization of the parameter space to give weak conditions for k-step estimators\nusing the efficient scores where sample splitting is used to estimate the “updates”; see\nalso Hubbard et al. (2016). Robins et al. (2008) and Robins et al. (2017) use sample\nsplitting in the construction of higher-order influence function corrections in semipara-\nmetric estimation. Some recent work in the targeted maximum likelihood literature, for\nexample Zheng and van der Laan (2011), also notes the utility of sample splitting in the\ncontext of k-step updating, though this sample splitting approach is different from the\ncross-fitting approach we pursue. Plan of the Paper. We organize the rest of the paper as follows. In Section 2, we\nformally define Neyman orthogonality and provide a brief discussion that synthesizes\nvarious models and frameworks that may be used to produce estimating equations sat-\nisfying this key condition. In Section 3, we carefully define DML estimators and develop\ntheir general theory.",
    "content_hash": "f57d2813cfa5a219bf4bb7eff859fb192456ae8e7dfea99d5f6a3c790a55076b",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "34e1545f-e1bd-4e5f-8f0e-f2cb2b517ebb",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "We then illustrate this general theory by applying it to provide\ntheoretical results for using DML to estimate and do inference for key parameters in the\npartially linear regression model and for using DML to estimate and do inference for\ncoefficients on endogenous variables in a partially linear instrumental variables model\nin Section 4. In Section 5, we provide a further illustration of the general theory by\napplying it to develop theoretical results for DML estimation and inference for average\ntreatment effects and average treatment effects on the treated under unconfoundedness\nand for DML estimation of local average treatment effects in an IV context within the\npotential outcomes framework; see Imbens and Rubin (2015). Finally, we apply DML in\nthree empirical illustrations in Section 6. In an appendix, we define additional notation\nand present proofs. Notation. The symbols P and E denote probability and expectation operators with\nrespect to a generic probability measure that describes the law of the data. If we need to\nsignify the dependence on a probability measure P, we use P as a subscript in PP and EP . We use capital letters, such as W, to denote random elements and use the corresponding\nlower case letters, such as w, to denote fixed values that these random elements can\ntake. In what follows, we use ∥· ∥P,q to denote the Lq(P) norm; for example, we denote\n∥f∥P,q := ∥f(W)∥P,q :=\nR\n|f(w)|qdP(w)\n\u00011/q , where ∥f∥P,∞stands for the essential\nsupremum. We use x′ to denote the transpose of a column vector x.",
    "content_hash": "ddebd39693b801f7d952c3be0f41873b06e7c1385de9640ef11b6dccb6dcbdfe",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4ca7daa8-48e2-4b79-babc-68574a34302f",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "For a differentiable\nmap x 7→f(x), mapping Rd to Rk, we use ∂x′f to abbreviate the partial derivatives\n(∂/∂x′)f, and we correspondingly use the expression ∂x′f(x0) to mean ∂x′f(x) |x=x0,\netc. 2. CONSTRUCTION OF NEYMAN ORTHOGONAL SCORE/MOMENT\nFUNCTIONS\nHere we formally introduce the model and discuss several methods for generating orthog-\nonal scores in a wide variety of settings, including the classical Neyman’s construction. We\nalso use this as an opportunity to synthesize some recent developments in the literature.",
    "content_hash": "df884d1054daa4ab63aa129fa26c74c096740194865024eae83046418de021a2",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a7324383-eb14-4eeb-a3af-90043c7a7004",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "12\nCCDDHNR\n2.1. Moment Condition/Estimating Equation Framework\nWe are interested in the true value θ0 of the low-dimensional target parameter θ ∈Θ,\nwhere Θ is a non-empty measurable subset of Rdθ. We assume that θ0 satisfies the\nmoment conditions\nEP [ψ(W; θ0, η0)] = 0,\n(2.1)\nwhere ψ = (ψ1, . . . , ψdθ)′ is a vector of known score functions, W is a random element\ntaking values in a measurable space (W, AW) with law determined by a probability\nmeasure P ∈PN, and η0 is the true value of the nuisance parameter η ∈T, where T is a\nconvex subset of some normed vector space with the norm denoted by ∥· ∥T . We assume\nthat the score functions ψj : W ×Θ×T →R are measurable once we equip Θ and T with\ntheir Borel σ-fields, and we assume that a random sample (Wi)N\ni=1 from the distribution\nof W is available for estimation and inference. As discussed in the Introduction, we require the Neyman orthogonality condition for\nthe score ψ. To introduce the condition, for eT = {η −η0 : η ∈T} we define the pathwise\n(or the Gateaux) derivative map Dr : eT →Rdθ,\nDr[η −η0] := ∂r\n\u001a\nEP\nh\nψ(W; θ0, η0 + r(η −η0)\ni\u001b\n,\nη ∈T,\nfor all r ∈[0, 1), which we assume to exist.",
    "content_hash": "01c70190ee9c900ccc001c164cc37387db67f80942c355167629305f56609fa5",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e62b8c36-dc67-41e5-9cbd-9f5beceadfa6",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": ", ψdθ)′ obeys the\northogonality condition at (θ0, η0) with respect to the nuisance realization set TN ⊂T if\n(2.1) holds and the pathwise derivative map Dr[η −η0] exists for all r ∈[0, 1) and η ∈TN\nand vanishes at r = 0; namely,\n∂ηEP ψ(W; θ0, η0)[η −η0] = 0,\nfor all η ∈TN. (2.3)\nWe remark here that condition (2.3) holds with TN = T when η is a finite-dimensional\nvector as long as ∂ηEP [ψj(W; θ0, η0)] = 0 for all j = 1, . . . , dθ, where ∂ηEP [ψj(W; θ0, η0)]\ndenotes the vector of partial derivatives of the function η 7→EP [ψj(W; θ0, η)] for η = η0. Sometimes it will also be helpful to use an approximate Neyman orthogonality condi-\ntion as opposed to the exact one given in Definition 2.1:\nDefinition 2.2. (Neyman Near-Orthogonality) The score ψ = (ψ1, . . . , ψdθ)′ obeys\nthe λN near-orthogonality condition at (θ0, η0) with respect to the nuisance realization set",
    "content_hash": "3f67ac36814108304975f79c80c2702fdcb6225dc02a4f1e5caabca461bd8e20",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fff079b7-12c1-4b75-99d6-ea7d9e7912ca",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Now consider the new score function,\nwhich we refer to as the Neyman orthogonal score,\nψ(W; θ, η) = ∂θℓ(W; θ, β) −µ∂βℓ(W; θ, β),\n(2.7)\nwhere the nuisance parameter is\nη = (β′, vec(µ)′)′ ∈T = B × Rdθdβ ⊂Rp,\np = dβ + dθdβ,\nand µ is the dθ × dβ orthogonalization parameter matrix whose true value µ0 solves the\nequation\nJθβ −µJββ = 0\n(2.8)\nfor\nJ =\n\u0012 Jθθ\nJθβ\nJβθ\nJββ\n\u0013\n= ∂(θ′,β′)EP\nh\n∂(θ′,β′)′ℓ(W; θ, β)\ni\nθ=θ0; β=β0. 7The C(α)-statistic, or the orthogonal score statistic, has been explicitly used for testing and estimation\nin high-dimensional sparse models in Belloni et al. (2015).",
    "content_hash": "f4bcb119c9d6d7537680ab078fe6f8b7b2f7564cbfb3d807ed4b17d76c605747",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "061acf76-f079-4348-ac02-a9cd091bc91d",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n13\nTN ⊂T if (2.1) holds and the pathwise derivative map Dr[η −η0] exists for all r ∈[0, 1)\nand η ∈TN and is small at r = 0; namely,\n∂ηEP ψ(W; θ0, η0)[η −η0]\n⩽λN,\nfor all η ∈TN,\n(2.4)\nwhere {λN}N⩾1 is a sequence of positive constants such that λN = o(N −1/2). 2.2. Construction of Neyman Orthogonal Scores\nIf we start with a score φ that does not satisfy the orthogonality condition above, we\nfirst transform it into a score ψ that does. Here we outline several methods for doing so. 2.2.1. Neyman Orthogonal Scores for Likelihood and Other M-Estimation Problems with\nFinite-Dimensional Nuisance Parameters\nFirst, we describe the construction used by Neyman (1959) to derive his celebrated\northogonal score and C(α)-statistic in a maximum likelihood setting.7 Such construction\nalso underlies the concept of local unbiasedness in construction of optimal tests in e.g. Ferguson (1967) and was extended to non-likelihood settings by Wooldridge (1991). The\ndiscussion of Neyman’s construction here draws on Chernozhukov et al. (2015a). To describe the construction, let θ ∈Θ ⊂Rdθ and β ∈B ⊂Rdβ, where B is a convex\nset, be the target and the nuisance parameters, respectively.",
    "content_hash": "4401bf99ca4742b881ad123da826d75512f99cbdeaccd22ed3694d515ff69f06",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0545f738-802b-46ca-b00b-7af5ba900548",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Further, suppose that the\ntrue parameter values θ0 and β0 solve the optimization problem\nmax\nθ∈Θ, β∈B EP [ℓ(W; θ, β)],\n(2.5)\nwhere ℓ(W; θ, β) is a known criterion function. For example, ℓ(W; θ, β) can be the log-\nlikelihood function associated to observation W. More generally, we refer to ℓ(W; θ, β)\nas the quasi-log-likelihood function. Then, under mild regularity conditions, θ0 and β0\nsatisfy\nEP [∂θℓ(W; θ0, β0)] = 0,\nEP [∂βℓ(W; θ0, β0)] = 0. (2.6)\nNote that the original score function φ(W; θ, β) = ∂θℓ(W; θ, β) for estimating θ0 will\nnot generally satisfy the orthogonality condition.",
    "content_hash": "b839e2c644a5d89dcc26ce2a0cfd9681dd16cb57cc54dade3c1796743f674083",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ff973e3f-1159-4a43-b409-507df6bf7282",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "14\nCCDDHNR\nThe true value of the nuisance parameter η is\nη0 = (β′\n0, vec(µ0)′)′;\n(2.9)\nand when Jββ is invertible, (2.8) has the unique solution,\nµ0 = JθβJ−1\nββ . (2.10)\nThe following lemma shows that the score ψ in (2.7) satisfies the Neyman orthogonality\ncondition. Lemma 2.1. (Neyman Orthogonal Scores for Quasi-Likelihood Settings) If (2.6)\nholds, J exists, and Jββ is invertible, the score ψ in (2.7) is Neyman orthogonal at (θ0, η0)\nwith respect to the nuisance realization set TN = T. Remark 2.1. (Additional nuisance parameters) Note that the orthogonal score ψ in\n(2.7) has nuisance parameters consisting of the elements of µ in addition to the elements\nof β, and Lemma 2.1 shows that Neyman orthogonality holds both with respect to β\nand with respect to µ. We will find that Neyman orthogonal scores in other settings,\nincluding infinite-dimensional ones, have a similar property. Remark 2.2. (Efficiency) Note that in this example, µ0 not only creates the necessary\northogonality but also creates the efficient score for inference on the target parameter\nθ when the quasi-log-likelihood function is the true (possibly conditional) log-likelihood,\nas demonstrated by Neyman (1959). Example 2.1.",
    "content_hash": "d7ba32a5fea1de456b36ee120f398fab0502c6af6087a410264559f5017af514",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3267f2ea-69c2-4b05-9cf4-b0d839690716",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(High-Dimensional Linear Regression) As an application of the con-\nstruction above, consider the following linear predictive model:\nY = Dθ0 + X′β0 + U,\nEP [U(X′, D)′] = 0,\n(2.11)\nD = X′γ0 + V,\nEP [V X] = 0,\n(2.12)\nwhere for simplicity we assume that θ0 is a scalar. The first equation here is the main\npredictive model, and the second equation only plays a role in the construction of the\nNeyman orthogonal scores. It is well-known that θ0 and β0 in this model solve the opti-\nmization problem (2.5) with\nℓ(W; θ, β) = −(Y −Dθ −X′β)2\n2\n,\nθ ∈Θ = R, β ∈B = Rdβ,\nwhere we denoted W = (Y, D, X′)′. Hence, equations (2.6) hold with\n∂ℓθ(W; θ, β) = (Y −Dθ −X′β)D,\n∂ℓβ(W; θ, β) = (Y −Dθ −X′β)X,\nand the matrix J satisfies\nJθβ = −EP [DX′],\nJββ = −EP [XX′].",
    "content_hash": "8b707c91733cda6c03e8025d3cb1d07446f3b0644a70bb076bcd491ddbaabb4f",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c30a6ae7-ea64-45da-a567-f9f5aff7df59",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "The Neyman orthogonal score is then given by\nψ(W; θ, η) = (Y −Dθ −X′β)(D −µX);\nη = (β′, vec(µ)′)′;\nψ(W; θ0, η0) = U(D −µ0X);\nµ0 = EP [DX′](EP [XX′])−1 = γ′\n0. (2.13)\nIf the vector of covariates X here is high-dimensional but the vectors of parameters β0",
    "content_hash": "71285353070682f7ee47e0a8f5374cdd09e2293044403f338ef6643cd55d314f",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "09acc15e-24f6-4bb9-99f1-9277ded2deab",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n15\nand γ0 are approximately sparse, we can use ℓ1-penalized least squares, ℓ2-boosting, or\nforward selection methods to estimate β0 and γ0 = µ′\n0, and hence µ0 = (β′\n0, vec(µ0)′)′;\nsee references cited in the Introduction. ■\nIf Jββ is not invertible, equation (2.8) typically has multiple solutions. In this case, it\nis convenient to focus on a minimal norm solution,\nµ0 = arg min ∥µ∥such that ∥Jθβ −µJββ∥q = 0\nfor a suitably chosen norm ∥·∥q on the space of dθ ×dβ matrices. With an eye on solving\nthe empirical version of this problem, we may also consider the relaxed version of this\nproblem,\nµ0 = arg min ∥µ∥such that ∥Jθβ −µJββ∥q ⩽rN\n(2.14)\nfor some rN > 0 such that rN →0 as N →∞. This relaxation is also helpful when Jββ is\ninvertible but ill-conditioned. The following lemma shows that using µ0 in (2.14) leads to\nNeyman near-orthogonal scores. The proof of this lemma can be found in the Appendix. Lemma 2.2.",
    "content_hash": "956a9121ce93043e4f8d0d7a141277bf667cc4c20a8007990ebcb1571c837da8",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "23256d4a-a7ab-4e5e-91e1-d4feccd24e37",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(Neyman Near-Orthogonal Scores for Quasi-Likelihood Settings)\nIf (2.6) holds, J exists, the solution of the optimization problem (2.14) exists, and µ0 is\ntaken to be this solution, the score ψ defined in (2.7) is Neyman λN near-orthogonal at\n(θ0, η0) with respect to the nuisance realization set TN = {β ∈B: ∥β −β0∥∗\nq ⩽λN/rN} ×\nRdθdβ, where the norm ∥· ∥∗\nq on Rdβ is defined by ∥β∥∗\nq = supA ∥Aβ∥with the supremum\nbeing taken over all dθ × dβ matrices A such that ∥A∥q ⩽1. Example 2.1. (High-Dimensional Linear Regression, Continued) In the high-\ndimensional linear regression example above, the relaxation (2.14) is helpful when Jββ =\nEP [XX′] is ill-conditioned. Specifically, if one suspects that EP [XX′] is ill-conditioned,\none can define µ0 as the solution to the following optimization problem:\nmin ∥µ∥such that ∥EP [DX′] −µEP [XX′]∥∞⩽rN. (2.15)\nLemma 2.2 above then shows that using this µ0 leads to a score ψ that obeys the Ney-\nman near-orthogonality condition. Alternatively, one can define µ0 as the solution of the\nfollowing closely related optimization problem,\nmin\nµ\n\u0010\nµEP [XX′]µ′ −µEP [DX] + rN∥µ∥1\n\u0011\n,\nwhose solution also obeys ∥EP [DX] −µEP [XX′]∥∞⩽rN which follows from the first\norder conditions.",
    "content_hash": "e01ef6ade7dab24365965d994c9eb1efe415c5c82a61c0c9c81f6c12bf09d656",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a1e802eb-2ba9-4061-b945-d24bb7c1f6ab",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "An empirical version of either problem leads to a Lasso-type estimator\nof the regularized solution µ0; see Javanmard and Montanari (2014a). ■\nRemark 2.3. (Giving up Efficiency) Note that the regularized µ0 in (2.14) creates the\nnecessary near-orthogonality at the cost of giving up somewhat on efficiency of the\nscore ψ. At the same time, regularization may generate additional robustness gains since\nachieving full efficiency by estimating µ0 in (2.10) may require stronger conditions. Remark 2.4. (Concentrating-out Approach) The approach for constructing Neyman\northogonal scores described above is closely related to the following concentrating-out\napproach which has been used, for example, in Newey (1994), to show Neyman orthogo-\nnality when β is infinite dimensional. For all θ ∈Θ, let βθ be the solution of the following",
    "content_hash": "46e865c3529e3a26bfd9bf6b4c243cefb7569b3ae869e5432805608c8e6991e7",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9e038dba-7d59-46f0-9405-1cfb1463d815",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "In this case, a Neyman orthogonal score function is\nψ(W; θ, η) = µm(W; θ, β),\n(2.18)\nwhere the nuisance parameter is\nη = (β′, vec(µ)′)′ ∈T = B × Rdθdm ⊂Rp,\np = dβ + dθdm,",
    "content_hash": "6f7a66077c65738ab99c35d5258e42837209ff09b799bfe774484f1382891f8b",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "dbfecbd6-23a3-41e4-85e0-eca2bf8003af",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Thus, maximiz-\ning the expected objective function with respect to the nuisance parameters, plugging\nthat maximum back in, and differentiating with respect to the parameters of interest\nproduces an orthogonal moment condition. See also Section 2.2.3. 2.2.2. Neyman Orthogonal Scores in GMM Problems\nThe construction in the previous section gives a Neyman orthogonal score whenever\nthe moment conditions (2.6) hold, and, as discussed in Remark 2.2, the resulting score\nis efficient as long as ℓ(W; θ, β) is the log-likelihood function. The question, however,\nremains about constructing the efficient score when ℓ(W; θ, β) is not necessarily a log-\nlikelihood function. In this section, we answer this question and describe a GMM-based\nmethod of constructing an efficient and Neyman orthogonal score in this more general\ncase. The discussion here is related to Lee (2005), Bera et al. (2010), and Chernozhukov\net al. (2015b). Since GMM does not require that the equations (2.6) are obtained from the first-order\nconditions of the optimization problem (2.5), we use a different notation for the moment\nconditions. Specifically, we consider parameters θ ∈Θ ⊂Rdθ and β ∈B ⊂Rdβ, where B\nis a convex set, whose true values, θ0 and β0, solve the moment conditions\nEP [m(W; θ0, β0)] = 0,\n(2.17)\nwhere m: W × Θ × B →Rdm is a known vector-valued function, and dm ⩾dθ + dβ is the\nnumber of moment conditions.",
    "content_hash": "ee61c3fe180934681b2cf45482a1cf794c091b88c0506549cd9a8d6c671a3b0f",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ad0042fb-6cec-479c-a55f-ef1ae21ca8f3",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n17\nand µ is the dθ × dm orthogonalization parameter matrix whose true value is\nµ0 =\n\u0010\nA′Ω−1 −A′Ω−1Gβ(G′\nβΩ−1Gβ)−1G′\nβΩ−1\u0011\n,\nwhere\nGγ = ∂γ′EP [m(W; θ, β)]\nγ=γ0\n=\nh\n∂θ′EP [m(W; θ, β)], ∂β′EP [m(W; θ, β)]\ni\nγ=γ0 =:\nh\nGθ, Gβ\ni\n,\nfor γ = (θ′, β′)′ and γ0 = (θ′\n0, β′\n0)′, A is a dm × dθ moment selection matrix, Ωis a\ndm × dm positive definite weighting matrix, and both A and Ωcan be chosen arbitrarily. Note that setting\nA = Gθ and Ω= VarP (m(W; θ0, β0)]) = EP\nh\nm(W; θ0, β0)m(W; θ0, β0)′i\nleads to the efficient score in the sense of yielding an estimator of θ0 having the smallest\nvariance in the class of GMM estimators (Hansen, 1982), and, in fact, to the semi-\nparametrically efficient score; see Levit (1975), Nevelson (1977), and Chamberlain (1987). Let η0 = (β′\n0, vec(µ0)′)′ be the true value of the nuisance parameter η = (β′, vec(µ)′)′.",
    "content_hash": "a28d3a09d8b75025f9a27e2e197eb3a1a21e853b35a165e15751b3fd46055c97",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "12cbb786-09fd-4843-bcff-759069119f99",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(Neyman Near-Orthogonal Scores for GMM settings) In the set-up\nabove, with γ0 denoting the solution of (2.19), we have for µ0 := A′Ω−1 −γ0L′Ω−1/2\nand η0 = (β′\n0, vec(µ0)′)′ that ψ defined in (2.18) is the Neyman λN near-orthogonal\nscore at (θ0, η0) with respect to the nuisance realization set TN = {β ∈B: ∥β −β0∥1 ⩽\nλN/rN} × Rdθdm. 2.2.3. Neyman Orthogonal Scores for Likelihood and Other M-Estimation Problems with\nInfinite-Dimensional Nuisance Parameters",
    "content_hash": "9b75aa5f5495f8f713864c253014e27d69ee0a199fed73405ea1e112e784915f",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f6525df0-53de-4071-96af-e6e1332f4d77",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Here, the symbol d/dθ denotes the full derivative with respect to θ, so that we differentiate\nwith respect to both θ arguments in ℓ(W; θ, η(θ)). The following lemma shows that the\nscore ψ in (2.21) satisfies the Neyman orthogonality condition. Lemma 2.5. (Neyman Orthogonal Scores via Concentrating-Out Approach)\nSuppose that (2.5) holds, and let T be a convex set of functions mapping Θ into B\nsuch that η0 ∈T. Also, suppose that for each η ∈T, the function θ 7→ℓ(W; θ, η(θ))\nis continuously differentiable almost surely. Then, under mild regularity conditions, the\nscore ψ in (2.21) is Neyman orthogonal at (θ0, η0) with respect to the nuisance realization\nset TN = T. As an example, consider the partially linear model from the Introduction. Let\nℓ(W; θ, β) = −1\n2(Y −Dθ −β(X))2,\nand let B be the set of functions of X with finite mean square. Then\n(θ0, β0) = arg\nmax\nθ∈Θ,β∈B EP [ℓ(W; θ, β)]\nand\nβθ(X) = EP [Y −Dθ|X],\nθ ∈Θ.",
    "content_hash": "f348dd8e652c348a018e75635e28fbadaaf20bd1fa5388fc39182b5915b471df",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cf993360-b98f-4b64-97c8-59cb474049ff",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "18\nCCDDHNR\nHere we show that the concentrating-out approach described in Remark 2.4 for the\ncase of finite-dimensional nuisance parameters can be extended to the case of infinite-\ndimensional nuisance parameters. Let ℓ(W; θ, β) be a known criterion function, where θ\nand β are the target and the nuisance parameters taking values in Θ and B, respectively\nand assume that the true values of these parameters, θ0 and β0, solve the optimization\nproblem (2.5). The function ℓ(W; θ, β) is analogous to that discussed above but now,\ninstead of assuming that B is a (convex) subset of a finite-dimensional space, we assume\nthat B is some (convex) set of functions, so that β is the functional nuisance parameter. For example, ℓ(W; θ, β) could be a semiparametric log-likelihood where β is the non-\nparametric part of the model. More generally, ℓ(W; θ, β) could be some other criterion\nfunction such as the negative of a squared residual. Also let\nβθ = arg max\nβ∈B EP [ℓ(W; θ, β)]\n(2.20)\nbe the “concentrated-out” nonparametric part of the model. Note that βθ is a function-\nvalued function. Now consider the score function\nψ(W; θ, η) = dℓ(W; θ, η(θ))\ndθ\n,\n(2.21)\nwhere the nuisance parameter is η: Θ →B, and its true value η0 is given by\nη0(θ) = βθ,\nfor all θ ∈Θ.",
    "content_hash": "53712ffd08a8e8dfb87301a8a537558020ba64c5421ce8a71f2f942fd5be74c6",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "98d10d38-b19c-4395-946d-7bdc953b6aa4",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Hence, (2.21) gives the following Neyman orthogonal score:\nψ(W; θ, βθ) = −1\n2\nd{Y −Dθ −EP [Y −Dθ|X]}2\ndθ\n= (D −EP [D|X]) × (Y −EP [Y |X] −(D −EP [D|X])θ)\n= (D −m0(X)) × (Y −Dθ −g0(X)),",
    "content_hash": "27cb5b3f7f625587c5eb2741ef68a0b7bcd5f7bb93976b5fabd354ebf8a82ba5",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4af86bc9-984e-4c56-a324-9c57aaab6769",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Now, consider what\nwould happen if we replaced B, which is the set of functions of X with finite mean square,\nby the set of functions Υ that is the mean square closure of functions that are additive\nin X1 and X2:\nΥ = {h(X1) + h(X2)}. Let ¯EP denote the least squares projection on Υ. Then, applying the previous calculation\nwith ¯EP replacing EP gives\nψ(W; θ, βθ) = (D −¯EP [D|X]) × (Y −¯EP [Y |X] + (D −¯EP [D|X])θ),\nwhich provides an orthogonal score based on additive function of X1 and X2. Here, it\nis important to note that the solution to EP [ψ(W, θ, βθ)] = 0 will be the true θ0 only\nwhen the true function of X in the partially linear model is additive. More generally,\nthe solution of the moment condition would be the coefficient of D in the least squares\nprojection of Y on functions of the form Dθ + h1(X1) + h1(X2). Note though that the\ncorresponding score is orthogonal by virtue of additivity being imposed in the estimation\nof ¯EP [Y |X] and ¯EP [D|X]. 2.2.4. Neyman Orthogonal Scores for Conditional Moment Restriction Problems with\nInfinite-Dimensional Nuisance Parameters\nNext we consider the conditional moment restrictions framework studied in Chamber-\nlain (1992). To define the framework, let W, R, and Z be random vectors taking values\nin W ⊂Rdw, R ⊂Rdr, and Z ⊂Rdz, respectively.",
    "content_hash": "bd726ca9f9b4330e16cd15452961a98a705ab5c015f0b6e9742904e32852b73b",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c8e0317c-ad2c-4cc4-9644-07fe7ea9d55f",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Assume that Z is a sub-vector of R\nand R is a sub-vector of W, so that dz ⩽dr ⩽dw. Also, let θ ∈Θ ⊂Rdθ be a finite-\ndimensional parameter whose true value θ0 is of interest, and let h be a vector-valued\nfunctional nuisance parameter taking values in a convex set of functions H mapping Z to\nRdh, with the true value of h being h0. The conditional moment restrictions framework",
    "content_hash": "03d3c348e64456d698bc4b705dd713a6ca8f378d71179eebe20d6b3917016e8f",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e054f9d6-87ba-4c17-a5b6-0a536f0a1fa4",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(Neyman Orthogonal Scores for Conditional Moment Settings)\nSuppose that (a) (2.22) holds, (b) the matrices EP [∥Γ(R)∥4], EP [∥G(Z)∥4], EP [∥A(R)∥2],\nand EP [∥Ω(R)∥−2] are finite, and (c) for all h ∈H, there exists a constant Ch > 0 such\nthat PP (EP [∥m(W; θ0, h(Z))∥| R] ⩽Ch) = 1. Then the score ψ in (2.26) is Neyman\northogonal at (θ0, η0) with respect to the nuisance realization set TN = T. As an application of the conditional moment restrictions framework, let us derive\nNeyman orthogonal scores in the partially linear regression example using this framework. The partially linear regression model (1.1) is equivalent to\nEP [Y −Dθ0 −g0(X) | X, D] = 0,",
    "content_hash": "8bb9da770b79947abc0f6c1ef4222b1837f38899ff5cfaad36acfadee761bf58",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d6c84e73-9851-417c-a752-9e860dfca2b4",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "20\nCCDDHNR\nassumes that θ0 and h0 satisfy the moment conditions\nEP [m(W; θ0, h0(Z)) | R] = 0,\n(2.22)\nwhere m: W × Θ × Rdh →Rdm is a known vector-valued function. This framework is\nof interest because it covers a rich variety of models without having to explicitly rely on\nthe likelihood formulation. To build a Neyman orthogonal score ψ(W; θ, η) for estimating θ0, consider the matrix-\nvalued functional parameter µ: R →Rdθ×dm whose true value is given by\nµ0(R) = A(R)′Ω(R)−1 −G(Z)Γ(R)′Ω(R)−1,\n(2.23)\nwhere the moment selection matrix-valued function A: R →Rdm×dθ and the weighting\npositive definite matrix-valued function Ω: R →Rdm×dm can be chosen arbitrarily, and\nthe matrix-valued functions Γ: R →Rdm×dθ and G: Z →Rdθ×dm are given by\nΓ(R) = ∂v′EP\nh\nm(W; θ0, v) | R\ni\nv=h0(Z), and\n(2.24)\nG(Z) = EP\nh\nA(R)′Ω(R)−1Γ(R) | Z\ni\n×\n\u0010\nEP [Γ(R)′Ω(R)−1Γ(R) | Z]\n\u0011−1\n.",
    "content_hash": "dff8cdcadad3c5258b519246951d7708bb41a84c18416d8cd04f58a0886ebe30",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "35728b85-95e8-43ad-84d4-478f2b7420d6",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(2.25)\nNote that µ0 in (2.23) is well-defined even though the right-hand side of (2.23) contains\nboth R and Z since Z is a sub-vector of R. Then a Neyman orthogonal score is\nψ(W; θ, η) = µ(R)m(W; θ, h(Z)),\n(2.26)\nwhere the nuisance parameter is\nη = (µ, h) ∈T = L1(R; Rdθ×dm) × H. Here, L1(R; Rdθ×dm) is the vector space of matrix-valued functions f : R →Rdθ×dm\nsatisfying EP [∥f(R)∥] < ∞. Also, note that even though the matrix-valued functions A\nand Ωcan be chosen arbitrarily, setting\nA(R) = ∂θ′EP\nh\nm(W; θ, h0(Z)) | R\ni\nθ=θ0 and\n(2.27)\nΩ(R) = EP\nh\nm(W; θ0, h0(Z))m(W; θ0, h0(Z))′ | R\ni\n(2.28)\nleads to an asymptotic variance equal to the semiparametric bound of Chamberlain\n(1992). Let η0 = (µ0, h0) be the true value of the nuisance parameter η = (µ, h). The\nfollowing lemma shows that the score ψ in (2.26) satisfies the Neyman orthogonality\ncondition. Lemma 2.6.",
    "content_hash": "63fd508ce2470f38425fdcd9b0d4bd19f9899b1b3a0f83988b6eb2ec2e9f78d7",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "540e238e-a2e3-43d3-86e0-b4c06b14abed",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n21\nwhich can be written in the form of the conditional moment restrictions framework\n(2.22) with W = (Y, D, X′)′, R = (D, X′)′, Z = X, h(Z) = g(X), and m(W; θ, v) =\nY −Dθ −v. Hence, using (2.27) and (2.28) and denoting σ(D, X)2 = EP [U 2 | D, X] for\nU = Y −Dθ0 −g0(X), we can take\nA(R) = −D,\nΩ(R) = EP [U 2 | D, X] = σ(D, X)2. With this choice of A(R) and Ω(R), we have\nΓ(R) = −1,\nG(Z) =\n\u0010\nEP\nh\nD\nσ(D, X)2 | X\ni\u0011\n×\n\u0010\nEP\nh\n1\nσ(D, X)2 | X\ni\u0011−1\n,\nand so (2.23) and (2.26) give\nψ(W; θ, η0)\n=\n1\nσ(D, X)2\n\u0010\nD −EP\nh\nD\nσ(D, X)2 | X\ni. EP\nh\n1\nσ(D, X)2 | X\ni\u0011\n×\n\u0010\nY −Dθ −g0(X)\n\u0011\n. By construction, the score ψ above is efficient and Neyman orthogonal. Note, however,\nthat using this score would require estimating the heteroscedasticity function σ(D, X)2\nwhich would requires the imposition of some additional smoothness assumptions over\nthis conditional variance function.",
    "content_hash": "d9937ff535bd6f7ca1bff51bf585b560c3c92dbf7e60750c0eb07b7850cc9c55",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9c6ef3e4-34c7-4195-bf2e-e5b6c684897b",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Instead, if are willing to give up on efficiency to gain\nsome robustness, we can take\nA(R) = −D,\nΩ(R) = 1;\nin which case we have\nΓ(R) = −1,\nG(Z) = EP [D | X]. (2.23) and (2.26) then give\nψ(W; θ, η0) = (D −EP [D | X]) × (Y −Dθ −g0(X))\n= (D −m0(X)) × (Y −Dθ −g0(X)). This score ψ is Neyman orthogonal and corresponds to the estimator of θ0 described in\nthe Introduction in (1.5). Note, however, that this score ψ is efficient only if σ(X, D) is\na constant. 2.2.5. Neyman Orthogonal Scores and Influence Functions\nNeyman orthogonality is a joint property of the score ψ(W; θ, η), the true parameter\nvalue η0, the parameter set T, and the distribution of W. It is not determined by any\nparticular model for the parameter θ. Nevertheless, it is possible to use semiparametric\nefficiency calculations to construct the orthogonal score from the original score as in\nChernozhukov et al. (2016). Specifically, an orthogonal score can be constructed by adding\nto the original score the influence function adjustment for estimation of the nuisance\nfunctions that is analyzed in Newey (1994). The resulting orthogonal score will be the\ninfluence function of the limit of the average of the original score.",
    "content_hash": "88ec78b6544f07fdf081cd7334e825b1f294978ded9f49bf5d5535f29e5ec58e",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6e5f3ba4-4049-41d5-8699-a3288435e0f9",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "To explain, consider the original score φ(W; θ, β), where β is some function, and let\nbβ0 be a nonparametric estimator of β0, the true value of β. Here, β is implicitly allowed\nto depend on θ, though we suppress that dependence for notational convenience. The",
    "content_hash": "fd77df645cbbd036950b74759c5990d110c6190e8aa7d941129b957ae1489282",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a7772f72-3d39-4cf7-85fe-6c92d98d8fbc",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(1998,\n2004) showed that n−1/2 Pn\ni=1 ψ(Wi; θ0, bη0) from equation (2.30) will have a second order\nremainder in bη0, which is the key asymptotic property of orthogonal scores. Orthogonality\nof influence functions in semiparametric models follows from van der Vaart (1991), as\nshown for higher order counterparts in Robins et al. (2008, 2017). Chernozhukov et al. (2016) point out that in general an orthogonal score can be constructed from an original\nscore and nonparametric estimator bβ0 by adding to the original score the adjustment term\nfor estimation of β0 as described above. This construction provides a way of obtaining\nan orthogonal score from any initial score φ(W; θ, β) and nonparametric estimator bβ0.",
    "content_hash": "9f9b119921ebd7dc4a3bb3de57d552628be151fd183b24000ae1f5415a218edd",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d763f595-d3a6-469f-b816-9e8556fbe090",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "22\nCCDDHNR\ncorresponding orthogonal score can be formed when there is ϕ(W; θ, η) such that\nZ\nφ(w; θ0, bβ0)dP(w) = 1\nn\nn\nX\ni=1\nϕ(Wi; θ0, η0) + oP (n−1/2),\n(2.29)\nwhere η is a vector of nuisance functions that includes β. ϕ(W; θ, η) is an adjustment\nfor the presence of the estimated function bβ0 in the original score φ(W; θ, β). The de-\ncomposition (2.29) typically holds when bβ is either a kernel or a series estimator with a\nsuitably chosen tuning parameter. The Neyman orthogonal score is given by\nψ(W; θ, η) = φ(W; θ, β) + ϕ(W; θ, η). (2.30)\nHere ψ(W; θ0, η0) is the influence function of the limit of n−1 Pn\ni=1 φ(Wi; θ0, bβ0), as\nanalyzed in Newey (1994), with the restriction EP [ψ(W; θ0, η0)] = 0 identifying θ0. The form of the adjustment term ϕ(W; θ, η) depends on the estimator bβ0 and, of course,\non the form of φ(W; θ, β). Such adjustment terms have been derived for various bβ0 by\nNewey (1994). Also Ichimura and Newey (2015) show how the adjustment term can be\ncomputed from the limit of a certain derivative.",
    "content_hash": "a7d3c4bbc4e75f361cdcc6fb59aaeee799c10368c0bc2a25f9c8204e3a3fb303",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e46effc5-0b4f-459e-b3fd-c1eca3186cd7",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Any of these results can be applied to a\nparticular starting score φ(W; θ, β) and estimator bβ0 to obtain an orthogonal score. For example, consider again the partially linear model with the original score\nφ(W; θ, β) = D(Y −Dθ −g0(X)). Here bβ0 = bg0 is a nonparametric regression estimator. From Newey (1994), we know\nthat we obtain the influence function adjustment by taking the conditional expectation\nof the derivative of the score with respect to g0(x) (obtaining −m0(X) = −EP [D|X])\nand multiplying the result by the nonparametric residual to obtain\nϕ(W, θ, η) = −m0(X){Y −Dθ −β(X, θ)}. The corresponding orthogonal score is then simply\nψ(W; θ, η) = {D −m0(X)}{Y −Dθ −β(X, θ)},\nβ0(X, θ) = EP [Y −Dθ|X],\nm0(X) = EP [D|X],\nillustrating that an orthogonal score for the partially linear model can be derived from\nan influence function adjustment. Influence functions have been used to estimate functionals of nonparametric estimators\nby Hasminskii and Ibragimov (1978) and Bickel and Ritov (1988). Newey et al.",
    "content_hash": "e4228ef8836a122e083dd773a98259710203a7854088a4d61430e84ad7c78e9a",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "afcc4617-afe0-43de-a188-dde6b8d42102",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n23\n3. DML: POST-REGULARIZED INFERENCE BASED ON\nNEYMAN-ORTHOGONAL ESTIMATING EQUATIONS\n3.1. Definition of DML and Its Basic Properties\nWe assume that we have a sample (Wi)N\ni=1, modeled as i.i.d. copies of W, whose law is\ndetermined by the probability measure P on W. Estimation will be carried out using the\nfinite-sample analog of the estimating equations (2.1). We assume that the true value η0 of the nuisance parameter η can be estimated by bη0\nusing a part of the data (Wi)N\ni=1. Different structured assumptions on η0 allow us to use\ndifferent machine-learning tools for estimating η0. For instance,\n1. approximate sparsity for η0 with respect to some dictionary calls for the use of\nforward selection, lasso, post-lasso, ℓ2-boosting, or some other sparsity-based tech-\nnique;\n2. well-approximability of η0 by trees calls for the use of regression trees and random\nforests;\n3. well-approximability of η0 by sparse neural and deep neural nets calls for the use\nof ℓ1-penalized neural and deep neural networks;\n4. well-approximability of η0 by at least one model mentioned in 1)-3) above calls for\nthe use of an ensemble/aggregated method over the estimation methods mentioned\nin 1)-3). There are performance guarantees for most of these ML methods that make it possible\nto satisfy the conditions stated below. Ensemble and aggregation methods ensure that\nthe performance guarantee is approximately no worse than the performance of the best\nmethod.",
    "content_hash": "22e58894f6e89c2154194c605bb9691f8216ffa9773f588d93ffca1a9f7e2b67",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "461763e8-605f-4eab-a286-2eb927f385cb",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "We assume that N is divisible by K in order to simplify the notation. The following\nalgorithm defines the simple cross-fitted DML as outlined in the Introduction. Definition 3.1. (DML1) 1) Take a K-fold random partition (Ik)K\nk=1 of observation\nindices [N] = {1, ..., N} such that the size of each fold Ik is n = N/K. Also, for each\nk ∈[K] = {1, . . . , K}, define Ic\nk := {1, ..., N} \\ Ik. 2) For each k ∈[K], construct a ML\nestimator\nbη0,k = bη0((Wi)i∈Ic\nk)\nof η0, where bη0,k is a random element in T, and where randomness depends only on the\nsubset of data indexed by Ic\nk. 3) For each k ∈[K], construct the estimator ˇθ0,k as the\nsolution of the following equation:\nEn,k[ψ(W; ˇθ0,k, bη0,k] = 0,\n(3.1)\nwhere ψ is the Neyman orthogonal score, and En,k is the empirical expectation over the\nk-th fold of the data; that is, En,k[ψ(W)] = n−1 P\ni∈Ik ψ(Wi).",
    "content_hash": "a2855468b7cce231d92bed101f21466f8da2d93df218d2e427b550aa24254c2d",
    "location": null,
    "page_start": 23,
    "page_end": 24,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d99d83f6-b515-408f-97be-1bd8fc6e15d3",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "If achievement of exact 0\nis not possible, define the estimator ˇθ0,k of θ0 as an approximate ϵN-solution:\nEn,k[ψ(W; ˇθ0,k, bη0,k)]\n⩽inf\nθ∈Θ\nEn,k[ψ(W; θ, bη0,k)]\n+ ϵN,\nϵN = o(δNN −1/2),\n(3.2)\nwhere (δN)N⩾1 is some sequence of positive constants converging to zero. 4) Aggregate",
    "content_hash": "d0955914a3a0247be961070d7180400c8596f61634d8c1be97162a31f5f40ad2",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "13e498e9-ea9e-4b44-bf44-a3558d17722e",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "If achievement of exact 0\nis not possible define the estimator ˜θ0 of θ0 as an approximate ϵN-solution:\n1\nK\nK\nX\nk=1\nEn,k[ψ(W; ˜θ0, bη0,k)]]\n⩽inf\nθ∈Θ\n1\nK\nK\nX\nk=1\nEn,k[ψ(W; θ0, bη0,k)]]\n+ ϵN,\n(3.5)\nfor ϵN = o(δNN −1/2), where (δN)N⩾1 is some sequence of positive constants converging\nto zero. Remark 3.1. (Recommendations) The choice of K has no asymptotic impact under our\nconditions but, of course, the choice of K may matter in small samples. Intuitively, larger\nvalues of K provide more observations in Ic\nk from which to estimate the high-dimensional\nnuisance functions, which seems to be the more difficult part of the problem. We have\nfound moderate values of K, such as 4 or 5, to work better than K = 2 in a variety of\nempirical examples and in simulations. Moreover, we generally recommend DML2 over\nDML1 though in some problems like estimation of ATE in the interactive model, which we\ndiscuss later, there is no difference between the two approaches. In most other problems,\nDML2 is better behaved since the pooled empirical Jacobian for the equation in (3.4)\nexhibits more stable behavior than the separate empirical Jacobians for the equation in\n(3.1). 3.2.",
    "content_hash": "71128a5bd7f06258c9432015776790dc5c9bd270592e2f4873a4dc5040d4b8df",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b39cc96b-207b-4430-837e-e53d948a72bf",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Moment Condition Models with Linear Scores\nWe first consider the case of linear scores, where\nψ(w; θ, η) = ψa(w; η)θ + ψb(w; η),\nfor all w ∈W, θ ∈Θ, η ∈T. (3.6)",
    "content_hash": "6f531f4f581898a7986a1eaec4bd84a8d99a882fcbaac53c22f606f9db34e93f",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4e4bf95d-2492-4e70-898c-e2d539b63ec7",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(d) The variance of the score ψ is non-degenerate: All eigenvalues of the matrix\nEP [ψ(W; θ0, η0)ψ(W; θ0, η0)′]\nare bounded from below by c0. Assumptions 3.2(a)-(c) state that the estimator of the nuisance parameter belongs to\nthe realization set TN ⊂T, which is a shrinking neighborhood of η0, which contracts\naround η0 with the rate determined by the “statistical” rates rN, r′\nN, and λ′\nN. These\nrates are not given in terms of the norm ∥· ∥T on T, but rather are the intrinsic rates",
    "content_hash": "0a36692f908d2898bdb815ba0810754dfe74dfbf3c91f0a76b8d31ecd015864b",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ed580a39-df67-4318-8170-70824b571771",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n25\nLet c0 > 0, c1 > 0, s > 0, q > 2 be some finite constants such that c0 ⩽c1; and let\n{δN}N⩾1 and {∆N}N⩾1 be some sequences of positive constants converging to zero such\nthat δN ⩾N −1/2. Also, let K ⩾2 be some fixed integer, and let {PN}N⩾1 be some\nsequence of sets of probability distributions P of W on W. Assumption 3.1. (Linear Scores with Approximate Neyman Orthogonality) For all N ⩾\n3 and P ∈PN, the following conditions hold. (a) The true parameter value θ0 obeys\n(2.1). (b) The score ψ is linear in the sense of (3.6). (c) The map η 7→EP [ψ(W; θ, η)]\nis twice continuously Gateaux-differentiable on T. (d) The score ψ obeys the Neyman\northogonality or, more generally, the Neyman λN near-orthogonality condition at (θ0, η0)\nwith respect to the nuisance realization set TN ⊂T for\nλN := sup\nη∈TN\n∂ηEP ψ(W; θ0, η0)[η −η0]\n⩽δNN −1/2. (e) The identification condition holds; namely, the singular values of the matrix\nJ0 := EP [ψa(W; η0)]\nare between c0 and c1. Assumption 3.1 requires scores to be Neyman orthogonal or near-orthogonal and im-\nposes mild smoothness requirements as well as the canonical identification condition. Assumption 3.2.",
    "content_hash": "1bf0565e7bc5748eac80c9c7ea3c8ec7f31770f889bb200099e927344fd74553",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e0505f3f-073f-41c7-8828-4012ff25d672",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(Score Regularity and Quality of Nuisance Parameter Estimators) For\nall N ⩾3 and P ∈PN, the following conditions hold. (a) Given a random subset I of\n[N] of size n = N/K, the nuisance parameter estimator bη0 = bη0((Wi)i∈Ic) belongs to\nthe realization set TN with probability at least 1 −∆N, where TN contains η0 and is\nconstrained by the next conditions. (b) The moment conditions hold:\nmN := sup\nη∈TN\n(EP [∥ψ(W; θ0, η)∥q])1/q ⩽c1,\nm′\nN := sup\nη∈TN\n(EP [∥ψa(W; η)∥q])1/q ⩽c1. (c) The following conditions on the statistical rates rN, r′\nN, and λ′\nN hold:\nrN := sup\nη∈TN\n∥EP [ψa(W; η)] −EP [ψa(W; η0)]∥⩽δN,\nr′\nN := sup\nη∈TN\n(EP [∥ψ(W; θ0, η) −ψ(W; θ0, η0)∥2])1/2 ⩽δN,\nλ′\nN :=\nsup\nr∈(0,1),η∈TN\n∥∂2\nrEP [ψ(W; θ0, η0 + r(η −η0))]∥⩽δN/\n√\nN.",
    "content_hash": "804ba1663a6ee3f12d2980c6e7a19136cdd29c08b85a44d54fdacc856cbc33cc",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c1c5c657-4c71-44f7-9c91-77de9c954aac",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(2012),\nand Belloni and Chernozhukov (2013) for ℓ1-penalized and related methods in a variety\nof sparse models; Kozbur (2016) for forward selection in sparse models; Luo and Spindler\n(2016) for L2-boosting in sparse linear models; Wager and Walther (2016) for concen-\ntration results for a class of regression trees and random forests; and Chen and White\n(1999) for a class of neural nets. However, the presented conditions allow for more refined statements than (3.8). We\nnote that many important structured problems – such as estimation of parameters in\npartially linear regression models, estimation of parameters in partially linear structural\nequation models, and estimation of average treatment effects under unconfoundedness\n– are such that some cross-derivatives vanish, allowing more refined requirements than\n(3.8). This feature allows us to require much finer conditions on the quality of the nuisance\nparameter estimators than the crude bound (3.8). For example, in many problems\nλ′\nN = 0,\n(3.9)\nbecause the second derivatives vanish,\n∂2\nrEP [ψ(W; θ0, η0 + r(η −η0))] = 0. This occurs in the following important examples:\n1. the optimal instrument problem; see Belloni et al. (2012). 2. the partially linear regression model when m0(X) = 0 or is otherwise known; see\nSection 4. 3. the treatment effect examples when the propensity score is known, which includes\nrandomized control trials as an important special case; see Section 5.",
    "content_hash": "4edd356c5ed5d76929ff2dec24f8711fde286655bdc2b9bfb71b3fe5329f446d",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3f778fa8-cd7c-4953-92f5-cb5968585892",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "If both (3.7) and (3.9) hold, Assumption 3.2, particularly rN = o(1) and r′\nN = o(1),\nimposes the weakest possible rate requirement:\nεN = o(1).",
    "content_hash": "ce9aa2014ebbecbf45e9123f2578f494714c6f3ac30fa89dbb36bcc32fb1dfa9",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5af61464-cd58-4dac-bcd6-6d4fffe531b3",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "26\nCCDDHNR\nthat are most connected to the statistical problem at hand. However, in smooth problems,\nas discussed below this translates, in the worst cases, to the crude requirement that the\nnuisance parameters are estimated at the rate o(N −1/4). The conditions in Assumption 3.2 embody refined requirements on the quality of nui-\nsance parameter estimators. In many applications, where (θ, η) 7→ψ(W; θ, η) is smooth,\nwe can bound\nrN ≲εN,\nr′\nN ≲εN,\nλ′\nN ≲ε2\nN,\n(3.7)\nwhere εN is the upper bound on the rate of convergence of bη0 to η0 with respect to the\nnorm ∥· ∥T = ∥· ∥P,2:\n∥bη0 −η∥T ≲εN. Note that TN can be chosen as the set of η that is within a neighborhood of size εN\nof η0, possibly with other restrictions, in this case. If only (3.7) holds, Assumption 3.2,\nparticularly λ′\nN = o(N −1/2), imposes the (crude) rate requirement\nεN = o(N −1/4). (3.8)\nThis rate is achievable for many ML methods under structured assumptions on the nui-\nsance parameters. See, among many others, Bickel et al. (2009), B¨uhlmann and van de\nGeer (2011), Belloni et al. (2011), Belloni and Chernozhukov (2011), Belloni et al.",
    "content_hash": "dc87a64a7b4c37d6aa190d16d9bc99005f7ed53fd3fe6cb3f2ec8c9094fc4d1e",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "850222fb-64b2-439a-9903-9703b59e3ad4",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "This estimator concentrates around\nthe true variance matrix σ2,\nbσ2 = σ2 + OP (ϱN),\nϱN := N −[(1−2/q)∧1/2] + rN + r′\nN ≲δN. Moreover, bσ2 can replace σ2 in the statement of Theorem 3.1 with the size of the remain-\nder term updated as ρN = N −[(1−2/q)∧1/2] + rN + r′\nN + N 1/2λN + N 1/2λ′\nN. Theorems 3.1 and 3.2 can be used for standard construction of confidence regions which\nare uniformly valid over a large, interesting class of models:",
    "content_hash": "9e0be0f2828eaa60da90ab402812ff8d83515d6f93e983518a83eacb0f1cb052",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "436331cc-5c77-4c95-bb97-8cffbcd1921b",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n27\nWe note that similar refined rates have appeared in the context of estimation of treatment\neffects in high-dimensional settings under sparsity; see Farrell (2015) and Athey et al. (2016) and related discussion in Remark 5.2. Our refined rate results complement this\nwork by applying to a broad class of estimation contexts, including estimation of average\ntreatment effects, and to a broad set of ML estimators. Theorem 3.1. (Properties of the DML) Suppose that Assumptions 3.1 and 3.2 hold. In addition, suppose that δN ⩾N −1/2 for all N ⩾1. Then the DML1 and DML2\nestimators ˜θ0 concentrate in a 1/\n√\nN neighborhood of θ0 and are approximately linear\nand centered Gaussian:\n√\nNσ−1(˜θ0 −θ0) =\n1\n√\nN\nN\nX\ni=1\n¯ψ(Wi) + OP (ρN) ; N(0, Id),\n(3.10)\nuniformly over P ∈PN, where the size of the remainder term obeys\nρN := N −1/2 + rN + r′\nN + N 1/2λN + N 1/2λ′\nN ≲δN,\n(3.11)\n¯ψ(·) := −σ−1J−1\n0 ψ(·, θ0, η0) is the influence function, and the approximate variance is\nσ2 := J−1\n0 EP [ψ(W; θ0, η0)ψ(W; θ0, η0)′](J−1\n0 )′.",
    "content_hash": "b04a31642522656cd32e64069aceb02647117d1387dbfeeb894f20711f035fbc",
    "location": null,
    "page_start": 27,
    "page_end": 30,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "64c60dc8-513c-443c-a71c-c6055dfb383e",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "The result establishes that the estimator based on the orthogonal scores achieves the\nroot-N rate of convergence and is approximately normally distributed. It is notewor-\nthy that this convergence result, both the rate of concentration and the distributional\napproximation, holds uniformly with respect to P varying over an expanding class of\nprobability measures PN. This means that the convergence holds under any sequence of\nprobability distributions (PN)N⩾1 with PN ∈PN for each N, which in turn implies that\nthe results are robust with respect to perturbations of a given P along such sequences. The same property can be shown to fail for methods not based on orthogonal scores. Theorem 3.2. (Variance Estimator for DML) Suppose that Assumptions 3.1 and\n3.2 hold. In addition, suppose that δN ⩾N −[(1−2/q)∧1/2] for all N ⩾1. Consider the\nfollowing estimator of the asymptotic variance matrix of\n√\nN(˜θ0 −θ0):\nbσ2 = bJ−1\n0\n1\nK\nK\nX\nk=1\nEn,k[ψ(W; ˜θ0, bη0,k)ψ(W; ˜θ0, bη0,k)′]( bJ−1\n0 )′,\nwhere\nbJ0 = 1\nK\nK\nX\nk=1\nEn,k[ψa(W; bη0,k)],\nand ˜θ0 is either the DML1 or the DML2 estimator.",
    "content_hash": "b3d5dd91f4501cad9108970ae22147eef9bda199b0a37d2746c309b435f052d8",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1a54a9bf-0be3-4db0-9360-5051fc1863a7",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Assumption\n3.3(b) only requires differentiability of the function (θ, η) 7→EP [ψ(W; θ, η)] and does not",
    "content_hash": "7de39f0ebafb0844f5275b3247e40482e8d98eceb41aa1d8e93c51a1be94c379",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1b1cb8c6-7e94-43ee-b339-2b99d37fcdd2",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Models with Nonlinear Scores\nLet c0 > 0, c1 > 0, a > 1, v > 0, s > 0, and q > 2 be some finite constants, and\nlet {δN}N⩾1, {∆N}N⩾1, and {τN}N⩾1 be some sequences of positive constants converg-\ning to zero. To derive the properties of the DML estimator, we will use the following\nassumptions. Assumption 3.3. (Nonlinear Moment Condition Problem with Approximate Neyman\nOrthogonality) For all N ⩾3 and P ∈PN, the following conditions hold. (a) The true\nparameter value θ0 obeys (2.1), and Θ contains a ball of radius c1N −1/2 log N centered\nat θ0. (b) The map (θ, η) 7→EP [ψ(W; θ, η)] is twice continuously Gateaux-differentiable\non Θ × T. (c) For all θ ∈Θ, the identification relation\n2∥EP [ψ(W; θ, η0)]∥⩾∥J0(θ −θ0)∥∧c0\nis satisfied, for the Jacobian matrix\nJ0 := ∂θ′\nn\nEP [ψ(W; θ, η0)]\no\nθ=θ0\nhaving singular values between c0 and c1. (d) The score ψ obeys the Neyman orthogonality\nor, more generally the Neyman near-orthogonality with λN = δNN −1/2 for the set TN ⊂\nT. Assumption 3.3 is mild and rather standard in moment condition problems. Assump-\ntion 3.3(a) requires θ0 to be sufficiently separated from the boundary of Θ.",
    "content_hash": "05fc411fc4e15c53971cf0ae852581cf60ffc653f4faec17f51f987f6919c824",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e1da5e53-5c71-45b8-87e0-f35812eacc2a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "28\nCCDDHNR\nCorollary 3.1. (Uniformly Valid Confidence Bands) Under the conditions of\nTheorem 3.2, suppose we are interested in the scalar parameter ℓ′θ0 for some dθ × 1\nvector ℓ. Then the confidence interval\nCI :=\nh\nℓ′˜θ0 ± Φ−1(1 −α/2)\np\nℓ′bσ2ℓ/N\ni\nobeys\nsup\nP ∈PN\nPP (ℓ′θ0 ∈CI) −(1 −α)\n→0. Indeed, the above theorem implies that CI obeys PPN (ℓ′θ0 ∈CI) →(1 −α) under any\nsequence {PN} ∈PN, which implies that these claims hold uniformly in P ∈PN. For\nexample, one may choose {PN} such that, for some ϵN →0\nsup\nP ∈PN\n|PP (ℓ′θ0 ∈CI) −(1 −α)| ⩽|PPN (ℓ′θ0 ∈CI) −(1 −α)| + ϵN →0. Next we note that the estimators need not be semi-parametrically efficient, but under\nsome conditions they can be. Corollary 3.2. (Cases with Semi-parametric Efficiency) Under the conditions of\nTheorem 3.1, if the score ψ is efficient for estimating θ0 at a given P ∈P ⊂PN, in the\nsemi-parametric sense as defined in van der Vaart (1998), then the large sample variance\nσ2\n0 of ˜θ0 reaches the semi-parametric efficiency bound at this P relative to the model P. 3.3.",
    "content_hash": "b54cc4b5ae170b8c458daeb79d1fddd34d32d9a0b191eeae4e238bd5070cb325",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1fb665e0-aa1f-4574-a391-cb66b5d733a7",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(c) The fol-\nlowing conditions on the statistical rates rN, r′\nN, and λ′\nN hold:\nrN :=\nsup\nη∈TN,θ∈Θ\n∥EP [ψ(W; θ, η) −EP [ψ(W; θ, η0)]∥⩽δNτN,\nr′\nN :=\nsup\nη∈TN,∥θ−θ0∥⩽τN\n(EP [∥ψ(W; θ, η) −ψ(W; θ0, η0)∥2])1/2 and r′\nN log1/2(1/r′\nN) ⩽δN,\nλ′\nN :=\nsup\nr∈(0,1),η∈TN,∥θ−θ0∥⩽τN\n∥∂2\nrEP [ψ(W; θ0, η0 + r(θ −θ0) + r(η −η0))]∥⩽δNN −1/2. (d) The variance of the score is non-degenerate: All eigenvalues of the matrix\nEP [ψ(W; θ0, η0)ψ(W; θ0, η0)′]\nare bounded from below by c0. Assumptions 3.3(a)-(c) state that the estimator of the nuisance parameter belongs to\nthe realization set TN ⊂T, which is a shrinking neighborhood of η0 that contracts at the\n“statistical” rates rN and r′\nN and λ′\nN. These rates are not given in terms of the norm ∥·∥T\non T, but rather are intrinsic rates that are connected to the statistical problem at hand.",
    "content_hash": "916aadee4c4a62b66cfe151d4cd74e1c6c4c6dc067900c51a5ab96032fee55c4",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0fb12ddb-3804-47aa-bd65-aecf3d9d8ea3",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n29\nrequire differentiability of the function (θ, η) 7→ψ(W; θ, η). Assumption 3.3(c) implies\nsufficient identifiability of θ0. Assumption 3.3(d) is the orthogonality condition that has\nalready been extensively discussed. Assumption 3.4. (Score Regularity and Requirements on the Quality of Estimation of\nNuisance Parameters) Let K be a fixed integer. For all N ⩾3 and P ∈PN, the following\nconditions hold. (a) Given a random subset I of {1, . . . , N} of size n = N/K, we have\nthat the nuisance parameter estimator bη0 = bη0((Wi)i∈Ic) belongs to the realization set TN\nwith probability at least 1 −∆N, where TN contains η0 and is constrained by conditions\ngiven below. (b) The parameter space Θ is bounded and for each η ∈TN, the function\nclass F1,η = {ψj(·, θ, η): j = 1, ..., dθ, θ ∈Θ} is suitably measurable and its uniform\ncovering entropy obeys\nsup\nQ\nlog N(ϵ∥F1,η∥Q,2, F1,η, ∥· ∥Q,2) ⩽v log(a/ϵ),\nfor all 0 < ϵ ⩽1,\n(3.12)\nwhere F1,η is a measurable envelope for F1,η that satisfies ∥F1,η∥P,q ⩽c1.",
    "content_hash": "137d886a9cc3a8030fe4b0e07217c4c4c7a18c147112cf380b95ce74ad917605",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ba291bb9-6fec-49e4-ae45-941abc5071a7",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "In smooth problems, these conditions translate to the crude requirement that nuisance\nparameters are estimated at the o(N −1/4) rate as discussed previously in the case with\nlinear scores. However, these conditions can be refined as, for example, when λ′\nN = 0 or\nwhen some cross-derivatives vanish in λ′\nN; see the linear case in the previous subsection for\nfurther discussion. Suitable measurability and pointwise entropy conditions, required in\nAssumption 3.4(b), are mild regularity conditions that are satisfied in all practical cases. The assumption of a bounded parameter space Θ in Assumption 3.4(b) is embedded in\nthe entropy condition, but we state it separately for clarity. This assumption was not\nneeded in the linear case, and it can be removed in the nonlinear case with the imposition\nof more complicated Huber-like regularity conditions. Assumption 3.4(c) is a set of mild\ngrowth conditions. Remark 3.2. (Rate Requirements on Nuisance Parameter Estimators) Similar to the\ndiscussion in the linear case, the conditions in Assumption 3.4 are very flexible and\nembody refined requirements on the quality of the nuisance parameter estimators. The",
    "content_hash": "a3796333a370003bc7834b54d7dbc55b3ed07f35a3fb28e89ca0c7c73bc5c6fe",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3f2ed4c1-fef1-42d9-b8f8-223e256f4666",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "To quantify and incorporate the\nvariation introduced by sample splitting, we consider variance estimators:\nbσ2,mean = 1\nS\nS\nX\ns=1\n\u0010\nbσ2\ns + (bθs −˜θmean)(bθs −˜θmean)′\u0011\n,\n(3.13)",
    "content_hash": "aea577c0ed39f2e27bb6f5318647f6011f70d8b17b05a5940f8252e474ed1ed7",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e5ddc202-bc83-4f4c-abfd-9201bb507295",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "30\nCCDDHNR\nconditions essentially reduce to the previous conditions in the linear case, with the excep-\ntion of compactness, which is imposed to make the conditions easy to verify in non-linear\ncases. Theorem 3.3. (Properties of the DML for Nonlinear Scores) Suppose that As-\nsumptions 3.3 and 3.4 hold. In addition, suppose that δN ⩾N −1/2+1/q log N and that\nN −1/2 log N ⩽τN ⩽δN for all N ⩾1. Then the DML1 and DML2 estimators ˜θ0\nconcentrate in a 1/\n√\nN neighborhood of θ0, and are approximately linear and centered\nGaussian:\n√\nNσ−1(˜θ0 −θ0) =\n1\n√\nN\nN\nX\ni=1\n¯ψ(Wi) + OP (ρN) ; N(0, I),\nuniformly over P ∈PN, where the size of the remainder term obeys\nρN := N −1/2+1/q log N + r′\nN log1/2(1/r′\nN) + N 1/2λN + N 1/2λ′\nN ≲δN,\n¯ψ(·) := −σ−1\n0 J−1\n0 ψ(·, θ0, η0) is the influence function, and the approximate variance is\nσ2 := J−1\n0 EP [ψ(W; θ0, η0)ψ(W; θ0, η0)′](J−1\n0 )′.",
    "content_hash": "2e9dd016de17eeb966f27e5a6592bee862f69e5aa1e53cc2a2a7899cd7d66a92",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "26536452-4d47-4f6c-b6ee-ac71a475bab6",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Moreover, in the statement above σ2 can be replaced by a consistent estimator bσ2, obeying\nbσ2 = σ2 + oP (ϱN) uniformly in P ∈PN, with the size of the remainder term updated\nas ρN = ρN + ϱN. Furthermore, Corollaries 3.1 and 3.2 continue to hold under the\nconditions of this theorem. 3.4. Finite-Sample Adjustments to Incorporate Uncertainty Induced by Sample Splitting\nThe estimation technique developed in this paper relies on subsamples obtained by ran-\ndomly partitioning the sample: an auxiliary sample for estimating the nuisance functions\nand a main sample for estimating the parameter of interest. Although the specific sample\npartition has no impact on estimation results asymptotically, the effect of the particular\nrandom split on the estimate can be important in finite samples. To make the results\nmore robust with respect to the partitioning, we propose to repeat the DML estimator\nS times, obtaining the estimates\n˜θs\n0,\ns = 1, . . . , S. Features of these estimates may then provide insight into the sensitivity of results to\nthe sample splitting, and we can report results that incorporate features of this set of\nestimates that should be less driven by any particular sample-splitting realization. Definition 3.3. (Incorporating the Impact of Sample Splitting using Mean\nand Median Methods) For point estimation, we define\n˜θmean\n0\n= 1\nS\nS\nX\ns=1\n˜θs\n0\nor\n˜θmedian\n0\n= median{ ˜θs\n0}S\ns=1,\nwhere the median operation is applied coordinatewise.",
    "content_hash": "52cd1e1f72a9f83f0cf17578b733bbd842633070ca01ae914802e0e8486044b8",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ca03aa9f-6a85-42fa-bc5d-8c6dd6bbffba",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "The first approach to inference on θ0, which we described in the Introduction, is to\nemploy the DML method using the score function\nψ(W; θ, η) := {Y −Dθ −g(X)}(D −m(X)),\nη = (g, m),\n(4.3)\nwhere W = (Y, D, X) and g and m are P-square-integrable functions mapping the sup-\nport of X to R. It is easy to see that θ0 satisfies the moment condition EP ψ(W; θ0, η0) = 0,\nand also the orthogonality condition ∂ηEP ψ(W; θ0, η0)[η −η0] = 0 where η0 = (g0, m0). A second approach employs the Robinson-style “partialling-out” score function\nψ(W; θ, η) := {Y −ℓ(X) −θ(D −m(X))}(D −m(X)),\nη = (ℓ, m),\n(4.4)\nwhere W = (Y, D, X) and ℓand m are P-square-integrable functions mapping the sup-\nport of X to R. This gives an alternative parameterization of the score function above,\nand using this score is first-order equivalent to using the previous score.",
    "content_hash": "53936b39ccb6316d075f02545f353724aa1dcad1806e817f45622050ddfa3471",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5356ad1d-4fa7-4562-82ac-e5ed4950ff1b",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n31\nand a more robust version,\nbσ2,median = median{bσ2\ns + ((bθs −˜θmedian)(bθs −˜θmedian)′}S\ns=1,\n(3.14)\nwhere the median picks out the matrix with median operator norm, which preserve non-\nnegative definiteness. We recommend using medians, reporting ˜θmedian\n0\nand bσ2Median, as these quantities are\nmore robust to outliers. Corollary 3.3. If S is fixed, as N →∞and maintaining either Assumptions 3.1 and\n3.2 or Assumptions 3.3 and 3.4 as appropriate, ˜θmean\n0\nand ˜θmedian\n0\nare first-order equiva-\nlent to ˜θ0 and obey the conclusions of Theorems 3.1 and 3.2 or of Theorem 3.3. Moreover,\nbσ2,median and bσ2,mean can replace bσ in the statement of the appropriate theorems. It would be interesting to investigate the behavior under the regime where S →∞as\nN →∞. 4. INFERENCE IN PARTIALLY LINEAR MODELS\n4.1. Inference in Partially Linear Regression Models\nHere we revisit the partially linear regression model\nY = Dθ0 + g0(X) + U,\nEP [U | X, D] = 0,\n(4.1)\nD = m0(X) + V,\nEP [V | X] = 0. (4.2)\nThe parameter of interest is the regression coefficient θ0. If D is conditionally exogenous\n(as good as randomly assigned conditional on covariates), then θ0 measures the average\ncausal/treatment effect of D on potential outcomes.",
    "content_hash": "74840fbe70601dcd880c7356fc75a7dc328143cddd8bc0ef288aa2e939fd0e16",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "65c96288-d07e-482a-92a9-e4e6f7839d41",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "It is easy to see\nthat θ0 satisfies the moment condition EP ψ(W; θ0, η0) = 0, and also the orthogonality\ncondition ∂ηEP ψ(W; θ0, η0)[η −η0] = 0, for η0 = (ℓ0, m0), where ℓ0(X) = EP [Y |X]. In the partially linear model, the DML approach complements Belloni et al. (2013),\nZhang and Zhang (2014), van de Geer et al. (2014), Javanmard and Montanari (2014b),\nand Belloni et al. (2014), Belloni et al. (2014), and Belloni et al. (2015), all of which\nconsider estimation and inference for parameters within the partially linear model using\nlasso-type methods without cross-fitting. By relying upon cross-fitting, the DML ap-\nproach allows for the use of a much broader collection of ML methods for estimating",
    "content_hash": "abd3e0c47c9f8d7f59f73146896a7043b7d65d366b0f93611936351bcbc38e9a",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "73cb8de2-a0f8-44c6-be65-4e963f5427e8",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(Regularity Conditions for Partially Linear Regression Model) Let P\nbe the collection of probability laws P for the triple W = (Y, D, X) such that (a) equations\n(4.1)-(4.2) hold; (b) ∥Y ∥P,q + ∥D∥P,q ⩽C; (c) ∥UV ∥P,2 ⩾c2 and EP [V 2] ⩾c; (d)\n∥EP [U 2 | X]∥P,∞⩽C and ∥EP [V 2 | X]∥P,∞⩽C; and (e) given a random subset I\nof [N] of size n = N/K, the nuisance parameter estimator bη0 = bη0((Wi)i∈Ic) obeys the\nfollowing conditions for all n ⩾1: With P-probability no less than 1 −∆N,\n∥bη0 −η0∥P,∞⩽C,\n∥bη0 −η0∥P,2 ⩽δN,\nand8\n(i) for the score ψ in (4.3), where bη0 = (bg0, bm0),\n∥bm0 −m0∥P,2 × ∥bg0 −g0∥P,2 ⩽δNN −1/2,\n(ii) for the score ψ in (4.4), where bη0 = (bℓ0, bm0),\n∥bm0 −m0∥P,2 ×\n\u0010\n∥bm0 −m0∥P,2 + ∥bℓ0 −ℓ0∥P,2\n\u0011\n⩽δNN −1/2. Remark 4.1. (Rate Conditions for Estimators of Nuisance Parameters) The only non-\nprimitive condition here is the assumption on the rate of estimating the nuisance param-\neters. These rates of convergence are available for most often used ML methods and are\ncase-specific, so we do not restate conditions that are needed to reach these rates.",
    "content_hash": "a31aa8ac39eba2d72bb8ddc5d5c04d7fa9d3711a2eed666a06064b276eb96d54",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "105e56b9-f85a-45f6-a393-a8dbb9c71700",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "32\nCCDDHNR\nthe nuisance functions and also allows relaxation of sparsity conditions in the case where\nlasso or other sparsity-based estimators are used. Both the DML approach and the ap-\nproaches taken in the aforementioned papers can be seen as heuristically “debiasing” the\nscore function (Y −Dθ −g(X))D, which does not possess the orthogonality property\nunless m0(X) = 0. Let (δN)∞\nn=1 and (∆N)∞\nn=1 be sequences of positive constants approaching 0 as before. Also, let c, C, and q be fixed strictly positive constants such that q > 4, and let K ⩾2\nbe a fixed integer. Moreover, for any η = (ℓ1, ℓ2), where ℓ1 and ℓ2 are functions mapping\nthe support of X to R, denote ∥η∥P,q = ∥ℓ1∥P,q ∨∥ℓ2∥P,q. For simplicity, assume that\nN/K is an integer. Assumption 4.1.",
    "content_hash": "de71522bb6fff30353d39ef9b58874c37c2ada33567b25562f717631fb1f5b1b",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d4fb486c-cb4b-4377-a28d-d33afcb68e28",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "The following theorem follows as a corollary to the results in Section 3 by verifying\nAssumptions 3.1 and 3.2 and will be proven as a special case of Theorem 4.2 below. Theorem 4.1. (DML Inference on Regression Coefficients in the Partially Lin-\near Regression Model) Suppose that Assumption 4.1 holds. Then the DML1 and\nDML2 estimators ˜θ0 constructed in Definitions 3.1 and 3.2 above using the score in\neither (4.3) or (4.4) are first-order equivalent and obey\nσ−1√\nN(˜θ0 −θ0) ; N(0, 1),\nuniformly over P ∈P, where σ2 = [EP V 2]−1EP [V 2U 2][EP V 2]−1. Moreover, the result\ncontinues to hold if σ2 is replaced by bσ2 defined in Theorem 3.2. Consequently, confidence\nregions based upon the DML estimators ˜θ0 have uniform asymptotic validity:\nlim\nN→∞sup\nP ∈P\nPP\n\u0010\nθ0 ∈[˜θ0 ± Φ−1(1 −α/2)bσ/\n√\nN]\n\u0011\n−(1 −α)\n= 0. 8We thank Rui Wang from the University of Washington for pointing out a mistake in the published\nversion of the paper: In Assumptions 4.1 and 4.2, the correct condition is ∥bη0 −η0∥P,∞⩽C rather than\n∥bη0 −η0∥P,q ⩽C appearing in the published version.",
    "content_hash": "a7bf5ff157f1d922696ba90a7908d43b10897f7be39b920b72bec787ad985941",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fdcaa21e-b0ba-4258-9d68-848e396c6c71",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n33\nRemark 4.2. (Asymptotic Efficiency under Homoscedasticity) Under conditional ho-\nmoscedasticity, i.e. E[U 2|Z] = E[U 2], the asymptotic variance σ2 reduces to E[V 2]−1E[U 2],\nwhich is the semi-parametric efficiency bound for θ. Remark 4.3. (Tightness of Conditions under Cross-Fitting) The conditions in Theorem\n4.1 are fairly sharp, though they are somewhat simplified for ease of presentation. The\nsharpness can be understood by examining the case where the regression function g0 and\nthe propensity function m0 are sparse with sparsity indices sg ≪N and sm ≪N and\nare estimated by ℓ1-penalized estimators bg0 and bm0 that have sparsity indices of orders\nsg and sm and converge to g0 and m0 at the rates\np\nsg/N and\np\nsm/N (ignoring logs). The rate conditions in Assumption 4.1 then require (ignoring logs) that\np\nsg/N\np\nsm/N ≪N −1/2 ⇔sgsm ≪N,\nwhich is much weaker than the condition\n(sg)2 + (sm)2 ≪N\n(ignoring logs) required without sample splitting. For example, if the propensity function\nm0 is very sparse (low sm), then the regression function is allowed to be quite dense (high\nsg), and vice versa. If the propensity function is known (sm = 0) or can be estimated at\nthe N −1/2 rate, then only consistency for bg0 is needed.",
    "content_hash": "d9aaf222a1b150080de660b38f987826cc2eb27017b8a688244683c517d1ef16",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "897d147f-d722-42fd-a46c-24089867971d",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Alternatively, we can use the Robinson-style score\nψ(W; θ, η) := (Y −ℓ(X) −θ(D −r(X)))(Z −m(X)),\nη = (ℓ, m, r),\n(4.8)\nwhere W = (Y, D, X, Z) and ℓ, m, and r are P-square-integrable functions mapping the\nsupport of X to R. It is straightforward to verify that both scores satisfy the moment\ncondition EP ψ(W; θ0, η0) = 0 and also the orthogonality condition ∂ηEP ψ(W; θ0, η0)[η−\nη0] = 0, for η0 = (g0, m0) in the former case and η0 = (ℓ0, m0, r0) for ℓ0 and r0 defined\nby ℓ0(X) = EP [Y | X] and r0(X) = EP [D | X], respectively, in the latter case.9\n9It is interesting to note that the methods for constructing Neyman orthogonal scores described in\nSection 2 may give scores that are different from those in (4.7) and (4.8). For example, applying the\nmethod for conditional moment restriction problems in Section 2.2.4 with Ω(R) = 1 gives the score",
    "content_hash": "e91191efff7efa7a22dd29daa7e79c7022f4f3e60dba6a3cb83a30cdb8a9c347",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "60eec00a-4983-4b88-868c-b1ecde0776a8",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Then the DML1 and DML2 esti-\nmators ˜θ0 constructed in Definitions 3.1 and 3.2 above using the score in either (4.7) or\n(4.8) are first-order equivalent and obey\nσ−1√\nN(˜θ0 −θ0) ; N(0, 1),\nuniformly over P ∈P, where σ2 = [EP DV ]−1EP [V 2U 2][EP DV ]−1. Moreover, the result\ncontinues to hold if σ2 is replaced by bσ2 defined in Theorem 3.2. Consequently, confidence\nregions based upon the DML estimators ˜θ0 have uniform asymptotic validity:\nlim\nN→∞sup\nP ∈P\nPP\n\u0010\nθ0 ∈[˜θ0 ± Φ−1(1 −α/2)bσ/\n√\nN]\n\u0011\n−(1 −α)\n= 0. ψ(W; θ, η) = (Y −Dθ−g(X))(r(Z, X)−f(X)), where the true values of r(Z, X) and f(X) are r0(Z, X) =\nEP [D | Z, X] and f0(X) = EP [D | X], respectively. It may be interesting to compare properties of the\nDML estimators ˜θ0 based on this score with those based on (4.7) and (4.8) in future work.",
    "content_hash": "eb1dd5fb747df6853d9ac9f336e01e51833ebb7aa3e29fca03760372e8258775",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c3e2039a-d420-46b9-9e20-3773ff3fa65c",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(Regularity Conditions for Partially Linear IV Model) For all prob-\nability laws P ∈P for the quadruple W = (Y, D, X, Z) the following conditions hold:\n(a) equations (4.5)-(4.6) hold; (b) ∥Y ∥P,q + ∥D∥P,q + ∥Z∥P,q ⩽C; (c) ∥UV ∥P,2 ⩾c2\nand |EP [DV ]| ⩾c; (d) ∥EP [U 2 | X]∥P,∞⩽C and ∥EP [V 2 | X]∥P,∞⩽C; and (e)\ngiven a random subset I of [N] of size n = N/K, the nuisance parameter estimator\nbη0 = bη0((Wi)i∈Ic) obeys the following conditions: With P-probability no less than 1−∆N,\n∥bη0 −η0∥P,∞⩽C,\n∥bη0 −η0∥P,2 ⩽δN,\nand\n(i) for the score ψ in (4.7), where bη0 = (bg0, bm0),\n∥bm0 −m0∥P,2 × ∥bg0 −g0∥P,2 ⩽δNN −1/2,\n(ii) for the score ψ in (4.8), where bη0 = (bℓ0, bm0, br0),\n∥bm0 −m0∥P,2 ×\n\u0010\n∥br0 −r0∥P,2 + ∥bℓ0 −ℓ0∥P,2\n\u0011\n⩽δNN −1/2. The following theorem follows as a corollary to the results in Section 3 by verifying\nAssumptions 3.1 and 3.2. Theorem 4.2. (DML Inference on Regression Coefficients in the Partially Lin-\near IV Model) Suppose that Assumption 4.2 holds.",
    "content_hash": "bf40b66f14350ec1193a46ba65dd53b89c4039a37a60b358127d4fece1595683",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "227e1ccd-eace-4640-b247-1ceb48a0b1e3",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "34\nCCDDHNR\nNote that the score in (4.8) has a minor advantage over the score in (4.7) because all of\nits nuisance parameters are conditional mean functions, which can be directly estimated\nby the ML methods. If one prefers to use the score in (4.7), one has to construct an\nestimator of g0 first. To do so, one can first obtain a DML estimator of θ0 based on\nthe score in (4.8), say ˜θ0. Then, using the fact that g0(X) = EP [Y −Dθ0 | X], one\ncan construct an estimator bg0 by applying an ML method to regress Y −D˜θ0 on X. Alternatively, one can use assumption-specific methods to directly estimate g0, without\nusing the score (4.8) first. For example, if g0 can be approximated by a sparse linear\ncombination of a large set of transformations of X, one can use the methods of Gautier\nand Tsybakov (2014) to obtain an estimator of g0. Let (δN)∞\nn=1 and (∆N)∞\nn=1 be sequences of positive constants approaching 0 as before. Also, let c, C, and q be fixed strictly positive constants such that q > 4, and let K ⩾2\nbe a fixed integer. Moreover, for any η = (ℓj)l\nj=1 mapping the support of X to Rl, denote\n∥η∥P,q = max1⩽j⩽l ∥ℓj∥P,q. For simplicity, assume that N/K is an integer. Assumption 4.2.",
    "content_hash": "4a07a2a046e2bc86e5c6385001bd5a42e145bac8714d248f5fcfe832b3035c14",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ba5fa80a-e0e9-4777-b581-99f3966e595f",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "This orthogonal moment\ncondition is based on the influence function for the mean for missing data from Robins\nand Rotnitzky (1995). For estimation of the ATTE, we use the score\nψ(W; θ, η) = D(Y −g(X))\np\n−m(X)(1 −D)(Y −g(X))\np(1 −m(X))\n−Dθ\np ,\n(5.4)\n10Without unconfoundedness/conditional exogeneity, these quantities measure association, and could\nbe referred to as average predictive effect (APE) and average predictive effect for the exposed (APEX). Inferential results for these objects would follow immediately from Theorem 5.1.",
    "content_hash": "25c81f8ca837cdb3f43ef94a3280d10438b12b0fb9b97baf30d3d364b1ff4d76",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a5be01a5-e120-4860-ad01-e95193a010da",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n35\n5. INFERENCE ON TREATMENT EFFECTS IN THE INTERACTIVE MODEL\n5.1. Inference on ATE and ATTE\nIn this section, we specialize the results of Section 3 to estimating treatment effects under\nthe unconfoundedness assumption of Rosenbaum and Rubin (1983). Within this setting,\nthere is a large classical literature focused on low-dimensional settings that provides\nmethods for adjusting for confounding variables including regression methods, propen-\nsity score adjustment methods, matching methods, and “doubly-robust” combinations\nof these methods; see, for example, Robins and Rotnitzky (1995), Hahn (1998), Hirano\net al. (2003), and Abadie and Imbens (2006) as well as the textbook overview provided\nin Imbens and Rubin (2015). In this section, we present results that complement this im-\nportant classic work as well as the rapidly expanding body of work on estimation under\nunconfoundedness using ML methods; see, among others, Athey et al. (2016), Belloni\net al. (2017), Belloni et al. (2014), Farrell (2015), and Imai and Ratkovic (2013). We specifically consider estimation of average treatment effects when treatment effects\nare fully heterogeneous and the treatment variable is binary, D ∈{0, 1}. We consider\nvectors (Y, D, X) such that\nY = g0(D, X) + U,\nEP [U | X, D] = 0,\n(5.1)\nD = m0(X) + V,\nEP [V | X] = 0.",
    "content_hash": "d61d00ae712fcabdcaa5a1fdd33fdb07ef1006a06714cff762e1205db5f7b33f",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e57836fa-71b6-431a-bd77-4ba1ae8e78f6",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(5.2)\nSince D is not additively separable, this model is more general than the partially linear\nmodel for the case of binary D. A common target parameter of interest in this model is\nthe average treatment effect (ATE),\nθ0 = EP [g0(1, X) −g0(0, X)].10\nAnother common target parameter is the average treatment effect for the treated (ATTE),\nθ0 = EP [g0(1, X) −g0(0, X)|D = 1]. The confounding factors X affect the policy variable via the propensity score m0(X)\nand the outcome variable via the function g0(D, X). Both of these functions are unknown\nand potentially complicated, and we can employ ML methods to learn them. We proceed to set up moment conditions with scores obeying orthogonality conditions. For estimation of the ATE, we employ\nψ(W; θ, η) := (g(1, X) −g(0, X)) + D(Y −g(1, X))\nm(X)\n−(1 −D)(Y −g(0, X))\n1 −m(X)\n−θ, (5.3)\nwhere the nuisance parameter η = (g, m) consists of P-square-integrable functions g and\nm mapping the support of (D, X) to R and the support of X to (ε, 1 −ε), respectively,\nfor some ε ∈(0, 1/2). The true value of η is η0 = (g0, m0).",
    "content_hash": "7f35116be325215e1c9541406effacae04ea85754bbdb9f7b05d17456f4d308a",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "857360b4-2c42-4f30-8b04-c5bf5fde8dfc",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "36\nCCDDHNR\nwhere the nuisance parameter η = (g, m, p) consists of P-square-integrable functions\ng and m mapping the support of X to R and to (ε, 1 −ε), respectively, and a con-\nstant p ∈(ε, 1 −ε), for some ε ∈(0, 1/2). The true value of η is η0 = (g0, m0, p0), where\ng0(X) = g0(0, X) and p0 = EP [D]. Note that estimating ATTE does not require estimat-\ning g0(1, X). Note also that since p is a constant, it does not affect the DML estimators\n˜θ0 based on the score ψ in (5.4) but having p simplifies the formula for the variance of\n˜θ0. Using their respective scores, it can be easily seen that true parameter values θ0 for\nATE and ATTE obey the moment condition EP ψ(W; θ0, η0) = 0, and also that the\northogonality condition ∂ηEP ψ(W; θ0, η0)[η −η0] = 0 holds. Let (δN)∞\nn=1 and (∆N)∞\nn=1 be sequences of positive constants approaching 0. Also, let\nc, ε, C and q be fixed strictly positive constants such that q > 2, and let K ⩾2 be a\nfixed integer. Moreover, for any η = (ℓ1, . . . , ℓl), denote ∥η∥P,q = max1⩽j⩽l ∥ℓj∥P,q.",
    "content_hash": "4f28d16224d1f3660e021336b0b422c65d0c73a56ca5882a21ba3c52d01a3c0c",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7afa4a1e-1b1c-436d-9fb1-a3724c0dffbf",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "These rates of convergence are available for most\noften used ML methods and are case-specific, so we do not restate conditions that are\nneeded to reach these rates. The conditions are not the tightest possible, but offer a\nset of simple conditions under which Theorem 5.1 follows as a special case of the gen-\neral theorem provided in Section 3. One could obtain more refined conditions by doing\ncustomized proofs. The following theorem follows as a corollary to the results in Section 3 by verifying\nAssumptions 3.1 and 3.2. Theorem 5.1. (DML Inference on ATE and ATTE) Suppose that either (a) the\ntarget parameter is ATE, θ0 = EP [g0(1, X) −g0(0, X)], and the score ψ in (5.3) is used,\nor (b) the target parameter is ATTE, θ0 = EP [g0(1, X)−g0(0, X) | D = 1], and the score\nψ in (5.4) is used. In addition, suppose that Assumption 5.1 holds. Then the DML1 and\nDML2 estimators ˜θ0, constructed in Definitions 3.1 and 3.2, are first-order equivalent\nand obey\nσ−1√\nN(˜θ0 −θ0) ⇝N(0, 1),\n(5.5)",
    "content_hash": "b7644bbba1ea0abc27034d84714a7c3247cc0b4b3147b7ca2d5e30480f9c85a1",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "16fd4f39-a8b1-44f9-a279-330d019e65c7",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "For example, if the propensity score m0 is very\nsparse, then the regression function is allowed to be quite dense with sg >\n√\nN, and\nvice versa. If the propensity score is known (sm = 0), then only consistency for bg0 is\nneeded. Such comparisons also extend to approximately sparse models. We note that\nsimilar refined rates appeared in Farrell (2015) who considers estimation of treatment\neffects in a setting where an approximately sparse model holds for both the regression\nand propensity score functions. In interesting related work, Athey et al. (2016) show\nthat\n√\nN consistent estimation of an average treatment effect is possible under very\nweak conditions on the propensity score - allowing for the possibility that the propensity\nscore may not be consistently estimated - under strong sparsity of the regression function\nsuch that sg ≪\n√\nN. Thus, the approach taken in this context and Athey et al. (2016) are\ncomplementary and one may prefer either depending on whether or not the regression\nfunction can be estimated extremely well based on a sparse method. 5.2. Inference on Local Average Treatment Effects\nIn this section, we consider estimation of local average treatment effects (LATE) with a\nbinary treatment variable, D ∈{0, 1}, and a binary instrument, Z ∈{0, 1}.11 As before,\nY denotes the outcome variable, and X is the vector of covariates.",
    "content_hash": "325b043fdc254a5b847ecd922eefc6cf1f7276590d8c8dcac5cfef66534e3132",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a702d37f-8b44-46be-b57b-117c7beb09e2",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n37\nuniformly over P ∈P, where σ2 = EP [ψ2(W; θ0, η0)]. Moreover, the result continues\nto hold if σ2 is replaced by bσ2 defined in Theorem 3.2. Consequently, confidence regions\nbased upon the DML estimators ˜θ0 have uniform asymptotic validity:\nlim\nN→∞sup\nP ∈P\nPP\n\u0010\nθ0 ∈[˜θ0 ± Φ−1(1 −α/2)bσ/\n√\nN]\n\u0011\n−(1 −α)\n= 0. The scores ψ in (5.3) and (5.4) are efficient, so both estimators are asymptotically effi-\ncient, reaching the semi-parametric efficiency bound of Hahn (1998). Remark 5.2. (Tightness of Conditions) The conditions in Assumption 5.1 are fairly\nsharp though somewhat simplified for ease of presentation. The sharpness can be un-\nderstood by examining the case where the regression function g0 and the propensity\nfunction m0 are sparse with sparsity indices sg ≪N and sm ≪N and are estimated\nby ℓ1-penalized estimators bg0 and bm0 that have sparsity indices of orders sg and sm and\nconverge to g0 and m0 at the rates\np\nsg/N and\np\nsm/N (ignoring logs). Then the rate\nconditions in Assumption 5.1 require\np\nsg/N\np\nsm/N ≪N −1/2 ⇔sgsm ≪N\n(ignoring logs) which is much weaker than the condition (sg)2 + (sm)2 ≪N (ignoring\nlogs) required without sample splitting.",
    "content_hash": "0cfee66ea34dddbf59a181cb6d1470245619ce6556cebf8109c04861cb00c6b5",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "79a72e6e-bf71-4c54-a988-2f8bc0c5eeb4",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Consider the functions µ0, m0, and p0, where µ0 maps the support of (Z, X) to R\nand m0 and p0 respectively map the support of (Z, X) and X to (ε, 1 −ε) for some\nε ∈(0, 1/2), such that\nY = µ0(Z, X) + U,\nEP [U | Z, X] = 0,\n(5.6)\nD = m0(Z, X) + V,\nEP [V | Z, X] = 0,\n(5.7)\nZ = p0(X) + ζ,\nEP [ζ | X] = 0. (5.8)\n11Similar results can be provided for the local average treatment effect on the treated (LATT) by\nadapting the following arguments to use the orthogonal scores for the LATT. See, for example, Belloni\net al. (2017).",
    "content_hash": "34a4bfd666dd638ee6b2d81f6fbd835bf7db083740b5c16f1cb524c2058b4ae5",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "494ebb7c-79b1-44f7-a2d9-e7c2cb9e5e9b",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Then\nthe DML1 and DML2 estimators ˜θ0 constructed in Definitions 3.1 and 3.2 and based on\nthe score ψ above are first-order equivalent and obey\nσ−1√\nN(˜θ0 −θ0) ⇝N(0, 1),\n(5.9)\nuniformly over P ∈P, where σ2 = (EP [m(1, X) −m(0, X)])−2EP [ψ2(W; θ0, η0)]. More-\nover, the result continues to hold if σ2 is replaced by bσ2 defined in Theorem 3.2. Conse-\nquently, confidence regions based upon the DML estimators ˜θ0 have uniform asymptotic",
    "content_hash": "4594b46b557de2d382d3a8c5eb4f1a5c9a867f123c10ad1b5b698cb33cd8d2d0",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "54fc60f5-d701-4b0f-9832-471fb2270a74",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "38\nCCDDHNR\nWe are interested in estimating\nθ0 = EP [µ(1, X)] −EP [µ(0, X)]\nEP [m(1, X)] −EP [m(0, X)]. Under the assumptions of Imbens and Angrist (1994) and Fr¨olich (2007), θ0 is the LATE -\nthe average treatment effect for compliers which are observations that would have D = 1\nif Z were 1 and would have D = 0 if Z were 0. To estimate θ0, we will use the score\nψ(W; θ, η) := µ(1, X) −µ(0, X) + Z(Y −µ(1, X))\np(X)\n−(1 −Z)(Y −µ(1, X))\n1 −p(X))\n−\n\u0010\nm(1, X) −m(0, X) + Z(D −m(1, X))\np(X)\n−(1 −Z)(D −m(0, X))\n1 −p(X)\n\u0011\n× θ,\nwhere W = (Y, D, X, Z) and the nuisance parameter η = (µ, m, p) consists of P-square-\nintegrable functions µ, m, and p, with µ mapping the support of (Z, X) to R and m and\np respectively mapping the support of (Z, X) and X to (ε, 1−ε) for some ε ∈(0, 1/2).",
    "content_hash": "67a1efced7edf3fc5df33faaf5434e397744ef9ec3aac56e1700d6f466fd20bc",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "46f49fe7-b262-436f-b3d5-b1611dc22c7d",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "It\nis easy to verify that this score satisfies the moment condition EP ψ(W; θ0, η0) = 0 and\nalso the orthogonality condition ∂ηEP ψ(W; θ0, η0)[η −η0] = 0 for η0 = (µ0, m0, p0). Let (δN)∞\nn=1 and (∆N)∞\nn=1 be sequences of positive constants approaching 0. Also, let\nc, C, and q be fixed strictly positive constants such that q > 4, and let K ⩾2 be a fixed\ninteger. Moreover, for any η = (ℓ1, ℓ2, ℓ3), where ℓ1 is a function mapping the support\nof (Z, X) to R and ℓ2 and ℓ3 are functions respectively mapping the support of (Z, X)\nand X to (ε, 1 −ε) for some ε ∈(0, 1/2), denote ∥η∥P,q = ∥ℓ1∥P,q ∨∥ℓ2∥P,2 ∨∥ℓ3∥P,q. For\nsimplicity, assume that N/K is an integer. Assumption 5.2.",
    "content_hash": "065ab1798aac3effa01904319fd93f710cc1b55117eb726d0ba03f4f92e0bfd6",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "82ae4f5f-f19d-46d1-ab3e-4617fc5db9e1",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(Regularity Conditions for LATE Estimation) For all probability laws\nP ∈P for the quadruple W = (Y, D, X, Z) the following conditions hold: (a) equations\n(5.6)-(5.8) hold, with D ∈{0, 1} and Z ∈{0, 1}; (b) ∥Y ∥P,q ⩽C; (c) PP (ε ⩽p0(X) ⩽\n1 −ε) = 1, (d) EP [m0(1, X) −m0(0, X)] ⩾c, (e) ∥U −θ0V ∥P,2 ⩾c; (f) ∥EP [U 2 |\nX]∥P,∞⩽C; and (g) given a random subset I of [N] of size n = N/K, the nuisance\nparameter estimator bη0 = bη0((Wi)i∈Ic) obeys the following conditions: with P-probability\nno less than 1 −∆N:\n∥bη0 −η0∥P,q ⩽C,\n∥bη0 −η0∥P,2 ⩽δN,\n∥bp0 −1/2∥P,∞⩽1/2 −ε,\nand\n∥bp0 −p0∥P,2 ×\n\u0010\n∥bµ0 −µ0∥P,2 + ∥bm0 −m0∥P,2\n\u0011\n⩽δNN −1/2. The following theorem follows as a corollary to the results in Section 3 by verifying\nAssumptions 3.1 and 3.2. Theorem 5.2. (DML Inference on LATE) Suppose that Assumption 5.2 holds.",
    "content_hash": "51b46c6e0a92619847c6b25d9ead1ce270635c5f67411568b9261efa9221cf5b",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "934e4869-5574-49d2-b2cc-a3112d913334",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "In these\nexperiments, UI claimants were randomly assigned either to a control group or one of\nfive treatment groups.12 In the control group, the standard rules of the UI system ap-\nplied. Individuals in the treatment groups were offered a cash bonus if they found a job\nwithin some pre-specified period of time (qualification period), provided that the job was\nretained for a specified duration. The treatments differed in the level of the bonus, the\nlength of the qualification period, and whether the bonus was declining over time in the\nqualification period; see Bilias and Koenker (2002) for further details. In our empirical example, we focus only on the most generous compensation scheme,\ntreatment 4, and drop all individuals who received other treatments. In this treatment,\nthe bonus amount is high and the qualification period is long compared to other treat-\nments, and claimants are eligible to enroll in a workshop. Our treatment variable, D,\nis an indicator variable for being assigned treatment 4, and the outcome variable, Y,\nis the log of duration of unemployment for the UI claimants. The vector of covariates,\nX, consists of age group dummies, gender, race, the number of dependents, quarter of\nthe experiment, location within the state, existence of recall expectations, and type of\noccupation. We report results based on five simple methods for estimating the nuisance functions\nused in forming the orthogonal estimating equations. We consider three tree-based meth-\nods, labeled “Random Forest”, “Reg. Tree”, and “Boosting”, one ℓ1-penalization based\nmethod, labeled “Lasso”, and a neural network method, labeled “Neural Net”.",
    "content_hash": "6317d02b559ae355a1604c1a31ac51ab011e4c220d55636633f1ca70252e8153",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "61652c8b-13e3-4842-9be9-c6c8fb2ea1e9",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "After obtaining\nestimates from the five simple methods and “Ensemble”, we select the best methods\nfor estimating each nuisance functions based on the average out-of-sample prediction\nperformance for the target variable associated with each nuisance function obtained from\neach of the previously described approaches. As a result, the reported estimate in the\nlast column uses different ML methods to estimate different nuisance functions. Note\nthat if a single method outperformed all the others in terms of prediction accuracy for all\nnuisance functions, the estimate in the “Best” column would be identical to the estimate\nreported under that method. Table 1 presents DML2 estimates of the ATE on unemployment duration using the\nmedian method described in Section 3.4. We report results for heterogeneous effect model\nin Panel A and for the partially linear model in Panel B. Because the treatment is\nrandomly assigned, we use the fraction of treated as the estimator of the propensity\nscore in forming the orthogonal estimating equations.14 For both the partially linear\nmodel and the interactive model, we report estimates obtained using 2-fold cross-fitting\nand 5-fold cross-fitting. All results are based on taking 100 different sample splits. We\nsummarize results across the sample splits using the median method. For comparison, we\nreport two different standard errors. In brackets, we report the median standard error\nfrom across the 100 splits; and we report standard errors adjusted for variability across\nthe sample splits using the median method in parentheses. The estimation results are consistent with the findings of previous studies which have\nanalyzed the Pennsylvania Bonus Experiment. The ATE on unemployment duration is\nnegative and significant across all estimation methods at the 5% level regardless of the\nstandard error estimator used.",
    "content_hash": "6bc18b616ef9d13a367c938e6b062de54dd489f1f6af0b9fdec7326bcd9cfc9b",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "caf307dc-b293-4b94-9542-0b5fd6a13e1d",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "40\nCCDDHNR\nobtained by estimating each nuisance function with a random forest which averages over\n1000 trees. The results in “Boosting” are obtained using boosted regression trees with\nregularization parameters chosen by 10-fold cross-validation. To estimate the nuisance\nfunctions using the neural networks, we use 2 neurons and a decay parameter of 0.02,\nand we set activation function as logistic for classification problems and as linear for\nregression problems.13 “Lasso” estimates an ℓ1-penalized linear regression model using\nthe data-driven penalty parameter selection rule developed in Belloni et al. (2012). For\n“Lasso”, we use a set of 96 potential control variables formed from the raw set of co-\nvariates and all second order terms, i.e. all squares and first-order interactions. For the\nremaining methods, we use the raw set of covariates as features. We also consider two hybrid methods labeled “Ensemble” and “Best”. “Ensemble”\noptimally combines four of the ML methods listed above by estimating the nuisance\nfunctions as weighted averages of estimates from “Lasso,” “Boosting,” “Random Forest,”\nand “Neural Net”. The weights are restricted to sum to one and are chosen so that the\nweighted average of these methods gives the lowest average mean squared out-of-sample\nprediction error estimated using 5-fold cross-validation. The final column in Table 1\n(“Best”) reports results that combine the methods in a different way.",
    "content_hash": "41c12738926178fb8860e6e19a7fed4927c429e4d39b98f7978df08ab6bdb460",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "168e34d6-91c8-4471-a470-0e1ad2a9bdd3",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Interestingly, we see that there is no practical difference\nacross the two different standard errors in this example. 13We also experimented with “Deep Learning” methods from which we obtained similar results for some\ntuning parameters. However, we ran into stability and computational issues and chose not to report\nthese results in the empirical section. 14We also estimated the effects using nonparametric estimates of the conditional propensity score ob-\ntained from the ML procedures given in the column labels. As expected due to randomization, the results\nare similar to those provided in Table 1 and are not reported for brevity.",
    "content_hash": "dafe0c9901d4a88405d1a1a2c5684ea00e9bb10ef89abed1959d5ba27413f16b",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "431c87bc-5bc5-4085-b44d-f7d2fbdf3f1a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "One might wonder\nwhether such specifications are able to adequately control for income and other related\nconfounds. At the same time, the power to learn about treatment effects decreases as one\nallows more flexible models. The principled use of flexible ML tools offers one resolution\nto this tension. The results presented below thus complement previous results which rely",
    "content_hash": "837fcd258ac0fd7816570b87273d1da35233620f584cc4e9bd2667d522213a24",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f065fad4-c4a5-4787-9a56-96d6b94ca84d",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n41\nTable 1. Estimated Effect of Cash Bonus on Unemployment Duration\nLasso\nReg. Tree\nForest\nBoosting\nNeural Net. Ensemble\nBest\nA. Interactive Regression Model\nATE (2 fold)\n-0.081\n-0.084\n-0.074\n-0.079\n-0.073\n-0.079\n-0.078\n[0.036]\n[0.036]\n[0.036]\n[0.036]\n[0.036]\n[0.036]\n[0.036]\n(0.036)\n(0.036)\n(0.036)\n(0.036)\n(0.036)\n(0.036)\n(0.036)\nATE (5 fold)\n-0.081\n-0.085\n-0.074\n-0.077\n-0.073\n-0.078\n-0.077\n[0.036]\n[0.036]\n[0.036]\n[0.035]\n[0.036]\n[0.036]\n[0.036]\n(0.036)\n(0.037)\n(0.036)\n(0.036)\n(0.036)\n(0.036)\n(0.036)\nB.",
    "content_hash": "e69008828cd648290fa85a34660261891ed74eaa1c7bf9e5b05c1f84976e56ae",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0c7486f7-ab76-4709-a0d4-246c32fd40e3",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Partially Linear Regression Model\nATE (2 fold)\n-0.080\n-0.084\n-0.077\n-0.076\n-0.074\n-0.075\n-0.075\n[0.036]\n[0.036]\n[0.035]\n[0.035]\n[0.035]\n[0.035]\n[0.035]\n(0.036)\n(0.036)\n(0.037)\n(0.036)\n(0.036)\n(0.036)\n(0.036)\nATE (5 fold)\n-0.080\n-0.084\n-0.077\n-0.074\n-0.073\n-0.075\n-0.074\n[0.036]\n[0.036]\n[0.035]\n[0.035]\n[0.035]\n[0.035]\n[0.035]\n(0.036)\n(0.037)\n(0.036)\n(0.035)\n(0.036)\n(0.035)\n(0.035)\nNote: Estimated ATE and standard errors from a linear model (Panel B) and heterogeneous effect model\n(Panel A) based on orthogonal estimating equations. Column labels denote the method used to estimate\nnuisance functions. Results are based on 100 splits with point estimates calculated the median method. The median standard error across the splits are reported in brackets and standard errors calculated using\nthe median method to adjust for variation across splits are provided in parentheses.",
    "content_hash": "96b0a76170e2d29b00ed18116f9142ee3976ca11a3416405e36bc98b8bc9e101",
    "location": null,
    "page_start": 41,
    "page_end": 42,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ed5888de-fd5b-4738-9eae-5e0df23986df",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "We consider the same methods with the\nsame tuning choices for estimating the nuisance functions as in the previous example,\nwith one exception, and so do not repeat details for brevity. The one exception is that",
    "content_hash": "17f3aff42f5b4fa6f4b1d41695ac428e8f5e39976189c2eb73dca158397ebff6",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4fb9940d-6d2e-4be3-b629-b96a59d8cc11",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "saving bonds, other interest-earning accounts in banks\nand other financial institutions, other interest-earning assets (such as bonds held person-\nally), stocks, and mutual funds less non-mortgage debt - as the outcome variable, Y , in\nour analysis. Our treatment variable, D, is an indicator for being eligible to enroll in a\n401(k) plan. The vector of raw covariates, X, consists of age, income, family size, years of\neducation, a married indicator, a two-earner status indicator, a defined benefit pension\nstatus indicator, an IRA participation indicator, and a home ownership indicator. In Table 2, we report DML2 estimates of ATE of 401(k) eligibility on net financial assets\nboth in the partially linear model as in (1.1) and allowing for heterogeneous treatment\neffects using the interactive model outlined in Section 5.1. To reduce the disproportionate\nimpact of extreme propensity score weights in the interactive model, we trim the propen-\nsity scores at 0.01 and 0.99. We present two sets of results based on sample-splitting\nas discussed in Section 3 using 2-fold cross-fitting and 5-fold cross-fitting. As in the\nprevious section, we consider 100 different sample partitions and summarize the results\nacross different sample splits using the median method. For comparison, we report two\ndifferent standard errors. In brackets, we report the median standard error from across\nthe 100 splits; and we report standard errors adjusted for variability across the sample\nsplits using the median method in parentheses.",
    "content_hash": "08628e30f00b46d7f351d2bb1305515c3e4521c2da3df4ffe5e57383321f71ed",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b48bd848-4cf0-4bc8-80f8-f7f49db9233f",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Partially Linear Regression Model\nATE (2 fold)\n7717\n8709\n9116\n8759\n8950\n9010\n9125\n[1346]\n[1363]\n[1302]\n[1339]\n[1335]\n[1309]\n[1304]\n(1749)\n(1427)\n(1377)\n(1382)\n(1408)\n(1344)\n(1357)\nATE (5 fold)\n8187\n8871\n9247\n9110\n9038\n9166\n9215\n[1298]\n[1358]\n[1295]\n[1314]\n[1322]\n[1299]\n[1294]\n(1558)\n(1418)\n(1328)\n(1328)\n(1355)\n(1310)\n(1312)\nNote: Estimated ATE and standard errors from a linear model (Panel B) and heterogeneous effect model\n(Panel A) based on orthogonal estimating equations. Column labels denote the method used to estimate\nnuisance functions. Results are based on 100 splits with point estimates calculated the median method. The median standard error across the splits are reported in brackets and standard errors calculated using\nthe median method to adjust for variation across splits are provided in parentheses. Further details about\nthe methods are provided in the main text. on the assumption that confounding effects can adequately be controlled for by a small\nnumber of variables chosen ex ante by the researcher. In the example in this paper, we use the same data as in Chernozhukov and Hansen\n(2004). We use net financial assets - defined as the sum of IRA balances, 401(k) bal-\nances, checking accounts, U.S.",
    "content_hash": "cef9a127f41eda71a05468090dabb65c287675473dcc353c4de77b14e48362ae",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bc9980d9-187a-4a2b-aec4-3ad9b06a4008",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "42\nCCDDHNR\nTable 2. Estimated Effect of 401(k) Eligibility on Net Financial Assets\nLasso\nReg. Tree\nForest\nBoosting\nNeural Net. Ensemble\nBest\nA. Interactive Regression Model\nATE (2 fold)\n6830\n7713\n7770\n7806\n7764\n7702\n7546\n[1282]\n[1208]\n[1276]\n[1159]\n[1328]\n[1149]\n[1360]\n(1530)\n(1271)\n(1363)\n(1202)\n(1468)\n(1170)\n(1533)\nATE (5 fold)\n7170\n7993\n8105\n7713\n7788\n7839\n7753\n[1201]\n[1198]\n[1242]\n[1155]\n[1238]\n[1134]\n[1237]\n(1398)\n(1236)\n(1299)\n(1177)\n(1293)\n(1148)\n(1294)\nB.",
    "content_hash": "9e60d0a45a3bdae6a68335a1628c4335b19c30bbb004d9adba139593041efcfb",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c3cbb98b-ef5d-4aab-95c7-6d171ed3d28b",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n43\nTable 3. Estimated Effect of 401(k) Participation on Net Financial Assets\nLasso\nReg. Tree\nForest\nBoosting\nNeural Net. Ensemble\nBest\nLATE (2 fold)\n8978\n11073\n11384\n11329\n11094\n11119\n10952\n[2192]\n[1749]\n[1832]\n[1666]\n[1903]\n[1653]\n[1657]\n(3014)\n(1849)\n(1993)\n(1718)\n(2098)\n(1689)\n(1699)\nLATE (5 fold)\n8944\n11459\n11764\n11133\n11186\n11173\n11113\n[2259]\n[1717]\n[1788]\n[1661]\n[1795]\n[1641]\n[1645]\n(3307)\n(1786)\n(1893)\n(1710)\n(1890)\n(1678)\n(1675)\nNote: Estimated LATE based on orthogonal estimating equations. Column labels denote the method\nused to estimate nuisance functions. Results are based on 100 splits with point estimates calculated the\nmedian method. The median standard error across the splits are reported in brackets and standard errors\ncalculated using the median method to adjust for variation across splits are provided in parentheses. Further details about the methods are provided in the main text. we implement neural networks with 8 neurons and a decay parameter of 0.01 in this\nexample. Turning to the results, it is first worth noting that the estimated ATE of 401(k) eligi-\nbility on net financial assets is $19,559 with an estimated standard error of 1413 when\nno control variables are used.",
    "content_hash": "90f178e951ab2aaf819e6c309a525b509bce978cee03bcb4d2563d0d62f02e9e",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6a84f0e8-6d32-414e-a5da-34664f68b8b7",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Of course, this number is not a valid estimate of the causal\neffect of 401(k) eligibility on financial assets if there are neglected confounding variables\nas suggested by Poterba et al. (1994a) and Poterba et al. (1994b). When we turn to the\nestimates that flexibly account for confounding reported in Table 2, we see that they are\nsubstantially attenuated relative to this baseline that does not account for confounding,\nsuggesting much smaller causal effects of 401(k) eligibility on financial asset holdings. It\nis interesting and reassuring that the results obtained from the different flexible methods\nare broadly consistent with each other. This similarity is consistent with the theory that\nsuggests that results obtained through the use of orthogonal estimating equations and\nany sensible method of estimating the necessary nuisance functions should be similar. Finally, it is interesting that these results are also broadly consistent with those reported\nin the original work of Poterba et al. (1994a) and Poterba et al. (1994b) which used a\nsimple intuitively motivated functional form, suggesting that this intuitive choice was\nsufficiently flexible to capture much of the confounding variation in this example. As a further illustration, we also report the LATE in this example where we take the\nendogenous treatment variable to be participating in a 401(k) plan. Even after controlling\nfor features related to job choice, it seems likely that the actual choice of whether to\nparticipate in an offered plan would be endogenous.",
    "content_hash": "0b7579a0a1171a265e46195aafe1a2e5a73c41ca42824a8fe6a0c9864a33e07f",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b61f599f-54db-4885-8d5c-f47869517fb4",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Of course, we can use eligibility for\na 401(k) plan as an instrument for participation in a 401(k) plan under the conditions\nthat were used to justify the exogeneity of eligibility for a 401(k) plan provided above in\nthe discussion of estimation of the ATE of 401(k) eligibility. We report DML2 results of estimating the LATE of 401(k) participation using 401(k)\neligibility as an instrument in Table 3. We employ the procedure outlined in Section 5.2\nusing the same ML estimators to estimate the quantities used to form the orthogonal\nestimating equation as we employed to estimate the ATE of 401(k) eligibility outlined\npreviously, so we omit the details for brevity. Looking at the results, we see that the es-\ntimated causal effect of 401(k) participation on net financial assets is uniformly positive\nand statistically significant across all of the considered methods. As when looking at the\nATE of 401(k) eligibility, it is reassuring that the results obtained from the different flex-",
    "content_hash": "eddd8c8325c390926cf30fc241911da21fb5abedd419a73822b4d986a799c654",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fd924fc0-73d6-4778-9f7e-a9ba0b9ed74d",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "The validity of this instrument hinges on the argument that set-\ntlers set up better institutions in places where they are more likely to establish long-term\nsettlements; that where they are likely to settle for the long term is related to settler\nmortality at the time of initial colonization; and that institutions are highly persistent. The exclusion restriction for the instrumental variable is then motivated by the argu-\nment that GDP, while persistent, is unlikely to be strongly influenced by mortality in\nthe previous century, or earlier, except through institutions.",
    "content_hash": "0bdc155e23c187e91b7ab529d696ddbf2d7d027560cad340f6d95e75afa07f5f",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0934c00d-29e0-4b36-89da-dc89619ba670",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "44\nCCDDHNR\nible methods are broadly consistent with each other. It is also interesting that the results\nbased on flexible ML methods are broadly consistent with, though somewhat attenuated\nrelative to, those obtained by applying the same specification for controls as used in\nPoterba et al. (1994a) and Poterba et al. (1994b) and using a linear IV model which\nreturns an estimated effect of participation of $13,102 with estimated standard error of\n(1922). The mild attenuation may suggest that the simple intuitive control specification\nused in the original baseline specification is somewhat too simplistic. Looking at Tables 2 and 3, there are other interesting observations that can provide\nuseful insights into understanding the finite sample properties of the DML estimation\nmethod. First, the standard errors of the estimates obtained using 5-fold cross-fitting are\nlower than those obtained from 2-fold cross-fitting for all methods across all cases. This\nfact suggests that having more observations in the auxiliary sample may be desirable. Specifically, the 5-fold cross-fitting estimates use more observations to learn the nuisance\nfunctions than 2-fold cross-fitting and thus likely learn them more precisely. This increase\nin precision in learning the nuisance functions may then translate into more precisely\nestimated parameters of interest. While intuitive, we note that this statement does not\nseem to be generalizable in that there does not appear to be a general relationship\nbetween the number of folds in cross-fitting and the precision of the estimate of the\nparameter of interest; see the next example. Second, we also see that the standard errors\nof the Lasso estimates after adjusting for variation due to sample splitting are noticeably\nlarger than the standard errors coming from the other ML methods.",
    "content_hash": "3bea7642d6c87cdbc61bbb3e1700cc50352f8c0c7d3200f984a7872c1b9afde5",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8a8a38b4-6598-49d0-b442-d6b83018923f",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "We believe that\nthis is due to the fact that the out-of-sample prediction errors from a linear model\ntend to be larger when there is a need to extrapolate. In our framework, if the main\nsample includes observations that are outside of the range of the observations in the\nauxiliary sample, the model has to extrapolate to those observations. The fact that the\nstandard errors are lower in 5-fold cross-fitting than in 2-fold cross-fitting for the “Lasso”\nestimations also supports this hypothesis, because the higher number of observations in\nthe auxiliary sample reduces the degree of extrapolation. We also see that there is a\nnoticeable increase in the standard errors that account for variability due to sample\nsplitting relative to the simple unadjusted standard errors in this case, though these\ndifferences do not qualitatively change the results. 6.3. The Effect of Institutions on Economic Growth\nTo demonstrate DML estimation of partially linear structural equation models with in-\nstrumental variables, we consider estimation of the effect of institutions on aggregate\noutput following the work of Acemoglu et al. (2001) (AJR). Estimating the effect of\ninstitutions on output is complicated by the clear potential for simultaneity between in-\nstitutions and output: Specifically, better institutions may lead to higher incomes, but\nhigher incomes may also lead to the development of better institutions. To help overcome\nthis simultaneity, AJR use mortality rates for early European settlers as an instrument\nfor institution quality.",
    "content_hash": "f2fdc07a23476d8c7810e7429050a76033ba61c90c40f4bba49b078c77afdb24",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "759cf83a-e3b2-42d2-91be-74af84aa17bc",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "A leading candidate for such a factor, as\nthey discuss, is geography. AJR address this by assuming that the confounding effect of\ngeography is adequately captured by a linear term in distance from the equator and a set\nof continent dummy variables. Using DML allows us to relax this assumption and replace\nit by a weaker assumption that geography can be sufficiently controlled by an unknown\nfunction of distance from the equator and continent dummies which can be learned by\nML methods. We use the same set of 64 country-level observations as AJR. The data set contains\nmeasurements of GDP, settler morality, an index which measures protection against ex-\npropriation risk and geographic information. The outcome variable, Y, is the logarithm of\nGDP per capita and the endogenous explanatory variable, D, is a measure of the strength\nof individual property rights that is used as a proxy for the strength of institutions. To\ndeal with endogeneity, we use an instrumental variable Z, which is mortality rates for\nearly European settlers. Our raw set of control variables, X, include distance from the\nequator and dummy variables for Africa, Asia, North America, and South America. We report results from applying DML2 following the procedure outlined in Section\n4.2 in Table 4. The considered ML methods and tuning parameters are the same as\nthe previous examples except for the Ensemble method, from which we exclude Neural\nNetwork since the small sample size causes stability problems in training the Neural\nNetwork. We use the raw set of covariates and all second order terms when doing lasso\nestimation, and we simply use the raw set of covariates in the remaining methods.",
    "content_hash": "317f0c41b1b04b9e95f3328e18467a77d9e20a5eb6e41e83e20b99eeb50accba",
    "location": null,
    "page_start": 45,
    "page_end": 45,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "792063bf-8d5e-403c-b1fd-05707ebf4818",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n45\nTable 4. Estimated Effect of Institutions on Output\nLasso\nReg. Tree\nForest\nBoosting\nNeural Net. Ensemble\nBest\n2 fold\n0.85\n0.81\n0.84\n0.77\n0.94\n0.8\n0.83\n[0.28]\n[0.42]\n[0.38]\n[0.33]\n[0.32]\n[0.35]\n[0.34]\n(0.22)\n(0.29)\n(0.3)\n(0.27)\n(0.28)\n(0.3)\n(0.29)\n5 fold\n0.77\n0.95\n0.9\n0.73\n1.00\n0.83\n0.88\n[0.24]\n[0.46]\n[0.41]\n[0.33]\n[0.33]\n[0.37]\n[0.41]\n(0.17)\n(0.45)\n(0.4)\n(0.27)\n(0.3)\n(0.34)\n(0.39)\nNote: Estimated coefficient from a linear instrumental variables model based on orthogonal estimating\nequations. Column labels denote the method used to estimate nuisance functions. Results are based on\n100 splits with point estimates calculated the median method. The median standard error across the\nsplits are reported in brackets and standard errors calculated using the median method to adjust for\nvariation across splits are provided in parentheses. Further details about the methods are provided in\nthe main text. In their paper, AJR note that their instrumental variable strategy will be invalidated\nif other factors are also highly persistent and related to the development of institutions\nwithin a country and to the country’s GDP.",
    "content_hash": "e5fc1542a26a22281aed22e3a35a80ca0a950372e930061632afb756fa30aa52",
    "location": null,
    "page_start": 45,
    "page_end": 45,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e7c4baac-9cd2-4dd5-9cff-06166e56664f",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "As in\nthe previous examples, we consider 100 different sample splits and report the “Median”\nestimates of the coefficient and two different standard error estimates. In brackets, we\nreport the median standard error from across the 100 splits; and we report standard errors\nadjusted for variability across the sample splits using the median method in parentheses. Finally, we report results from both 2-fold cross-fitting and 5-fold cross-fitting as in the\nother examples. In this example, we see uniformly large and positive point estimates across all proce-\ndures considered, and estimated effects are statistically significant at the 5% level. As\nin the second example, we see that adjusting for variability across sample splits leads",
    "content_hash": "9f6d04e30e739cadc0dc8a4198f77f6d57831aadcac65dd3475f144d59ad59f9",
    "location": null,
    "page_start": 45,
    "page_end": 45,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6ff668ed-5bae-4f10-b8f1-adf01db3fce9",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "46\nCCDDHNR\nto noticeable increases in estimated standard errors but does not result in qualitatively\ndifferent conclusions. Interestingly, we see that the estimated standard errors based on\n5-fold cross-fitting are larger than those on twofold cross-fitting in all procedures except\nlasso, which differs from the finding in the 401(k) example. Further understanding these\ndifferences and the impact of the number of folds on inference for objects of interest\nseems like an interesting question for future research. Finally, although the estimates are\nsomewhat smaller than the baseline estimates reported in AJR - an estimated coefficient\nof 1.10 with estimated standard error of 0.46 (Acemoglu et al. (2001), Table 4, Panel A,\ncolumn 7) - the results are qualitatively similar, indicating a strong and positive effect\nof institutions on output. 6.4. Comments on Empirical Results\nBefore closing this section we want to emphasize some important conclusions that can\nbe drawn from these empirical examples. First, the choice of the ML method used in\nestimating nuisance functions does not substantively change the conclusion in any of\nthe examples, and we obtained broadly consistent results regardless of which method\nwe employ. The robustness of the results to the different methods is implied by the\ntheory assuming that all of the employed methods are able to deliver sufficiently high-\nquality approximations to the underlying nuisance functions. Second, the incorporation\nof uncertainty due to sample-splitting using the median method increases the standard\nerrors relative to a baseline that does not account for this uncertainty, though these\ndifferences do not alter the main results in any of the examples.",
    "content_hash": "eaf09e4f9bfd0f36dd65210771bb6f947a86ac754d306f05d95495dd8c5893ab",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "226c17ba-4ff5-4ce8-9491-a0c32a9c34c6",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "This lack of variation\nsuggests that the parameter estimates are robust to the particular sample split used in\nthe estimation in these examples. ACKNOWLEDGEMENTS\nWe would like to acknowledge research support from the National Science Foundation. We also thank participants of the MIT Stochastics and Statistics seminar, the Kansas\nEconometrics conference, the Royal Economic Society Annual Conference, The Hannan\nLecture at the Australasian Econometric Society meeting, The Econometric Theory lec-\nture at the EC2 meetings 2016 in Toulouse, The CORE 50th Anniversary Conference,\nThe Becker-Friedman Institute Conference on Machine Learning and Economics, The\nINET conferences at USC on Big Data, the World Congress of Probability and Statistics\n2016, the Joint Statistical Meetings 2016, the New England Day of Statistics Conference,\nCEMMAP’s Masterclass on Causal Machine Learning, and St. Gallen’s summer school\non “Big Data”, for many useful comments and questions. We would like to thank Susan\nAthey, Peter Aronow, Jin Hahn, Guido Imbens, Mark van der Laan, Matt Taddy, and\nRui Wang for constructive comments. We thank Peter Aronow for pointing us to the\nliterature on targeted learning on which, along with prior works of Neyman, Bickel, and\nthe many other contributions to semiparametric learning theory, we build. REFERENCES\nAbadie, A. and G. W. Imbens (2006). Large sample properties of matching estimators\nfor average treatment effects. Econometrica 74, 235–267. Acemoglu, D., S. Johnson, and J. A. Robinson (2001). The colonial origins of comparative\ndevelopment: An empirical investigation.",
    "content_hash": "03c05a83549d8614b77a8f0c70b1d550be5c084e7c5f1cc5a9eb5bb86028122e",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1e565f78-655d-4fa8-892f-87ba939a017d",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "American Economic Review 91, 1369–1401.",
    "content_hash": "9acb5ec831e32c434d3f2a3d1f5209a3c05b625e78c74cefe11b23d73a084e1e",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d5548a6c-ec37-4289-9680-c41ec55f9334",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n47\nAi, C. and X. Chen (2012). The semiparametric efficiency bound for models of sequential\nmoment restrictions containing unknown functions. Journal of Econometrics 170, 442–\n457. Andrews, D. W. K. (1994a). Asymptotics for semiparametric econometric models via\nstochastic equicontinuity. Econometrica 62, 43–72. Andrews, D. W. K. (1994b). Empirical process methods in econometrics. Handbook of\nEconometrics, Volume IV, Chapter 37, 2247–2294. Angrist, J. D. and A. B. Krueger (1995). Split-sample instrumental variables estimates\nof the return to schooling. Journal of Business and Economic Statistics 13, 225–235. Athey, S., G. Imbens, and S. Wager (2016). Approximate residual balancing: De-biased\ninference of average treatment effects in high-dimensions. arXiv:1604.07125v3. arXiv,\n2016. Ayyagari, R. (2010). Applications of influence functions to semiparametric regression\nmodels. Ph.D. Thesis, Harvard School of Public Health, Harvard University. Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen (2012). Sparse models and meth-\nods for optimal instruments with an application to eminent domain. Econometrica 80,\n2369–2429. arXiv, 2010. Belloni, A. and V. Chernozhukov (2011).",
    "content_hash": "162edc6d3bfd38f66a3f7c024ca6b129524fd41d8078d52a93b491d6d7954dae",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fa3f11e9-b7d8-4496-9bda-50150609c905",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "ℓ1-penalized quantile regression for high di-\nmensional sparse models. Annals of Statistics 39, 82–130. arXiv, 2009. Belloni, A. and V. Chernozhukov (2013). Least squares after model selection in high-\ndimensional sparse models. Bernoulli 19, 521–547. arXiv, 2009. Belloni, A., V. Chernozhukov, I. Fern´andez-Val, and C. Hansen (2017). Program evalu-\nation with high-dimensional data. Econometrica 85, 233—-298. arXiv, 2013. Belloni, A., V. Chernozhukov, and C. Hansen (2010). Lasso methods for gaussian instru-\nmental variables models. arXiv:1012.1297. Belloni, A., V. Chernozhukov, and C. Hansen (2013). Inference for high-dimensional\nsparse econometric models. Advances in Economics and Econometrics. 10th World\nCongress of Econometric Society. August 2010, III:245–295. arXiv, 2011. Belloni, A., V. Chernozhukov, and C. Hansen (2014). Inference on treatment effects after\nselection amongst high-dimensional controls. Review of Economic Studies 81, 608–650. arXiv, 2011. Belloni, A., V. Chernozhukov, and K. Kato (2015). Uniform post selection inference for\nlad regression models and other z-estimators. Biometrika 102, 77–94.",
    "content_hash": "844e0c59ad9973a4da3b641ffa93356bad5bf7e08f49c63bcd2f256b8fb596a6",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d2a77d3f-1deb-4f94-89ef-71ba97890404",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Tsybakov (2009). Simultaneous analysis of Lasso and\nDantzig selector. Annals of Statistics 37, 1705–1732. arXiv, 2008.",
    "content_hash": "0e98cf4b902c9f00315c3e0b5722d5aaf27e38c88942e2e07c85c94ed5cb4d22",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "06a27a9c-d8af-48d5-a810-1ccf0d15679a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Gaussian approximation of\nsuprema of empirical processes. The Annals of Statistics 42, 1564–1597. arXiv, 2012. Chernozhukov, V., J. Escanciano, H. Ichimura, W. Newey, and J. Robins (2016). Locally\nrobust semiparametric estimation. arXiv:1608.00033. arXiv, 2016. Chernozhukov, V. and C. Hansen (2004). The effects of 401 (k) participation on the\nwealth distribution: an instrumental quantile regression analysis. Review of Economics\nand statistics 86, 735–751. Chernozhukov, V., C. Hansen, and M. Spindler (2015a). Post-selection and post-\nregularization inference in linear models with very many controls and instruments. Americal Economic Review: Papers and Proceedings 105, 486–490. Chernozhukov, V., C. Hansen, and M. Spindler (2015b). Valid post-selection and post-\nregularization inference: An elementary, general approach. Annual Review of Eco-\nnomics 7, 649–688. DasGupta, A. (2008). Asymptotic Theory of Statistics and Probability. Springer Texts\nin Statistics. Fan, J., S. Guo, and K. Yu (2012). Variance estimation using refitted cross-validation in\nultrahigh dimensional regression. Journal of the Royal Statistical Society, Series B 74,\n37–65. Farrell, M. (2015). Robust inference on average treatment effects with possibly more\ncovariates than observations.",
    "content_hash": "9a208c89dac1ec4844a58394bd8cc1be4d560c7dd098a312ee4200f633268e69",
    "location": null,
    "page_start": 48,
    "page_end": 48,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "26450377-ab2d-4aa8-99e6-21d3e3b83972",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Montanari (2014b). Hypothesis testing in high-dimensional regres-\nsion under the gaussian random design model: Asymptotic theory. IEEE Transactions\non Information Theory 60, 6522–6554. arXiv, 2013. Kozbur, D. (2016). Testing-based forward model selection. arXiv:1512.02666. arXiv,\n2015. Lee, L. (2005). A c(α)-type gradient test in the GMM approach. Working paper, The\nOhio State University. Levit, B. Y. (1975). On the efficiency of a class of nonparametric estimates. Theory of\nProbability and Its Applications 20, 723–740. Linton, O. (1996). Edgeworth approximation for MINPIN estimators in semiparametric\nregression models. Econometric Theory 12, 30–60. Luedtke, A. R. and M. J. van der Laan (2016). Optimal individualized treatments in\nresource-limited settings. The International Journal of Biostatistics 12, 283–303. Luo, Y. and M. Spindler (2016). High-dimensional l2 boosting: Rate of convergence. arXiv:1602.08927. arXiv, 2016. Nevelson, M. (1977). On one informational lower bound. Problemy Peredachi Informat-\nsii 13, 26–31. Newey, W. (1990). Semiparametric efficiency bounds. Journal of Applied Econometrics 5,\n99–135. Newey, W. (1994).",
    "content_hash": "56975b60a59069d5db66418e24433a5384fb29ca8c934ce65e2be65ea501ebf2",
    "location": null,
    "page_start": 49,
    "page_end": 49,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "737cfa4c-de7f-4e77-ab8d-d55df90a4e8a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n49\nHirano, K., G. W. Imbens, and G. Ridder (2003). Efficient estimation of average treat-\nment effects using the estimated propensity score. Econometrica 71, 1161–1189. Hubbard, A. E., S. Kherad-Pajouh, and M. J. van der Laan (2016). Statistical inference\nfor data adaptive target parameters. International Journal of Biostatistics 12, 3–19. Ibragimov, I. A. and R. Z. Hasminskii (1981). Statistical Estimation: Asymptotic Theory. Springer-Verlag, New York. Ichimura, H. and W. Newey (2015). The influence function of semiparametric estimators. arXiv:1508.01378. arXiv, 2015. Imai, K. and M. Ratkovic (2013). Estimating treatment effect heterogeneity in random-\nized program evaluation. Annals of Applied Statistics 7, 443–470. Imbens, G. and J. Angrist (1994). Identification and estimation of local average treatment\neffects. Econometrica, 467–475. Imbens, G. W. and D. B. Rubin (2015). Causal Inference for Statistics, Social, and\nBiomedical Sciences: An Introduction. Cambridge University Press. Javanmard, A. and A. Montanari (2014a). Confidence intervals and hypothesis testing\nfor high-dimensional regression. Journal of Machine Learning Research 15, 2869–2909. Javanmard, A. and A.",
    "content_hash": "2fb238a19f4c793782e907f4c85bf4a5b0ae717db2da5788b602164e27d6061e",
    "location": null,
    "page_start": 49,
    "page_end": 49,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4bbbb038-8005-4a5e-b24d-528eb4e145b1",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "50\nCCDDHNR\nPoterba, J. M., S. F. Venti, and D. A. Wise (1994b). Do 401(k) contributions crowd out\nother personal saving? Journal of Public Economics 58, 1–32. Robins, J., L. Li, R. Mukherjee, E. Tchetgen, and A. van der Vaart (2017). Minimax\nestimation of a functional on a structured high dimensional model. Annals of Statistics,\nforthcoming. Robins, J., L. Li, E. Tchetgen, and A. van der Vaart (2008). Higher order influence\nfunctions and minimax estimation of nonlinear functionals. In D. Nolan and T. Speed\n(Eds.), Probability and Statistics: Essays in Honor of David A. Freedman, pp. 335–421. Institute of Mathematical Statistics. Robins, J. and A. Rotnitzky (1995). Semiparametric efficiency in multivariate regression\nmodels with missing data. Journal of the American Statistical Association 90, 122–129. Robins, J., P. Zhang, R. Ayyagari, R. Logan, E. Tchetgen, L. Li, A. Lumley, and\nA. van der Vaart (2013). New statistical approaches to semiparametric regression with\napplication to air pollution research. Research Report 175, Health Effects Institute. Robinson, P. M. (1988). Root-N-consistent semiparametric regression. Econometrica 56,\n931–954. Rosenbaum, P. R. and D. B. Rubin (1983).",
    "content_hash": "bee06e1c2a098053e206c672fde2d91c291d3ddef176c0dc15d09d563222a94f",
    "location": null,
    "page_start": 50,
    "page_end": 50,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3d371ac0-573a-43dd-a1ce-f3658e2135df",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "The central role of the propensity score in\nobservational studies for causal effects. Biometrika 70, 41–55. Scharfstein, D. O., A. Rotnitzky, and J. M. Robins (1999). Rejoinder to “adjusting for\nnon-ignorable drop-out using semiparametric non-response models”. Journal of the\nAmerican Statistical Association 94, 1135–1146. Schick, A. (1986). On asymptotically efficient estimation in semiparametric models. Annals of Statistics 14, 1139–1151. Severini, T. A. and W. H. Wong (1992). Profile likelihood and conditionally parametric\nmodels. The Annals of Statistics 20, 1768–1802. Toth, B. and M. J. van der Laan (2016). TMLE for marginal structural models based on\nan instrument. Working Paper 350, U.C. Berkeley Division of Biostatistics Working\nPaper Series. van de Geer, S., P. B¨uhlmann, Y. Ritov, and R. Dezeure (2014). On asymptotically opti-\nmal confidence regions and tests for high-dimensional models. Annals of Statistics 42,\n1166–1202. arXiv, 2013. van der Laan, M. and D. Rubin (2006). Targeted maximum likelihood learning. Working\nPaper 213, UC Berkeley Division of Biostatistics Working Paper Series. van der Laan, M. J. (2015). A generally efficient targeted minimum loss based estimator. Working Paper 343, U.C.",
    "content_hash": "3fd5d1cb2977438e135535932e9d8590b72b62f2a5cea576e1b60b660d6a794d",
    "location": null,
    "page_start": 50,
    "page_end": 50,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8d585f78-d043-4e7f-9d3b-05eecc6126b6",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n51\nZhang, C. and S. Zhang (2014). Confidence intervals for low-dimensional parameters with\nhigh-dimensional data. Journal of the Royal Statistical Society, Series B 76, 217–242. arXiv, 2012. Zheng, W., Z. Luo, and M. J. van der Laan (2016). Marginal structural models with coun-\nterfactual effect modifiers. Working Paper 348, U.C. Berkeley Division of Biostatistics\nWorking Paper Series. Zheng, W. and M. J. van der Laan (2011). Cross-validated targeted minimum-loss-based\nestimation. In Targeted Learning, pp. 459–474. Springer. APPENDIX: PROOFS OF RESULTS\nIn this appendix, we use C to denote a strictly positive constant that is independent\nof n and P ∈PN. The value of C may change at each appearance. Also, the notation\naN ≲bN means that aN ⩽CbN for all n and some C. The notation aN ≳bN means that\nbN ≲aN. Moreover, the notation aN = o(1) means that there exists a sequence (bN)n⩾1\nof positive numbers such that (a) |aN| ⩽bN for all n, (b) bN is independent of P ∈PN\nfor all n, and (c) bN →0 as n →∞. Finally, the notation aN = OP (bN) means that for\nall ϵ > 0, there exists C such that PP (aN > CbN) ⩽1 −ϵ for all n. Using this notation\nallows us to avoid repeating “uniformly over P ∈PN” many times in the proofs.",
    "content_hash": "85648cf9583ca9b1d61c3078b9f37d9de6fe69647706f506f52603e05751b03a",
    "location": null,
    "page_start": 51,
    "page_end": 51,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ddfa2dc0-2c73-458c-95dc-38c90dc11ad8",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "For any ϵ > 0 P(∥Xm∥> ϵm) ⩽E[P(∥Xm∥> ϵm | Ym)] →0, since the\nsequence {P(∥Xm∥> ϵm | Ym)} is uniformly integrable. To show the second part note\nthat P(∥Xm∥> ϵm | Ym) ⩽E[∥Xm∥q/ϵq\nm | Ym] ∨1 →P 0 by Markov’s inequality. Part\n(b). This follows from Part (a). ■\nLet (Wi)n\ni=1 be a sequence of independent copies of a random element W taking values\nin a measurable space (W, AW) according to a probability law P. Let F be a set of",
    "content_hash": "c1ebdf5ae355bb1f64ec37739b8b8ab69ed725d706f62105f006af293b267cbb",
    "location": null,
    "page_start": 51,
    "page_end": 51,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "69502b05-6f9d-4ac0-8228-db85851a17ae",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Define the empirical process Gn(ψ(W)) as a linear operator acting on measurable\nfunctions ψ : W →R such that ∥ψ∥P,2 < ∞via,\nGn(ψ(W)) := Gn,I(ψ(W)) :=\n1\n√n\nX\ni∈I\nψ(Wi) −\nZ\nψ(w)dP(w). Analogously, we defined the empirical expectation as:\nEn(ψ(W)) := En,I(ψ(W)) := 1\nn\nX\ni∈I\nψ(Wi). A.5. Useful Lemmas\nThe following lemma is useful particularly in the sample-splitting contexts. Lemma 6.1. (Conditional Convergence Implies Unconditional) Let {Xm} and\n{Ym} be sequences of random vectors. (a) If for ϵm →0, P(∥Xm∥> ϵm | Ym) →P 0,\nthen P(∥Xm∥> ϵm) →0. In particular, this occurs if E[∥Xm∥q/ϵq\nm | Ym] →P 0 for\nsome q ⩾1, by Markov’s inequality. (b) Let {Am} be a sequence of positive constants. If\n∥Xm∥= OP (Am) conditional on Ym, namely, that for any ℓm →∞, P(∥Xm∥> ℓmAm |\nYm) →P 0, then ∥Xm∥= OP (Am) unconditionally, namely, that for any ℓm →∞,\nP(∥Xm∥> ℓmAm) →0. Proof. Part (a).",
    "content_hash": "c79fdca2a60ba431da20e264de279f10302e2f55561cebcafd303adab7a2c2e5",
    "location": null,
    "page_start": 51,
    "page_end": 51,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "006bdba9-acb9-482d-9fbf-be015e3f7dc6",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Then\n∂ηEP ψ(W, θ0, η0)[η −η0]\n=\n(Jθβ −µ0Jββ)(β −β0)\n⩽∥Jθβ −µ0Jββ∥q × ∥β −β0∥∗\nq ⩽rn × (λN/rN) = λN. This completes the proof of the lemma. ■",
    "content_hash": "d295bf5cd59ad2ed148181d74c8394e6aee9dcfbbaeb03d46bc0afc78e1c5651",
    "location": null,
    "page_start": 52,
    "page_end": 52,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b0b032e8-af45-4402-a7ad-1e0457f3e857",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Then\nψ(W; θ, η0 + r(η −η0)) = ∂θQ(W; θ, r),\nand so\n∂rEP [ψ(W; θ, η0 + r(η −η0))] = ∂rEP [∂θQ(W; θ, r)]\n= ∂r∂θEP [Q(W; θ, r)] = ∂θ∂rEP [Q(W; θ, r)]\n(A.2)\n= ∂θ∂rEP [ℓ(W; θ, η0(θ) + r(η(θ) −η0(θ)))]. Hence,\n∂rEP [ψ(W; θ, η0 + r(η −η0))]\nr=0 = 0\nsince\n∂rEP [ℓ(W; θ, η0(θ) + r(η(θ) −η0(θ)))]\nr=0 = 0,\nfor all θ ∈Θ,\nas η0(θ) = βθ solves the optimization problem\nmax\nβ∈B EP [ℓ(W; θ, β)],\nfor all θ ∈Θ. Here the regularity conditions are needed to make sure that we can interchange EP and\n∂θ and also ∂θ and ∂r in (A.2). This completes the proof of the lemma.",
    "content_hash": "5e799a194ac8661d7734a3dad5a8302fef1e96583ed9ee3ae1d518bc460d5d03",
    "location": null,
    "page_start": 53,
    "page_end": 53,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "23f2d915-7b00-4490-a8e6-427b87ab0c5d",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n53\nA.8. Proof of Lemma 2.3\nThe proof is similar to that of Lemma 2.1, except that now we have\n∂η′EP ψ(W, θ0, η0) = [µ0Gβ, EP m(W, θ0, β0)′ ⊗Idθ×dθ] = 0,\nwhere Idθ×dθ is the dθ × dθ identity matrix and ⊗is the Kronecker product. ■\nA.9. Proof of Lemma 2.4\nThe proof follows similarly to that of Lemma 2.2, except that now for any β ∈B such\nthat ∥β −β0∥1 ⩽λN/rN, any dθ × k matrix µ, and η = (β′, vec(µ)′)′, we have\n∂ηEP ψ(W, θ0, η0)[η −η0]\n= ∥µ0Gβ(β −β0)∥\n⩽∥A′Ω−1/2L −γ0L′L∥∞× ∥β −β0∥1\n⩽rn × (λN/rN) = λN. This completes the proof of the lemma. A.10. Proof of Lemma 2.5\nTake any η ∈T, and consider the function\nQ(W; θ, r) := ℓ(W; θ, η0(θ) + r(η(θ) −η0(θ))),\nθ ∈Θ, r ∈[0, 1].",
    "content_hash": "2d6792fb4e886b9220cebda3c3def2177dcec6ac56d66cc4f24263c24c047d06",
    "location": null,
    "page_start": 53,
    "page_end": 53,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "51a70b31-e868-4d6b-91ee-f6b4a5816c85",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Then\nEP [ψ(W, θ0, η0 + r(η −η0)]\n= EP\nh\n(µ0(R) + r(µ(R) −µ0(R)))m(W, θ0, h0(Z) + r(h(Z) −h0(Z)))\ni\n,\nand so\n∂ηEP ψ(W, θ0, η0)[η −η0] = I1 + I2,\nwhere\nI1 = EP\nh\n(µ(R) −µ0(R))m(W, θ0, h0(Z)\ni\n,\nI2 = EP\nh\nµ0(R)∂v′m(W, θ0, v)|v=h0(Z)(h(Z) −h0(Z))\ni\n.",
    "content_hash": "5d969a57455023c23d7debada5aab29c772a852120b44937121f762faf0bb4a8",
    "location": null,
    "page_start": 54,
    "page_end": 54,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f12b5ff5-a02f-4646-a762-0c31cce33265",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "54\nCCDDHNR\nA.11. Proof of Lemma 2.6\nFirst, we demonstrate that µ0 ∈L1(R; Rdθ×dm). Indeed,\nEP [∥µ0(R)∥] ⩽EP\nh\n∥A(R)′Ω(R)−1∥\ni\n+ EP\nh\n∥G(Z)Γ(R)Ω(R)−1∥\ni\n⩽EP\nh\n∥A(R)∥× ∥Ω(R)∥−1i\n+ EP\nh\n∥G(Z)∥× ∥Γ(R)∥× ∥Ω(R)∥−1i\n⩽\n\u0010\nEP [∥A(R)∥2] × EP [∥Ω(R)∥−2]\n\u00111/2\n+\n\u0010\nEP\nh\n∥G(Z)∥2 × ∥Γ(R)∥2i\n× EP [∥Ω(R)∥−2]\n\u00111/2\n,\nwhich is finite by assumptions of the lemma since\nEP\nh\n∥G(Z)∥2 × ∥Γ(R)∥2i\n⩽\n\u0010\nEP [∥G(Z)∥4] × EP [Γ(R)∥4]\n\u00111/2\n< ∞. Next, we demonstrate that\nEP [∥ψ(W, θ0, η)∥] < ∞\nfor all η ∈T.",
    "content_hash": "63dfa0e9ac27b86f0ce524fbd93a463ed6aed6a4685f7a6dbbd02f1e2e50373e",
    "location": null,
    "page_start": 54,
    "page_end": 54,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "89e17b09-c49f-43ea-9560-ecdce9a5e286",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Indeed, for all η ∈T, there exist µ ∈L1(R; Rdθ×dm) and h ∈H such that η = (µ, h),\nand so\nEP [∥ψ(W, θ0, η)∥] = EP [∥µ(X)m(W, θ0, h(Z))∥]\n⩽EP\nh\n∥µ(R)∥× ∥m(W, θ0, h(Z))∥\ni\n= EP\nh\n∥µ(R)∥× EP [∥m(W, θ0, h(Z)) | R]\ni\n⩽ChE[∥µ(R)∥],\nwhich is finite by assumptions of the lemma. Further, (2.1) holds because\nEP [ψ(W, θ0, η0)] = EP\nh\nµ0(R)m(W, θ0, h0(Z))\ni\n= EP\nh\nµ0(R)EP [m(W, θ0, h0(Z)) | R]\ni\n= 0,\n(A.3)\nwhere the last equality follows from (2.22). Finally, we demonstrate that (2.3) holds. To do so, take any η = (µ, h) ∈TN = T.",
    "content_hash": "18ffacdc6b8eed5a2998abd6340d1dbea27718c42e324f9d67a5d9873a23dd31",
    "location": null,
    "page_start": 54,
    "page_end": 54,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "55d277a8-3418-404f-9462-76ad5466f849",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "In Steps 2, 3, 4, and 5 below, we will show that\n∥RN,1∥= OPN (N −1/2 + rN),\n(A.5)\n∥RN,2∥= OPN (N −1/2r′\nN + λN + λ′\nN),\n(A.6)\n∥N −1/2 PN\ni=1 ψ(Wi; θ0, η0)∥= OPN (1),\n(A.7)\n∥σ−1∥= OPN (1),\n(A.8)\nrespectively. Since N −1/2 + rN ⩽ρN = o(1) and all singular values of J0 are bounded\nbelow from zero by Assumption 3.1, it follows from (A.5) that with PN-probability 1 −",
    "content_hash": "555e7d3336298fcd21cfdc557d65f8294c2794ad9f3caa4f5433c53d5cac2da8",
    "location": null,
    "page_start": 55,
    "page_end": 55,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a4874f3e-2be6-45ed-82f1-27040ef91934",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n55\nHere I1 = 0 by the same argument as that in (A.3) and I2 = 0 because\nI2 = EP\nh\nµ0(R)EP [∂v′m(W, θ0, v)|v=h0(Z) | X](h(Z) −h0(Z))\ni\n= EP\nh\nµ0(R)Γ(X)(h(Z) −h0(Z))\ni\n= EP\nh\nEP [µ0(R)Γ(R) | Z](h(Z) −h0(Z))\ni\n= 0\nsince\nEP [µ0(R)Γ(X) | Z] = EP [A(R)′Ω(R)−1Γ(R) | Z] −EP [G(Z)Γ(R)′Ω(R)−1Γ(R) | Z]\n= EP [A(R)′Ω(R)−1Γ(R) | Z] −G(Z)EP [Γ(R)′Ω(R)−1Γ(R) | Z]\n= EP [A(R)′Ω(R)−1Γ(R) | Z] −EP [A(R)′Ω(R)−1Γ(R) | Z]\n×\n\u0010\nEP [Γ(R)′Ω(R)−1Γ(R) | Z]\n\u0011−1\n× EP [Γ(R)′Ω(R)−1Γ(R) | Z]\n= EP [A(R)′Ω(R)−1Γ(R) | Z] −EP [A(R)′Ω(R)−1Γ(R) | Z] = 0.",
    "content_hash": "ac1ac80c7536351b66d70e4de1efbf308f66574e59b42324406fac71b626991c",
    "location": null,
    "page_start": 55,
    "page_end": 55,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "302c21e0-2894-4acd-80ae-9c50536f688e",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "56\nCCDDHNR\no(1), all singular values of bJ0 are bounded below from zero as well. Therefore, with the\nsame PN-probability,\n˜θ0 = −bJ−1\n0\n1\nK\nK\nX\nk=1\nEn,k[ψb(W; bη0,k)]\nand\n√\nN(˜θ0 −θ0) = −\n√\nN bJ−1\n0\n\u0010 1\nK\nK\nX\nk=1\nEn,k[ψb(W; bη0,k)] + bJ0θ0\n\u0011\n= −\n√\nN bJ−1\n0\n1\nK\nK\nX\nk=1\nEn,k[ψ(W; θ0, bη0,k)]\n= −\n\u0010\nJ0 + RN,1\n\u0011−1\n×\n\u0010 1\n√\nN\nN\nX\ni=1\nψ(Wi; θ0, η0) +\n√\nNRN,2\n\u0011\n. (A.9)\nIn addition, given that\n(J0 + RN,1)−1 −J−1\n0\n= (J0 + RN,1)−1(J0 −(J0 + RN,1))J−1\n0\n= −(J0 + RN,1)−1RN,1J−1\n0 ,\nit follows from (A.5) that\n∥(J0 + RN,1)−1 −J−1\n0 ∥⩽∥(J0 + RN,1)−1∥× ∥RN,1∥× ∥J−1\n0 ∥\n= OPN (1)OPN (N −1/2 + rN)OPN (1) = OPN (N −1/2 + rN).",
    "content_hash": "900724b22bbe7e554ffab903edbf15a45a02295988a304084296c2323c7ebe19",
    "location": null,
    "page_start": 56,
    "page_end": 56,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ef0acc3f-3bf5-44ba-904a-2a65f65c9f75",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(A.10)\nMoreover, since r′\nN +\n√\nN(λN + λ′\nN) ⩽ρN = o(1), it follows from (A.6) and (A.7) that\n1\n√\nN\nN\nX\ni=1\nψ(Wi; θ0, η0) +\n√\nNRN,2\n⩽\n1\n√\nN\nN\nX\ni=1\nψ(Wi; θ0, η0)\n+\n√\nNRN,2\n= OPN (1) + oPN (1) = OPN (1). (A.11)\nCombining (A.10) and (A.11) gives\n\u0010\n(J0 + RN,1)−1 −J−1\n0\n\u0011\n×\n\u0010 1\n√\nN\nN\nX\ni=1\nψ(Wi; θ0, η0) +\n√\nNRN,2\n\u0011\n⩽\n(J0 + RN,1)−1 −J−1\n0\n×\n1\n√\nN\nN\nX\ni=1\nψ(Wi; θ0, η0) +\n√\nNRN,2\n= OPN (N −1/2 + rN). Now, substituting the last bound into (A.9) yields\n√\nN(˜θ0 −θ0) = −J−1\n0\n×\n\u0010 1\n√\nN\nN\nX\ni=1\nψ(Wi; θ0, η0) +\n√\nNRN,2\n\u0011\n+ OPN (N −1/2 + rN)\n= −J−1\n0\n×\n1\n√\nN\nN\nX\ni=1\nψ(Wi; θ0, η0) + OPN (ρN),\nwhere in the second line we used (A.6) and the definition of ρN. Combining this with",
    "content_hash": "543f77b558616c35a885bb4579769386ec89d6c0f3ae41a1c129865c5c942527",
    "location": null,
    "page_start": 56,
    "page_end": 56,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "28ae2642-2399-4620-8a60-e292a91c7214",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Hence, I1,k = OPN (N −1/2) by Lemma\n6.1 in the Appendix. Combining the bounds I1,k = OPN (N −1/2) and I2,k = OPN (rN)\nwith (A.14) gives (A.13). Step 3. Here we establish (A.6). This is the step where we invoke the Neyman orthog-\nonality (or near-orthogonality) condition. Again, since K is a fixed integer, which is\nindependent of N, it suffices to show that for any k ∈[K],\nEn,k[ψ(W; θ0, bη0,k)] −1\nn\nX\ni∈Ik\nψ(Wi; θ0, η0) = OPN (N −1/2r′\nN + λN + λ′\nN). (A.15)\nTo do so, fix any k ∈[K] and introduce the following additional empirical process nota-\ntion:\nGn,k[ϕ(W)] =\n1\n√n\nX\ni∈Ik\n\u0010\nϕ(Wi) −\nZ\nϕ(w)dPN\n\u0011\n,",
    "content_hash": "863a500b16dab2a3638ecf28c1a7bc8a743f71710cdfccfe5f48053d634c974d",
    "location": null,
    "page_start": 57,
    "page_end": 57,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4f6e910f-534d-4f99-b5f7-3e845325e1b6",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n57\n(A.8) gives\n√\nNσ−1(˜θ0 −θ0) =\n1\n√\nN\nN\nX\ni=1\n¯ψ(Wi) + OPN (ρN)\n(A.12)\nby the definition of ¯ψ given in the statement of the theorem. In turn, since ρN = o(1),\ncombining (A.12) with the Lindeberg-Feller CLT and the Cramer-Wold device yields\n(A.4). To complete the proof of the theorem, it remains to establish the bounds (A.5)–\n(A.8). We do so in four steps below. Step 2. Here we establish (A.5). Since K is a fixed integer, which is independent of N,\nit suffices to show that for any k ∈[K],\nEn,k[ψa(W; bη0,k)] −EPN [ψa(W; η0)]\n= OPN (N −1/2 + rN).",
    "content_hash": "7ae8e0bf7c1df10108ac8f91fedf1bc4e7dfdae0e7af0647d44aa09fe1d73075",
    "location": null,
    "page_start": 57,
    "page_end": 57,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fbecf5ae-e220-4e46-a779-fb92dd103c3e",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(A.13)\nTo do so, fix any k ∈[K] and observe that by the triangle inequality,\nEn,k[ψa(W; bη0,k)] −EPN [ψa(W; η0)]\n⩽I1,k + I2,k,\n(A.14)\nwhere\nI1,k :=\nEn,k[ψa(W; bη0,k)] −EPN [ψa(W; bη0,k) | (Wi)i∈Ic\nk]\n,\nI2,k :=\nEPN [ψa(W; bη0,k) | (Wi)i∈Ic\nk] −EPN [ψa(W; η0)]\n. To bound I2,k, note that on the event EN, which holds with PN-probability 1 −o(1),\nI2,k ⩽sup\nη∈TN\nEPN [ψa(W; η)] −EPN [ψa(W; η0)]\n= rN,\nand so I2,k = OPN (rN). To bound I1,k, note that conditional on (Wi)i∈Ic\nk, the estimator\nbη0,k is non-stochastic, and so on the event EN,\nEPN [I2\n1,k | (Wi)i∈Ic\nk] ⩽n−1EPN [∥ψa(W; bη0,k)∥2 | (Wi)i∈Ic\nk]\n⩽sup\nη∈TN\nn−1EPN [∥ψa(W; η)∥2] ⩽c2\n1/n,\nwhere the last inequality holds by Assumption 3.2.",
    "content_hash": "ea9e0c3d4f866a964cd1eb0cdddcd0ec15048fc430e34b6b1e7c7720564573d9",
    "location": null,
    "page_start": 57,
    "page_end": 57,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9f9c75c6-f3f5-4c2a-aadf-b9ea9bc6c9b6",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "To bound I3,k, note that, as above, conditional on (Wi)i∈Ic\nk, the estimator bη0,k is non-\nstochastic, and so on the event EN,\nEPN [I2\n3,k | (Wi)i∈Ic\nk] = EPN\nh\n∥ψ(W; θ0, bη0,k) −ψ(W; θ0, η0)∥2 | (Wi)i∈Ic\nk\ni\n⩽sup\nη∈TN\nEPN\nh\n∥ψ(W; θ0, η) −ψ(W; θ0, η0)∥2 | (Wi)i∈Ic\nk\ni\n⩽sup\nη∈TN\nEPN\nh\n∥ψ(W; θ0, η) −ψ(W; θ0, η0)∥2i\n= (r′\nN)2\nby the definition of r′\nN in Assumption 3.2. Hence, I3,k = OPN (r′\nN) by Lemma 6.1 in the\nAppendix. To bound I4,k, introduce the function\nfk(r) := EPN [ψ(W; θ0, η0 + r(bη0,k −η0)) | (Wi)i∈Ic\nk] −EPN [ψ(W; θ0, η0)],\nr ∈[0, 1]. Then, by Taylor’s expansion,\nfk(1) = fk(0) + f ′\nk(0) + f ′′\nk (˜r)/2,\nfor some ˜r ∈(0, 1).",
    "content_hash": "725fd2a39f9ae3e6158aa3282a3e33163c1909d92173a3ef66b9d2af1a2abc09",
    "location": null,
    "page_start": 58,
    "page_end": 58,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fc7b71d1-6c88-44da-9c22-591cc652924d",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "58\nCCDDHNR\nwhere ϕ is any PN-integrable function on W. Then observe that by the triangle inequality,\nEn,k[ψ(W; θ0, bη0,k)] −1\nn\nX\ni∈Ik\nψ(Wi; θ0, η0)\n⩽I3,k + I4,k\n√n\n,\n(A.16)\nwhere\nI3,k :=\nGn,k[ψ(W; θ0, bη0,k)] −Gn,k[ψ(W; θ0, η0)]\n,\nI4,k := √n\nEPN [ψ(W; θ0, bη0,k) | (Wi)i∈Ic\nk] −EPN [ψ(W; θ0, η0)]\n.",
    "content_hash": "3f024fc5ba0ccd3fb2db031e25c037fe3b80a9a9e5a096ee505a48a1e1ee977a",
    "location": null,
    "page_start": 58,
    "page_end": 58,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7d70ae50-e730-4146-b930-04baf10297ec",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n59\nStep 5. Here we establish (A.8). Note that all eigenvalues of the matrix\nσ2 = J−1\n0 EP [ψ(W; θ0, η0)ψ(W; θ0, η0)′](J−1\n0 )′\nare bounded from below by c0/c2\n1 since all singular values of J0 are bounded from above\nby c1 by Assumption 3.1 and all eigenvalues of EP [ψ(W; θ0, η0)ψ(W; θ0, η0)′] are bounded\nfrom below by c0 by Assumption 3.2. Hence, given that ∥σ−1∥is the largest eigenvalue\nof σ−1, it follows that ∥σ−1∥= c1/√c0. This gives (A.8) and completes the proof of the\ntheorem. ■\nA.13. Proof of Theorem 3.1 (DML1 case)\nAs in the case of the DML2 version, note that (3.11) follows immediately from the\nassumptions, and so it suffices to show that (3.10) holds uniformly over P ∈PN. Fix any sequence {PN}N⩾1 such that PN ∈PN for all N ⩾1. Since this sequence is\nchosen arbitrarily, to show that (3.10) holds uniformly over P ∈PN, it suffices to show\nthat\n√\nNσ−1(˜θ0 −θ0) =\n1\n√\nN\nN\nX\ni=1\n¯ψ(Wi) + OPN (ρN) ; N(0, Id).",
    "content_hash": "70455e6583e449c5178218bc584fc343a7a8fc14d55634335536dbe249ca4017",
    "location": null,
    "page_start": 59,
    "page_end": 59,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f7116bb5-800a-460c-ab5a-d9beddc9cf9a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(A.17)\nTo do so, for all k ∈[K], denote\nbJ0,k := En,k[ψa(W; bη0,k)],\nRN,1,k := bJ0,k −J0,\nRN,2,k := En,k[ψ(W; θ0, bη0,k)] −1\nn\nX\ni∈Ik\nψ(Wi; θ0, η0). Since K is a fixed integer, which is independent of n, it follows by the same arguments\nas those in Steps 2–5 in Section A.12 that\nmax\nk∈[K] ∥RN,1,k∥= OPN (N −1/2 + rN),\n(A.18)\nmax\nk∈[K] ∥RN,2,k∥= OPN (N −1/2r′\nN + λN + λ′\nN),\n(A.19)\nmax\nk∈[K] ∥n−1/2 P\ni∈Ik ψ(Wi; θ0, η0)∥= OPN (1),\n(A.20)\n∥σ−1∥= OPN (1).",
    "content_hash": "4aa734ee7da94a69894c2962afc82f579a49968acef84dc151ad698badaebe40",
    "location": null,
    "page_start": 59,
    "page_end": 59,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c90c1060-8391-44ca-86bb-3b6fc815c476",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(A.21)\nSince N −1/2 + rN ⩽ρN = o(1) and all singular values of J0 are bounded below from\nzero by Assumption 3.1, it follows from (A.18) that for all k ∈[K], with PN-probability\n1 −o(1), all singular values of bJ0,k are bounded below from zero, and so with the same\nPN-probability,\nˇθ0,k = −bJ−1\n0,kEn,k[ψb(W; bη0,k)]. Hence, by the same arguments as those in Step 1 in Section A.12, it follows from the\nbounds (A.18)–(A.21) that for all k ∈[K],\n√nσ−1(ˇθ0,k −θ0) =\n1\n√n\nX\ni∈Ik\n¯ψ(Wi) + OPN (ρN).",
    "content_hash": "11d2d9131ec0449ebe7f12e21577a152190a9103adf1390b23c27c4f3a55b6cb",
    "location": null,
    "page_start": 59,
    "page_end": 59,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "152ec30f-c354-44db-9d7a-84ced64e52d3",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "60\nCCDDHNR\nTherefore,\n√\nNσ−1(˜θ0 −θ0) =\n√\nNσ−1\u0010 1\nK\nK\nX\nk=1\nˇθ0,k −θ0\n\u0011\n=\n1\n√\nN\nN\nX\ni=1\n¯ψ(Wi) + OPN (ρN). (A.22)\nIn turn, since ρN = o(1), combining (A.22) with the Lindeberg-Feller CLT and the\nCramer-Wold device yields (A.17) and completes the proof of the theorem. ■\nA.14. Proof of Theorem 3.2. In this proof, all bounds hold uniformly in P ∈PN for N ⩾3, and we do not repeat this\nqualification throughout. Also, the second asserted claim follows immediately from the\nfirst one and Theorem 3.1. Hence, it suffices to prove the first asserted claim. In the proof of Theorem 3.1 in Section A.12, we established that ∥bJ0 −J0∥= OP (rN +\nN −1/2). Hence, since ∥J−1\n0 ∥⩽c−1\n0\nby Assumption 3.1 and\nEP [ψ(W; θ0, η0)ψ(W; θ0, η0)′]\n⩽EP [∥ψ(W; θ0, η0)∥2] ⩽c2\n1\nby Assumption 3.2, it suffices to show that\n1\nK\nK\nX\nk=1\nEn,k[ψ(W; ˜θ0, bη0,k)ψ(W; ˜θ0, bη0,k)′] −EP [ψ(W; θ0, η0)ψ(W; θ0, η0)′]\n= OP (ϱN).",
    "content_hash": "d1d962cc1c38b830acc4b2597d2a98a78b69fb950d7ad2cceb944ff766ea8986",
    "location": null,
    "page_start": 60,
    "page_end": 60,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "17223f66-b7b4-4fcf-befe-693962c8ede2",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Moreover, since both K and dθ, the dimension of ψ, are fixed integers, which are inde-\npendent of N, the last bound will follow if we show that for all k ∈[K] and all j, k ∈[dθ],\nIkjl :=\nEn,k[ψj(W; ˜θ0, bη0,k)ψl(W; ˜θ0, bη0,k)] −EP [ψj(W; θ0, η0)ψl(W; θ0, η0)]\nsatisfies\nIkjl = OP (ϱN). (A.23)\nTo do so, observe that by the triangle inequality,\nIkjl ⩽Ikjl,1 + Ikjl,2,\n(A.24)\nwhere\nIkjl,1 :=\nEn,k[ψj(W; ˜θ0, bη0,k)ψl(W; ˜θ0, bη0,k)] −En,k[ψj(W; θ0, η0)ψl(W; θ0, η0)]\n,\nIkjl,2 :=\nEn,k[ψj(W; θ0, η0)ψl(W; θ0, η0)] −EP [ψj(W; θ0, η0)ψl(W; θ0, η0)]\n. We bound Ikjl,2 first.",
    "content_hash": "64b275b7d3df45f665093908e724bcfaea741510395836a8f971354890c1694c",
    "location": null,
    "page_start": 60,
    "page_end": 60,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9d060542-8b0b-44cc-be0a-c87ca301757b",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "If q ⩾4, then\nEP [I2\nkjl,2] ⩽n−1EP\nh\n(ψj(W; θ0, η0)ψl(W; θ0, η0))2i\n⩽n−1\u0010\nEP [ψ4\nj (W; θ0, η0)] × EP [ψ4\nl (W; θ0, η0)]\n\u00111/2\n⩽n−1EP [∥ψ(W; θ0, η0)∥4] ⩽c4\n1,\nwhere the second line holds by H¨older’s inequality, and the third one by Assumption\n3.2. Hence, Ikjl,2 = OP (N −1/2). If q ∈(2, 4), we apply the following von Bahr-Esseen\ninequality with p = q/2: if X1, . . . , Xn are independent random variables with mean zero,",
    "content_hash": "3f222632883b4198255af1661fdccf40ba36209ac1854809d890c91c3e009be1",
    "location": null,
    "page_start": 60,
    "page_end": 60,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "915cb105-b8cd-49b7-806a-8c656b067054",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Denoting\nψhi := ψh(Wi; θ0, η0) and bψhi := ψh(Wi; ˜θ0, bη0,k), for (h, i) ∈{j, l} × Ik,\nand applying the inequality above with a := ψji, b := ψli, a + δa := bψji, b + δb := bψli,\nr := | bψji −ψji| ∨| bψli −ψli|, and c := |ψji| ∨|ψli| gives\nIkjl,1 =\n1\nn\nX\ni∈Ik\nbψji bψli −ψjiψli\n⩽1\nn\nX\ni∈Ik\n| bψji bψli −ψjiψli|\n⩽2\nn\nX\ni∈Ik\n\u0010\n| bψji −ψji| ∨| bψli −ψli|\n\u0011\n×\n\u0010\n|ψji| ∨|ψli| + | bψji −ψji| ∨| bψli −ψli|\n\u0011\n⩽\n\u0010 2\nn\nX\ni∈Ik\n\u0010\n| bψji −ψji|2 ∨| bψli −ψli|2\u0011\u00111/2\n×\n\u0010 2\nn\nX\ni∈Ik\n\u0010\n|ψji| ∨|ψli| + | bψji −ψji| ∨| bψli −ψli|\n\u00112\u00111/2\n.",
    "content_hash": "af1f8ed5c31fe6243339a2aacb6ea4786e1cc98161f24fda5ac91882059f5e16",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c6086f0d-9d64-444e-8771-ec06c3c2c9b4",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "In addition, the expression in the last line above is bounded by\n\u0010 2\nn\nX\ni∈Ik\n|ψji|2 ∨|ψli|2\u00111/2\n+\n\u0010 2\nn\nX\ni∈Ik\n| bψji −ψji|2 ∨| bψli −ψli|2\u00111/2\n,\nand so\nI2\nkjl,1 ≲RN ×\n\u0010 1\nn\nX\ni∈Ik\n∥ψ(Wi; θ0, η0)∥2 + RN\n\u0011\n,\nwhere\nRN := 1\nn\nX\ni∈Ik\n∥ψ(Wi; ˜θ0, bη0,k) −ψ(Wi; θ0, η0)∥2. Moreover,\n1\nn\nX\ni∈Ik\n∥ψ(Wi; θ0, η0)∥2 = OP (1),",
    "content_hash": "8c334f76b583c25ea56446ed221b64fe2e98a0dc778f060cf05262432996980b",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "75dbad07-2688-4668-bb26-f93a6de8af4c",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n61\nthen for any p ∈[1, 2],\nE\n\"\nn\nX\ni=1\nXi\np#\n⩽\n\u0010\n2 −1\nn\n\u0011\nn\nX\ni=1\nE[|Xi|p];\nsee DasGupta (2008), p. 650. This gives\nEP [Iq/2\nkjl,2] ≲n−q/2+1EP\nh\n(ψj(W; θ0, η0)ψl(W; θ0, η0))q/2i\n⩽n−q/2+1EP [∥ψ(W; θ0, η0)∥q] ≲n−q/2+1\nby Assumption 3.2. Hence, Ikjl,2 = OP (N 2/q−1). Conclude that\nIkjl,2 = OP\n\u0010\nN −[(1−2/q)∧(1/2)]\u0011\n. (A.25)\nNext, we bound Ikjl,1. To do so, observe that for any numbers a, b, δa, and δb such that\n|a| ∨|b| ⩽c and |δa| ∨|δb| ⩽r, we have\n(a + δa)(b + δb) −ab\n⩽2r(c + r).",
    "content_hash": "c1071d6fe1f8427348bbaacfedcd03cd6a31d014c8fb2a59766492107de4d622",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ec2a1a32-ce18-4c35-815d-755f5f22e6c0",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "62\nCCDDHNR\nby Markov’s inequality since\nEP\nh 1\nn\nX\ni∈Ik\n∥ψ(Wi; θ0, η0)∥2i\n= EP [∥ψ(W; θ0, η0∥2] ⩽c2\n1\nby Assumption 3.2. It remains to bound RN. We have\nRN ≲1\nn\nX\ni∈Ik\nψa(Wi; bη0,k)(˜θ0 −θ0)\n2\n+ 1\nn\nX\ni∈Ik\nψ(Wi; θ0, bη0,k)−ψ(Wi; θ0, η0)\n2\n. (A.26)\nThe first term on the right-hand side of (A.26) is bounded from above by\n\u0010 1\nn\nX\ni∈Ik\n∥ψa(Wi; bη0,k)∥2\u0011\n× ∥˜θ0 −θ0∥2 = OP (1) × OP (N −1) = OP (N −1),\nand the conditional expectation of the second term given (Wi)i∈Ic\nk on the event that\nbη0,k ∈TN is equal to\nEP\nh\n∥ψ(W; θ0, bη0,k) −ψ(W; θ0, η0)∥2 | (Wi)i∈Ic\nk\ni\n⩽sup\nη∈TN\nEP\nh\n∥ψ(W; θ0, η) −ψ(W; θ0, η0)∥2 | (Wi)i∈Ic\nk\ni\n= (r′\nN)2.",
    "content_hash": "fef9e6e8072749c68332117012fa62a9bd30dbbf190b75dd5d8f3f7988adfd9a",
    "location": null,
    "page_start": 62,
    "page_end": 62,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "51c609d1-c863-4a26-804c-33a335db8f70",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": ", K, the estimator ˇθ0 = ˇθ0,k defined by\nequation (3.2) obeys\n√nσ−1\n0 (ˇθ0 −θ0) =\n1\n√n\nX\ni∈I\n¯ψ(Wi) + OP (ρ′\nn)\n(A.28)\nuniformly over P ∈PN, where ρ′\nn = n−1/2 + rN + r′\nN + n1/2λN + n1/2λ′\nN ≲δN and\nwhere ¯ψ(·) := −σ−1\n0 J−1\n0 ψ(·, θ0, η0). Proof of Lemma 6.3. Fix any k = 1, . . . , K and any sequence {PN}N⩾1 such that\nPN ∈PN for all N ⩾1. To prove the asserted claim, it suffices to show that the\nestimator ˇθ0 = ˇθ0,k satisfies (A.28) with P replaced by PN. To do so, we split the proof",
    "content_hash": "36ef311f41c21f51876a4dc131da7a243dc7493e4936b9bbdd10456d0dd7dea3",
    "location": null,
    "page_start": 62,
    "page_end": 62,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "dceb0d93-504c-4547-a607-467b930fad7c",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Since the event that bη0,k ∈TN holds with probability 1 −∆N = 1 −o(1), it follows that\nRN = OP (N −1 + (r′\nN)2), and so\nIkjl,1 = OP\n\u0010\nN −1/2 + r′\nN\n\u0011\n. (A.27)\nCombining the bounds (A.25) and (A.27) with (A.24) gives (A.23) and completes the\nproof of the theorem. ■\nProof of Theorem 3.3. We only consider the case of the DML1 estimator and note that the DML2 estimator\ncan be treated similarly. The main part of the proof is the same as that in the linear case (Theorem 3.1, DML1\ncase, presented in Section A.13), once we have the following lemma that establishes\napproximate linearity of the subsample DML estimators ˇθ0,k. Lemma 6.3. (Linearization for Subsample DML in Nonlinear Problems) Under\nthe conditions of Theorem 3.3, for any k = 1, . . .",
    "content_hash": "f2484b51ebda1dfde2df90aa7640340c654e661ea24340bb41f4878aa16a55da",
    "location": null,
    "page_start": 62,
    "page_end": 62,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9b4d7916-3e67-488a-8853-40b70e986dcb",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n63\ninto four steps. In the proof, we will use En, Gn, I, and bη0 instead of En,k, Gn,k, Ik, and\nbη0,k, respectively. Step 1. (Preliminary Rate Result). We claim that with PN-probability 1 −o(1),\n∥ˇθ0 −θ0∥⩽τN. (A.29)\nTo show this claim, note that the definition of ˇθ0 implies that\nEn[ψ(W; ˇθ0, bη0)]\n⩽\nEn[ψ(W; θ0, bη0)]\n+ ϵN,\nwhich in turn implies via the triangle inequality that, with PN-probability 1 −o(1),\nEPN [ψ(W; θ, η0)]|θ=ˇθ0\n⩽ϵN + 2I1 + 2I2,\n(A.30)\nwhere\nI1 :=\nsup\nθ∈Θ,η∈TN\nEPN [ψ(W; θ, η)] −EPN [ψ(W; θ, η0)]\n,\nI2 :=\nmax\nη∈{η0,bη0} sup\nθ∈Θ\nEn[ψ(W; θ, η)] −EPN [ψ(W; θ, η)]\n. Here ϵN = o(τN) because ϵN = o(δNN −1/2), δN = o(1), and τN ⩾c0N −1/2 log n. Also,\nI1 = rN ⩽δNτN = o(τN) by Assumption 3.4(c).",
    "content_hash": "981ca2b5f143af6af2fd0aa055eb61f24f9d4d79fcac64f6c26f7fe47883a37b",
    "location": null,
    "page_start": 63,
    "page_end": 63,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2854021e-9c9b-4dab-9848-3c49356a0957",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(A.33)\nMoreover, for any θ ∈Θ and η ∈TN, we have\n√nEn[ψ(W; θ, η)] = √nEn[ψ(W; θ0, η0)] + Gn[ψ(W; θ, η) −ψ(W; θ0, η0)]\n+ √n\n\u0010\nEPN [ψ(W; θ, η)\n\u0011\n,\n(A.34)\nwhere we are using that EPN [ψ(W; θ0, η0)]= 0. Finally, by Taylor’s expansion of the",
    "content_hash": "0af9c94da81839bb196269653b438991604e2f40454627d11301d8bfca891c0b",
    "location": null,
    "page_start": 63,
    "page_end": 63,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5e0ecddb-821b-4b36-8ceb-75ff60a587eb",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Moreover, applying Lemma 6.2 to the\nfunction class F1,η for η = η0 and η = bη0 defined in Assumption 3.4, conditional on\n(Wi)i∈Ic and Ic, so that bη0 is fixed after conditioning, shows that with PN-probability\n1 −o(1),\nI2 ≲N −1/2(1 + N −1/2+1/q log n) ≲N −1/2 = o(τN). Hence, it follows from (A.30) and Assumption 3.3 that with PN-probability 1 −o(1),\n∥J0(ˇθ0 −θ0)∥∧c0 ⩽\nEPN [ψ(W; θ, η0)]|θ=ˇθ0\n= o(τN). (A.31)\nCombining this bound with the fact that the singular values of J0 are bounded away\nfrom zero, which holds by Assumption 3.3, gives the claim of this step. Step 2. (Linearization) Here we prove the claim of the lemma. First, by definition of ˇθ0,\nwe have\n√n\nEn[ψ(W; ˇθ0, bη0)]\n⩽inf\nθ∈Θ\n√n\nEn[ψ(W; θ, bη0)]\n+ ϵN\n√n. (A.32)\nAlso, it will be shown in Step 4 that\nI3 := inf\nθ∈Θ\n√n∥En[ψ(W; θ, bη0)]∥\n= OPN (n−1/2+1/q log n + r′\nN log1/2(1/r′\nN) + λN\n√n + λ′\nN\n√n).",
    "content_hash": "ae4912f3bdf26968bbcbc2590efbec382f23b7e2285d9fb4535011ad0a42183d",
    "location": null,
    "page_start": 63,
    "page_end": 63,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "683bcb99-dfc8-4433-85b8-8295d3836915",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(A.35)\nTherefore, since ∥ˇθ0 −θ0∥⩽τN and η ∈TN with PN-probability 1 −o(1), and since by\nNeyman λN-near orthogonality,\n∥∂ηEPN [ψ(W; θ0, η0)][bη0 −η0]∥⩽λN,\napplying (A.34) with θ = ˇθ0 and η = bη0, we have with PN-probability 1 −o(1),\n√n\nEn[ψ(W; θ0, η0)] + J0(ˇθ0 −θ0)\n⩽λN\n√n + ϵN\n√n + I3 + I4 + I5,\nwhere by Assumption 3.4,\nI4 := √n\nsup\n∥θ−θ0∥⩽τN ,η∈TN\nZ 1\n0\n2−1∂2\nrEPN [W; θ0 + r(θ −θ0), η0 + r(η −η0)]dr\n⩽λ′\nN\n√n,\nand by Step 3 below, with PN-probability 1 −o(1),\nI5 :=\nsup\n∥θ−θ0∥⩽τN\nGn\n\u0010\nψ(W; θ, bη0) −ψ(W; θ0, η0)\n\u0011\n(A.36)\n⩽r′\nN log1/2(1/r′\nN) + n−1/2+1/q log n.",
    "content_hash": "f42a0de32259df79bbb9e1ea1c63facab63236efd723a5b9f04ede3b698ef565",
    "location": null,
    "page_start": 64,
    "page_end": 64,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4a2b993b-23e3-4ae7-aa85-b74fe738d058",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Thus, an application of Lemma 6.2 to the empirical\nprocess {Gn(f), f ∈F2} with an envelope F2 = F1,bη0 +F1,η0 and σ = Cr′\nN for sufficiently\nlarge constant C conditional on (Wi)i∈Ic and Ic yields that with PN-probability 1−o(1),\nsup\nf∈F2\n|Gn(f)| ≲r′\nN log1/2(1/r′\nN) + n−1/2+1/q log n. (A.38)\nThis follows since ∥F2∥P,q = ∥F1,bη0 + F1,η0∥P,q ⩽2C1 by Assumption 3.4(b) and the\ntriangle inequality, and\nlog sup\nQ\nN(ϵ∥F2∥Q,2, F2, ∥· ∥Q,2) ⩽2v log(2a/ϵ),\nfor all 0 < ϵ ⩽1,",
    "content_hash": "21076c049cf351cfd2921e53c8382c155a32becf149758cc96ffcbe3fbcabbbd",
    "location": null,
    "page_start": 64,
    "page_end": 64,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ccad9e54-34c9-4fb5-8020-30ddb7593328",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "64\nCCDDHNR\nfunction r 7→EPN [ψ(W; θ0 + r(θ −θ0), η0 + r(η −η0))], which vanishes at r = 0,\nEPN [ψ(W; θ, η)] = J0(θ −θ0) + ∂ηEPN ψ(W; θ0, η0)[η −η0]\n+\nZ 1\n0\n2−1∂2\nrEPN [W; θ0 + r(θ −θ0), η0 + r(η −η0)]dr.",
    "content_hash": "a53a694a3403085606e6596abe9d2cf08166ec12c77b558f0786af514e12d2a0",
    "location": null,
    "page_start": 64,
    "page_end": 64,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "98abe4c2-89c5-4c6d-af7e-c476fcaefbc0",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "(A.37)\nTherefore, since all singular values of J0 are bounded below from zero by Assumption\n3.3(d), it follows that\nJ−1\n0\n√nEn[ψ(W; θ0, η0)] + √n(ˇθ0 −θ0)\n= OPN (n−1/2+1/q log n + r′\nN log1/2(1/r′\nN) + (ϵN + λN + λ′\nN)√n. The asserted claim now follows by multiplying both parts of the display by Σ−1/2\n0\n(under\nthe norm on the left-hand side) and noting that singular values of Σ0 are bounded below\nfrom zero by Assumptions 3.3 and 3.4. Step 3. Here we derive a bound on I5 in (A.37). We have\nI5 ≲sup\nf∈F2\n|Gn(f)|,\nF2 =\n\b\nψj(·, θ, bη0) −ψj(·, θ0, η0): j = 1, ..., dθ, ∥θ −θ0∥⩽τn\n. To bound supf∈F2 |Gn(f)|, we apply Lemma 6.2 conditional on (Wi)i∈Ic and Ic so that bη0\ncan be treated as fixed. Observe that with PN-probability 1−o(1), supf∈F2 ∥f∥PN,2 ≲r′\nN\nwhere we used Assumption 3.4.",
    "content_hash": "1546f6101ab25a9187f679ba7ff6eae7a0237aeea47fdae40e43289a9ddc45e5",
    "location": null,
    "page_start": 64,
    "page_end": 64,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "dd1a8321-935e-4705-9d85-c53b88356191",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Also, we replace the constant q and the sequence (δN)N⩾1 in Assumptions 3.1 and 3.2 by\nq/2 and (δ′\nN)N⩾1 with δ′\nN = (C + 2\n√\nC + 2)(δN ∨N −[(1−4/q)∧(1/2)]) for all N (recall that\nwe assume that q > 4, and the analysis in Section 3 only requires that q > 2; also, δ′\nN\nsatisfies δ′\nN ⩾N −[(1−4/q)∧(1/2)], which is required in Theorems 3.1 and 3.2). We proceed",
    "content_hash": "0259b4dfd9f1b8deb85a654738dbdbb59af0550be6e33e1a2730b310d75f3356",
    "location": null,
    "page_start": 65,
    "page_end": 65,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a7d41e2e-c2a7-46f4-9687-df746545611a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Observe that the score ψ in (4.7) is linear in θ:\nψ(W; θ, η) = (Y −Dθ −g(X))(Z −m(X)) = ψa(W; η)θ + ψb(W; η),\nψa(W; η) = D(m(X) −Z),\nψb(W; η) = (Y −g(X))(Z −m(X)). Therefore, all asserted claims of Theorem 4.2 follow from Theorems 3.1 and 3.2 and\nCorollary 3.1 as long as we can verify Assumptions 3.1 and 3.2, which we do here. We\ndo so with TN being the set of all η = (g, m) consisting of P-square-integrable functions\ng and m such that\n∥η −η0∥P,∞⩽C,\n∥η −η0∥P,2 ⩽δN,\n∥m −m0∥P,2 × ∥g −g0∥P,2 ⩽δNN −1/2.",
    "content_hash": "406b030816917c8dcd0c62c5cd3ec88551bcd21ec0b8a98d9f6a3d345d1fab81",
    "location": null,
    "page_start": 65,
    "page_end": 65,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0ca980ad-d349-4a7d-8be8-2498d6efe21d",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n65\nbecause F2 ⊂F1,bη0 −F1,η0 for F1,η defined in Assumption 3.4(b), and\nlog sup\nQ\nN(ϵ∥F1,bη0 + F1,η0∥Q,2, F1,bη0 −F1,η0, ∥· ∥Q,2)\n⩽log sup\nQ\nN((ϵ/2)∥F1,bη0∥Q,2, F1,bη0, ∥· ∥Q,2) + log sup\nQ\nN((ϵ/2)∥F1,η0∥Q,2, F1,η0, ∥· ∥Q,2)\nby the proof of Theorem 3 in Andrews (1994b). The claim of this step follows. Step 4. Here we derive a bound on I3 in (A.33). Let ¯θ0 = θ0−J−1\n0 En[ψ(W; θ0, η0)]. Then\n∥¯θ0 −θ0∥= OPN (1/√n) = oPN (τn) since EPN [∥√nEn[ψ(W; θ0, η0)]∥] is bounded and the\nsingular values of J0 are bounded below from zero by Assumption 3.3(d). Therefore, ¯θ0 ∈\nΘ with PN-probability 1 −o(1) by Assumption 3.3(a).",
    "content_hash": "c53fa71204b050511dca7c8b9334674258e3c6f413ff600cbca16af5713fa246",
    "location": null,
    "page_start": 65,
    "page_end": 65,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "84a3948a-9fd5-40ab-8457-abf5d275b419",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Hence, with the same probability,\ninf\nθ∈Θ\n√n\nEn[ψ(W; θ, bη0)]\n⩽√n\nEn[ψ(W; ¯θ0, bη0)]\n,\nand so it suffices to show that with PN-probability 1 −o(1),\n√n\nEn[ψ(W; ¯θ0, bη0)]\n= O(n−1/2+1/q log n + r′\nN log1/2(1/r′\nN) + λN\n√n + λ′\nN\n√n). To prove it, substitute θ = ¯θ0 and η = bη0 into (A.34) and use Taylor’s expansion in\n(A.35). This shows that with PN-probability 1 −o(1),\n√n\nEn[ψ(W; ¯θ0, bη0)]\n⩽√n\nEn[ψ(W; θ0, η0)] + J0(¯θ0 −θ0)\n+ λN\n√n + I4 + I5\n= λN\n√n + I4 + I5,\nCombining this with the bounds on I4 and I5 derived above gives the claim of this step\nand completes the proof of the theorem. ■\nProof of Theorems 4.1 and 4.2\nSince Theorem 4.1 is a special case of Theorem 4.2 (with Z = D), it suffices to prove\nthe latter. Also, we only consider the DML estimators based on the score (4.7) and note\nthat the estimators based on the score (4.8) can be treated similarly.",
    "content_hash": "339cdaf67e4d39a8df247b9821c3fee8efe651e117813b48bfbc0d0a03378ea7",
    "location": null,
    "page_start": 65,
    "page_end": 65,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "587a70a5-589a-4079-b209-063d220fbd0a",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "We\ndo so with TN being the set of all η = (g, m) consisting of P-square-integrable functions\ng and m such that\n∥η −η0∥P,q ⩽C,\n∥η −η0∥P,2 ⩽δN,\n∥m −1/2∥P,∞⩽1/2 −ε,\n∥m −m0∥P,2 × ∥g −g0∥P,2 ⩽δNN −1/2. Also, we replace the sequence (δN)N⩾1 in Assumptions 3.1 and 3.2 by (δ′\nN)N⩾1 with\nδ′\nN = Cε(δN ∨N −[(1−4/q)∧(1/2)]) for all N, where Cε is a sufficiently large constant that\ndepends only on ε and C (note that δ′\nN satisfies δ′\nN ⩾N −[(1−4/q)∧(1/2)], which is required\nin Theorems 3.1 and 3.2). We proceed in five steps. All bounds in the proof hold uniformly\nover P ∈P but we omit this qualifier for brevity. Step 1. We first verify Neyman orthogonality. We have that Eψ(W; θ0, η0) = 0 by\ndefinition of θ0 and η0.",
    "content_hash": "1a44fb0f60f7b9b02e94238f7dbb8f0dea84979c4108ebcc10ecce7cf23f157e",
    "location": null,
    "page_start": 65,
    "page_end": 68,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7b749bbe-7742-4415-a59b-39e0b8670e40",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "In addition,\n|EP [ψa(W; η0)]| = |EP [D(m0(X) −Z)]| ⩽∥D∥P,2∥m0(X)∥P,2 + ∥D∥P,2∥Z∥P,2\n⩽2∥D∥P,2∥Z∥P,2 ⩽2∥D∥P,q∥Z∥P,q ⩽2C2\nby the triangle inequality, H¨older’s inequality, Jensen’s inequality, and Assumption 4.2(b). This gives Assumption 3.1(e). Hence, given that Assumptions 3.1(i,ii,iii) hold trivially,\nSteps 1 and 2 together show that all conditions of Assumption 3.1 hold. Step 3. Note that Assumption 3.2(a) holds by construction of the set TN and Assumption\n4.2(e). Also, note that ψ(W; θ0, η0) = UV , and so\nEP [ψ(W; θ0, η0)ψ(W; θ0, η0)′] = EP [U 2V 2] ⩾c4 > 0,\nby Assumption 4.2(c), which gives Assumption 3.2(d). Step 4. Here we verify Assumption 3.2(b).",
    "content_hash": "0f23fddd32eedf8705db2b511ed06cd372452176bd6255e677e009a2b3b8acbd",
    "location": null,
    "page_start": 66,
    "page_end": 66,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "525814f4-893e-4d37-8d21-5899b7f2fbe7",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "For any η = (g, m) ∈TN, we have\n(EP [∥ψa(W; η)∥q/2])2/q = ∥ψa(W; η)∥P,q/2 = ∥D(m(X) −Z)∥P,q/2\n⩽∥D(m(X) −m0(X))∥P,q/2 + ∥Dm0(X)∥P,q/2 + ∥DZ∥P,q/2\n⩽∥D∥P,q∥m(X) −m0(X)∥P,q + ∥D∥P,q∥m0(X)∥P,q + ∥D∥P,q∥Z∥P,q\n⩽C∥D∥P,q + 2∥D∥P,q∥Z∥P,q ⩽3C2\nby Assumption 4.2(b), which gives the bound on m′\nN in Assumption 3.2(b). Also, since\n|EP [(D −r0(X))(Z −m0(X))]| = |EP [DV ]| ⩾c\nby Assumption 4.2(c), it follows that θ0 satisfies\n|θ0| = |EP [(Y −ℓ0(X))(Z −m0(X))]|\n|EP [(D −r0(X))(Z −m0(X))]|\n⩽c−1\u0010\n∥Y ∥P,2 + ∥ℓ0(X)∥P,2\n\u0011\u0010\n∥Z∥P,2 + ∥m0(X)∥P,2\n\u0011\n⩽4c−1∥Y ∥P,2∥Z∥P,2 ⩽4C2/c.",
    "content_hash": "ed2d1bdbb2ba9e74d892e69397ff143b68a947a824090b6093ed6ce50de31cd8",
    "location": null,
    "page_start": 66,
    "page_end": 66,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a97c9eb1-147a-4226-82e0-70122eb3ed4f",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Also, for any η = (g, m) ∈TN, the Gateaux derivative in the\ndirection η −η0 = (g −g0, m −m0) is given by\n∂ηEP ψ(W; θ0, η0)[η −η0] = EP\nh\ng(1, X) −g0(1, X)\ni\n−EP\nh\ng(0, X) −g0(0, X)\ni\n−EP\nhD(g(1, X) −g0(1, X))\nm0(X)\ni\n+ EP\nh(1 −D)(g(0, X) −g0(0, X))\n1 −m0(X)\ni\n−EP\nhD(Y −g0(1, X))(m(X) −m0(X))\nm2\n0(X)\ni\n−EP\nh(1 −D)(Y −g0(0, X))(m(X) −m0(X))\n(1 −m0(X))2\ni\n,\nwhich is 0 by the law of iterated expectations, since\nEP [D | X] = m0(X),\nEP [1 −D | X] = 1 −m0(X),\nEP [D(Y −g0(1, X)) | X] = 0,\nEP [(1 −D)(Y −g0(0, X)) | X] = 0. This gives Assumption 3.1(d) with λN = 0. Step 2.",
    "content_hash": "c3cf32db542b88be4dc53bb472b0aa03074f87ed0a039f619e94acaad29d6587",
    "location": null,
    "page_start": 66,
    "page_end": 66,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2e9b47a9-b8e8-4138-a57b-c455a083d0ea",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "66\nCCDDHNR\nin five steps. All bounds in the proof hold uniformly over P ∈P but we omit this qualifier\nfor brevity). Step 1. We first verify Neyman orthogonality. We have that EP ψ(W; θ0, η0) = 0 by\ndefinition of θ0 of η0. Also, for any η = (g, m) ∈TN, the Gateaux derivative in the\ndirection η −η0 = (g −g0, m −m0) is given by\n∂ηEP ψ(W; θ0, η0)[η −η0] = EP\nh\n(g(X) −g0(X))(m0(X) −Z)\ni\n+ EP\nh\n(m0(X) −m(X))(Y −Dθ0 −g0(X))\ni\n= 0,\nby the law of iterated expectations, since V = Z −m0(X) and U = (Y −Dθ0 −g0(X))\nobey EP [V |X] = 0 and EP [U|Z, X] = 0. This gives Assumption 3.1(d) with λN = 0. Step 2. Note that\n|J0| = |EP [ψa(W; η0)]| = |EP [D(m0(X) −Z)]| = |EP [DV ]| ⩾c > 0\nby Assumption 4.2(c).",
    "content_hash": "15202e8129de0919c625eb8d70f2b7e5f9491998d1e8345b25bf93db777c4c79",
    "location": null,
    "page_start": 66,
    "page_end": 66,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "be81bc46-5a87-44e2-86fc-3d714cb0c8eb",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Further,\n(EP [∥ψ(W; θ0, η) −ψ(W; θ0, η0)∥2])1/2\n= ∥ψ(W; θ0, η) −ψ(W; θ0, η0)∥P,2\n= ∥(U + g0(X) −g(X))(Z −m(X)) −U(Z −m0(X))∥P,2\n⩽∥U(m(X) −m0(X))∥P,2 + ∥(g(X) −g0(X))(Z −m(X))∥P,2\n⩽\n√\nC∥m(X) −m0(X)∥P,2 + ∥V (g(X) −g0(X))∥P,2\n+ ∥(g(X) −g0(X))(m(X) −m0(X))∥P,2\n⩽\n√\nC∥m(X) −m0(X)∥P,2 +\n√\nC∥g(X) −g0(X)∥P,2 + C∥m(X) −m0(X)∥P,2\n⩽(2\n√\nC + C)δN ⩽δ′\nN,\nwhich gives the bound on r′\nN in Assumption 3.2(c). Finally, let\nf(r) := EP [ψ(W; θ0, η0 + r(η −η0)],\nr ∈(0, 1).",
    "content_hash": "88fb44688faadee3adf61c1ada4b53300c14e857e3f63335e004b5abd17bd68d",
    "location": null,
    "page_start": 67,
    "page_end": 67,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "022b43f1-70e7-4613-a5b6-dd50c8c39ae0",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "For any η = (g, m) ∈TN, we have\n∥EP [ψa(W; η)] −EP [ψa(W; η0)]∥= |EP [ψa(W; η) −ψa(W; η0)]|\n= |EP [D(m(X) −m0(X))]|\n⩽∥D∥P,2∥m(X) −m0(X)∥P,2 ⩽CδN ⩽δ′\nN,\nwhich gives the bound on rN in Assumption 3.2(c).",
    "content_hash": "7c81db61e52700db4976e8c341439228adca850d48b85c3124014fc961ee3b2e",
    "location": null,
    "page_start": 67,
    "page_end": 67,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9af7e9bd-bdba-49df-97fb-e90a5160fcab",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n67\nHence,\n(EP [∥ψ(W; θ0, η)∥q/2])2/q = ∥ψ(W; θ0, η)∥P,q/2\n= ∥(Y −Dθ0 −g(X))(Z −m(X))∥P,q/2\n⩽∥U(Z −m(X))∥P,q/2 + ∥(g(X) −g0(X))(Z −m(X))∥P,q/2\n⩽∥U∥P,q∥Z −m(X)∥P,q + ∥g(X) −g0(X)∥P,q∥Z −m(X)∥P,q\n⩽(∥U∥P,q + C)∥Z −m(X)∥P,q\n⩽(∥Y −Dθ0∥P,q + ∥g0(X)∥P,q + C)\n× (∥Z∥P,q + ∥m0(X)∥P,q + ∥m(X) −m0(X)∥P,q)\n⩽(2∥Y −Dθ0∥P,q + C)(2∥Z∥P,q + C)\n⩽(2∥Y ∥P,q + 2∥D∥P,q|θ0| + C)(2∥Z∥P,q + C)\n⩽3C(3C + 8C3/c),\nwhere we used the fact that since g0(X) = EP [Y −Dθ0 | X], ∥g0(X)∥P,q ⩽∥Y −\nDθ0∥P,q by Jensen’s inequality. This gives the bound on mN in Assumption 3.2(b). Hence, Assumption 3.2(b) holds. Step 5. Finally, we verify Assumption 3.2(c).",
    "content_hash": "fd8933125ffd1d29ad6212da5fb6221c73600c7bf0f189afd1288421abe4b652",
    "location": null,
    "page_start": 67,
    "page_end": 67,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "533fb52e-f75e-45a4-9145-15f72a7a04fa",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Then for any r ∈(0, 1),\nf(r) = EP [(U −r(g(X) −g0(X)))(V −r(m(X) −m0(X)))],\nand so\n∂f(r) = −EP [(g(X) −g0(X))(V −r(m(X) −m0(X)))]\n−EP [(U −r(g(X) −g0(X)))(m(X) −m0(X))],\n∂2f(r) = 2EP [(g(X) −g0(X))(m(X) −m0(X))].",
    "content_hash": "08bb56fc50b1d5202c03df04eb781930e3c178252981b3c1b182017d905c49a0",
    "location": null,
    "page_start": 67,
    "page_end": 67,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5d343654-7af1-4020-a797-c907b8e5c0bc",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Note that J0 = −1, and so Assumption 3.1(e) holds trivially. Hence, given\nthat Assumptions 3.1(i,ii,iii) hold trivially as well, Steps 1 and 2 together show that all\nconditions of Assumption 3.1 hold.",
    "content_hash": "01b657057f550b1e836c1615a0f000cfd076854b3173988af00063769cf31959",
    "location": null,
    "page_start": 68,
    "page_end": 68,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c63ae336-f512-44a8-936f-944a33696a16",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "68\nCCDDHNR\nHence,\n|∂2f(r)| ⩽2∥g(X) −g0(X)∥P,2 × ∥m(X) −m0(X)∥P,2 ⩽2δNN −1/2 ⩽δ′\nNN −1/2,\nwhich gives the bound on λ′\nN in Assumption 3.2(c). Thus, all conditions of Assumptions\n3.1 are verified. This completes the proof. ■\nProof of Theorems 5.1 and 5.2\nThe proof of Theorem 5.2 is similar to that of Theorem 5.1 and therefore omitted. In\nturn, regarding Theorem 5.1, we show the proof for the case of ATE and note that the\nproof for the case of ATTE is similar. Observe that the score ψ in (5.3) is linear in θ:\nψ(W; θ, η) = ψa(W; η)θ + ψb(W; η),\nψa(W; η) = −1,\nψb(W; η) = (g(1, X) −g(0, X)) + D(Y −g(1, X))\nm(X)\n−(1 −D)(Y −g(0, X))\n1 −m(X)\n. Therefore, all asserted claims of Theorem 5.1 follow from Theorems 3.1 and 3.2 and\nCorollary 3.1 as long as we can verify Assumptions 3.1 and 3.2, which we do here.",
    "content_hash": "08c98d1510fc5ff20215f49f11e73c7b789b2be38fd57c69a340571124f3ad07",
    "location": null,
    "page_start": 68,
    "page_end": 68,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9bb29999-d34a-4ba6-8c46-b837450787fa",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n69\nStep 3. Note that Assumption 3.2(a) holds by construction of the set TN and Assumption\n5.1(f). Also,\nEP\nh\nψ2(W; θ0, η0)\ni\n= EP\nh\nEP [ψ2(W; θ0, η0) | X]\ni\n= EP\nh\nEP [(g0(1, X) −g0(0, X) −θ0)2 | X]\n+ EP\nh\u0010D(Y −g0(1, X))\nm0(X)\n−(1 −D)(Y −g0(0, X))\n1 −m0(X)\n\u00112\n| X\nii\n⩾EP\nh\u0010D(Y −g0(1, X))\nm0(X)\n−(1 −D)(Y −g0(0, X))\n1 −m0(X)\n\u00112i\n= EP\nhD2(Y −g0(1, X))2\nm0(X)2\n+ (1 −D)2(Y −g0(0, X))2\n(1 −m0(X))2\ni\n⩾\n1\n(1 −ε)2 EP\nh\nD2(Y −g0(1, X))2 + (1 −D)2(Y −g0(0, X))2i\n=\n1\n(1 −ε)2 EP [DU 2 + (1 −D)U 2] =\n1\n(1 −ε)2 EP [U 2] ⩾\nc2\n(1 −ε)2 . This gives Assumption 3.2(d). Step 4. Here we verify Assumption 3.2(b).",
    "content_hash": "d5918d251aa6ab0c3e775fab69913f635684155a22181e321ae6e63b816179bf",
    "location": null,
    "page_start": 69,
    "page_end": 69,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "13844175-d7c5-4fcd-958c-0c7b153c7fc2",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Similarly, for any η ∈(g, m) ∈TN,\n∥g(1, X) −g0(1, X)∥P,q ⩽C/ε1/q\nand\n∥g(0, X) −g0(0, X)∥P,q ⩽C/ε1/q\nsince ∥g(D, X) −g0(D, X)∥P,q ⩽C. In addition,\n|θ0| = |EP [g0(1, X) −g0(0, X)]| ⩽∥g0(1, X)∥P,2 + ∥g0(0, X)∥P,2 ⩽2C/ε1/q.",
    "content_hash": "7d82fbcec96c471f07f09def666d10462c465e75876cf33f4539dfc2c45292cb",
    "location": null,
    "page_start": 69,
    "page_end": 69,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9d9d02e2-0b2f-4cd4-baeb-ec899aa1a930",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "We have\n∥g0(D, X)∥P,q = (EP [|g0(D, X)|q])1/q\n⩾\n\u0010\nEP\nh\n|g0(1, X)|qPP (D = 1 | X) + |g0(0, X)|qPP (D = 0 | X)\ni\u00111/q\n⩾ε1/q\u0010\nEP [|g0(1, X)|q] + EP [|g0(0, X)|q]\n\u00111/q\n⩾ε1/q\u0010\nEP [|g0(1, X)|q] ∨EP [|g0(0, X)|q]\n\u00111/q\n⩾ε1/q\u0010\n∥g0(1, X)∥P,q ∨∥g0(0, X)∥P,q\n\u0011\n,\nwhere in the third line, we used the facts that PP (D = 1 | X) = m0(X) ⩾ε and\nPP (D = 0 | X) = 1 −m0(X) ⩾ε. Hence, given that ∥g0(D, X)∥P,q ⩽∥Y ∥P,q ⩽C by\nJensen’s inequality and Assumption 5.1(b), it follows that\n∥g0(1, X)∥P,q ⩽C/ε1/q\nand\n∥g0(0, X)∥P,q ⩽C/ε1/q.",
    "content_hash": "7f8e43740f7fac6f4dbf17c7762738fe7af9799ad9af5fab819135bb3617ff82",
    "location": null,
    "page_start": 69,
    "page_end": 69,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "47dbe3e5-7f4e-4998-bf93-89f9d2fb575b",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Further, by the triangle inequality,\n(EP [∥ψ(W; θ0, η) −ψ(W; θ0, η0)∥2])1/2 = ∥ψ(W; θ0, η) −ψ(W; θ0; η0)∥P,2\n⩽I1 + I2 + I3,\nwhere\nI1 :=\ng(1, X) −g0(1, X)\nP,2 +\ng(0, X) −g0(0, X)\nP,2,\nI2 :=\nD(Y −g(1, X))\nm(X)\n−D(Y −g0(1, X))\nm0(X)\nP,2,\nI3 :=\n(1 −D)(Y −g(0, X))\n1 −m(X)\n−(1 −D)(Y −g0(0, X))\n1 −m0(X)\nP,2. To bound I1, note that by the same argument as that used in Step 4,\n∥g(1, X) −g0(1, X)∥P,2 ⩽δN/ε1/2\nand\n∥g(0, X) −g0(0, X)∥P,2 ⩽δN/ε1/2, (A.39)\nand so I1 ⩽2δN/ε1/2.",
    "content_hash": "308b153ee8fae5e337c3e735fa7b9effc4efc73a348a4b38684641077a3950ac",
    "location": null,
    "page_start": 70,
    "page_end": 70,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bd99b949-bb3d-4f3a-88ea-def2ad491f8c",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "70\nCCDDHNR\nTherefore, for any η = (g, m) ∈TN, we have\n(EP [|ψ(W; θ0, η)|q])1/q = ∥ψ(W; θ0, η)∥P,q\n⩽(1 + ε−1)\n\u0010\n∥g(1, X)∥P,q + ∥g(0, X)∥P,q\n\u0011\n+ 2∥Y ∥P,q/ε + |θ0|\n⩽(1 + ε−1)\n\u0010\n∥g(1, X) −g0(1, X)∥P,q + ∥g(0, X) −g0(0, X)∥P,q\n\u0011\n+ (1 + ε−1)\n\u0010\n∥g0(1, X)∥P,q + ∥g0(0, X)∥P,q\n\u0011\n+ 2C/ε + 2C/ε1/q\n⩽4C(1 + ε−1)/ε1/q + 2C/ε + 2C/ε1/q. This gives the bound on mN in Assumption 3.2(b). Also, we have\n(EP [|ψa(W; η)|q])1/q = 1. This gives the bound on m′\nN in Assumption 3.2(b). Hence, Assumption 3.2(b) holds. Step 5. Finally, we verify Assumption 3.2(c). For any η = (g, m) ∈TN, we have\n∥EP [ψa(W; η) −ψa(W; η0)]∥= |1 −1| = 0 ⩽δ′\nN,\nwhich gives the bound on rN in Assumption 3.2(c).",
    "content_hash": "00385f4980a0c71074d31e546aeee7010201916cfdb5041d30b5365a09ab3398",
    "location": null,
    "page_start": 70,
    "page_end": 70,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "70a7852f-75ee-4364-b052-d9bc7a657ca8",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "To bound I2, we have\nI2 ⩽ε−2\r\r\rDm0(X)(Y −g(1, X)) −Dm(X)(Y −g0(1, X))\nP,2\n⩽ε−2\r\r\rm0(X)(g0(1, X) + U −g(1, X)) −m(X)U\nP,2\n⩽ε−2\u0010\r\r\rm0(X)(g(1, X) −g0(1, X))\nP,2 +\n(m(X) −m0(X))U\nP,2\n\u0011\n⩽ε−2\u0010\n∥g(1, X) −g0(1, X)∥P,2 +\n√\nC∥m(X) −m0(X)∥P,2\n\u0011\n⩽ε−2(ε−1/2 +\n√\nC)δN,\nwhere the first inequality follows from the bounds ε ⩽m0(X) ⩽1 −ε and ε ⩽m(X) ⩽\n1 −ε, the second from the facts that D ∈{0, 1} and for D = 1, Y = g0(1, X) + U, the\nthird from the triangle inequality, the fourth from the facts that m0(X) ⩽1 and EP [U 2 |\nX] ⩽C, and the fifth from (A.39). Similarly, I3 ⩽ε−2(ε−1/2 +\n√\nC)δN. Combining these\ninequalities shows that\n(EP [∥ψ(W; θ0, η) −ψ(W; θ0, η0)∥2])1/2 ⩽2(ε−1/2 + ε−5/2 +\n√\nCε−2)δN ⩽δ′\nN,",
    "content_hash": "a584b530f442c7e2ee1a6073b1890bcc88855b9256f01d6e04cfcf726e6a9c7c",
    "location": null,
    "page_start": 70,
    "page_end": 70,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8b3a74b3-3e61-4363-9471-c47868afabb1",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "This gives the bound\non λ′\nN in Assumption 3.2(c). Thus, all conditions of Assumptions 3.1 are verified. This\ncompletes the proof. ■",
    "content_hash": "005176c0c95065c1c199caf5bbd3b6fc097f5f4e1de8f5be9b250a51769eeeb4",
    "location": null,
    "page_start": 71,
    "page_end": 71,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ea7318a9-3017-4e13-8f71-b1aab2698b86",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "DML\n71\nas long as Cε in the definition of δ′\nN satisfies Cε ⩾2(ε−1/2 + ε−5/2 +\n√\nCε−2). This gives\nthe bound on r′\nN in Assumption 3.2(c). Finally, let\nf(r) := EP [ψ(W; θ0, η0 + r(η −η0))],\nr ∈(0, 1).",
    "content_hash": "3c42b9cfb55bc335909f5703c6801d1f8dd493fb4c803626d9374f49ec2d0e40",
    "location": null,
    "page_start": 71,
    "page_end": 71,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ea4466c8-19ca-4587-bdbd-33b83fbb0a06",
    "source_id": "2bc757a2-8fc9-4622-a733-a18814df7a0b",
    "content": "Then for any r ∈(0, 1),\n∂2f(r) = EP\nhD(g(1, X) −g0(1, X))(m(X) −m0(X))\n(m0(X) + r(m(X) −m0(X)))2\ni\n+ EP\nh(1 −D)(g(0, X) −g0(0, X))(m(X) −m0(X))\n(1 −m0(X) −r(m(X) −m0(X)))2\ni\n+ EP\nh(g(1, X) −g0(1, X))(m(X) −m0(X))\n(m0(X) + r(m(X) −m0(X)))2\ni\n+ 2EP\nhD(Y −g0(1, X) −r(g(1, X) −g0(1, X)))(m(X) −m0(X))2\n(m0(X) + r(m(X) −m0(X)))3\ni\n+ EP\nh(g(0, X) −g0(0, X))(m(X) −m0(X))\n(1 −m0(X) −r(m(X) −m0(X)))2\ni\n−2EP\nh(1 −D)(Y −g0(0, X) −r(g(0, X) −g0(0, X)))(m(X) −m0(X))2\n(1 −m0(X) −r(m(X) −m0(X)))3\ni\n,\nand so, given that\nD(Y −g0(1, X)) = DU,\n(1 −D)(Y −g0(0, X)) = (1 −D)U,\nEP [U | D, X] = 0,\n|m(X) −m0(X)| ⩽2,\nit follows that for some constant C′\nε that depends only on ε and C,\n|∂2f(r)| ⩽C′\nε∥m −m0∥P,2 × ∥g −g0∥P,2 ⩽δ′\nNN −1/2,\nas long as the constant Cε in the definition of δ′\nN satisfies Cε ⩾C′\nε.",
    "content_hash": "0b614c6206fa0b16d8561266524f392075a55a8dfa58958284bae96d0ae6e0ca",
    "location": null,
    "page_start": 71,
    "page_end": 71,
    "metadata": {
      "section": "arXiv:1608.00060v7  [stat.ML]  3 Nov 2024",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7dc326dc-45f8-4a0e-83fe-b8c18298ebd6",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Meinshausen [2006], who showed that diam(L(x)) →p 0 for regular trees. Lemma 2. Let T be a regular, random-split tree and let L(x) denote its leaf containing x. Suppose that X1, ..., Xs ∼U\n[0, 1]d\u0001\nindependently. Then, for any 0 < η < 1, and for large\nenough s,\nP\n\ndiamj (L(x)) ≥\n\u0012\ns\n2k −1\n\u0013−\n0.99 (1−η) log((1−α)−1)\nlog(α−1)\nπ\nd\n\n≤\n\u0012\ns\n2k −1\n\u0013−η2\n2\n1\nlog(α−1)\nπ\nd\n. This lemma then directly translates into a bound on the bias of a single regression tree. Since a forest is an average of independently-generated trees, the bias of the forest is the\nsame as the bias of a single tree. Theorem 3. Under the conditions of Lemma 2, suppose moreover that µ (x) is Lipschitz\ncontinuous and that the trees T in the random forest are honest. Then, provided that α ≤0.2,\nthe bias of the random forest at x is bounded by\n|E [ˆµ (x)] −µ (x)| = O\ns\n−1\n2\nlog((1−α)−1)\nlog(α−1)\nπ\nd\n! ;\nthe constant in the O-bound is given in the proof.",
    "content_hash": "7616eded127921005d7a584fd6582cf0e2ba621f465e622d007eb2dcbf50390a",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "to be asymptotically normal under weak conditions. Thus whenever the ratio of the variance",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "26e043b8-d8d8-45a3-aa01-c6c695f483a6",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Estimation and Inference of Heterogeneous Treatment\nEﬀects using Random Forests∗\nStefan Wager\nDepartment of Statistics\nStanford University\nswager@stanford.edu\nSusan Athey\nGraduate School of Business\nStanford University\nathey@stanford.edu\nJuly 11, 2017\nAbstract\nMany scientiﬁc and engineering challenges—ranging from personalized medicine to\ncustomized marketing recommendations—require an understanding of treatment eﬀect\nheterogeneity. In this paper, we develop a non-parametric causal forest for estimat-\ning heterogeneous treatment eﬀects that extends Breiman’s widely used random for-\nest algorithm. In the potential outcomes framework with unconfoundedness, we show\nthat causal forests are pointwise consistent for the true treatment eﬀect, and have an\nasymptotically Gaussian and centered sampling distribution. We also discuss a prac-\ntical method for constructing asymptotic conﬁdence intervals for the true treatment\neﬀect that are centered at the causal forest estimates. Our theoretical results rely on a\ngeneric Gaussian theory for a large family of random forest algorithms. To our knowl-\nedge, this is the ﬁrst set of results that allows any type of random forest, including\nclassiﬁcation and regression forests, to be used for provably valid statistical inference. In experiments, we ﬁnd causal forests to be substantially more powerful than classical\nmethods based on nearest-neighbor matching, especially in the presence of irrelevant\ncovariates. Keywords: Adaptive nearest neighbors matching; asymptotic normality; potential\noutcomes; unconfoundedness.",
    "content_hash": "e9d18ce1d310e9294e4731d5652378e7a4f5f475e8a5caf2c7992092dbb4eb9c",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "Estimation and Inference of Heterogeneous Treatment",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "47daa92a-52ee-4cff-88c3-5a1b893806b7",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "1\nIntroduction\nIn many applications, we want to use data to draw inferences about the causal eﬀect of a\ntreatment: Examples include medical studies about the eﬀect of a drug on health outcomes,\nstudies of the impact of advertising or marketing oﬀers on consumer purchases, evaluations\nof the eﬀectiveness of government programs or public policies, and “A/B tests” (large-scale\nrandomized experiments) commonly used by technology ﬁrms to select algorithms for ranking\nsearch results or making recommendations. Historically, most datasets have been too small\nto meaningfully explore heterogeneity of treatment eﬀects beyond dividing the sample into\n∗Part of the results developed in this paper were made available as an earlier technical report “Asymptotic\nTheory for Random Forests”, available at http://arxiv.org/abs/1405.0352. 1\narXiv:1510.04342v4  [stat.ME]  10 Jul 2017",
    "content_hash": "56d557d87cdcd7733a459e610e8d9048052d7877fd9ea8658b884e73b2be3e58",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "Introduction",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b26a2099-e84e-4326-ab18-c8cda9d67227",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Procedure 1. Double-Sample Trees\nDouble-sample trees split the available training data into two parts: one half for esti-\nmating the desired response inside each leaf, and another half for placing splits. Input: n training examples of the form (Xi, Yi) for regression trees or (Xi, Yi, Wi)\nfor causal trees, where Xi are features, Yi is the response, and Wi is the treatment\nassignment. A minimum leaf size k. 1. Draw a random subsample of size s from {1, ..., n} without replacement, and then\ndivide it into two disjoint sets of size |I| = ⌊s/2⌋and |J | = ⌈s/2⌉. 2. Grow a tree via recursive partitioning. The splits are chosen using any data from\nthe J sample and X- or W-observations from the I sample, but without using\nY -observations from the I-sample. 3. Estimate leaf-wise responses using only the I-sample observations. Double-sample regression trees make predictions ˆµ(x) using (4) on the leaf containing x,\nonly using the I-sample observations. The splitting criteria is the standard for CART\nregression trees (minimizing mean-squared error of predictions). Splits are restricted so\nthat each leaf of the tree must contain k or more I-sample observations. Double-sample causal trees are deﬁned similarly, except that for prediction we estimate\nˆτ(x) using (5) on the I sample.",
    "content_hash": "68185aa8cfcb9196eec8a016aef30d69c61a08092f329b03d71a45591b09c59d",
    "location": null,
    "page_start": 1,
    "page_end": 9,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7e2f0926-8b7f-485a-8a4b-bc8145e87730",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Procedure 2. Propensity Trees\nPropensity trees use only the treatment assignment indicator Wi to place splits, and\nsave the responses Yi for estimating τ. Input: n training examples (Xi, Yi, Wi), where Xi are features, Yi is the response, and\nWi is the treatment assignment. A minimum leaf size k. 1. Draw a random subsample I ∈{1, ..., n} of size |I| = s (no replacement). 2. Train a classiﬁcation tree using sample I where the outcome is the treatment\nassignment, i.e., on the (Xi, Wi) pairs with i ∈I. Each leaf of the tree must have\nk or more observations of each treatment class. 3. Estimate τ(x) using (5) on the leaf containing x. In step 2, the splits are chosen by optimizing, e.g., the Gini criterion used by CART for\nclassiﬁcation [Breiman et al., 1984]. We also have access to a regression tree T which can be used to get estimates of the con-\nditional mean function at x of the form T (x; ξ, Z1, ..., Zn), where ξ ∼Ξ is a source of\nauxiliary randomness. Our goal is to use this tree-growing scheme to build a random forest\nthat can be used for valid statistical inference about µ(x). We begin by precisely describing how we aggregate individual trees into a forest. For\nus, a random forest is an average of trees trained over all possible size-s subsamples of the\ntraining data, marginalizing over the auxiliary noise ξ.",
    "content_hash": "76eba2fba186eb04442b7f32dbc062e107eb333fd655207be54b0c7314b12a67",
    "location": null,
    "page_start": 1,
    "page_end": 10,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fd2cdac0-1fe0-4dc2-bd4c-5b1046010ba2",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Now, in order to verify (37), we ﬁrst recall that by Lemma 4\nVar\n\u0002\nE\n\u0002\nS1\nZ1\n\u0003\u0003\n= Ω\n1\ns log (s)d\n! . (38)\nThus, any terms that decay faster than the right-hand-side rate can safely be ignored in\nestablishing (37). We begin by verifying that we can take the leaf L(x) containing x to have\na small diameter diam(L(x)). Deﬁne the truncated tree predictor\nT ′ (x; Z) = T (x; Z) 1\n\b\ndiam (L(x)) ≤s−ω\t\u0001\n, where ω = 1\n2\nπ\nd\nlog\n\u0010\n(1 −α)−1\u0011\nlog (α−1)\n,\nand deﬁne similarly the truncated selection variables S′\ni = Si 1({diam (L(x)) ≤s−ω}). Now,\nthanks to the ANOVA decomposition (3rd line), we see that\nVar\n\u0002\nE\n\u0002\nT ′ (x; Z)\nZ1\n\u0003\n−E\n\u0002\nT (x; Z)\nZ1\n\u0003\u0003\n= Var\n\u0002\nE\n\u0002\nT (x; Z) 1\n\b\ndiam (L(x)) > s−ω\t\u0001 \f\f Z1\n\u0003\u0003\n≤1\ns Var\n\u0002\nT (x; Z) 1\n\b\ndiam (L(x)) > s−ω\t\u0001\u0003\n≤\nsupx∈[0, 1]d\n\b\nE\n\u0002\nY 2 \f\f X = x\n\u0003\ns\nP\n\u0002\ndiam (L(x)) > s−ω\u0003\n,\nwhere the sup term is bounded by Lipschitz-continuity of the second moment of Y .",
    "content_hash": "04127559b3d4e2992bcd47125361fa7b7183b10c7ba7a2e353f1f0a797b41ae0",
    "location": null,
    "page_start": 1,
    "page_end": 39,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ff5c1cba-c988-4901-9463-639f1a08359c",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "We used s = 2500 (i.e., |I| = 1250) and grew B = 2000\ntrees. One weakness of nearest neighbor approaches in general, and random forests in particular,\nis that they can ﬁll the valleys and ﬂatten the peaks of the true τ(x) function, especially\nnear the edge of feature space. We demonstrate this eﬀect using an example similar to the\n20",
    "content_hash": "bd170c8509e7a4bda3c2bb69ba025989898d6b1a28628171847a20ee805b27eb",
    "location": null,
    "page_start": 1,
    "page_end": 20,
    "metadata": {
      "section": ", and computed ˆ",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b37f1aa8-d1af-4221-9ef5-e567680ab098",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Leo Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996. Leo Breiman. Random forests. Machine Learning, 45(1):5–32, 2001a. Leo Breiman. Statistical modeling: The two cultures (with comments and a rejoinder by\nthe author). Statistical Science, 16(3):199–231, 2001b. Leo Breiman. Consistency for a simple model of random forests. Statistical Department,\nUniversity of California at Berkeley. Technical Report, (670), 2004. Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen. Classiﬁcation and\nRegression Trees. CRC press, 1984. Peter B¨uhlmann and Bin Yu. Analyzing bagging. The Annals of Statistics, 30(4):927–961,\n2002. Andreas Buja and Werner Stuetzle. Observations on bagging. Statistica Sinica, 16(2):323,\n2006. Song Xi Chen and Peter Hall. Eﬀects of bagging and bias correction on estimators deﬁned\nby estimating equations. Statistica Sinica, 13(1):97–110, 2003. Victor Chernozhukov, Christian Hansen, and Martin Spindler. Valid post-selection and post-\nregularization inference: An elementary, general approach. Annual Review of Economics,\n7(1):649–688, 2015. Hugh A Chipman, Edward I George, and Robert E McCulloch. BART: Bayesian additive\nregression trees. The Annals of Applied Statistics, 4(1):266–298, 2010.",
    "content_hash": "facbb6b8ac69f99f28ab26ce10a344f7239d0f033dcc497330fdfd47e62d96dd",
    "location": null,
    "page_start": 1,
    "page_end": 26,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c588be8f-3811-48eb-8eee-a12d569ab9bf",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "David I Cook, Val J Gebski, and Anthony C Keech. Subgroup analysis in clinical trials. Medical Journal of Australia, 180(6):289–292, 2004. Richard K Crump, V Joseph Hotz, Guido W Imbens, and Oscar A Mitnik. Nonparametric\ntests for treatment eﬀect heterogeneity. The Review of Economics and Statistics, 90(3):\n389–405, 2008. Rajeev H Dehejia. Program evaluation as a decision problem. Journal of Econometrics, 125\n(1):141–173, 2005. Misha Denil, David Matheson, and Nando De Freitas. Narrowing the gap: Random forests\nin theory and in practice. In Proceedings of The 31st International Conference on Machine\nLearning, pages 665–673, 2014. Jiangtao Duan. Bootstrap-Based Variance Estimators for a Bagging Predictor. PhD thesis,\nNorth Carolina State University, 2011. Miroslav Dud´ık, John Langford, and Lihong Li. Doubly robust policy evaluation and learn-\ning. In Proceedings of the 28th International Conference on Machine Learning, pages\n1097–1104, 2011. Bradley Efron. Estimation and accuracy after model selection (with discussion). Journal of\nthe American Statistical Association, 109(507), 2014. Bradley Efron and Charles Stein. The jackknife estimate of variance. The Annals of Statis-\ntics, 9(3):586–596, 1981. Jared C Foster, Jeremy MG Taylor, and Stephen J Ruberg.",
    "content_hash": "f99689b37b8109b8dc870ae0901c373ac7196a5e38dee240392eab23e7ce5a30",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "75e429eb-1cf5-4c35-83df-4a833696cdd8",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Journal of Machine Learning Research, 17(26):1–41, 2016. Jersey Neyman. Sur les applications de la th´eorie des probabilit´es aux experiences agricoles:\nEssai des principes. Roczniki Nauk Rolniczych, 10:1–51, 1923. Dimitris N. Politis, Joseph P. Romano, and Michael Wolf. Subsampling. Springer Series in\nStatistics. Springer New York, 1999. Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in obser-\nvational studies for causal eﬀects. Biometrika, 70(1):41–55, 1983. Donald B Rubin. Estimating causal eﬀects of treatments in randomized and nonrandomized\nstudies. Journal of Educational Psychology, 66(5):688, 1974. Richard J Samworth. Optimal weighted nearest neighbour classiﬁers. The Annals of Statis-\ntics, 40(5):2733–2763, 2012. 27",
    "content_hash": "4e91f6b87fd3b8c8bc16dbd919c3c3303f98fe408c8659719e7fadbb8bddf80d",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "beccb946-d8f0-45b3-9d11-4399fae82ef8",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Journal of Clinical Epidemiology, 63(8):826–833, 2010. Richard J Willke, Zhiyuan Zheng, Prasun Subedi, Rikard Althin, and C Daniel Mullins. From concepts, theory, and evidence of heterogeneity of treatment eﬀects to methodolog-\nical approaches: a primer. BMC Medical Research Methodology, 12(1):185, 2012. Achim Zeileis, Torsten Hothorn, and Kurt Hornik. Model-based recursive partitioning. Journal of Computational and Graphical Statistics, 17(2):492–514, 2008. Ruoqing Zhu, Donglin Zeng, and Michael R Kosorok. Reinforcement learning trees. Journal\nof the American Statistical Association, 110(512):1770–1784, 2015. 28",
    "content_hash": "b911da2e1f2b618a9cbc2991d667790550799f1a17882af787baea23509b02f5",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c97e51cc-8c17-4fd0-8ea4-39c7cd05f19b",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "C\nProofs\nNotation. Throughout the appendix we use the following notation to describe asymp-\ntotic scalings: f(s) ∼g(s) means that lims→∞f(s)/g(s) = 1, f(s) ≳g(s) means that\nlim infs→∞f(s)/g(s) ≥1 and f(s) ≲g(s) is analogous, f(s) = O(g(s)) means that f(s) ≲\nC g(s) for some C > 0, f(s) = Ω(g(s)) means that f(s) ≳c g(s) for some c > 0, and ﬁnally\nf(s) = o(g(s)) means that lim sups→∞f(s)/g(s) = 0. 32",
    "content_hash": "d938ab515b5276abee8c4eb4e828ab465f4a09b59ab60651b66680ef3dbf5866",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2ee8fbf0-2864-426c-a907-0ce8e8c350e0",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "because S1 ≤1. Meanwhile, because E\n\u0002\nY\nX = x\n\u0003\nis Lipschitz, we can deﬁne u :=\nsup\n\b\f\fE\n\u0002\nY\nX = x\n\u0003\f\f : x ∈[0, 1]d\n, and see that\nE\n\n\nE\n\" n\nX\ni=1\nSiE\n\u0002\nYi\nXi\n\u0003 \f\f Z1\n#\n−E [T]\n2+δ\n≤(2u)δ Var\n\"\nE\n\" n\nX\ni=1\nSiE\n\u0002\nYi\nXi\n\u0003 \f\f Z1\n##\n≤21+δu2+δ \u0010\nE\nh\nE\n\u0002\nS1\nZ1\n\u00032i\n+ Var\n\u0002\n(n −1)E\n\u0002\nS2\nZ1\n\u0003\u0003\u0011\n≤(2u)2+δE\nh\nE\n\u0002\nS1\nX1\n\u00032i\n. Thus, the condition we need to check simpliﬁes to\nlim\nn→∞n E\nh\nE\n\u0002\nS1\nX1\n\u00032i \u000e n Var\n\u0002\nE\n\u0002\nT\nZ1\n\u0003\u0003\u00011+δ/2 = 0. Finally, as argued in the proofs of Theorem 5 and Corollary 6,\nVar\n\u0002\nE\n\u0002\nT\nZ1\n\u0003\u0003\n= Ω\n\u0010\nE\nh\nE\n\u0002\nS1\nX1\n\u00032i\nVar\n\u0002\nY\nX = x\n\u0003\u0011\n. Because Var\n\u0002\nY\nX = x\n\u0003\n> 0 by assumption, we can use Lemma 4 to conclude our argument,\nnoting that\n\u0010\nn E\nh\nE\n\u0002\nS1\nX1\n\u00032i\u0011−δ/2\n≲\n\u0012Cf, d\n2k\nn\ns log(s)d\n\u0013−δ/2\n,\nwhich goes to 0 thanks to our assumptions on the scaling of s (and the factor 2 comes from\npotentially using a double-sample tree). Proof of Theorem 9. Let F denote the distribution from which we drew Z1, ..., Zn.",
    "content_hash": "fd1f246177a37746e37461cbfdf189ff509d017d8df4387b0875bd5b17b67c16",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0c046a08-c164-4d2b-80b1-f3ac548558ef",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "As we show in Lemma 12, the main eﬀects Ai give us σ2\nn, in that\n1\nσ2n\ns2\nn2\nn\nX\ni=1\nA2\ni →p 1. (43)\nMeanwhile, Lemma 13 establishes that the Bi all satisfy\nE\n\u0002\nR2\ni\n\u0003\n≲2\nn Var [T (x; , Z1, ..., Zs)] ,\n(44)\nand so\nE\n\"\ns2\nn2\nn\nX\ni=1\nR2\ni\n#\n≲2 s2\nn2 Var [T (x; , Z1, ..., Zs)]\n≲2 s\nn log (n)d σ2\nn. Because all terms are positive and s log (n)d /n goes to zero by hypothesis, Markov’s inequal-\nity implies that\n1\nσ2n\ns2\nn2\nn\nX\ni=1\nR2\ni →p 0. Using Cauchy-Schwarz to bound the cross terms of the form AiRi, and noting that limn→∞n(n−\n1)/(n −s)2 = 1, we can thus conclude that bVIJ/σ2\nn converges in probability to 1. Lemma 12. Under the conditions of Theorem 9, (43) holds. Proof.",
    "content_hash": "2534540806587b5d263ea742cc7c62b8c74f64e3c7d1bc94fe26a0d2f2265318",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0961f39c-a07e-4ef0-89d5-b4771797e8c3",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Lemma 13. Under the conditions of Theorem 9, (44) holds. Proof. Without loss of generality, we establish (44) for R1. Using the ANOVA decomposition\n(40), we can write our term of interest as\nR1 = EZ∗⊂b\nF\nh\nT −˚\nT\nZ∗\n1 = Z1\ni\n−EZ∗⊂b\nF\nh\nT −˚\nT\ni\n=\n\u0012 s −1\nn −1 −\n\u0012s\n2\n\u0013\u000e\u0012n\n2\n\u0013\u0013\nn\nX\ni=2\nT2 (Z1, Zi)\n+\n\u0012\u0012s −1\n2\n\u0013\u000e\u0012n −1\n2\n\u0013\n−\n\u0012s\n2\n\u0013\u000e\u0012n\n2\n\u0013\u0013\nX\n2≤i<j≤n\nT2 (Zi, Zj)\n+\n\u0012\u0012s −1\n2\n\u0013\u000e\u0012n −1\n2\n\u0013\n−\n\u0012s\n3\n\u0013\u000e\u0012n\n3\n\u0013\u0013\nX\n2≤i<j≤n\nT3 (Z1, Zi, Zj)\n+\n\u0012\u0012s −1\n3\n\u0013\u000e\u0012n −1\n3\n\u0013\n−\n\u0012s\n3\n\u0013\u000e\u0012n\n3\n\u0013\u0013\nX\n2≤i<j<k≤n\nT3 (Zi, Zj, Zk)\n+ . . . Because all the terms in the ANOVA expansion are mean-zero and uncorrelated, we see\nusing notation from (41) that\nE\n\u0002\nR2\n1\n\u0003\n= (n −1)\n\u0012 s −1\nn −1 −\n\u0012s\n2\n\u0013\u000e\u0012n\n2\n\u0013\u00132\nV2\n+\n\u0012n −1\n2\n\u0013 \u0012\u0012s −1\n2\n\u0013\u000e\u0012n −1\n2\n\u0013\n−\n\u0012s\n2\n\u0013\u000e\u0012n\n2\n\u0013\u00132\nV2\n+\n\u0012n −1\n2\n\u0013 \u0012\u0012s −1\n2\n\u0013\u000e\u0012n −1\n2\n\u0013\n−\n\u0012s\n3\n\u0013\u000e\u0012n\n3\n\u0013\u00132\nV3\n+\n\u0012n −1\n3\n\u0013 \u0012\u0012s −1\n3\n\u0013\u000e\u0012n −1\n3\n\u0013\n−\n\u0012s\n3\n\u0013\u000e\u0012n\n3\n\u0013\u00132\nV3\n+ . . .",
    "content_hash": "9c5fa8a73b2feed529e306eea5feb1abc668b5fe43139d3d4d2f577edc016927",
    "location": null,
    "page_start": 1,
    "page_end": 45,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e27fb7a9-9336-4e62-aa3f-8e1ae3193903",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Recall that\ns\nX\nk=1\n\u0012s\nk\n\u0013\nVk = Var [T (Z1, ..., Zs)] . The above sum is maximized when all the variance is contained in second-order terms, and\ns\n2\n\u0001\nV2 = Var [T]. This implies that\nE\n\u0002\nR2\n1\n\u0003\n≲(n −1)\n\u0012 s −1\nn −1 −\n\u0012s\n2\n\u0013\u000e\u0012n\n2\n\u0013\u00132 \u0012s\n2\n\u0013−1\nVar [T (Z1, ..., Zs)]\n∼2\nn Var [T (Z1, ..., Zs)] ,\nthus completing the proof. 45",
    "content_hash": "681f735f28e64abd6efb9dd2a0ef6cd7bcc77f878449c368acfbda2e1f3953a4",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cf50a046-41ff-4d24-b339-aebebc9f0bbf",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "a few subgroups. Recently, however, there has been an explosion of empirical settings where\nit is potentially feasible to customize estimates for individuals. An impediment to exploring heterogeneous treatment eﬀects is the fear that researchers\nwill iteratively search for subgroups with high treatment levels, and then report only the re-\nsults for subgroups with extreme eﬀects, thus highlighting heterogeneity that may be purely\nspurious [Assmann et al., 2000, Cook et al., 2004]. For this reason, protocols for clinical\ntrials must specify in advance which subgroups will be analyzed, and other disciplines such\nas economics have instituted protocols for registering pre-analysis plans for randomized ex-\nperiments or surveys. However, such procedural restrictions can make it diﬃcult to discover\nstrong but unexpected treatment eﬀect heterogeneity. In this paper, we seek to address\nthis challenge by developing a powerful, nonparametric method for heterogeneous treatment\neﬀect estimation that yields valid asymptotic conﬁdence intervals for the true underlying\ntreatment eﬀect. Classical approaches to nonparametric estimation of heterogeneous treatment eﬀects in-\nclude nearest-neighbor matching, kernel methods, and series estimation; see, e.g., Crump\net al. [2008], Lee [2009], and Willke et al. [2012]. These methods perform well in applica-\ntions with a small number of covariates, but quickly break down as the number of covariates\nincreases. In this paper, we explore the use of ideas from the machine learning literature to\nimprove the performance of these classical methods with many covariates.",
    "content_hash": "cf7117db1ef091926f7859109fefbe5bdb26eaee4db6f2b4740ad8d0e9597cdd",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "a few subgroups. Recently, however, there has been an explosion of empirical settings where",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1a4b9d83-d540-4759-ba5a-87259211ce87",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "This paper addresses these limitations, developing a forest-based method for treatment\neﬀect estimation that allows for a tractable asymptotic theory and valid statistical inference. Following Athey and Imbens [2016], our proposed forest is composed of causal trees that\nestimate the eﬀect of the treatment at the leaves of the trees; we thus refer to our algorithm\nas a causal forest. In the interest of generality, we begin our theoretical analysis by developing the desired\nconsistency and asymptotic normality results in the context of regression forests. We prove\nthese results for a particular variant of regression forests that uses subsampling to generate a\nvariety of diﬀerent trees, while it relies on deeply grown trees that satisfy a condition we call\n“honesty” to reduce bias. An example of an honest tree is one where the tree is grown using\none subsample, while the predictions at the leaves of the tree are estimated using a diﬀerent\n2",
    "content_hash": "3869a0d95221d35a4dfcd2e1c67763d6d38e85d187d337f556e03b2354473a8c",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "eﬀect estimation that allows for a tractable asymptotic theory and valid statistical inference.",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "86573749-4fa4-43ed-803a-a9f7b03b2b52",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "We focus on the\nfamily of random forest algorithms introduced by Breiman [2001a], which allow for ﬂexible\nmodeling of interactions in high dimensions by building a large number of regression trees\nand averaging their predictions. Random forests are related to kernels and nearest-neighbor\nmethods in that they make predictions using a weighted average of “nearby” observations;\nhowever, random forests diﬀer in that they have a data-driven way to determine which nearby\nobservations receive more weight, something that is especially important in environments\nwith many covariates or complex interactions among covariates. Despite their widespread success at prediction and classiﬁcation, there are important\nhurdles that need to be cleared before random forests are directly useful to causal inference. Ideally, an estimator should be consistent with a well-understood asymptotic sampling dis-\ntribution, so that a researcher can use it to test hypotheses and establish conﬁdence intervals. For example, when deciding to use a drug for an individual, we may wish to test the hypothe-\nsis that the expected beneﬁt from the treatment is less than the treatment cost. Asymptotic\nnormality results are especially important in the causal inference setting, both because many\npolicy applications require conﬁdence intervals for decision-making, and because it can be\ndiﬃcult to directly evaluate the model’s performance using, e.g., cross validation, when es-\ntimating causal eﬀects. Yet, the asymptotics of random forests have been largely left open,\neven in the standard regression or classiﬁcation contexts.",
    "content_hash": "2979a85783fa24ebdc58c5fb8fe8e4ee02610fd34886e0be6d3c59f54efb4a2e",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "2001a",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0b9a6ad3-71c0-43e0-a972-27fa639df72e",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "subsample. We also show that the heuristically motivated inﬁnitesimal jackknife for random\nforests developed by Efron [2014] and Wager et al. [2014] is consistent for the asymptotic\nvariance of random forests in this setting. Our proof builds on classical ideas from Efron and\nStein [1981], H´ajek [1968], and Hoeﬀding [1948], as well as the adaptive nearest neighbors\ninterpretation of random forests of Lin and Jeon [2006]. Given these general results, we\nnext show that our consistency and asymptotic normality results extend from the regression\nsetting to estimating heterogeneous treatment eﬀects in the potential outcomes framework\nwith unconfoundedness [Neyman, 1923, Rubin, 1974]. Although our main focus in this paper is causal inference, we note that there are a variety\nof important applications of the asymptotic normality result in a pure prediction context. For example, Kleinberg et al. [2015] seek to improve the allocation of medicare funding\nfor hip or knee replacement surgery by detecting patients who had been prescribed such a\nsurgery, but were in fact likely to die of other causes before the surgery would have been\nuseful to them. Here we need predictions for the probability that a given patient will survive\nfor more than, say, one year that come with rigorous conﬁdence statements; our results are\nthe ﬁrst that enable the use of random forests for this purpose.",
    "content_hash": "82898c4ae55befe78092c81a2a7e08bdded476e07fe508b0770d283e870489bc",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "Efron",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a8dfb6d0-e1f2-46a4-a48e-53ca31520261",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Finally, we compare the performance of the causal forest algorithm against classical k-\nnearest neighbor matching using simulations, ﬁnding that the causal forest dominates in\nterms of both bias and variance in a variety of settings, and that its advantage increases\nwith the number of covariates. We also examine coverage rates of our conﬁdence intervals\nfor heterogeneous treatment eﬀects. 1.1\nRelated Work\nThere has been a longstanding understanding in the machine learning literature that predic-\ntion methods such as random forests ought to be validated empirically [Breiman, 2001b]: if\nthe goal is prediction, then we should hold out a test set, and the method will be considered\nas good as its error rate is on this test set. However, there are fundamental challenges with\napplying a test set approach in the setting of causal inference. In the widely used potential\noutcomes framework we use to formalize our results [Neyman, 1923, Rubin, 1974], a treat-\nment eﬀect is understood as a diﬀerence between two potential outcomes, e.g., would the\npatient have died if they received the drug vs. if they didn’t receive it. Only one of these\npotential outcomes can ever be observed in practice, and so direct test-set evaluation is in\ngeneral impossible.1 Thus, when evaluating estimators of causal eﬀects, asymptotic theory\nplays a much more important role than in the standard prediction context. From a technical point of view, the main contribution of this paper is an asymptotic nor-\nmality theory enabling us to do statistical inference using random forest predictions.",
    "content_hash": "3f0a14a4414c02c5f3783e1b3f6bc56a3d5f5b392acbe3e0be4dad1979472a5a",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "nearest neighbor matching using simulations, ﬁnding that the causal forest dominates in",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6bc4639a-6338-49f8-8797-dab441568775",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Recent\nresults by Biau [2012], Meinshausen [2006], Mentch and Hooker [2016], Scornet et al. [2015],\nand others have established asymptotic properties of particular variants and simpliﬁcations\nof the random forest algorithm. To our knowledge, however, we provide the ﬁrst set of con-\nditions under which predictions made by random forests are both asymptotically unbiased\nand Gaussian, thus allowing for classical statistical inference; the extension to the causal\nforests proposed in this paper is also new. We review the existing theoretical literature on\nrandom forests in more detail in Section 3.1. 1Athey and Imbens [2016] have proposed indirect approaches to mimic test-set evaluation for causal\ninference. However, these approaches require an estimate of the true treatment eﬀects and/or treatment\npropensities for all the observations in the test set, which creates a new set of challenges. In the absence of\nan observable ground truth in a test set, statistical theory plays a more central role in evaluating the noise\nin estimates of causal eﬀects. 3",
    "content_hash": "356706e0de5314bad93f3cc1271d9bae2cd9ea6f9612933d92cf111554616ba4",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "and others have established asymptotic properties of particular variants and simpliﬁcations",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b704329a-df93-46c9-87e8-7e086c7f7acb",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "In the econo-\nmetrics literature, Bhattacharya and Dupas [2012], Dehejia [2005], Hirano and Porter [2009],\nManski [2004] estimate parametric or semi-parametric models for optimal policies, relying on\nregularization for covariate selection in the case of Bhattacharya and Dupas [2012]. Taddy\net al. [2016] use Bayesian nonparametric methods with Dirichlet priors to ﬂexibly estimate\nthe data-generating process, and then project the estimates of heterogeneous treatment ef-\nfects down onto the feature space using regularization methods or regression trees to get\nlow-dimensional summaries of the heterogeneity; but again, there are no guarantees about\nasymptotic properties. 4",
    "content_hash": "c64d4ac8ab94c70bc995448334b9c50014cbde67e95e9473aba4e84acbfffd41",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Bhattacharya and Dupas",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "554fb7cf-ee62-4088-9e01-9fcda65c46fc",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "A small but growing literature, including Green and Kern [2012], Hill [2011] and Hill and\nSu [2013], has considered the use of forest-based algorithms for estimating heterogeneous\ntreatment eﬀects. These papers use the Bayesian Additive Regression Tree (BART) method\nof Chipman et al. [2010], and report posterior credible intervals obtained by Markov-chain\nMonte Carlo (MCMC) sampling based on a convenience prior. Meanwhile, Foster et al. [2011] use regression forests to estimate the eﬀect of covariates on outcomes in treated and\ncontrol groups separately, and then take the diﬀerence in predictions as data and project\ntreatment eﬀects onto units’ attributes using regression or classiﬁcation trees (in contrast, we\nmodify the standard random forest algorithm to focus on directly estimating heterogeneity\nin causal eﬀects). A limitation of this line of work is that, until now, it has lacked formal\nstatistical inference results. We view our contribution as complementary to this literature, by showing that forest-\nbased methods need not only be viewed as black-box heuristics, and can instead be used\nfor rigorous asymptotic analysis. We believe that the theoretical tools developed here will\nbe useful beyond the speciﬁc class of algorithms studied in our paper. In particular, our\ntools allow for a fairly direct analysis of variants of the method of Foster et al. [2011]. Using BART for rigorous statistical analysis may prove more challenging since, although\nBART is often successful in practice, there are currently no results guaranteeing posterior\nconcentration around the true conditional mean function, or convergence of the MCMC\nsampler in polynomial time. Advances of this type would be of considerable interest.",
    "content_hash": "9ba6dc1de1ede45d72576475cc23cb2904e4725911fb1aa19667719735190c5d",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "treatment eﬀects. These papers use the Bayesian Additive Regression Tree (BART) method",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1d717acb-7ff8-4626-94f3-634cb5f5f137",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Several papers use tree-based methods for estimating heterogeneous treatment eﬀects. In\ngrowing trees to build our forest, we follow most closely the approach of Athey and Imbens\n[2016], who propose honest, causal trees, and obtain valid conﬁdence intervals for average\ntreatment eﬀects for each of the subpopulations (leaves) identiﬁed by the algorithm. (Instead\nof personalizing predictions for each individual, this approach only provides treatment eﬀect\nestimates for leaf-wise subgroups whose size must grow to inﬁnity.) Other related approaches\ninclude those of Su et al. [2009] and Zeileis et al. [2008], which build a tree for treatment\neﬀects in subgroups and use statistical tests to determine splits; however, these papers do\nnot analyze bias or consistency properties. Finally, we note a growing literature on estimating heterogeneous treatment eﬀects using\ndiﬀerent machine learning methods. Imai and Ratkovic [2013], Signorovitch [2007], Tian\net al. [2014] and Weisberg and Pontes [2015] develop lasso-like methods for causal inference\nin a sparse high-dimensional linear setting. Beygelzimer and Langford [2009], Dud´ık et al. [2011], and others discuss procedures for transforming outcomes that enable oﬀ-the-shelf loss\nminimization methods to be used for optimal treatment policy estimation.",
    "content_hash": "373f8e74b7a33a12a8838a1d0022264aab9781e60f8c4fe74c92cee1150f7f92",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "], who propose honest, causal trees, and obtain valid conﬁdence intervals for average",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7e62a717-f8ec-4a87-929a-aa4195d1a535",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "(2)\nThe motivation behind unconfoundedness is that, given continuity assumptions, it eﬀectively\nimplies that we can treat nearby observations in x-space as having come from a randomized\nexperiment; thus, nearest-neighbor matching and other local methods will in general be\nconsistent for τ(x). An immediate consequence of unconfoundedness is that\nE\n\u0014\nYi\n\u0012 Wi\ne (x) −1 −Wi\n1 −e (x)\n\u0013 \f\f Xi = x\n\u0015\n= τ (x) ,\nwhere e (x) = E\n\u0002\nWi\nXi = x\n\u0003\n(3)\nis the propensity of receiving treatment at x. Thus, if we knew e(x), we would have access\nto a simple unbiased estimator for τ(x); this observation lies at the heart of methods based\non propensity weighting [e.g., Hirano et al., 2003]. Many early applications of machine\nlearning to causal inference eﬀectively reduce to estimating e(x) using, e.g., boosting, a\nneural network, or even random forests, and then transforming this into an estimate for\nτ(x) using (3) [e.g., McCaﬀrey et al., 2004, Westreich et al., 2010]. In this paper, we take a\nmore indirect approach: We show that, under regularity assumptions, causal forests can use\nthe unconfoundedness assumption (2) to achieve consistency without needing to explicitly\nestimate the propensity e(x). 2.2\nFrom Regression Trees to Causal Trees and Forests\nAt a high level, trees and forests can be thought of as nearest neighbor methods with an\nadaptive neighborhood metric.",
    "content_hash": "8cee861cd73fbcd90657264985dd1a25987f109c7c80e4fa31f5e324661f7bef",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "where",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "78278dea-1afc-4676-b8ad-9e1924416816",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "2\nCausal Forests\n2.1\nTreatment Estimation with Unconfoundedness\nSuppose we have access to n independent and identically distributed training examples\nlabeled i = 1, ..., n, each of which consists of a feature vector Xi ∈[0, 1]d, a response\nYi ∈R, and a treatment indicator Wi ∈{0, 1}. Following the potential outcomes framework\nof Neyman [1923] and Rubin [1974] (see Imbens and Rubin [2015] for a review), we then\nposit the existence of potential outcomes Y (1)\ni\nand Y (0)\ni\ncorresponding respectively to the\nresponse the i-th subject would have experienced with and without the treatment, and deﬁne\nthe treatment eﬀect at x as\nτ (x) = E\nh\nY (1)\ni\n−Y (0)\ni\nXi = x\ni\n. (1)\nOur goal is to estimate this function τ(x). The main diﬃculty is that we can only ever\nobserve one of the two potential outcomes Y (0)\ni\n, Y (1)\ni\nfor a given training example, and so\ncannot directly train machine learning methods on diﬀerences of the form Y (1)\ni\n−Y (0)\ni\n. In general, we cannot estimate τ(x) simply from the observed data (Xi, Yi, Wi) without\nfurther restrictions on the data generating distribution. A standard way to make progress\nis to assume unconfoundedness [Rosenbaum and Rubin, 1983], i.e., that the treatment as-\nsignment Wi is independent of the potential outcomes for Yi conditional on Xi:\nn\nY (0)\ni\n, Y (1)\ni\no\n⊥⊥Wi\nXi.",
    "content_hash": "88cd297b79a44afa11ee20353c0d78570734a2c61367c17125a1839800e6f41a",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": ". Following the potential outcomes framework",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "41698c0e-15da-44ff-bdcb-fb9608300ccb",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Given a test point x, classical methods such as k-nearest\nneighbors seek the k closest points to x according to some pre-speciﬁed distance measure,\ne.g., Euclidean distance. In contrast, tree-based methods also seek to ﬁnd training examples\nthat are close to x, but now closeness is deﬁned with respect to a decision tree, and the\n5",
    "content_hash": "a67c520e2016476c1d5411a3ad559b66c6250a0e0db0d9f4f9d953efd8ae665a",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "closest points to",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "eb45d0d0-3889-4126-9aab-219f2d7c246c",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Then, it is natural to estimate the treatment\neﬀect for any x ∈L as\nˆτ (x) =\n1\n|{i : Wi = 1, Xi ∈L}|\nX\n{i:Wi=1, Xi∈L}\nYi\n−\n1\n|{i : Wi = 0, Xi ∈L}|\nX\n{i:Wi=0, Xi∈L}\nYi. (5)\nIn the following sections, we will establish that such trees can be used to grow causal forests\nthat are consistent for τ(x).2\nFinally, given a procedure for generating a single causal tree, a causal forest generates\nan ensemble of B such trees, each of which outputs an estimate ˆτb(x). The forest then\naggregates their predictions by averaging them: ˆτ(x) = B−1 PB\nb=1 ˆτb(x). We always assume\nthat the individual causal trees in the forest are built using random subsamples of s training\nexamples, where s/n ≪1; for our theoretical results, we will assume that s ≍nβ for some\nβ < 1. The advantage of a forest over a single tree is that it is not always clear what\nthe “best” causal tree is. In this case, as shown by Breiman [2001a], it is often better to\ngenerate many diﬀerent decent-looking trees and average their predictions, instead of seeking\na single highly-optimized tree. In practice, this aggregation scheme helps reduce variance\nand smooths sharp decision boundaries [B¨uhlmann and Yu, 2002].",
    "content_hash": "88b0e8f88e1b5bf285e89331976e8aba4638e6e029fdd2dc36c8cb8f01fcac7d",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": ") =",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8ee8a76b-d7bb-4402-b26d-077216d26d4a",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "closest points to x are those that fall in the same leaf as it. The advantage of trees is that\ntheir leaves can be narrower along the directions where the signal is changing fast and wider\nalong the other directions, potentially leading a to a substantial increase in power when the\ndimension of the feature space is even moderately large. In this section, we seek to build causal trees that resemble their regression analogues as\nclosely as possible. Suppose ﬁrst that we only observe independent samples (Xi, Yi), and\nwant to build a CART regression tree. We start by recursively splitting the feature space\nuntil we have partitioned it into a set of leaves L, each of which only contains a few training\nsamples. Then, given a test point x, we evaluate the prediction ˆµ(x) by identifying the leaf\nL(x) containing x and setting\nˆµ (x) =\n1\n|{i : Xi ∈L(x)}|\nX\n{i:Xi∈L(x)}\nYi. (4)\nHeuristically, this strategy is well-motivated if we believe the leaf L(x) to be small enough\nthat the responses Yi inside the leaf are roughly identically distributed. There are several\nprocedures for how to place the splits in the decision tree; see, e.g., Hastie et al. [2009]. In the context of causal trees, we analogously want to think of the leaves as small enough\nthat the (Yi, Wi) pairs corresponding to the indices i for which i ∈L(x) act as though they\nhad come from a randomized experiment.",
    "content_hash": "9e11dfda231ff2bbc3f81ef56d2699d934cccd9d4be846385537e9001b214f52",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "closely as possible. Suppose ﬁrst that we only observe independent samples (",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fb3b6d1c-c283-461b-8a39-ae6401027c42",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "2.3\nAsymptotic Inference with Causal Forests\nOur results require some conditions on the forest-growing scheme: The trees used to build\nthe forest must be grown on subsamples of the training data, and the splitting rule must\n2The causal tree algorithm presented above is a simpliﬁcation of the method of Athey and Imbens [2016]. The main diﬀerence between our approach and that of Athey and Imbens [2016] is that they seek to build a\nsingle well-tuned tree; to this end, they use fairly large leaves and apply a form propensity weighting based\non (3) within each leaf to correct for variations in e(x) inside the leaf. In contrast, we follow Breiman [2001a]\nand build our causal forest using deep trees. Since our leaves are small, we are not required to apply any\nadditional corrections inside them. However, if reliable propensity estimates are available, using them as\nweights for our method may improve performance (and would not conﬂict with the theoretical results). 6",
    "content_hash": "a604575d94eb7313dc61f99d592b86fe43925b6493527d70bc8f4bb91f51f294",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": ") within each leaf to correct for variations in",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "315b73d1-08fc-4978-9151-1e5ef9f165e5",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "To deﬁne the variance estimates, let ˆτ ∗\nb (x) be the treatment eﬀect estimate given by the\nb-th tree, and let N ∗\nib ∈{0, 1} indicate whether or not the i-th training example was used\nfor the b-th tree.3 Then, we set\nbVIJ (x) = n −1\nn\n\u0012\nn\nn −s\n\u00132\nn\nX\ni=1\nCov∗[ˆτ ∗\nb (x) , N ∗\nib]2 ,\n(8)\nwhere the covariance is taken with respect to the set of all the trees b = 1, .., B used in\nthe forest. The term n(n −1)/(n −s)2 is a ﬁnite-sample correction for forests grown by\nsubsampling without replacement; see Proposition 10. We show that this variance estimate\nis consistent, in the sense that bVIJ (x) / Var [ˆτ(x)] →p 1. 2.4\nHonest Trees and Forests\nIn our discussion so far, we have emphasized the ﬂexible nature of our results: for a wide\nvariety of causal forests that can be tailored to the application area, we achieve both con-\nsistency and centered asymptotic normality, provided the sub-sample size s scales at an\n3For double-sample trees deﬁned in Procedure 1, N∗\nib = 1 if the i-th example appears in either the\nI-sample or the J -sample. 7",
    "content_hash": "f9e4eecbc5144e5d84a2384f583186219aefe3e1ae120792b0971985bf9e11a1",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Honest Trees and Forests",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7080bde3-06b4-4f54-a75c-78d104fae69d",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Using\nthe potential nearest neighbors construction of Lin and Jeon [2006] and classical analysis\ntools going back to Hoeﬀding [1948] and H´ajek [1968], we show that—provided the sub-\nsample size s scales appropriately with n—the predictions made by a causal forest are\nasymptotically Gaussian and unbiased. Speciﬁcally, we show that\n(ˆτ (x) −τ (x))\n\u000e p\nVar [ˆτ(x)] ⇒N (0, 1)\n(7)\nunder the conditions required for consistency, provided the subsample size s scales as s ≍nβ\nfor some βmin < β < 1\nMoreover, we show that the asymptotic variance of causal forests can be accurately\nestimated. To do so, we use the inﬁnitesimal jackknife for random forests developed by\nEfron [2014] and Wager et al. [2014], based on the original inﬁnitesimal jackknife procedure\nof Jaeckel [1972]. This method assumes that we have taken the number of trees B to be large\nenough that the Monte Carlo variability of the forest does not matter; and only measures\nthe randomness in ˆτ(x) due to the training sample.",
    "content_hash": "ad28e3acd64abc87f1c9b3513993c7bf36789fc32a954d2c8be89f02b8f26116",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Wager et al.",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4b691c2a-d8aa-4847-a54c-92838a5edb1b",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "not “inappropriately” incorporate information about the outcomes Yi as discussed formally\nin Section 2.4. However, given these high level conditions, we obtain a widely applicable\nconsistency result that applies to several diﬀerent interesting causal forest algorithms. Our ﬁrst result is that causal forests are consistent for the true treatment eﬀect τ(x). To achieve pointwise consistency, we need to assume that the conditional mean functions\nE\n\u0002\nY (0) \f\f X = x\n\u0003\nand E\n\u0002\nY (1) \f\f X = x\n\u0003\nare both Lipschitz continuous. To our knowledge, all\nexisting results on pointwise consistency of regression forests [e.g., Biau, 2012, Meinshausen,\n2006] require an analogous condition on E\n\u0002\nY\nX = x\n\u0003\n. This is not particularly surprising,\nas forests generally have smooth response surfaces [B¨uhlmann and Yu, 2002]. In addition to\ncontinuity assumptions, we also need to assume that we have overlap, i.e., for some ε > 0\nand all x ∈[0, 1]d,\nε < P\n\u0002\nW = 1\nX = x\n\u0003\n< 1 −ε. (6)\nThis condition eﬀectively guarantees that, for large enough n, there will be enough treatment\nand control units near any test point x for local methods to work. Beyond consistency, in order to do statistical inference on the basis of the estimated\ntreatment eﬀects ˆτ(x), we need to understand their asymptotic sampling distribution.",
    "content_hash": "b17654871e204c9dcf8472eedce599a204ec6105165ec16f9e3c99291f2cbb70",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "existing results on pointwise consistency of regression forests [e.g.,",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e3cf8671-d7eb-4303-be7e-16c7554564c7",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "(9)\nThus, ﬁnding the squared-error minimizing split is equivalent to maximizing the variance of\nˆµ(Xi) for i ∈J ; note that P\ni∈J ˆµ(Xi) = P\ni∈J Yi for all trees, and so maximizing variance\nis equivalent to maximizing the sum of the ˆµ(Xi)2. In Procedure 1, we emulate this algorithm\nby picking splits that maximize the variance of ˆτ(Xi) for i ∈J .4\nRemark 2. In Appendix B, we present evidence that adaptive forests with small leaves\ncan overﬁt to outliers in ways that make them inconsistent near the edges of sample space. Thus, the forests of Breiman [2001a] need to be modiﬁed in some way to get pointwise\n4Athey and Imbens [2016] also consider “honest splitting rules” that anticipate honest estimation, and\ncorrect for the additional sampling variance in small leaves using an idea closely related to the Cp penalty of\nMallows [1973]. Although it could be of interest for further work, we do not study the eﬀect of such splitting\nrules here. 8",
    "content_hash": "335c3117544e56f0c90d2a17093dfb9a45471ca9ccb1aab428a44b57527d32b4",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "1973",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1adcb4b6-7213-4b5f-9073-7ec71c0ce4dd",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Thus,\nalthough no data point can be used for split selection and leaf estimation in a single tree,\neach data point will participate in both I and J samples of some trees, and so will be used\nfor both specifying the structure and treatment eﬀect estimates of the forest. Although our\noriginal motivation for considering double-sample trees was to eliminate bias and thus enable\ncentered conﬁdence intervals, we ﬁnd that in practice, double-sample trees can improve upon\nstandard random forests in terms of mean-squared error as well. Another way to build honest trees is to ignore the outcome data Yi when placing splits,\nand instead ﬁrst train a classiﬁcation tree for the treatment assignments Wi (Procedure 2). Such propensity trees can be particularly useful in observational studies, where we want to\nminimize bias due to variation in e(x). Seeking estimators that match training examples\nbased on estimated propensity is a longstanding idea in causal inference, going back to\nRosenbaum and Rubin [1983]. Remark 1. For completeness, we brieﬂy outline the motivation for the splitting rule of\nAthey and Imbens [2016] we use for our double-sample trees. This method is motivated by\nan algorithm for minimizing the squared-error loss in regression trees. Because regression\ntrees compute predictions ˆµ by averaging training responses over leaves, we can verify that\nX\ni∈J\n(ˆµ (Xi) −Yi)2 =\nX\ni∈J\nY 2\ni −\nX\ni∈J\nˆµ (Xi)2 .",
    "content_hash": "8b6442f56ad9ce27d93d44f4f328cd6db1d0d46022e85bfbbaff2d51537f7d83",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "minimize bias due to variation in",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a1cf3480-a9f1-4fb4-b20a-9ce86d4ebd83",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "appropriate rate. Our results do, however, require the individual trees to satisfy a fairly\nstrong condition, which we call honesty: a tree is honest if, for each training example i, it\nonly uses the response Yi to estimate the within-leaf treatment eﬀect τ using (5) or to decide\nwhere to place the splits, but not both. We discuss two causal forest algorithms that satisfy\nthis condition. Our ﬁrst algorithm, which we call a double-sample tree, achieves honesty by dividing\nits training subsample into two halves I and J . Then, it uses the J -sample to place the\nsplits, while holding out the I-sample to do within-leaf estimation; see Procedure 1 for\ndetails. In our experiments, we set the minimum leaf size to k = 1. A similar family of\nalgorithms was discussed in detail by Denil et al. [2014], who showed that such forests could\nachieve competitive performance relative to standard tree algorithms that do not divide their\ntraining samples. In the semiparametric inference literature, related ideas go back at least\nto the work of Schick [1986]. We note that sample splitting procedures are sometimes criticized as ineﬃcient because\nthey “waste” half of the training data at each step of the estimation procedure. However, in\nour case, the forest subampling mechanism enables us to achieve honesty without wasting any\ndata in this sense, because we re-randomize the I/J -data splits over each subsample.",
    "content_hash": "e1e6b2bf126067e5223cd908c66ed9b3554b40f7da6344aa4793ced9421460e1",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "Denil et al.",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ff160eb5-e4c6-4520-a760-ce6d3e30b92d",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Following Athey and Imbens [2016], the splits of the\ntree are chosen by maximizing the variance of ˆτ(Xi) for i ∈J ; see Remark 1 for details. In addition, each leaf of the tree must contain k or more I-sample observations of each\ntreatment class. consistency results; here, we use honesty following, e.g., Wasserman and Roeder [2009]. We note that there have been some recent theoretical investigations of non-honest forests,\nincluding Scornet et al. [2015] and Wager and Walther [2015]. However, Scornet et al. [2015]\ndo not consider pointwise properties of forests; whereas Wager and Walther [2015] show\nconsistency of adaptive forests with larger leaves, but their bias bounds decay slower than\nthe sampling variance of the forests and so cannot be used to establish centered asymptotic\nnormality. 3\nAsymptotic Theory for Random Forests\nIn order to use random forests to provide formally valid statistical inference, we need an\nasymptotic normality theory for random forests. In the interest of generality, we ﬁrst de-\nvelop such a theory in the context of classical regression forests, as originally introduced by\nBreiman [2001a]. In this section, we assume that we have training examples Zi = (Xi, Yi)\nfor i = 1, ..., n, a test point x, and we want to estimate true conditional mean function\nµ (x) = E\n\u0002\nY\nX = x\n\u0003\n. (10)\n9",
    "content_hash": "f195e2a4eeef7c128369f2e0231749f984a3baca5a580af697fe6ef8cea4f305",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "asymptotic normality theory for random forests. In the interest of generality, we ﬁrst de-",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5e4cdf0c-8c99-48fb-a351-7ff27299ff56",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "In practice, we compute such a\nrandom forest by Monte Carlo averaging, and set\nRF (x; Z1, ..., Zn) ≈1\nB\nB\nX\nb=1\nT (x; ξ∗\nb , Z∗\nb1, ..., Z∗\nbs) ,\n(11)\nwhere {Z∗\nb1, ..., Z∗\nbs} is drawn without replacement from {Z1, ..., Zn}, ξ∗\nb is a random draw\nfrom Ξ, and B is the number of Monte Carlo replicates we can aﬀord to perform. The\nformulation (12) arises as the B →∞limit of (11); thus, our theory eﬀectively assumes that\nB is large enough for Monte Carlo eﬀects not to matter. The eﬀects of using a ﬁnite B are\nstudied in detail by Mentch and Hooker [2016]; see also Wager et al. [2014], who recommend\ntaking B on the order of n. Deﬁnition 1. The random forest with base learner T and subsample size s is\nRF (x; Z1, ..., Zn) =\n\u0012n\ns\n\u0013−1\nX\n1≤i1<i2<...<is≤n\nEξ∼Ξ [T (x; ξ, Zi1, ..., Zis)] . (12)\nNext, as described in Section 2, we require that the trees T in our forest be honest. Double-sample trees, as deﬁned in Procedure 1, can always be used to build honest trees with\nrespect to the I-sample.",
    "content_hash": "32d17cb0cfff94c0ccb5152311f1bf4197f880a77cc04bf4670c50179f42551e",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": ") provide a simple recipe for building honest trees without sample splitting.",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b106cf8a-7af7-47c9-80d8-6a6bed137964",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "In the context of causal trees for observational studies, propensity\ntrees (Procedure 2) provide a simple recipe for building honest trees without sample splitting. 10",
    "content_hash": "185040cdb1894abdc61a696666c7b307b44ab01d8282b0d5d9a09707ce660b03",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "marginalizing over",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "282a1b5e-8774-4e8b-8c73-037f4c175dc3",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Deﬁnition 2. A tree grown on a training sample (Z1 = (X1, Y1) , ..., Zs = (Xs, Ys)) is\nhonest if (a) (standard case) the tree does not use the responses Y1, ..., Ys in choosing where\nto place its splits; or (b) (double sample case) the tree does not use the I-sample responses\nfor placing splits. In order to guarantee consistency, we also need to enforce that the leaves of the trees\nbecome small in all dimensions of the feature space as n gets large.5\nHere, we follow\nMeinshausen [2006], and achieve this eﬀect by enforcing some randomness in the way trees\nchoose the variables they split on: At each step, each variable is selected with probability\nat least π/d for some 0 < π ≤1 (for example, we could satisfy this condition by completely\nrandomizing the splitting variable with probability π). Formally, the randomness in how to\npick the splitting features is contained in the auxiliary random variable ξ. Deﬁnition 3. A tree is a random-split tree if at every step of the tree-growing procedure,\nmarginalizing over ξ, the probability that the next split occurs along the j-th feature is\nbounded below by π/d for some 0 < π ≤1, for all j = 1, ..., d. The remaining deﬁnitions are more technical. We use regularity to control the shape of\nthe tree leaves, while symmetry is used to apply classical tools in establishing asymptotic\nnormality. Deﬁnition 4.",
    "content_hash": "61fb78759806905440b598f41edaa5813f52aa749747964dd81f9cabfee33c2b",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "The remaining deﬁnitions are more technical. We use regularity to control the shape of",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3e22c7ae-2efa-4417-8e4f-81939a78353b",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "A tree predictor grown by recursive partitioning is α-regular for some α > 0\nif either (a) (standard case) each split leaves at least a fraction α of the available training\nexamples on each side of the split and, moreover, the trees are fully grown to depth k for\nsome k ∈N, i.e., there are between k and 2k −1 observations in each terminal node of the\ntree; or (b) (double sample case) if the predictor is a double-sample tree as in Procedure 1,\nthe tree satisﬁes part (a) for the I sample. Deﬁnition 5. A predictor is symmetric if the (possibly randomized) output of the predictor\ndoes not depend on the order (i = 1, 2, ...) in which the training examples are indexed. Finally, in the context of classiﬁcation and regression forests, we estimate the asymptotic\nvariance of random forests using the original inﬁnitesimal jackknife of Wager et al. [2014],\ni.e.,\nbVIJ (x) = n −1\nn\n\u0012\nn\nn −s\n\u00132\nn\nX\ni=1\nCov∗[ˆµ∗\nb (x) , N ∗\nib]2 ,\n(13)\nwhere ˆµ∗\nb(x) is the estimate for µ(x) given by a single regression tree. We note that the\nﬁnite-sample correction n(n −1)/(n −s)2 did not appear in Wager et al. [2014], as their\npaper focused on subsampling with replacement, whereas this correction is only appropriate\nfor subsampling without replacement.",
    "content_hash": "bfcda37d04767a8ab9da8531ff0d590454d0d2672fbab876e5d0762071f48e7c",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Given these preliminaries, we can state our main result on the asymptotic normality of",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "81c5e69d-eda9-47d0-9c45-443626c7bde6",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Given these preliminaries, we can state our main result on the asymptotic normality of\nrandom forests. As discussed in Section 2.3, we require that the conditional mean function\nµ (x) = E\n\u0002\nY\nX = x\n\u0003\nbe Lipschitz continuous. The asymptotic normality result requires\nfor the subsample size s to scale within the bounds given in (14). If the subsample size\ngrows slower than this, the forest will still be asymptotically normal, but the forest may be\nasymptotically biased. For clarity, we state the following result with notation that makes\n5Biau [2012] and Wager and Walther [2015] consider the estimation of low-dimensional signals embedded\nin a high-dimensional ambient space using random forests; in this case, the variable selection properties of\ntrees also become important. We leave a study of asymptotic normality of random forests in high dimensions\nto future work. 11",
    "content_hash": "31b362efa58b0c74fd2b454879f4adeb2b5fa14e1f8891a6f4d496946bf4ad21",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "are Lipschitz-continuous, and ﬁnally that",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c670de4f-f3d5-4e7d-9642-892b80df429d",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "the dependence of ˆµn(x) and sn on n explicit; in most of the paper, however, we drop the\nsubscripts to ˆµn(x) and sn when there is no risk of confusion. Theorem 1. Suppose that we have n independent and identically distributed training\nexamples Zi = (Xi, Yi) ∈[0, 1]d × R. Suppose moreover that the features are inde-\npendently and uniformly distributed 6 Xi ∼U([0, 1]d), that µ(x) = E\n\u0002\nY\nX = x\n\u0003\nand\nµ2(x) = E\n\u0002\nY 2 \f\f X = x\n\u0003\nare Lipschitz-continuous, and ﬁnally that Var\n\u0002\nY\nX = x\n\u0003\n> 0\nand E[|Y −E[Y\nX = x]|2+δ \f\f X = x] ≤M for some constants δ, M > 0, uniformly over all\nx ∈[0, 1]d. Given this data-generating process, let T be an honest, α-regular with α ≤0.2,\nand symmetric random-split tree in the sense of Deﬁnitions 2, 3, 4, and 5, and let ˆµn(x) be\nthe estimate for µ(x) given by a random forest with base learner T and a subsample size sn. Finally, suppose that the subsample size sn scales as\nsn ≍nβ\nfor some βmin := 1 −\n\n1 + d\nπ\nlog\nα−1\u0001\nlog\n\u0010\n(1 −α)−1\u0011\n\n\n−1\n< β < 1.",
    "content_hash": "acff686a908d1dfcb0262293b9b545e5eab466eb2d4693b9a38045b5ba6fbd07",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "]. Here, we treat the output RF(",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d8434372-be50-43fa-8201-9cba4026fa9d",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "(14)\nThen, random forest predictions are asymptotically Gaussian:\nˆµn(x) −µ (x)\nσn(x)\n⇒N (0, 1) for a sequence σn(x) →0. (15)\nMoreover, the asymptotic variance σn can be consistently estimated using the inﬁnitesimal\njackknife (8):\nbVIJ (x)\n\u000e\nσ2\nn(x) →p 1. (16)\nRemark 3 (binary classiﬁcation). We note that Theorem 1 also holds for binary classiﬁ-\ncation forests with leaf size k = 1, as is default in the R-package randomForest [Liaw and\nWiener, 2002]. Here, we treat the output RF(x) of the random forests as an estimate for the\nprobability P\n\u0002\nY = 1\nX = x\n\u0003\n; Theorem 1 then lets us construct valid conﬁdence intervals\nfor this probability. For classiﬁcation forests with k > 1, the proof of Theorem 1 still holds\nif the individual classiﬁcation trees are built by averaging observations within a leaf, but not\nif they are built by voting. Extending our results to voting trees is left as further work. The proof of this result is organized as follows. In Section 3.2, we provide bounds for the\nbias E [ˆµn(x) −µ (x)] of random forests, while Section 3.3 studies the sampling distributions\nof ˆµn(x) −E [ˆµn(x)] and establishes Gaussianity. Given a subsampling rate satisfying (14),\nthe bias decays faster than the variance, thus allowing for (15).",
    "content_hash": "c6f340196a10e97b5b557978ea6d320265958f6c1cec36a7d4c1475f7d92908c",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "forests. The convergence and consistency properties of trees and random forests have been",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4749379a-4c20-4018-96e8-8537aabca7b0",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Before beginning the proof,\nhowever, we relate our result to existing results about random forests in Section 3.1. 3.1\nTheoretical Background\nThere has been considerable work in understanding the theoretical properties of random\nforests. The convergence and consistency properties of trees and random forests have been\nstudied by, among others, Biau [2012], Biau et al. [2008], Breiman [2004], Breiman et al. [1984], Meinshausen [2006], Scornet et al. [2015], Wager and Walther [2015], and Zhu et al. [2015]. Meanwhile, their sampling variability has been analyzed by Duan [2011], Lin and\nJeon [2006], Mentch and Hooker [2016], Sexton and Laake [2009], and Wager et al. [2014]. 6The result also holds with a density that is bounded away from 0 and inﬁnity; however, we assume\nuniformity for simpler exposition. 12",
    "content_hash": "642b47fb09e01b1a2d1077106ce913ac0e3b0a595704e9d37cc560413b125724",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "bias of the random forest decays as 1",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4d1bd3c7-205d-4f14-a7b0-5136bbe47138",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "However, to our knowledge, our Theorem 1 is the ﬁrst result establishing conditions under\nwhich predictions made by random forests are asymptotically unbiased and normal. Probably the closest existing result is that of Mentch and Hooker [2016], who showed\nthat random forests based on subsampling are asymptotically normal under substantially\nstronger conditions than us: they require that the subsample size s grows slower than √n,\ni.e., that sn/√n →0. However, under these conditions, random forests will not in general\nbe asymptotically unbiased. As a simple example, suppose that d = 2, that µ(x) = ∥x∥1,\nand that we evaluate an honest random forest at x = 0. A quick calculation shows that the\nbias of the random forest decays as 1/√sn, while its variance decays as sn/n. If sn/√n →0,\nthe squared bias decays slower than the variance, and so conﬁdence intervals built using\nthe resulting Gaussian limit distribution will not cover µ(x). Thus, although the result of\nMentch and Hooker [2016] may appear qualitatively similar to ours, it cannot be used for\nvalid asymptotic statistical inference about µ(x). The variance estimator bVIJ was studied in the context of random forests by Wager et al. [2014], who showed empirically that the method worked well for many problems of interest. Wager et al. [2014] also emphasized that, when using bVIJ in practice, it is important to\naccount for Monte Carlo bias.",
    "content_hash": "9d16d67a57a8b9c30fa8456b653728bbae1c3b32a6a6e534b93738796b3a7406",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": ") of random forest",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7dc9f811-8211-4860-a135-1e81f8c59ad0",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Our analysis provides theoretical backing to these results,\nby showing that bVIJ is in fact a consistent estimate for the variance σ2\nn(x) of random forest\npredictions. The earlier work on this topic [Efron, 2014, Wager et al., 2014] had only mo-\ntivated the estimator bVIJ by highlighting connections to classical statistical ideas, but did\nnot establish any formal justiﬁcation for it. Instead of using subsampling, Breiman originally described random forests in terms of\nbootstrap sampling, or bagging [Breiman, 1996]. Random forests with bagging, however,\nhave proven to be remarkably resistant to classical statistical analysis. As observed by Buja\nand Stuetzle [2006], Chen and Hall [2003], Friedman and Hall [2007] and others, estimators\nof this form can exhibit surprising properties even in simple situations; meanwhile, using\nsubsampling rather than bootstrap sampling has been found to avoid several pitfalls [e.g.,\nPolitis et al., 1999]. Although they are less common in the literature, random forests based\non subsampling have also been occasionally studied and found to have good practical and\ntheoretical properties [e.g., B¨uhlmann and Yu, 2002, Mentch and Hooker, 2016, Scornet\net al., 2015, Strobl et al., 2007]. Finally, an interesting question for further theoretical study is to understand the optimal\nscaling of the subsample size sn for minimizing the mean-squared error of random forests.",
    "content_hash": "bf9b07e67b006514153803dacf14cbd145dc6debce761fd13bca2355e595d24c",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "bounds for bias developed in the following section. Now, as shown by",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c2852fe2-b365-4276-8b30-a7c0e0f8713b",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "For subsampled nearest-neighbors estimation, the optimal rate for sn is sn ≍n1−(1+d/4)−1\n[Biau et al., 2010, Samworth, 2012]. Here, our speciﬁc value for βmin depends on the upper\nbounds for bias developed in the following section. Now, as shown by Biau [2012], under\nsome sparsity assumptions on µ(x), it is possible to get substantially stronger bounds for\nthe bias of random forests; thus, it is plausible that under similar conditions we could push\nback the lower bound βmin on the growth rate of the subsample size. 3.2\nBias and Honesty\nWe start by bounding the bias of regression trees. Our approach relies on showing that as the\nsample size s available to the tree gets large, its leaves get small; Lipschitz-continuity of the\nconditional mean function and honesty then let us bound the bias. In order to state a formal\nresult, deﬁne the diameter diam(L(x)) of a leaf L(x) as the length of the longest segment\ncontained inside L(x), and similarly let diamj(L(x)) denote the length of the longest such\nsegment that is parallel to the j-th axis. The following lemma is a reﬁnement of a result of\n13",
    "content_hash": "ad11cbd5bde98321ea6bd4abf292ccd98016d35903d48029460eca3d4331af47",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "same as the bias of a single tree.",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8db718c9-451f-4a07-a8c0-ce0d10f2559f",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "3.3\nAsymptotic Normality of Random Forests\nOur analysis of the asymptotic normality of random forests builds on ideas developed by\nHoeﬀding [1948] and H´ajek [1968] for understanding classical statistical estimators such as\nU-statistics. We begin by brieﬂy reviewing their results to give some context to our proof. Given a predictor T and independent training examples Z1, ..., Zn, the H´ajek projection of\nT is deﬁned as\n˚\nT = E [T] +\nn\nX\ni=1\nE\n\u0002\nT\nZi\n\u0003\n−E [T]\n\u0001\n. (17)\nIn other words, the H´ajek projection of T captures the ﬁrst-order eﬀects in T. Classical\nresults imply that Var\nh\n˚\nT\ni\n≤Var [T], and further:\nlim\nn→∞Var\nh\n˚\nT\ni\u000e\nVar [T] = 1 implies that\nlim\nn→∞E\n\u0014\r\r\r˚\nT −T\n2\n2\n\u0015\u000e\nVar [T] = 0. (18)\nSince the H´ajek projection ˚\nT is a sum of independent random variables, we should expect it\nto be asymptotically normal under weak conditions. Thus whenever the ratio of the variance\nof ˚\nT to that of T tends to 1, the theory of H´ajek projections almost automatically guarantees\nthat T will be asymptotically normal.7\nIf T is a regression tree, however, the condition from (18) does not apply, and we cannot\nuse the classical theory of H´ajek projections directly. Our analysis is centered around a\nweaker form of this condition, which we call ν-incrementality.",
    "content_hash": "63a62f6d7814b8f26858fac1d65b9e4baedce5c645c316c6de313b6608662724",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "). In our notation,",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "18f64fa2-ebcb-48e5-a691-28dd052d4061",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "With our deﬁnition, predictors\nT to which we can apply the argument (18) directly are 1-incremental. 7The moments deﬁned in (17) depend on the data-generating process for the Zi, and so cannot be\nobserved in practice. Thus, the H´ajek projection is mostly useful as an abstract theoretical tool. For a\nreview of classical projection arguments, see Chapter 11 of Van der Vaart [2000]. 14",
    "content_hash": "fe73028a5c075d4d0a1d20518f5e18d904648f099227ed2ce1fd98f2c0d8aad3",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "Consider a set of points",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5ce3ea3e-c946-4761-8353-0b869eba37db",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "We thus\nfollow the lead of Lin and Jeon [2006], and analyze a more general class of predictors—\npotential nearest neighbors predictors—that operate by doing a nearest-neighbor search over\nrectangles; see also Biau and Devroye [2010]. The study of potential (or layered) nearest\nneighbors goes back at least to Barndorﬀ-Nielsen and Sobel [1966]. Deﬁnition 7. Consider a set of points X1, ..., Xs ∈Rd and a ﬁxed x ∈Rd. A point Xi\nis a potential nearest neighbor (PNN) of x if the smallest axis-aligned hyperrectangle with\nvertices x and Xi contains no other points Xj. Extending this notion, a PNN k-set of x is a\nset of points Λ ⊆{X1, ..., Xs} of size k ≤|L| < 2k −1 such that there exists an axis aligned\nhyperrectangle L containing x, Λ, and no other training points. A training example Xi is\ncalled a k-PNN of x if there exists a PNN k-set of x containing Xi. Finally, a predictor T\nis a k-PNN predictor over {Z} if, given a training set\n{Z} = {(X1, Y1) , ..., (Xs, Ys)} ∈\n\b\nRd × Y\ns\nand a test point x ∈Rd, T always outputs the average of the responses Yi over a k-PNN set\nof x. This formalism allows us to describe a wide variety of tree predictors.",
    "content_hash": "989c719fc961bae28802b624b5af150a933cb69c0d0049dede90b891ccf06d3b",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "the quantity",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cf97c0f7-3c62-406d-9096-b9b08f0a05b5",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "For example, as\nshown by Lin and Jeon [2006], any decision tree T that makes axis-aligned splits and has\nleaves of size between k and 2k −1 is a k-PNN predictor. In particular, the base learners\noriginally used by Breiman [2001a], namely CART trees grown up to a leaf size k [Breiman\net al., 1984], are k-PNN predictors. Predictions made by k-PNN predictors can always be\nwritten as\nT (x; ξ, Z1, ..., Zs) =\ns\nX\ni=1\nSiYi,\n(19)\nwhere Si is a selection variable that takes the value 1/|{i : Xi ∈L(x)}| for indices i in the\nselected leaf-set L(x) and 0 for all other indices. If the tree is honest, we know in addition\nthat, for each i, Si is independent of Yi conditional on Xi. 15",
    "content_hash": "e343a559609f93e7d621590c2858827c75b4aa5e915a0eca656de8cfcef763a8",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "hold and that",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2aab566b-9eed-4098-8537-ee8ea7f36c54",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Deﬁnition 6. The predictor T is ν(s)-incremental at x if\nVar\nh\n˚\nT (x; Z1, ..., Zs)\ni\u000e\nVar [T (x; Z1, ..., Zs)] ≳ν(s),\nwhere ˚\nT is the H´ajek projection of T (17). In our notation,\nf(s) ≳g(s) means that lim inf\ns→∞f(s)\n\u000e\ng(s) ≥1. Our argument proceeds in two steps. First we establish lower bounds for the incremen-\ntality of regression trees in Section 3.3.1. Then, in Section 3.3.2 we show how we can turn\nweakly incremental predictors T into 1-incremental ensembles by subsampling (Lemma 7),\nthus bringing us back into the realm of classical theory. We also establish the consistency of\nthe inﬁnitesimal jackknife for random forests. Our analysis of regression trees is motivated\nby the “potential nearest neighbors” model for random forests introduced by Lin and Jeon\n[2006]; the key technical device used in Section 3.3.2 is the ANOVA decomposition of Efron\nand Stein [1981]. The discussion of the inﬁnitesimal jackknife for random forest builds on\nresults of Efron [2014] and Wager et al. [2014]. 3.3.1\nRegression Trees and Incremental Predictors\nAnalyzing speciﬁc greedy tree models such as CART trees can be challenging.",
    "content_hash": "79e2d8378e4aac31f17086df697e3547bffe4ba87ef1e1de0556bcacb21589c7",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": ", Λ, and no other training points. A training example",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bd79fa95-a3d7-4eb7-8a8d-f0fbad645f51",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Thanks to this result, we are now ready to show that all honest and regular random-split\ntrees are incremental. Notice that any symmetric k-regular tree following Deﬁnition 4 is also\na symmetric k-PNN predictor. Theorem 5. Suppose that the conditions of Lemma 4 hold and that T is an honest k-regular\nsymmetric tree in the sense of Deﬁnitions 2 (part a), 4 (part a), and 5. Suppose moreover\nthat the conditional moments µ (x) and µ2(x) are both Lipschitz continuous at x. Finally,\nsuppose that Var\n\u0002\nY\nX = x\n\u0003\n> 0. Then T is ν (s)-incremental at x with\nν (s) = Cf, d\n\u000e\nlog (s)d,\n(21)\nwhere Cf, d is the constant from Lemma 4. Finally, the result of Theorem 5 also holds for double-sample trees of the form described\nin Procedure 1. To establish the following result, we note that a double-sample tree is an\nhonest, symmetric k-PNN predictor with respect to the I-sample, while all the data in the\nJ -sample can be folded into the auxiliary noise term ξ; the details are worked out in the\nproof. Corollary 6. Under the conditions of Theorem 5, suppose that T is instead a double-\nsample tree (Procedure 1) satisfying Deﬁnitions 2 (part b), 4 (part b), and 5. Then, T is\nν-incremental, with ν (s) = Cf, d/(4 log (s)d).",
    "content_hash": "905d89f95901d39c19908107733619f56d0511e525fe7bf2ea0b31b6a9d6a55f",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "for the central limit theorem.",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2823c90c-d21d-44c8-93d4-5b799329b147",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "An important property of k-PNN predictors is that we can often get a good idea about\nwhether Si is non-zero even if we only get to see Zi; more formally, as we show below,\nthe quantity s Var\n\u0002\nE\n\u0002\nS1\nZ1\n\u0003\u0003\ncannot get too small. Establishing this fact is a key step\nin showing that k-PNNs are incremental. In the following result, T can be an arbitrary\nsymmetric k-PNN predictor. Lemma 4. Suppose that the observations X1, X2, . . . are independent and identically dis-\ntributed on [0, 1]d with a density f that is bounded away from inﬁnity, and let T be any\nsymmetric k-PNN predictor. Then, there is a constant Cf, d depending only on f and d such\nthat, as s gets large,\ns Var\n\u0002\nE\n\u0002\nS1\nZ1\n\u0003\u0003\n≳1\nk Cf, d\n\u000e\nlog (s)d ,\n(20)\nwhere Si is the indicator for whether the observation is selected in the subsample. When f\nis uniform over [0, 1]d, the bound holds with Cf, d = 2−(d+1) (d −1)!. When k = 1 we see that, marginally, S1 ∼Bernoulli(1/s) and so s Var [S1] ∼1; more\ngenerally, a similar calculation shows that 1/(2k −1) ≲s Var [S1] ≲1/k. Thus, (20) can\nbe interpreted as a lower bound on how much information Z1 contains about the selection\nevent S1.",
    "content_hash": "f9206ba64e8d321f5c41f2e4bc1a817438b23131a664d7770d23991e7bbeda57",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "Under the conditions of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cd2e585d-0747-47cd-a687-c3f10f97bb6f",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "3.3.2\nSubsampling Incremental Base Learners\nIn the previous section, we showed that decision trees are ν-incremental, in that the H´ajek\nprojection ˚\nT of T preserves at least some of the variation of T. In this section, we show that\nrandomly subsampling ν-incremental predictors makes them 1-incremental; this then lets us\nproceed with a classical statistical analysis. The following lemma, which ﬂows directly from\nthe ANOVA decomposition of Efron and Stein [1981], provides a ﬁrst motivating result for\nour analysis. 16",
    "content_hash": "c6d26efbde418bd190fc539ed3f810996ef89aca60a4f74e47c7c82dd2e21b48",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "]. We show below that, for trivial trees",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9e383873-6e61-4865-afe1-45da341a0c09",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Lemma 7. Let ˆµ(x) be the estimate for µ(x) generated by a random forest with base learner\nT as deﬁned in (12), and let ˚ˆµ be the H´ajek projection of ˆµ (17). Then\nE\n\u0014\u0010\nˆµ (x) −˚ˆµ (x)\n\u00112\u0015\n≤\n\u0010 s\nn\n\u00112\nVar [T (x; ξ, Z1, ..., Zs)]\nwhenever the variance Var [T] of the base learner is ﬁnite. This technical result paired with Theorem 5 or Corollary 6 leads to an asymptotic Gaus-\nsianity result; from a technical point of view, it suﬃces to check Lyapunov-style conditions\nfor the central limit theorem. Theorem 8. Let ˆµ(x) be a random forest estimator trained according the conditions of\nTheorem 5 or Corollary 6. Suppose, moreover, that the subsample size sn satisﬁes\nlim\nn→∞sn = ∞and\nlim\nn→∞sn log (n)d\u000e\nn = 0,\nand that E[|Y −E[Y\nX = x]|2+δ \f\f X = x] ≤M for some constants δ, M > 0, uniformly over\nall x ∈[0, 1]d. Then, there exists a sequence σn(x) →0 such that\nˆµn (x) −E [ˆµn (x)]\nσn(x)\n⇒N (0, 1) ,\n(22)\nwhere N (0, 1) is the standard normal distribution.",
    "content_hash": "1fa78b1a9d7879806bffdbd03811b7527c44031efc620ab645607316a6dc1353",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "We now return to our main topic, namely estimating heterogeneous treatment eﬀects using",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "216cd73e-0b82-429d-b7cc-6ccbd8580c29",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Of\ncourse, n(n−1)/(n−s)2 →1, and so Theorem 9 would hold even without this ﬁnite-sample\ncorrection; however, we ﬁnd it to substantially improve the performance of our method in\npractice. Proposition 10. For trivial trees T(x; ξ, Zi1, ..., Zis) = s−1 Ps\nj=1 Yij, the variance esti-\nmate bVIJ (13) is equivalent to the standard variance estimator bVsimple, and E[bVIJ] = Var [ˆµ]. 17",
    "content_hash": "58e94b6858c0fa8468e380b3ffdba95d522e6078f7071a4778c8ae1681657257",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "tions from each treatment group (",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1f640174-0a40-4776-b7be-a801fedf1bbd",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Moreover, as we show below, it is possible to accurately estimate the variance of a random\nforest using the inﬁnitesimal jackknife for random forests [Efron, 2014, Wager et al., 2014]. Theorem 9. Let bVIJ (x; , Z1, ..., Zn) be the inﬁnitesimal jackknife for random forests as\ndeﬁned in (8). Then, under the conditions of Theorem 8,\nbVIJ (x; Z1, ..., Zn)\n\u000e\nσ2\nn(x) →p 1. (23)\nFinally, we end this section by motivating the ﬁnite sample correction n(n −1)/(n −s)2\nappearing in (13) by considering the simple case where we have trivial trees that do not\nmake any splits: T(x; ξ, Zi1, ..., Zis) = s−1 Ps\nj=1 Yij. In this case, we can verify that the\nfull random forest is nothing but ˆµ = n−1 Pn\ni=1 Yi, and the standard variance estimator\nbVsimple =\n1\nn (n −1)\nn\nX\ni=1\nYi −Y\n\u00012 ,\nY = 1\nn\nn\nX\ni=1\nYi\nis well-known to be unbiased for Var [ˆµ]. We show below that, for trivial trees bVIJ = bVsimple,\nimplying that our correction makes bVIJ exactly unbiased in ﬁnite samples for trivial trees.",
    "content_hash": "edc4bce2c4b125320f50d568c1791fe17e5e17ec1af3ebfb3e66febc6a80886a",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "the notions of honesty and regularity need to be adapted slightly. Speciﬁcally, an honest",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e56f48da-d231-4468-a6c1-eb0213bbdd01",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "The\nmain diﬀerence relative to our ﬁrst result about regression forests is that we now rely on\nunconfoundedness and overlap to achieve consistent estimation of τ(x). To see how these\nassumptions enter the proof, recall that an honest causal tree uses the features Xi and the\ntreatment assignments Wi in choosing where to place its splits, but not the responses Yi. Writing I(1)(x) and I(0)(x) for the indices of the treatment and control units in the leaf\naround x, we then ﬁnd that after the splitting stage\nE\n\u0002\nΓ (x)\nX, W\n\u0003\n(25)\n=\nP\n{i∈I(1)(x)} E\n\u0002\nY (1) \f\f X = Xi, W = 1\n\u0003\nI(1)(x)\n−\nP\n{i∈I(0)(x)} E\n\u0002\nY (0) \f\f X = Xi, W = 0\n\u0003\nI(0)(x)\n=\nP\n{i∈I(1)(x)} E\n\u0002\nY (1) \f\f X = Xi\n\u0003\nI(1)(x)\n−\nP\n{i∈I(0)(x)} E\n\u0002\nY (0) \f\f X = Xi\n\u0003\nI(0)(x)\n,\n18",
    "content_hash": "32c175c2a230d28354064bc880b844002ca62161cb85aceb2f96c25d441caacf",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": ". In practice, if we want to build a causal tree that can",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e4a0ab8c-de4e-46aa-9842-dc5c06ed427f",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Meanwhile, a regular causal tree must have at least k examples\nfrom both treatment classes in each leaf; in other words, regular causal trees seek to act as\nfully grown trees for the rare treatment assignment, while allowing for more instances of the\ncommon treatment assignment. Deﬁnition 2b. A causal tree grown on a training sample (Z1 = (X1, Y1, W1), ...,\nZs = (Xs, Ys, Ws)) is honest if (a) (standard case) the tree does not use the responses\nY1, ..., Ys in choosing where to place its splits; or (b) (double sample case) the tree does not\nuse the I-sample responses for placing splits. Deﬁnition 4b. A causal tree grown by recursive partitioning is α-regular at x for some\nα > 0 if either: (a) (standard case) (1) Each split leaves at least a fraction α of the available\ntraining examples on each side of the split, (2) The leaf containing x has at least k observa-\ntions from each treatment group (Wi ∈{0, 1}) for some k ∈N, and (3) The leaf containing\nx has either less than 2k −1 observations with Wi = 0 or 2k −1 observations with Wi = 1;\nor (b) (double-sample case) for a double-sample tree as deﬁned in Procedure 1, (a) holds for\nthe I sample. Given these assumptions, we show a close analogue to Theorem 1, given below.",
    "content_hash": "47e7755874cd13786d13001e6e472eb7903fd2504b66f1ff1d4be129c73c96c1",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "assignment is unconfounded",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9c1c1064-4a63-49cc-87f3-58ccef63247e",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "4\nInferring Heterogeneous Treatment Eﬀects\nWe now return to our main topic, namely estimating heterogeneous treatment eﬀects using\nrandom forests in the potential outcomes framework with unconfoundedness, and adapt our\nasymptotic theory for regression forests to the setting of causal inference. Here, we again\nwork with training data consisting of tuples Zi = (Xi, Yi, Wi) for i = 1, ..., n, where Xi\nis a feature vector, Yi is the response, and Wi is the treatment assignment. Our goal is\nto estimate the conditional average treatment eﬀect τ(x) = E\n\u0002\nY (1) −Y (0) \f\f X = x\n\u0003\nat a\npre-speciﬁed test point x. By analogy to Deﬁnition 1, we build our causal forest CF by\naveraging estimates for τ obtained by training causal trees Γ over subsamples:\nCF (x; Z1, ..., Zn) =\n\u0012n\ns\n\u0013−1\nX\n1≤i1<i2<...<is≤n\nEξ∼Ξ [Γ (x; ξ, Zi1, ..., Zis)] . (24)\nWe seek an analogue to Theorem 1 for such causal forests. Most of the deﬁnitions used to state Theorem 1 apply directly to this context; however,\nthe notions of honesty and regularity need to be adapted slightly. Speciﬁcally, an honest\ncausal tree is not allowed to look at the responses Yi when making splits but can look at the\ntreatment assignments Wi.",
    "content_hash": "3df2c72840bb45b6fb86ecab82fdd987598354e584691ab58b04ffb71a4de704",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": ", given below. The",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1d151729-9076-4c64-b308-3813345e296d",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Then, the predictions ˆτ(x)\nare consistent and asymptotically both Gaussian and centered, and the variance of the causal\nforest can be consistently estimated using the inﬁnitesimal jackknife for random forests, i.e.,\n(7) holds. Remark 4. (testing at many points) We note that it is not in general possible to construct\ncausal trees that are regular in the sense of Deﬁnition 4b for all x simultaneously. As a\nsimple example, consider the situation where d = 1, and Wi = 1 ({Xi ≥0}); then, the tree\ncan have at most 1 leaf for which it is regular. In the proof of Theorem 11, we avoided this\nissue by only considering a single test point x, as it is always possible to build a tree that\nis regular at a single given point x. In practice, if we want to build a causal tree that can\nbe used to predict at many test points, we may need to assign diﬀerent trees to be valid for\ndiﬀerent test points. Then, when predicting at a speciﬁc x, we treat the set of trees that\nwere assigned to be valid at that x as the relevant forest and apply Theorem 11 to it. 5\nSimulation Experiments\nIn observational studies, accurate estimation of heterogeneous treatment eﬀects requires\novercoming two potential sources of bias. First, we need to identify neighborhoods over\nwhich the actual treatment eﬀect τ(x) is reasonably stable and, second, we need to make\nsure that we are not biased by varying sampling propensities e(x). The simulations here\naim to test the ability of causal forests to respond to both of these factors.",
    "content_hash": "44d49f908bbba831079c1111e6842a8baec6d7a1de1915de6ca69ce7a4511ece",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "Data Analysis Challenge at the 2016 Atlantic Causal Inference Conference. We hope that",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "38d89b94-057e-40b6-93c1-518c7869c580",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Since causal forests are adaptive nearest neighbor estimators, it is natural to use a non-\nadaptive nearest neighborhood method as our baseline. We compare our method to the\nstandard k nearest neighbors (k-NN) matching procedure, which estimates the treatment\neﬀect as\nˆτKNN(x) = 1\nk\nX\ni∈S1(x)\nYi −1\nk\nX\ni∈S0(x)\nYi,\n(26)\nwhere S1 and S0 are the k nearest neighbors to x in the treatment (W = 1) and control\n(W = 0) samples respectively. We generate conﬁdence intervals for the k-NN method by\nmodeling ˆτKNN(x) as Gaussian with mean τ(x) and variance (bV (S0) + bV (S1))/(k (k −1)),\nwhere bV\nS0/1\n\u0001\nis the sample variance for S0/1. The goal of this simulation study is to verify that forest-based methods can be used build\nrigorous, asymptotically valid conﬁdence intervals that improve over non-adaptive methods\n19",
    "content_hash": "48f63554d41c8fae0b679afbb93f58f01fa5efc94d5e769d911a492b85a8ea5d",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "5 ﬁxed. Thanks to unconfoundedness,",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d86dc3fe-7c04-4f2b-88a9-837c5ed7e562",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "where the second equality follows by unconfoundedness (2). Thus, it suﬃces to show that the\ntwo above terms are consistent for estimating E\n\u0002\nY (0) \f\f X = x\n\u0003\nand E\n\u0002\nY (1) \f\f X = x\n\u0003\n. To do\nso, we can essentially emulate the argument leading to Theorem 1, provided we can establish\nan analogue to Lemma 2 and give a fast enough decaying upper bound to the diameter of\nL(x); this is where we need the overlap assumption. A proof of Theorem 11 is given in the\nappendix. Theorem 11. Suppose that we have n independent and identically distributed training ex-\namples Zi = (Xi, Yi, Wi) ∈[0, 1]d × R × {0, 1}. Suppose, moreover, that the treatment\nassignment is unconfounded (2) and has overlap (6). Finally, suppose that both potential\noutcome distributions (Xi, Y (0)\ni\n) and (Xi, Y (1)\ni\n) satisfy the same regularity assumptions as\nthe pair (Xi, Yi) did in the statement of Theorem 1. Given this data-generating process, let Γ\nbe an honest, α-regular with α ≤0.2, and symmetric random-split causal forest in the sense\nof Deﬁnitions 2b, 3, 4b, and 5, and let ˆτ(x) be the estimate for τ(x) given by a causal forest\nwith base learner Γ and a subsample size sn scaling as in (14).",
    "content_hash": "042f24e0d73abde0badc929e8c52c7eb7a1c223da0f0f8e4d270633349a4a09f",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "Since causal forests are adaptive nearest neighbor estimators, it is natural to use a non-",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7fe8a854-136e-4b5b-b41e-c4d28851f600",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "like k-NN in ﬁnite samples. The fact that forest-based methods hold promise for treatment\neﬀect estimation in terms of predictive error has already been conclusively established else-\nwhere; for example, BART methods following Hill [2011] won the recent Causal Inference\nData Analysis Challenge at the 2016 Atlantic Causal Inference Conference. We hope that\nthe conceptual tools developed in this paper will prove to be helpful in analyzing a wide\nvariety of forest-based methods. 5.1\nExperimental Setup\nWe describe our experiments in terms of the sample size n, the ambient dimension d, as well\nas the following functions:\nmain eﬀect: m(x) = 2−1 E\nh\nY (0) + Y (1) \f\f X = x\ni\n,\ntreatment eﬀect: τ(x) = E\nh\nY (1) −Y (0) \f\f X = x\ni\n,\ntreatment propensity: e(x) = P\n\u0002\nW = 1\nX = x\n\u0003\n. In all our examples, we respect unconfoundedness (2), use X ∼U([0, 1]d), and have ho-\nmoscedastic noise Y (0/1) ∼N\nE[Y (0/1) \f\f X], 1\n\u0001\n. We evaluate performance in terms of ex-\npected mean-squared error for estimating τ(X) at a random test example X, as well as\nexpected coverage of τ(X) with a target coverage rate of 0.95. In our ﬁrst experiment, we held the treatment eﬀect ﬁxed at τ(x) = 0, and tested the\nability of our method to resist bias due to an interaction between e(x) and m(x).",
    "content_hash": "e7824cedfdcc6f48e2ae038b64e08370d5f19ef1ddc2d795ff90bf235e1e61c4",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "0.95 (0)",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "20485bab-8f20-4267-9999-9459354ffbfd",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "This\nexperiment is intended to emulate the problem that in observational studies, a treatment\nassignment is often correlated with potential outcomes, creating bias unless the statistical\nmethod accurately adjusts for covariates. k-NN matching is a popular approach for per-\nforming this adjustment in practice. Here, we set\ne(X) = 1\n4 (1 + β2, 4(X1)) ,\nm(X) = 2X1 −1,\n(27)\nwhere βa, b is the β-density with shape parameters a and b. We used n = 500 samples and\nvaried d between 2 and 30. Since our goal is accurate propensity matching, we use propensity\ntrees (Procedure 2) as our base learner; we grew B = 1000 trees with s = 50. For our second experiment, we evaluated the ability of causal forests to adapt to hetero-\ngeneity in τ(x), while holding m(x) = 0 and e(x) = 0.5 ﬁxed. Thanks to unconfoundedness,\nthe fact that e(x) is constant means that we are in a randomized experiment. We set τ to\nbe a smooth function supported on the ﬁrst two features:\nτ (X) = ς (X1) ς (X2) ,\nς (x) = 1 +\n1\n1 + e−20(x−1/3) . (28)\nWe took n = 5000 samples, while varying the ambient dimension d from 2 to 8. For causal\nforests, we used double-sample trees with the splitting rule of Athey and Imbens [2016] as\nour base learner (Procedure 1).",
    "content_hash": "9bef2e6410a3fd5cb2f0fd730d3779598b919af086e0687425e5389457868652",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": ". Causal forests succeed in maintaining a mean-squared error of 0.02 as",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "37c0d4cc-5cb1-4b8e-b95a-f4836af0010f",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "mean-squared error\ncoverage\nd\nCF\n10-NN\n100-NN\nCF\n10-NN\n100-NN\n2\n0.02 (0)\n0.21 (0)\n0.09 (0)\n0.95 (0)\n0.93 (0)\n0.62 (1)\n5\n0.02 (0)\n0.24 (0)\n0.12 (0)\n0.94 (1)\n0.92 (0)\n0.52 (1)\n10\n0.02 (0)\n0.28 (0)\n0.12 (0)\n0.94 (1)\n0.91 (0)\n0.51 (1)\n15\n0.02 (0)\n0.31 (0)\n0.13 (0)\n0.91 (1)\n0.90 (0)\n0.48 (1)\n20\n0.02 (0)\n0.32 (0)\n0.13 (0)\n0.88 (1)\n0.89 (0)\n0.49 (1)\n30\n0.02 (0)\n0.33 (0)\n0.13 (0)\n0.85 (1)\n0.89 (0)\n0.48 (1)\nTable 1: Comparison of the performance of a causal forests (CF) with that of the k-nearest\nneighbors (k-NN) estimator with k = 10, 100, on the setup (27). The numbers in parenthe-\nses indicate the (rounded) standard sampling error for the last printed digit, obtained by\naggregating performance over 500 simulation replications.",
    "content_hash": "1753e1d04d9557f5c862efebd57d7b6f8ac0cc0875f742657a8938a0a08d785e",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "forests, available on",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cdc948b9-4bbc-469a-9ebd-03d8e781838d",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "one studied above, except now τ(x) has a sharper spike in the x1, x2 ≈1 region:\nτ (X) = ς (X1) ς (X2) ,\nς (x) =\n2\n1 + e−12(x−1/2) . (29)\nWe used the same training method as with (28), except with n = 10000, s = 2000, and\nB = 10000. We implemented our simulations in R, using the packages causalTree [Athey and Imbens,\n2016] for building individual trees, randomForestCI [Wager et al., 2014] for computing bVIJ,\nand FNN [Beygelzimer et al., 2013] for k-NN regression. All our trees had a minimum leaf\nsize of k = 1. Software replicating the above simulations is available from the authors.8\n5.2\nResults\nIn our ﬁrst setup (27), causal forests present a striking improvement over k-NN matching;\nsee Table 1. Causal forests succeed in maintaining a mean-squared error of 0.02 as d grows\nfrom 2 to 30, while 10-NN and 100-NN do an order of magnitude worse. We note that\nthe noise of k-NN due to variance in Y after conditioning on X and W is already 2/k,\nimplying that k-NN with k ≤100 cannot hope to match the performance of causal forests. Here, however, 100-NN is overwhelmed by bias, even with d = 2.",
    "content_hash": "306e29430a4d87827ea3f70e0c72775ec138267309c0ef63e70cbd6ee82df19d",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "sample quantiles",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "77342224-c4a4-49c8-aa42-e319417776ef",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Meanwhile, in terms of\nuncertainty quantiﬁcation, our method achieves nominal coverage up to d = 10, after which\nthe performance of the conﬁdence intervals starts to decay. The 10-NN method also achieves\ndecent coverage; however, its conﬁdence intervals are much wider than ours as evidenced by\nthe mean-squared error. Figure 1 oﬀers some graphical diagnostics for causal forests in the setting of (27). In\nthe left panel, we observe how the causal forest sampling variance σ2\nn(x) goes to zero with\nn; while the center panel depicts the decay of the relative root-mean squared error of the\ninﬁnitesimal jackknife estimate of variance, i.e., E[(ˆσ2\nn(x)−σ2\nn(x))2]1/2/σ2\nn(x). The boxplots\ndisplay aggregate results for 1,000 randomly sampled test points x. Finally, the right-most\npanel evaluates the Gaussianity of the forest predictions. Here, we ﬁrst drew 1,000 random\ntest points x, and computed ˆτ(x) using forests grown on many diﬀerent training sets. The\nplot shows standardized Gaussian QQ-plots aggregated over all these x; i.e., for each x, we\nplot Gaussian theoretical quantiles against sample quantiles of (ˆτ(x)−E(ˆτ(x)))/\np\nVar[ˆτ(x)]. 8The R package grf [Athey et al., 2016] provides a newer, high-performance implementation of causal\nforests, available on CRAN. 21",
    "content_hash": "9028ec43829a8c560d260585cd0858a6c5bfe5786976db91df7bacda8ddf6dbb",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "). The numbers in parenthe-",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bc983b0a-e8a7-46e4-bc4b-007590e17fb6",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "The numbers in parenthe-\nses indicate the (rounded) standard sampling error for the last printed digit, obtained by\naggregating performance over 25 simulation replications. In our second setup (28), causal forests present a similar improvement over k-NN match-\ning when d > 2, as seen in Table 2.9 Unexpectedly, we ﬁnd that the performance of causal\nforests improves with d, at least when d is small. To understand this phenomenon, we note\nthat the variance of a forest depends on the product of the variance of individual trees times\nthe correlation between diﬀerent trees [Breiman, 2001a, Hastie et al., 2009]. Apparently,\nwhen d is larger, the individual trees have more ﬂexibility in how to place their splits, thus\nreducing their correlation and decreasing the variance of the full ensemble. Finally, in the setting (29), Table 3 shows that causal forests still achieve an order of\nmagnitude improvement over k-NN in terms of mean-squared error when d > 2, but struggle\nmore in terms of coverage. This appears to largely be a bias eﬀect: especially as d gets larger,\nthe random forest is dominated by bias instead of variance and so the conﬁdence intervals\n9When d = 2, we do not expect causal forests to have a particular advantage over k-NN since the\ntrue τ also has 2-dimensional support; our results mirror this, as causal forests appear to have comparable\nperformance to 50-NN. 22",
    "content_hash": "5cd5d87a42af47d7311506d1597bb18d2cdacf92a2c2bf8341e3ca659fd958e1",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "sample quantiles",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8bdd3ed3-6af8-45ed-804d-2e52d6ebac8e",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "The right-most panel shows\nstandardized Gaussian QQ-plots for predictions at the same 1000 test points, with n = 800\nand d = 20. The ﬁrst two panels are computed over 50 randomly drawn training sets, and\nthe last one over 20 training sets. mean-squared error\ncoverage\nd\nCF\n7-NN\n50-NN\nCF\n7-NN\n50-NN\n2\n0.04 (0)\n0.29 (0)\n0.04 (0)\n0.97 (0)\n0.93 (0)\n0.94 (0)\n3\n0.03 (0)\n0.29 (0)\n0.05 (0)\n0.96 (0)\n0.93 (0)\n0.92 (0)\n4\n0.03 (0)\n0.30 (0)\n0.08 (0)\n0.94 (0)\n0.93 (0)\n0.86 (1)\n5\n0.03 (0)\n0.31 (0)\n0.11 (0)\n0.93 (1)\n0.92 (0)\n0.77 (1)\n6\n0.02 (0)\n0.34 (0)\n0.15 (0)\n0.93 (1)\n0.91 (0)\n0.68 (1)\n8\n0.03 (0)\n0.38 (0)\n0.21 (0)\n0.90 (1)\n0.90 (0)\n0.57 (1)\nTable 2: Comparison of the performance of a causal forests (CF) with that of the k-nearest\nneighbors (k-NN) estimator with k = 7, 50, on the setup (28).",
    "content_hash": "22023418563ecd5d4f5b99c591f6d140eacaee3ca0df250f1f162e05b1d99009",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "= 6",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bda44581-bdd8-4ce5-a0cf-c85f4b281842",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "G\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\n100\n200\n400\n800\n1600\n0.00\n0.02\n0.04\n0.06\nn\nforest sampling variance\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGGGGGGG\nG\nG\nGG\nG\nG\nGG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nGG\nG\nG\nGGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nGG\nGGG\nG\nG\nG\nG\nGG\nG\nG\nGGG\nGG\nG\nGGGG\nG\nGG\nG\nG\nG\nG\nG\nG\n100\n200\n400\n800\n1600\n0.0\n0.5\n1.0\n1.5\nn\ninf.",
    "content_hash": "2efc281d04c0e5ab65d151aac16b63dabde6fffc06e09bcb12a03fefa3d9c322",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "also has 2-dimensional support; our results mirror this, as causal forests appear to have comparable",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5c6879ad-8669-43b0-aeb7-9c4ba3c30502",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "jack. relative rmse\n−2\n−1\n0\n1\n2\n−2\n−1\n0\n1\n2\ntheoretical quantiles\nsample quantiles\nGG\nGG\nGG\nG\nG\nGG\nG\nG\nG\nG\nGGG\nGG\nGG\nGGGGG\nGGGGGGG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGGG\nG\nGG\nGG\nG\nGGG\nGG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nGG\nGG\nG\nG\nGG\nG\nGGGGGG\nG\nG\nGG\nG\nGG\nG\nG\nGGG\nG\nGGG\nGGGGGGG\nG\nG\nGGG\nG\nG\nGGGGGG\nGGG\nGG\nG\nG\nGGGGG\nG\nGGG\nGGG\nG\nGGGG\nGGG\nG\nGG\nG\nGGGGGGGGGGGGG\nG\nGGGGGG\nGGGG\nG\nG\nG\nGGG\nG\nGG G\nG\nG\nG\nGGG\nG G\nG\nG\nG\nG\nGG\nGG\nG\nG\nGG\nGGGGGG\nGG\nG\nGG\nG\nG\nGG\nG\nG\nG\nGGG\nG\nG\nG\nGG\nG\nGGGGGG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nGGGGGGGG\nG\n−2\n−1\n0\n1\n2\n−2\n−1\n0\n1\n2\nCausal forest variance\nInf. jack. relative RMSE\nPrediction QQ-plot\nFigure 1: Graphical diagnostics for causal forests in the setting of (27). The ﬁrst two\npanels evaluate the sampling error of causal forests and our inﬁnitesimal jackknife estimate\nof variance over 1,000 randomly draw test points, with d = 20.",
    "content_hash": "4b34e6207bb4ee78d88ae880408995f580bfa9314e383ed917c255a5dc28268c",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "= 6",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c6f373bb-9bbf-4db9-972d-8a7b51224489",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "d = 6\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nd = 20\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue eﬀect τ(x)\nCausal forest\nk∗-NN\nFigure 2: The true treatment eﬀect τ(Xi) at 10,000 random test examples Xi, along with\nestimates ˆτ(Xi) produced by a causal forest and optimally-tuned k-NN, on data drawn\naccording to (29) with d = 6, 20.",
    "content_hash": "ba6b1605c84e8b2294212d485e17ed6681995c32ca977a54393a66b0204a2cec",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "= 6",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f76e0928-4790-4481-b65b-7d06896094ca",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "The test points are plotted according to their ﬁrst two\ncoordinates; the treatment eﬀect is denoted by color, from dark (low) to light (high). On\nthis simulation instance, causal forests and k∗-NN had a mean-squared error of 0.03 and\n0.13 respectively for d = 6, and of 0.05 and 0.62 respectively for d = 20. The optimal tuning\nchoices for k-NN were k∗= 39 for d = 6, and k∗= 24 for d = 20. are not centered. Figure 2 illustrates this phenomenon: although the causal forest faithfully\ncaptures the qualitative aspects of the true τ-surface, it does not exactly match its shape,\nespecially in the upper-right corner where τ is largest. Our theoretical results guarantee that\nthis eﬀect will go away as n →∞. Figure 2 also helps us understand why k-NN performs\nso poorly in terms of mean-squared error: its predictive surface is both badly biased and\nnoticeably “grainy,” especially for d = 20. It suﬀers from bias not only at the boundary\n23",
    "content_hash": "35057cd09a39b017ed2a2cad20b38c8a9cdd2fa32eb6d32a9bceac2365098430",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2b87acc6-7d4a-4c52-b6a8-986b21623986",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "mean-squared error\ncoverage\nd\nCF\n10-NN\n100-NN\nCF\n10-NN\n100-NN\n2\n0.02 (0)\n0.20 (0)\n0.02 (0)\n0.94 (0)\n0.93 (0)\n0.94 (0)\n3\n0.02 (0)\n0.20 (0)\n0.03 (0)\n0.90 (0)\n0.93 (0)\n0.90 (0)\n4\n0.02 (0)\n0.21 (0)\n0.06 (0)\n0.84 (1)\n0.93 (0)\n0.78 (1)\n5\n0.02 (0)\n0.22 (0)\n0.09 (0)\n0.81 (1)\n0.93 (0)\n0.67 (0)\n6\n0.02 (0)\n0.24 (0)\n0.15 (0)\n0.79 (1)\n0.92 (0)\n0.58 (0)\n8\n0.03 (0)\n0.29 (0)\n0.26 (0)\n0.73 (1)\n0.90 (0)\n0.45 (0)\nTable 3: Comparison of the performance of a causal forests (CF) with that of the k-nearest\nneighbors (k-NN) estimator with k = 10, 100, on the setup (29). The numbers in parenthe-\nses indicate the (rounded) standard sampling error for the last printed digit, obtained by\naggregating performance over 40 simulation replications.",
    "content_hash": "c162114deb2106b27a822aa02b1d1c0ebc3eadd23ee7b4f722c93e9fd8929b19",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "forests, available on",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0105c9d0-55a8-44b4-b9cd-d502386c9058",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Our causal forest estimator can be thought of as an adaptive nearest\nneighbor method, where the data determines which dimensions are most important to con-\nsider in selecting nearest neighbors. Such adaptivity seems essential for modern large-scale\napplications with many features. In general, the challenge in using adaptive methods as the basis for valid statistical infer-\nence is that selection bias can be diﬃcult to quantify; see Berk et al. [2013], Chernozhukov\net al. [2015], Taylor and Tibshirani [2015], and references therein for recent advances. In this\npaper, pairing “honest” trees with the subsampling mechanism of random forests enabled\nus to accomplish this goal in a simple yet principled way. In our simulation experiments,\nour method provides dramatically better mean-squared error than classical methods while\nachieving nominal coverage rates in moderate sample sizes. A number of important extensions and reﬁnements are left open. Our current results only\nprovide pointwise conﬁdence intervals for τ(x); extending our theory to the setting of global\nfunctional estimation seems like a promising avenue for further work. Another challenge is\nthat nearest-neighbor non-parametric estimators typically suﬀer from bias at the boundaries\nof the support of the feature space. A systematic approach to trimming at the boundaries,\nand possibly correcting for bias, would improve the coverage of the conﬁdence intervals. In\ngeneral, work can be done to identify methods that produce accurate variance estimates even\nin more challenging circumstances, e.g., with small samples or a large number of covariates,\nor to identify when variance estimates are unlikely to be reliable. 24",
    "content_hash": "ee1729aaee6753109a587619e49eed6e51ffa8d15c8f03e39aa9be1b60eb6073",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5a5e43c7-da66-421c-855c-e1aa551460ad",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "where the treatment eﬀect is largest, but also where the slope of the treatment eﬀect is high\nin the interior. These results highlight the promise of causal forests for accurate estimation of hetero-\ngeneous treatment eﬀects, all while emphasizing avenues for further work. An immediate\nchallenge is to control the bias of causal forests to achieve better coverage. Using more\npowerful splitting rules is a good way to reduce bias by enabling the trees to focus more\nclosely on the coordinates with the greatest signal. The study of splitting rules for trees\ndesigned to estimate causal eﬀects is still in its infancy and improvements may be possible. A limitation of the present simulation study is that we manually chose whether to use\ndouble-sample forests or propensity forests, depending on which procedure seemed more\nappropriate in each problem setting. An important challenge for future work is to design\nsplitting rules that can automatically choose which characteristic of the training data to\nsplit on. A principled and automatic rule for choosing s would also be valuable. We present additional simulation results in the supplementary material. Appendix A has\nextensive simulations in the setting of Table 2 while varying both s and n; and also considers\na simulation setting where the signal is spread out over many diﬀerent features, meaning\nthat forests have less upside over baseline methods. Finally, in Appendix B, we study the\neﬀect of honesty versus adaptivity on forest predictive error. 6\nDiscussion\nThis paper proposed a class of non-parametric methods for heterogeneous treatment eﬀect\nestimation that allow for data-driven feature selection all while maintaining the beneﬁts\nof classical methods, i.e., asymptotically normal and unbiased point estimates with valid\nconﬁdence intervals.",
    "content_hash": "4c8e198097e4c0a0ddc7b0cbbd9f21d4caf06e5e23af03b36aaf176d6e7261a9",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "733e0b06-689a-4047-8b6e-cbef8c94bd5d",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Acknowledgment\nWe are grateful for helpful feedback from Brad Efron, Trevor Hastie, Guido Imbens, Guen-\nther Walther, as well as the associate editor, two anonymous referees, and seminar partici-\npants at Atlantic Causal Inference Conference, Berkeley Haas, Berkeley Statistics, California\nEconometrics Conference, Cambridge, Carnegie Mellon, COMPSTAT, Cornell, Columbia\nBusiness School, Columbia Statistics, CREST, EPFL, ISAT/DARPA Machine Learning for\nCausal Inference, JSM, London Business School, Microsoft Conference on Digital Economics,\nMerck, MIT IDSS, MIT Sloan, Northwestern, SLDM, Stanford GSB, Stanford Statistics,\nUniversity College London, University of Bonn, University of Chicago Booth, University of\nChicago Statistics, University of Illinois Urbana–Champaign, University of Pennsylvania,\nUniversity of Southern California, University of Washington, Yale Econometrics, and Yale\nStatistics. S. W. was partially supported by a B. C. and E. J. Eaves Stanford Graduate\nFellowship. References\nSusan F Assmann, Stuart J Pocock, Laura E Enos, and Linda E Kasten. Subgroup analysis\nand other (mis) uses of baseline data in clinical trials. The Lancet, 355(9209):1064–1069,\n2000. Susan Athey and Guido Imbens. Recursive partitioning for heterogeneous causal eﬀects. Proceedings of the National Academy of Sciences, 113(27):7353–7360, 2016. Susan Athey, Julie Tibshirani, and Stefan Wager. Generalized random forests. arXiv preprint\narXiv:1610.01271, 2016.",
    "content_hash": "8193b96d47c309a7caef2737ae0eaef346647fa674a07e6b9a32e2e183698a21",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7373dc11-a2fc-4178-ac39-66d21e5da677",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "G´erard Biau, Luc Devroye, and G´abor Lugosi. Consistency of random forests and other\naveraging classiﬁers. The Journal of Machine Learning Research, 9:2015–2033, 2008. G´erard Biau, Fr´ed´eric C´erou, and Arnaud Guyader. On the rate of convergence of the bagged\nnearest neighbor estimate. The Journal of Machine Learning Research, 11:687–712, 2010. Patrick Billingsley. Probability and Measure. John Wiley & Sons, 2008. 25",
    "content_hash": "7b60d0c2ec38b5b33f5bb791e9ac5b2851f59c5fa937409deb95cd34cb9ec2a4",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fc966267-18a3-46e1-acb7-9d4df29de2c9",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Ole Barndorﬀ-Nielsen and Milton Sobel. On the distribution of the number of admissible\npoints in a vector random sample. Theory of Probability & Its Applications, 11(2):249–269,\n1966. Richard Berk, Lawrence Brown, Andreas Buja, Kai Zhang, and Linda Zhao. Valid post-\nselection inference. The Annals of Statistics, 41(2):802–837, 2013. Alina Beygelzimer and John Langford. The oﬀset tree for learning with partial labels. In Proceedings of the 15th International Conference on Knowledge Discovery and Data\nMining, pages 129–138. ACM, 2009. Alina Beygelzimer, Sham Kakadet, John Langford, Sunil Arya, David Mount, and Shengqiao\nLi. FNN: Fast Nearest Neighbor Search Algorithms and Applications, 2013. URL http:\n//CRAN.R-project.org/package=FNN. R package version 1.1. Debopam Bhattacharya and Pascaline Dupas. Inferring welfare maximizing treatment as-\nsignment under budget constraints. Journal of Econometrics, 167(1):168–196, 2012. G´erard Biau. Analysis of a random forests model. The Journal of Machine Learning Re-\nsearch, 13:1063–1095, 2012. G´erard Biau and Luc Devroye. On the layered nearest neighbour estimate, the bagged\nnearest neighbour estimate and the random forest method in regression and classiﬁcation. Journal of Multivariate Analysis, 101(10):2499–2518, 2010.",
    "content_hash": "cc3d98bb8e06ce9b8e2ae0e1afaf4824a0aadeb235d2fc5fb63296b6ccaf4e63",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c36fbbff-81e4-4677-bf2a-b30716e0d6d1",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Subgroup identiﬁcation from\nrandomized clinical trial data. Statistics in Medicine, 30(24):2867–2880, 2011. Jerome H Friedman and Peter Hall. On bagging and nonlinear estimation. Journal of\nStatistical Planning and Inference, 137(3):669–683, 2007. Donald P Green and Holger L Kern. Modeling heterogeneous treatment eﬀects in survey\nexperiments with Bayesian additive regression trees. Public Opinion Quarterly, 76(3):\n491–511, 2012. Jaroslav H´ajek. Asymptotic normality of simple linear rank statistics under alternatives. The Annals of Mathematical Statistics, 39(2):325–346, 1968. 26",
    "content_hash": "ff953113728134c8640f773d64a8f340be5c15de69fb2ed26d8ef20f7078d0a6",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "48cba87e-6e53-4af1-9dc7-6e5b76275d8b",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, and Ziad Obermeyer. Prediction policy\nproblems. American Economic Review, 105(5):491–95, 2015. Myoung-jae Lee. Non-parametric tests for distributional treatment eﬀect for randomly cen-\nsored responses. Journal of the Royal Statistical Society: Series B (Statistical Methodol-\nogy), 71(1):243–264, 2009. Andy Liaw and Matthew Wiener. Classiﬁcation and regression by randomForest. R News,\n2(3):18–22, 2002. URL http://CRAN.R-project.org/doc/Rnews/. Yi Lin and Yongho Jeon. Random forests and adaptive nearest neighbors. Journal of the\nAmerican Statistical Association, 101(474):578–590, 2006. Colin L Mallows. Some comments on Cp. Technometrics, 15(4):661–675, 1973. Charles F Manski. Statistical treatment rules for heterogeneous populations. Econometrica,\n72(4):1221–1246, 2004. Daniel F McCaﬀrey, Greg Ridgeway, and Andrew R Morral. Propensity score estimation\nwith boosted regression for evaluating causal eﬀects in observational studies. Psychological\nMethods, 9(4):403, 2004. Nicolai Meinshausen. Quantile regression forests. The Journal of Machine Learning Re-\nsearch, 7:983–999, 2006. Lucas Mentch and Giles Hooker. Quantifying uncertainty in random forests via conﬁdence\nintervals and hypothesis tests.",
    "content_hash": "7d3a9ea64135c8f5ad4d323701c2b8873d296c33d290d6ae9507b30be73a249a",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "179d457c-19af-4922-95f2-0a5cc8588ec5",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learn-\ning. New York: Springer, 2009. Jennifer Hill and Yu-Sung Su. Assessing lack of common support in causal inference us-\ning bayesian nonparametrics: Implications for evaluating the eﬀect of breastfeeding on\nchildrens cognitive outcomes. The Annals of Applied Statistics, 7(3):1386–1420, 2013. Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Compu-\ntational and Graphical Statistics, 20(1), 2011. Keisuke Hirano and Jack R Porter. Asymptotics for statistical treatment rules. Economet-\nrica, 77(5):1683–1701, 2009. Keisuke Hirano, Guido W Imbens, and Geert Ridder. Eﬃcient estimation of average treat-\nment eﬀects using the estimated propensity score. Econometrica, 71(4):1161–1189, 2003. Wassily Hoeﬀding. A class of statistics with asymptotically normal distribution. The Annals\nof Mathematical Statistics, 19(3):293–325, 1948. Kosuke Imai and Marc Ratkovic. Estimating treatment eﬀect heterogeneity in randomized\nprogram evaluation. The Annals of Applied Statistics, 7(1):443–470, 2013. Guido W Imbens and Donald B Rubin. Causal Inference in Statistics, Social, and Biomedical\nSciences. Cambridge University Press, 2015. Louis A Jaeckel. The Inﬁnitesimal Jackknife. 1972.",
    "content_hash": "4549a7733d4a9ef4b052de61c63d928c262c0e79574f538fe3962dea633e9be5",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "74f3e6a6-cfc6-46bb-b285-642e41e9a878",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Lu Tian, Ash A Alizadeh, Andrew J Gentles, and Robert Tibshirani. A simple method for\nestimating interactions between a treatment and a large number of covariates. Journal of\nthe American Statistical Association, 109(508):1517–1532, 2014. Aad W. Van der Vaart. Asymptotic Statistics. Number 3. Cambridge Univ Pr, 2000. Stefan Wager. Asymptotic theory for random forests. arXiv preprint arXiv:1405.0352, 2014. Stefan Wager and Guenther Walther. Adaptive concentration of regression trees, with ap-\nplication to random forests. arXiv preprint arXiv:1503.06388, 2015. Stefan Wager, Trevor Hastie, and Bradley Efron. Conﬁdence intervals for random forests:\nThe jackknife and the inﬁnitesimal jackknife. The Journal of Machine Learning Research,\n15, 2014. Larry Wasserman and Kathryn Roeder. High-dimensional variable selection. The Annals of\nStatistics, 37(5A):2178–2201, 2009. Herbert I Weisberg and Victor P Pontes. Post hoc subgroups in clinical trials: Anathema\nor analytics? Clinical Trials, 12(4):357–364, 2015. Daniel Westreich, Justin Lessler, and Michele J Funk. Propensity score estimation: neu-\nral networks, support vector machines, decision trees (CART), and meta-classiﬁers as\nalternatives to logistic regression.",
    "content_hash": "08ae92511c90b7808c351006e6e2453f881e5139e642f8c27d558948ed73105a",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bbf18bbe-7dd5-4d03-86f2-3f84efa58f4a",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Anton Schick. On asymptotically eﬃcient estimation in semiparametric models. The Annals\nof Statistics, pages 1139–1151, 1986. Erwan Scornet, G´erard Biau, and Jean-Philippe Vert. Consistency of random forests. The\nAnnals of Statistics, 43(4):1716–1741, 2015. Joseph Sexton and Petter Laake. Standard errors for bagged and random forest estimators. Computational Statistics & Data Analysis, 53(3):801–811, 2009. James Edward\nSignorovitch. Identifying Informative Biological Markers in High-\nDimensional Genomic Data and Clinical Trials. PhD thesis, Harvard University, 2007. Carolin Strobl, Anne-Laure Boulesteix, Achim Zeileis, and Torsten Hothorn. Bias in ran-\ndom forest variable importance measures: Illustrations, sources and a solution. BMC\nBioinformatics, 8(1):25, 2007. Xiaogang Su, Chih-Ling Tsai, Hansheng Wang, David M Nickerson, and Bogong Li. Sub-\ngroup analysis via recursive partitioning. The Journal of Machine Learning Research, 10:\n141–158, 2009. Matt Taddy, Matt Gardner, Liyun Chen, and David Draper. A nonparametric Bayesian\nanalysis of heterogeneous treatment eﬀects in digital experimentation. Journal of Business\n& Economic Statistics, (just-accepted), 2016. Jonathan Taylor and Robert J Tibshirani. Statistical learning and selective inference. Pro-\nceedings of the National Academy of Sciences, 112(25):7629–7634, 2015.",
    "content_hash": "6e1f8e1752bf6a8df7afb91c1a4175de519b52bfd37e443580df84c2efa4b960",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9d3e7f47-ba12-4beb-acc6-22a7345efe57",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "The results presented in Table 2\nare a subset of these results. The numbers in parentheses indicate the (rounded) estimates for standard sampling error for\nthe last printed digit, obtained by aggregating performance over 10 simulation replications. Mean-squared error (MSE)\nand coverage denote performance for estimating τ on a random test set; the variance column denotes the mean variance\nestimate obtained by the inﬁnitesimal jackknife on the test set. Target coverage is 0.95. We always grew B = n trees.",
    "content_hash": "b383cbeaaac43e1718ae2f573e0ff78af7eaa533aec81cacfca9c955b4a6d438",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b44983d1-8d2b-4a93-9bba-c9457227c070",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "n\nd\ns\nMSE\nCoverage Variance\nn\nd\ns\nMSE\nCoverage Variance\nn\nd\ns\nMSE\nCoverage Variance\n1000 2 100 0.16 (1) 0.29 (3)\n0.01 (0) 2000 4 200 0.14 (1) 0.25 (3)\n0.01 (0)\n5000 6 500 0.05 (0) 0.52 (3)\n0.01 (0)\n1000 2 200 0.09 (1) 0.69 (5)\n0.03 (0) 2000 4 400 0.06 (1) 0.70 (4)\n0.02 (0)\n5000 6 1000 0.03 (0) 0.75 (3)\n0.01 (0)\n1000 2 250 0.07 (1) 0.79 (4)\n0.03 (0) 2000 4 500 0.05 (1) 0.81 (3)\n0.02 (0)\n5000 6 1250 0.02 (0) 0.79 (3)\n0.01 (0)\n1000 2 333 0.07 (1) 0.89 (3)\n0.04 (0) 2000 4 667 0.04 (0) 0.90 (2)\n0.03 (0)\n5000 6 1667 0.02 (0) 0.86 (2)\n0.02 (0)\n1000 2 500 0.07 (1) 0.94 (2)\n0.07 (0) 2000 4 1000 0.04 (0) 0.95 (1)\n0.04 (0)\n5000 6 2500 0.02 (0) 0.92 (1)\n0.02 (0)\n1000 2 667 0.08 (1) 0.90 (4)\n0.08 (1) 2000 4 1333 0.05 (0) 0.96 (1)\n0.06 (0)\n5000 6 3333 0.03 (0) 0.96 (1)\n0.03 (0)\n1000 3 100 0.25 (2) 0.16 (2)\n0.01 (0) 2000 5 200 0.17 (1) 0.18 (2)\n0.01 (0)\n5000 8 500 0.06 (1) 0.42 (3)\n0.01 (0)\n1000 3 200 0.13 (2) 0.53 (5)\n0.02 (0) 2000 5 400 0.07 (1) 0.65 (6)\n0.02 (0)\n5000 8 1000 0.03 (0) 0.69 (2)\n0.01 (0)\n1000 3 250 0.11 (2) 0.66 (6)\n0.03 (0) 2000 5 500 0.05 (1) 0.75 (5)\n0.02 (0)\n5000 8 1250 0.03 (0) 0.73 (3)\n0.01 (0)\n1000 3 333 0.09 (1) 0.81 (5)\n0.04 (0) 2000 5 667 0.05 (0) 0.84 (3)\n0.03 (0)\n5000 8 1667 0.03 (0) 0.81 (2)\n0.01 (0)\n1000 3 500 0.08 (1) 0.90 (3)\n0.06 (0) 2000 5 1000 0.04 (0) 0.91 (1)\n0.04 (0)\n5000 8 2500 0.03 (0) 0.88 (2)\n0.02 (0)\n1000 3 667 0.08 (1) 0.73 (5)\n0.04 (0) 2000 5 1333 0.05 (0) 0.92 (2)\n0.05 (0)\n5000 8 3333 0.03 (0) 0.92 (1)\n0.03 (0)\n1000 4 100 0.32 (1) 0.12 (1)\n0.01 (0) 2000 6 200 0.22 (1) 0.12 (1)\n0.01 (0) 10000 2 1000 0.01 (0) 0.95 (1)\n0.01 (0)\n1000 4 200 0.16 (1) 0.43 (3)\n0.03 (0) 2000 6 400 0.09 (1) 0.52 (5)\n0.02 (0) 10000 2 2000 0.01 (0) 0.96 (1)\n0.02 (0)\n1000 4 250 0.13 (1) 0.58 (4)\n0.03 (0) 2000 6 500 0.07 (1) 0.68 (4)\n0.02 (0) 10000 2 2500 0.02 (0) 0.96 (0)\n0.02 (0)\n1000 4 333 0.09 (1) 0.76 (3)\n0.04 (0) 2000 6 667 0.05 (0) 0.82 (3)\n0.03 (0) 10000 2 3333 0.02 (0) 0.96 (0)\n0.02 (0)\n1000 4 500 0.07 (1) 0.91 (2)\n0.06 (0) 2000 6 1000 0.04 (0) 0.89 (2)\n0.03 (0) 10000 2 5000 0.03 (0) 0.97 (0)\n0.04 (0)\n1000 4 667 0.07 (1) 0.79 (3)\n0.04 (1) 2000 6 1333 0.05 (0) 0.94 (1)\n0.05 (0) 10000 2 6667 0.04 (0) 0.98 (0)\n0.06 (0)\n1000 5 100 0.34 (2) 0.10 (1)\n0.01 (0) 2000 8 200 0.24 (1) 0.12 (1)\n0.01 (0) 10000 3 1000 0.01 (0) 0.84 (1)\n0.01 (0)\n1000 5 200 0.16 (2) 0.41 (6)\n0.02 (0) 2000 8 400 0.08 (0) 0.61 (4)\n0.02 (0) 10000 3 2000 0.01 (0) 0.91 (1)\n0.01 (0)\n1000 5 250 0.12 (2) 0.59 (5)\n0.03 (0) 2000 8 500 0.06 (0) 0.78 (2)\n0.02 (0) 10000 3 2500 0.01 (0) 0.92 (1)\n0.01 (0)\n1000 5 333 0.09 (1) 0.80 (4)\n0.04 (0) 2000 8 667 0.05 (0) 0.85 (1)\n0.02 (0) 10000 3 3333 0.02 (0) 0.94 (1)\n0.02 (0)\n1000 5 500 0.07 (1) 0.89 (3)\n0.06 (0) 2000 8 1000 0.04 (0) 0.91 (1)\n0.04 (0) 10000 3 5000 0.02 (0) 0.95 (0)\n0.03 (0)\n1000 5 667 0.07 (1) 0.77 (4)\n0.04 (0) 2000 8 1333 0.04 (0) 0.89 (3)\n0.04 (0) 10000 3 6667 0.03 (0) 0.97 (0)\n0.04 (0)\n1000 6 100 0.41 (3) 0.07 (1)\n0.01 (0) 5000 2 500 0.02 (0) 0.86 (2)\n0.01 (0) 10000 4 1000 0.02 (0) 0.73 (3)\n0.01 (0)\n1000 6 200 0.22 (3) 0.31 (4)\n0.02 (0) 5000 2 1000 0.02 (0) 0.92 (1)\n0.02 (0) 10000 4 2000 0.02 (0) 0.85 (2)\n0.01 (0)\n1000 6 250 0.17 (3) 0.48 (6)\n0.03 (0) 5000 2 1250 0.02 (0) 0.94 (1)\n0.02 (0) 10000 4 2500 0.02 (0) 0.87 (1)\n0.01 (0)\n1000 6 333 0.12 (2) 0.68 (7)\n0.04 (0) 5000 2 1667 0.03 (0) 0.95 (1)\n0.03 (0) 10000 4 3333 0.02 (0) 0.90 (1)\n0.01 (0)\n1000 6 500 0.08 (2) 0.89 (3)\n0.05 (0) 5000 2 2500 0.04 (0) 0.96 (0)\n0.05 (0) 10000 4 5000 0.02 (0) 0.93 (1)\n0.02 (0)\n1000 6 667 0.07 (1) 0.75 (6)\n0.04 (0) 5000 2 3333 0.05 (0) 0.97 (0)\n0.06 (0) 10000 4 6667 0.02 (0) 0.95 (0)\n0.03 (0)\n1000 8 100 0.51 (2) 0.06 (0)\n0.01 (0) 5000 3 500 0.02 (0) 0.75 (3)\n0.01 (0) 10000 5 1000 0.02 (0) 0.65 (3)\n0.01 (0)\n1000 8 200 0.29 (1) 0.20 (2)\n0.02 (0) 5000 3 1000 0.02 (0) 0.89 (2)\n0.01 (0) 10000 5 2000 0.02 (0) 0.79 (2)\n0.01 (0)\n1000 8 250 0.23 (1) 0.31 (2)\n0.03 (0) 5000 3 1250 0.02 (0) 0.91 (1)\n0.02 (0) 10000 5 2500 0.02 (0) 0.83 (2)\n0.01 (0)\n1000 8 333 0.16 (1) 0.53 (4)\n0.04 (0) 5000 3 1667 0.02 (0) 0.94 (1)\n0.02 (0) 10000 5 3333 0.02 (0) 0.87 (1)\n0.01 (0)\n1000 8 500 0.10 (1) 0.86 (2)\n0.06 (0) 5000 3 2500 0.03 (0) 0.96 (1)\n0.03 (0) 10000 5 5000 0.02 (0) 0.92 (1)\n0.02 (0)\n1000 8 667 0.08 (1) 0.70 (5)\n0.04 (0) 5000 3 3333 0.03 (0) 0.97 (0)\n0.05 (0) 10000 5 6667 0.02 (0) 0.94 (0)\n0.02 (0)\n2000 2 200 0.05 (0) 0.64 (4)\n0.01 (0) 5000 4 500 0.03 (0) 0.61 (3)\n0.01 (0) 10000 6 1000 0.02 (0) 0.62 (3)\n0.00 (0)\n2000 2 400 0.03 (0) 0.88 (1)\n0.02 (0) 5000 4 1000 0.02 (0) 0.84 (2)\n0.01 (0) 10000 6 2000 0.02 (0) 0.75 (2)\n0.01 (0)\n2000 2 500 0.03 (0) 0.92 (1)\n0.03 (0) 5000 4 1250 0.02 (0) 0.88 (2)\n0.01 (0) 10000 6 2500 0.02 (0) 0.79 (2)\n0.01 (0)\n2000 2 667 0.04 (0) 0.95 (1)\n0.04 (0) 5000 4 1667 0.02 (0) 0.91 (1)\n0.02 (0) 10000 6 3333 0.02 (0) 0.84 (1)\n0.01 (0)\n2000 2 1000 0.05 (0) 0.97 (1)\n0.06 (0) 5000 4 2500 0.03 (0) 0.95 (1)\n0.03 (0) 10000 6 5000 0.02 (0) 0.90 (1)\n0.01 (0)\n2000 2 1333 0.06 (0) 0.98 (0)\n0.08 (0) 5000 4 3333 0.03 (0) 0.96 (1)\n0.04 (0) 10000 6 6667 0.02 (0) 0.93 (1)\n0.02 (0)\n2000 3 200 0.09 (1) 0.40 (5)\n0.01 (0) 5000 5 500 0.04 (0) 0.56 (4)\n0.01 (0) 10000 8 1000 0.03 (0) 0.56 (2)\n0.00 (0)\n2000 3 400 0.04 (1) 0.78 (4)\n0.02 (0) 5000 5 1000 0.02 (0) 0.81 (2)\n0.01 (0) 10000 8 2000 0.02 (0) 0.70 (2)\n0.01 (0)\n2000 3 500 0.04 (1) 0.85 (3)\n0.02 (0) 5000 5 1250 0.02 (0) 0.86 (3)\n0.01 (0) 10000 8 2500 0.02 (0) 0.74 (2)\n0.01 (0)\n2000 3 667 0.04 (1) 0.90 (2)\n0.03 (0) 5000 5 1667 0.02 (0) 0.90 (2)\n0.02 (0) 10000 8 3333 0.02 (0) 0.80 (2)\n0.01 (0)\n2000 3 1000 0.04 (0) 0.94 (1)\n0.05 (0) 5000 5 2500 0.02 (0) 0.94 (1)\n0.02 (0) 10000 8 5000 0.02 (0) 0.87 (1)\n0.01 (0)\n2000 3 1333 0.05 (0) 0.96 (1)\n0.06 (0) 5000 5 3333 0.03 (0) 0.96 (1)\n0.03 (0) 10000 8 6667 0.02 (0) 0.91 (1)\n0.02 (0)\nTable 4: Simulations for the generative model described in (28), while varying s and n.",
    "content_hash": "4b2c8b352a8b83ca9b13eb1d1b704c82e900a0952487399419effcecfaa994fe",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "baee6f9b-a16a-405c-90be-e16f1879149e",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "When the signal is dense,\nall surveyed methods have bad coverage except for 10-NN which, as always, simply has very\nwide intervals. B\nIs Honesty Necessary for Consistency? Our honesty assumption is the largest divergence between our framework and main-stream\napplications of random forests. Following Breiman [2001a], almost all practical implemen-\ntations of random forests are not honest. Moreover, there has been a stream of recent work\nproviding theoretical guarantees for adaptive random forests: Scornet et al. [2015] establish\nrisk consistency under assumptions on the data-generating function, i.e., they show that\n30",
    "content_hash": "da8623059758d02e5b074ae06bf74c9adb7b5c360a83a5327b8d679fd380b042",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1b84acf9-f14d-4fa2-bcbd-24cd1ce11bf4",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "As expected, we observe a\nbias-variance trade-oﬀ: the causal forest is more aﬀected by bias when s is small relative\nto n, and by variance when s is larger relative to n. Reassuringly, we observe that our\nconﬁdence intervals obtain close-to-nominal coverage when the mean-squared error matches\nthe average variance estimate ˆσ2(X) generated by the inﬁnitesimal jackknife, corroborating\nthe hypothesis that failures in coverage mostly arise when the causal forest is bias- instead\nof variance-dominated. Finally, all our experiments relied on settings with strong, low-dimensional structure that\nforests could pick up on to improve over k-NN matching. This intuition is formally supported\nby, e.g., the theory developed by Biau [2012]. Here, we consider how forests perform when\nthe signal is spread out over a larger number of features, and so forests have less upside\nover other methods. We ﬁnd that—as expected—they do not improve much over baselines. Speciﬁcally, we generate data with a treatment eﬀect function\nτ(x) = 4\nq\nq\nX\nj=1\n\u0012\n1\n1 + e−12(xj−0.5) −1\n2\n\u0013\n,\n(30)\nwhere we vary both the number of signal dimensions q and ambient dimensions d. As seen\nin Table 5, forests vastly improve over k-NN in terms of mean-squared error when q is much\nsmaller than d, but that this advantage decreases when d and q are comparable; and actually\ndo worse than k-NN when we have a dense signal with d = q = 6.",
    "content_hash": "556b37aa651e03fc2612ea4fa0ea7ff03d95eef3bc8b4eee0b0e53d5d159d030",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "68bd6996-d5ca-44a7-8d79-155ee1a1f400",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "mean-squared error\ncoverage\nq\nd\nCF\n10-NN\n100-NN\nCF\n10-NN\n100-NN\n2\n6\n0.04 (0)\n0.24 (0)\n0.13 (0)\n0.92 (1)\n0.92 (0)\n0.59 (1)\n4\n6\n0.06 (0)\n0.22 (0)\n0.07 (0)\n0.87 (1)\n0.93 (0)\n0.72 (1)\n6\n6\n0.08 (0)\n0.22 (0)\n0.05 (0)\n0.75 (1)\n0.93 (0)\n0.78 (1)\n2\n12\n0.04 (0)\n0.38 (0)\n0.34 (0)\n0.86 (1)\n0.88 (0)\n0.45 (0)\n4\n12\n0.08 (0)\n0.30 (0)\n0.18 (0)\n0.76 (1)\n0.90 (0)\n0.51 (1)\n6\n12\n0.12 (0)\n0.26 (0)\n0.12 (0)\n0.59 (1)\n0.91 (0)\n0.59 (1)\nTable 5: Results for a data-generating design where we vary both the number of signal\nfeatures q and the number of ambient features d. All simulations have n = 5, 000, B = 2, 000\nand a minimum leaf size of 1, and are aggregated over 20 simulation replicates. A\nAdditional Simulations\nIn Table 4, we expand on the simulation results given in Table 2, and present results for\ndata generated according to (28) all while varying s and n.",
    "content_hash": "0a2a2442afefa0f05d6b75d15536f4206b39c63272a9edb31b08702232beb55f",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "afcf84e0-c8b4-4164-85be-c098f432af31",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "0.0\n0.2\n0.4\n0.6\nn\nerror\nHonest bias\nHonest RMSE\nAdapt. bias\nAdapt. RMSE\n100\n200\n400\n800\n1600\n3200\nFigure 3: Comparison of the performance of honest and adaptive causal forests when pre-\ndicting at x0 = (0, 0, . . . , 0), which is a corner of the support of the features Xi. Both forests\nhave B = 500 trees, and use a leaf-size of k = 1. We use a subsample size s = n0.8 for adap-\ntive forests and s = 2 n0.8 for honest forests. All results are averaged over 40 replications;\nwe report both bias and root-mean-squared error (RMSE). the test-set error of forests converges asymptotically to the Bayes risk, Mentch and Hooker\n[2016] provide results about uncertainty quantiﬁcation, and Wager and Walther [2015] ﬁnd\nthat adaptive trees with growing leaves are in general consistent under fairly weak conditions\nand provide bounds on the decay rate of their bias. However, if we want pointwise centered asymptotic Gaussianity results, then honesty\nappears to be necessary. Consider the following simple example, where there is no treatment\nheterogeneity—and in fact X is independent of W and Y . We are in a randomized controlled\ntrial, where X ∼Uniform([0, 1]p) with p = 10 and W ∼Bernoulli(0.5). The distribution of\nY is Yi = 2WiAi +εi, where εi ∼N\n0, 0.12\u0001\nand Ai ∼Bernoulli(0.05).",
    "content_hash": "c58b11a9735d91be1e09442f982927abe427aefc00ad4d8b7275c1e6d4f4fd51",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b972e270-63ce-46cd-ae09-614a9d87d7ec",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Thus, the treatment\neﬀect is τ(x) = 0.1 for all x ∈[0, 1]p. Our goal is to estimate the treatment eﬀect τ(x0) at\nx0 = (0, 0, . . . , 0). Results from running both honest and adaptive forests are shown in Figure 3. We see\nthat honest forests are unbiased regardless of n, and their mean-squared error decreases\nwith sample size, as expected. Adaptive forests, in contrast, perform remarkably badly. They have bias that far exceeds the intrinsic sampling variation; and, furthermore, this bias\nincreases with n. What is happening here is that CART trees aggressively seek to separate\noutliers (“Yi ≈1”) from the rest of the data (“Yi ≈0”) and, in doing so, end up over-\nrepresenting outliers in the corners of the feature space. As n increases, it appears that\nadaptive forests have more opportunities to push outliers into corners of features space and\nso the bias worsens. This phenomenon is not restricted to causal forests; an earlier technical\nreport [Wager, 2014] observed the same phenomenon in the context of plain regression forests. 31",
    "content_hash": "39995c588819764608cc99ce875a44a39ff2c143d45daf8934b7ccbc14cbd6c1",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e9bab935-e352-4353-b4f9-2b1c78321465",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Honest trees do not have this problem, as we do not know where the I-sample outliers will be\nwhen placing splits using only the J -sample. Thus, it appears that adaptive CART forests\nare pointwise biased in corners of x-space. Finally, we note that this bias phenomenon does not contradict existing consistency re-\nsults in the literature. Wager and Walther [2015] prove that this bias phenomenon discussed\nabove can be averted if we use a minimum leaf-size k that grows with n (in contrast, Figure\n3 uses k = 1). However, their bounds on the bias decays slower than the sampling variance\nof random forests, and so their results cannot be used to get centered conﬁdence intervals. Meanwhile, Scornet et al. [2015] prove that forests are risk-consistent at an average test\npoint, and, in fact, the test set error of adaptive forests does decay in the setting of Figure\n3 as the sample size n grows (although honest forests still maintain a lower test set error). The reason test set error can go to zero despite the bias phenomenon in Figure 3 is that,\nwhen n gets large, almost all test points will be far enough from corners that they will not\nbe aﬀected by the phenomenon from Figure 3. B.1\nAdaptive versus Honest Predictive Error\nThe discussion above implies that the theorems proved in this paper are not valid for adap-\ntive forests. That being said, it still remains interesting to ask whether our use of honest\nforest hurts us in terms of mean-squared error at a random test point, as in the formalism\nconsidered by, e.g., Scornet et al. [2015].",
    "content_hash": "817f84ca466d893ddead7a678078b038264f7c97c51bbfcf5c2a2f27385a8e96",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0db65872-abe9-4ca9-9e8f-e3e7c3ba26c7",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "In this setting, Denil et al. [2014] showed that\nhonesty can hurt the performance of regression forests on some classic datasets from the\nUCI repository; however, in a causal inference setting, we might be concerned that the risk\nof overﬁtting with adaptive forests is higher since our signals of interest are often quite weak. We compare the performance of honest and adaptive forests in the setting of Table 2,\nwith d = 8. Here, if we simply run adaptive forests out-of-the-box with the usual minimum\nleaf size parameter k = 1, they do extremely badly; in fact, they do worse than 50 nearest\nneighbors. However, if we are willing to increase the minimum leaf size, their performance\nimproves. Figure 4 depicts the root-mean-squared error for both adaptive and honest forests over a\nwide range of choices for the minimum leaf size parameter k. We see that, at their best, both\nmethods do comparably. However, honest forests attain good performance over a wide range\nof choices for k, including our default choice k = 1, whereas adaptive forests are extremely\nsensitive to choosing a good value of k. We also note that the optimum k = 64 for adaptive\nforests is quite far from standard choices advocated in practice; such as k = 5 recommended\nby Breiman [2001a] for regression forests, k = 7 in the cforest function used by Strobl\net al. [2007], or k = 10 recommended by Meinshausen [2006] for quantile regression. Thus,\nit appears that accurately tuning adaptive forests in this setting may present a challenge\nand, overall, a practitioner may prefer honest forests even based on their mean-squared error\ncharacteristics alone.",
    "content_hash": "4f002d05c1a1d92443752fc3346e2ddbb0f9b01ba13875de5bc907020e907133",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7e7847a5-86de-4ea9-98f6-41f7f0acbeae",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "By honesty of T, Var [T] ≳Var\n\u0002\nY\nX = x\n\u0003\n/ |{i : Xi ∈L(x)}| ≥Var\n\u0002\nY\nX = x\n\u0003\n/(2k), and\nso\nσ2\nn(x) ≳Cf, d\n2k\ns\nn\nVar\n\u0002\nY\nX = x\n\u0003\nlog(s)d\n= Ω\nnβ−1−ε\u0001\nfor any ε > 0. It follows that\n|E [ˆµn (x)] −µ(x)|\nσn (x)\n= O\n\nn\n1\n2\n1+ε−β\n1+\nlog((1−α)−1)\nπ−1d log(α−1)\n!!\n. 33",
    "content_hash": "95d9a90bb7ef40be385823213f9dcbab7cca39a87bf77df825ca501032556327",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ccc8936a-3338-4c1f-939c-c86c1220a301",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "0.2\n0.3\n0.4\n0.5\nMinimum Leaf Size\nRoot−Mean−Squared Error\n1\n2\n4\n8\n16\n32\n64\n128\n256\nHonest Forest\nAdaptive Forest\nFigure 4: Comparison of the root-mean-squared error of honest and adaptive forests in the\nsetting of Table 2, with d = 8. Honest forests use s = 2500 (i.e., |I| = 1250) while adaptive\nforests use s = 1250, such that both methods grow trees of the same depth. Both forests\nhave B = 500, and results are averaged over 100 simulation replications. Proof of Theorem 1. Given the statements of Theorem 8 and Theorem 9, it only remains to\nshow that (15) holds with E [ˆµn(x)] replaced with µ(x). To do so, it suﬃces to show that\n|E [ˆµn (x)] −µ(x)| /σn (x) →0; the rest follows from Slutsky’s lemma. Now, recall that by\nTheorem 3,\n|E [ˆµn (x)] −µ(x)| = O\nn\n−β\n2\nlog((1−α)−1)\nπ−1d log(α−1)\n! . Meanwhile, from Theorem 5 and the proof of Theorem 8, we see that\nσ2\nn(x) ≳Cf, d\ns\nn\nVar [T]\nlog(s)d .",
    "content_hash": "a4669304b512757ba8cd1358f55baddae5cd841a4ce264ab7fc384ac8fcf342b",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "04fb8968-8414-4cc0-819d-c27d0c721512",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Meanwhile, again by regularity, we might expect that diamj (L (x)) should less than (1 −α)cj(x),\nat least for large n. This condition would hold directly if the regularity condition from Deﬁni-\ntion 4 were framed in terms of Lebesgue measure instead of the number of training examples\nin the leaf; our task is to show that it still holds approximately in our current setup. Using the methods developed in Wager and Walther [2015], and in particular their\nLemma 12, we can verify that, with high probability and simultaneously for all but the\nlast O (log log n) parent nodes above L(x), the number of training examples inside the node\ndivided by n is within a factor 1 + o(1) of the Lebesgue measure of the node. From this, we\nconclude that, for large enough s, with probability greater than 1 −1/s\ndiamj (L (x)) ≤(1 −α + o(1))(1+o(1))cj(x) ,\nor, more prosaically, that\ndiamj (L (x)) ≤(1 −α)0.991 cj(x) . Combining this results with the above Chernoﬀbound yields the desired inequality. Here,\nreplacing 0.991 with 0.99 in the bound lets us ignore the 1/s asymptotic failure probability\nof the concentration result used above.",
    "content_hash": "a9a30e4d9d5f41e116fb755be5b508e51dcbe08bbfdc327a14b1324e2bf1cb5d",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6b3b2830-87b3-4bc0-98d6-2be2e783a693",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "The right-hand-side bound converges to 0 for some small enough ε > 0 provided that\nβ >\n\n1 +\nlog\n\u0010\n(1 −α)−1\u0011\nπ−1d log (α−1)\n\n\n−1\n= 1 −\n\n1 + d\nπ\nlog\nα−1\u0001\nlog\n\u0010\n(1 −α)−1\u0011\n\n\n−1\n= βmin. C.1\nBounding the Bias of Regression Trees\nProof of Lemma 2. Let c(x) be the number of splits leading to the leaf L(x), and let cj(x) be\nthe number of these splits along the j-th coordinate. By regularity, we know that sαc(x) ≤\n2k −1, and so c(x) ≥log(s/(2k −1))/ log(α−1). Thus, because the tree is a random split\ntree, cj(x) we have the following stochastic lower bound for cj(x):\ncj(x)\nd\n≥Binom\n\u0012log (s/(2k −1))\nlog (α−1)\n; π\nd\n\u0013\n. (31)\nBy Chernoﬀ’s inequality, it follows that\nP\n\u0014\ncj (x) ≤π\nd\nlog (s/(2k −1))\nlog (α−1)\n(1 −η)\n\u0015\n≤exp\n\u0014\n−η2\n2\nlog (s/(2k −1))\nπ−1d log (α−1)\n\u0015\n(32)\n=\n\u0012\ns\n2k −1\n\u0013−η2\n2\n1\nπ−1d log(α−1) .",
    "content_hash": "ec20ce4414da4f85236148bcc830954bb06a4628a6b1bc21aecc4b288efc8112",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a40282bb-2f1a-4310-94a0-9341c6baa11e",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Finally, we note that with double-sample trees, all the “s” terms in the above argument\nneed to be replaced by “s/2”; this additional factor 2, however, does not aﬀect the ﬁnal\nresult. 34",
    "content_hash": "c87a6625b4004f0699e5244a8cd9497e44998caca76ff02e3a97a1204866c765",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e754d89e-9780-4ef8-b3b2-824f3b2d6b7d",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Thus, for large s, we ﬁnd that\nE [T (x; Z)] −E\n\u0002\nY\nX = x\n\u0003\f\f ≲d\n\u0012\ns\n2k −1\n\u0013−1\n2\nlog((1−α)−1)\nlog(α−1)\nπ\nd\n×\nsup\nx∈[0, 1]d\n\b\nE\n\u0002\nY\nX = x\n\u0003\n−\ninf\nx∈[0, 1]d\n\b\nE\n\u0002\nY\nX = x\n\u0003\n! ,\nwhere supx E\n\u0002\nY\nX = x\n\u0003\n−infx E\n\u0002\nY\nX = x\n\u0003\n= O (1) because the conditional mean function\nis Lipschitz continuous. Finally, since a forest is just an average of trees, the above result\nalso holds for ˆµ(x). C.2\nBounding the Incrementality of Regression Trees\nProof of Lemma 4. First, we focus on the case where f is constant, i.e., the features Xi have\na uniform distribution over [0, 1]d. To begin, recall that the Si denote selection weights\nT (x; Z) =\ns\nX\ni=1\nSiYi where Si =\n(\n|{i : Xi ∈L(x; Z)}|−1\nif Xi ∈L(x; Z),\n0\nelse,\nwhere L(x; Z) denotes the leaf containing x. We also deﬁne the quantities\nPi = 1 ({Xi is a k-PNN of x}) . Because T is a k-PNN predictor, Pi = 0 implies that Si = 0, and, moreover, |{i : Xi ∈L(x; Z)}| ≥\nk; thus, we can verify that\nE\n\u0002\nS1\nZ1\n\u0003\n≤1\nk E\n\u0002\nP1\nZ1\n\u0003\n.",
    "content_hash": "a48baa5fcca732be2a4d5ceaf593ae44234ad9cb7e02c64c6893c1d730f533a1",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "638e6bae-bf1e-4482-b480-e3fe98a0d625",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Proof of Theorem 3. We begin with two observations. First, by honesty,\nE [T (x; Z)] −E\n\u0002\nY\nX = x\n\u0003\n= E\n\u0002\nE\n\u0002\nY\nX ∈L (x)\n\u0003\n−E\n\u0002\nY\nX = x\n\u0003\u0003\n. Second, by Lipschitz continuity of the conditional mean function,\nE\n\u0002\nY\nX ∈L (x)\n\u0003\n−E\n\u0002\nY\nX = x\n\u0003\f\f ≤C diam (L (x)) ,\nwhere C is the Lipschitz constant. Thus, in order to bound the bias under both Lipschitz\nand honesty assumptions, it suﬃces to bound the average diameter of the leaf L(x). To do so, we start by plugging in η =\np\nlog((1 −α)−1 in the bound from Lemma 2. Thanks our assumption that α ≤0.2, we see that η ≤0.48 and so 0.99 · (1 −η) ≥0.51; thus,\na union bound gives us that, for large enough s,\nP\n\ndiam (L(x)) ≥\n√\nd\n\u0012\ns\n2k −1\n\u0013−0.51\nlog((1−α)−1)\nlog(α−1)\nπ\nd\n\n≤d\n\u0012\ns\n2k −1\n\u0013−1\n2\nlog((1−α)−1)\nlog(α−1)\nπ\nd\n. The Lipschitz assumption lets us bound the bias on the event that that diam(L(x)) satisﬁes\nthis bound.",
    "content_hash": "171f75768da6d75622faa82f00921cf33d59ef535b70cb0e9e01aee7d7ad63ac",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3fcc7712-d3b8-4e81-a432-7e7280765eaa",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "The bulk of the proof involves showing that\nP\n\u0014\nE\n\u0002\nP1\nZ1\n\u0003\n≥1\ns2\n\u0015\n≲k 2d+1 log (s)d\n(d −1)! 1\ns;\n(33)\n35",
    "content_hash": "c0ccb5d304ea392853c24b61004459f6f4ac8d9ca880b78baf311a8b4be79963",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "218c9f9a-ae6a-45af-bf79-32036f2d40b7",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "We know that X1 is a k-PNN of 0 if and\nonly if there are at most 2k −2 other points Xi such that Xij ≤X1j for all j = 1, ..., d\n(because the X have a continuous density, there will almost surely be no ties). Thus,\nEx=0\n\u0002\nP1\nZ1\n\u0003\n= P\n\nBinomial\n\ns −1;\nd\nY\nj=1\nX1j\n\n≤2k −2\n\n\n(35)\n≤\n\u0012 s −1\n2k −2\n\u0013 \n1 −\nd\nY\nj=1\nX1j\n\n\ns−2k+1\n≤s2k−2\n\n1 −\nd\nY\nj=1\nX1j\n\n\ns−2k+1\n,\nwhere the second inequality can be understood as a union bound over all sets of s −2k + 1\nBernoulli realizations that could be simultaneously 0. We can check that X1j\nd= e−Ej where\nEj is a standard exponential random variable, and so\nEx=0\n\u0002\nP1\nZ1\n\u0003\nd\n≤s2k−2\n\n1 −exp\n\n−\nd\nX\nj=1\nEj\n\n\n\n\ns−2k+1\n,\n36",
    "content_hash": "cc204873b79847a952912d417f0f504fba12a0da7c78d626b9de189f7f60c3f5",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6873ae0a-0841-4187-b865-87a8d10ee73b",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "by the above argument, this immediately implies that\nP\n\u0014\nE\n\u0002\nS1\nZ1\n\u0003\n≥\n1\nk s2\n\u0015\n≲k 2d+1 log (s)d\n(d −1)! 1\ns. (34)\nNow, by exchangeability of the indices i, we know that\nE\n\u0002\nE\n\u0002\nS1\nZ1\n\u0003\u0003\n= E [S1] = 1\nsE\n\" s\nX\ni=1\nSi\n#\n= 1\ns,\nmoreover, we can verify that\nP\n\u0014\nE\n\u0002\nS1\nZ1\n\u0003\n≥\n1\nks2\n\u0015\nE\n\u0014\nE\n\u0002\nS1\nZ1\n\u0003 \f\f E\n\u0002\nS1\nZ1\n\u0003\n≥\n1\nks2\n\u0015\n∼1\ns. By Jensen’s inequality, we then see that\nE\nh\nE\n\u0002\nS1\nZ1\n\u00032i\n≥P\n\u0014\nE\n\u0002\nS1\nZ1\n\u0003\n≥\n1\nks2\n\u0015\nE\n\u0014\nE\n\u0002\nS1\nZ1\n\u0003 \f\f E\n\u0002\nS1\nZ1\n\u0003\n≥\n1\nks2\n\u00152\n∼\ns−2\nP\n\u0002\nE\n\u0002\nS1\nZ1\n\u0003\n≥\n1\nks2\n\u0003\nwhich, paired with (34), implies that\nE\nh\nE\n\u0002\nS1\nZ1\n\u00032i\n≳\n(d −1)! 2d+1 log (s)d\n1\nk s. This is equivalent to (20) because E\n\u0002\nE\n\u0002\nS1\nZ1\n\u0003\u00032 = 1/s2 is negligibly small. We now return to establishing (33). Recall that X1, ..., Xs are independently and uni-\nformly distributed over [0, 1]d, and that we are trying to ﬁnd points that are k-PNNs of a\nprediction point x. For now, suppose that x = 0.",
    "content_hash": "fe9b6602f40eddaa6c427ba3faafcb1c66477e2c7a4514e05fde41a68fa257f8",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "994f25a2-fcf6-4422-bc42-8f854db24e9e",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "(36)\nWe thus have obtained a tight expression for our quantity of interested for a prediction point\nat x = 0. In the case x ̸= 0, the ambient space around x can be divided into 2d quadrants. In\norder to check whether Xi is a PNN, we only need to consider other points in the same\nquadrant, as no point in a diﬀerent quadrant can prevent Xi from being a PNN. Now, index\nthe quadrants by l = 1, ..., 2d, and let vl be the volume of the l-th quadrant. By applying\n37",
    "content_hash": "c81319ad85279bb6c1903c3691522d0d20d54e979ae2cce3a59c0dc517142bf5",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "792d94b8-23ea-4783-a00e-e82222033e09",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "where A\nd\n≤B means that B −A ≥0 almost surely. Thus,\nPx=0\n\u0014\nE\n\u0002\nP1\nZ1\n\u0003\n≥1\ns2\n\u0015\n≤P\n\ns(2k−2)\n\n1 −exp\n\n−\nd\nX\nj=1\nEj\n\n\n\n\ns−2k+1\n≥1\ns2\n\n\n= P\n\nexp\n\n−\nd\nX\nj=1\nEj\n\n≤1 −\n\u0012 1\ns2k\n\u0013\n1\ns−2k+1\n\n\n= P\n\n\nd\nX\nj=1\nEj ≥−log\n\u0012\n1 −exp\n\u0014\n−2k\nlog (s)\ns −2k + 1\n\u0015\u0013\n. Notice that this quantity goes to zero as s gets large. The sum of d standard exponential\nrandom variables has a gamma distribution with shape d and scale 1, and\nP\n\n\nd\nX\nj=1\nEj ≥c\n\n= Γ (d, c)\n(d −1)!,\nwhere Γ is the upper incomplete gamma function. It is well known that\nlim\nc→∞\nΓ (d, c)\ncd−1 e−c = 1,\nand so\nPx=0\n\u0014\nE\n\u0002\nP1\nZ1\n\u0003\n≥1\ns2\n\u0015\n≲\n\u0010\n−log\n\u0010\n1 −exp\nh\n−2k\nlog(s)\ns−2k+1\ni\u0011\u0011d−1 \u0010\n1 −exp\nh\n−2k\nlog(s)\ns−2k+1\ni\u0011\n(d −1)! . We can check that\n1 −exp\n\u0014\n−2k\nlog (s)\ns −2k + 1\n\u0015\n∼2k log (s)\ns\n,\nletting us simplify the above expression to\nPx=0\n\u0014\nE\n\u0002\nP1\nZ1\n\u0003\n≥1\ns2\n\u0015\n≲\n2k\n(d −1)! log (s)d\ns\n.",
    "content_hash": "653b49b05eb3630bccaa49d4b03a17824000286f114978685d907f61c2d16c90",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9f132af6-f041-484e-b333-519644cc76df",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Moreover, by Theorem 3, we know that\nE\n\u0002\nYi\nXi ∈L (x; Z)\n\u0003\n→p E\n\u0002\nY\nX = x\n\u0003\n, and\nE\n\u0002\nY 2\ni\nXi ∈L (x; Z)\n\u0003\n→p E\n\u0002\nY 2 \f\f X = x\n\u0003\n,\nand so\nk Var [T (x; Z)] ≤|{i : Xi ∈L (x; Z)}| · Var [T (x; Z)] →p Var\n\u0002\nY\nX = x\n\u0003\n,\nbecause k remains ﬁxed while the leaf size gets smaller. Thus, we conclude that\nVar\nh\n˚\nT (x; Z)\ni\nVar [T (x; Z)] ≳k s Var\n\u0002\nE\n\u0002\nT (x; Z)\nZ1\n\u0003\u0003\nVar\n\u0002\nY\nX = x\n\u0003\n≳ν (s) ,\nas claimed. 38",
    "content_hash": "42bf5f00bcbc372eb166ceed8e5bfc7f68b24d32178697997034cf7aaf91e0a6",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e947b110-2e9f-4ad6-a215-87e794ec2e1a",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "(36) on the l-th quadrant alone, we see that the probability of E\n\u0002\nP1\nZ1\n\u0003\n≥\n1\ns2 given that\nX1 is in the l-the quadrant is asymptotically bounded on the order of\n2k\n(d −1)! log (s)d\nvl s\n. Summing over all quadrants, we ﬁnd that\nPx=0\n\u0014\nE\n\u0002\nP1\nZ1\n\u0003\n≥1\ns2\n\u0015\n≲\nX\n{l:vl>0}\nvl\n2k\n(d −1)! log (s)d\nvl s\n= |{l : vl > 0}|\n2k\n(d −1)! log (s)d\ns\n≤k\n2d+1\n(d −1)! log (s)d\ns\n,\nthus establishing (33). Finally, to generalize to bounded densities f, we note that if f(x) ≤C\nfor all x ∈[0, 1]d, then\nEx=0\n\u0002\nP1\nZ1\n\u0003\n≤P\n\nBinomial\n\ns −1; C\nd\nY\nj=1\nX1j\n\n≤2k −2\n\n,\nand the previous argument goes though. Proof of Theorem 5. Our main task is to show that if T is a regular tree and Var\n\u0002\nY\nX = x\n\u0003\n>\n0, then\nVar\n\u0002\nE\n\u0002\nT (x; Z)\nZ1\n\u0003\u0003\n≳Var\n\u0002\nE\n\u0002\nS1\nZ1\n\u0003\u0003\nVar\n\u0002\nY\nX = x\n\u0003\n. (37)\nGiven this result, Lemma 4 then implies that\nVar\n\u0002\nE\n\u0002\nT (x; Z)\nZ1\n\u0003\u0003\n≳1\nk\nν (s)\ns\nVar\n\u0002\nY\nX = x\n\u0003\n.",
    "content_hash": "6fb0ce686e9961e115ac287ed8c3d66aeb3563d0efd093891e00626e2dbb3c96",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b83f14ce-dbac-47eb-802b-f3b3fd0b1315",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Thus,\nby Lemma 2, the variance of the diﬀerence between E\n\u0002\nT ′ \f\f Z1\n\u0003\nand E\n\u0002\nT\nZ1\n\u0003\ndecays faster\nthan the target rate (38), and so\nVar\n\u0002\nE\n\u0002\nT (x; Z)\nZ1\n\u0003\u0003\n∼Var\n\u0002\nE\n\u0002\nT ′ (x; Z)\nZ1\n\u0003\u0003\n,\nprovided that T ′ satisﬁes (37), as we will see it does. By the same argument, we also note\nthat\nVar\n\u0002\nE\n\u0002\nS′\n1\nZ1\n\u0003\u0003\n∼Var\n\u0002\nE\n\u0002\nS1\nZ1\n\u0003\u0003\n. We can now proceed to analyze T ′ instead of T. Recall that our goal is to provide a lower bound on the variance of the expectation of\nT ′(x; Z) conditionally on Z1. First, an elementary decomposition shows that\nVar\n\u0002\nE\n\u0002\nT ′(x; Z)\nZ1\n\u0003\u0003\n= Var\n\u0002\nE\n\u0002\nT ′(x; Z)\nX1\n\u0003\u0003\n+ Var\n\u0002\nE\n\u0002\nT ′(x; Z)\nX1, Y1\n\u0003\n−E\n\u0002\nT ′(x; Z)\nX1\n\u0003\u0003\n≥Var\n\u0002\nE\n\u0002\nT ′(x; Z)\nX1, Y1\n\u0003\n−E\n\u0002\nT ′(x; Z)\nX1\n\u0003\u0003\n,\nand so it suﬃces to provide a lower bound for the latter term. Next we note that, thanks\nto honesty as in Deﬁnition 2, part (a), and i.i.d. sampling,\nE\n\u0002\nT ′(x; Z)\nX1, Y1\n\u0003\n−E\n\u0002\nT ′(x; Z)\nX1\n\u0003\n= E\n\u0002\nS′\n1\nX1\n\u0003 Y1 −E\n\u0002\nY1\nX1\n\u0003\u0001\n.",
    "content_hash": "00ca3c57e00ac4bd8cd0d99b289cfa1756cd4905c878339452ddb2ea354d43d6",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9ad75c4c-3d85-4eac-8e34-1dbd2337b7ba",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Because of honesty and our Lipschitz assumption, the above implies that\nVar\n\u0002\nE\n\u0002\nT ′(x; Z)\nX1, Y1\n\u0003\n−E\n\u0002\nT ′(x; Z)\nX1\n\u0003\u0003\n= Var\n\u0002\nE\n\u0002\nS′\n1\nX1\n\u0003\n(Y1 −µ(x))\n\u0003\n+ O\nE\n\u0002\nS′2\n1\n\u0003\ns−2ω\u0001\n,\n39",
    "content_hash": "58f68dd8b99cbe61333527ede1c196eb4b10378be090d5690bcc8bcd0371f2e7",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6cdf4f70-d016-4c35-ae10-d2e8e86ad424",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Suppose we have any symmetric function T : Ωn →R,\nand suppose that Z1, ..., Zn are independent and identically distributed on Ωsuch that\nVar [T(Z1, ..., Zn)] < ∞. Then Efron and Stein [1981] show that there exist functions\nT1, ..., Tn such that\nT (Z1, ..., Zn) = E [T] +\nn\nX\ni=1\nT1 (Zi) +\nX\ni<j\nT2 (Zi, Zj) + ... + Tn (Z1, ..., Zn) ,\n(40)\n40",
    "content_hash": "f91005f506f2d9d1234d4c16bbda762a9a9f9ddf3fc1e58eddd3f70f49897513",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f6db932e-b26e-4dea-8e9a-8d4f5fdf793c",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "For a double sample tree, we start by noting that\nVar\nh\n˚\nT\ni\n= s Var\n\u0002\nE\n\u0002\nT\nZ1\n\u0003\u0003\n= s Var\n\u0002\nE\n\u0002\n1 ({1 ∈I}) T\nZ1\n\u0003\n+ E\n\u0002\n1 ({1 ̸∈I}) T\nZ1\n\u0003\u0003\n≥s\n2 Var\n\u0002\nE\n\u0002\n1 ({1 ∈I}) T\nZ1\n\u0003\u0003\n−s Var\n\u0002\nE\n\u0002\n1 ({1 ̸∈I}) T\nZ1\n\u0003\u0003\n∼s\n8 Var\n\u0002\nE\n\u0002\nT\nZ1\n\u0003 \f\f 1 ∈I\n\u0003\n−s\n4 Var\n\u0002\nE\n\u0002\nT\nZ1\n\u0003 \f\f 1 ̸∈I\n\u0003\n,\nwhere to verify the last line we note that P\n\u0002\n1 ∈I\nZ1\n\u0003\n= ⌊s/2⌋regardless of Z1. Now, an\nimmediate application of Theorem 5 shows us that\n⌊s/2⌋Var\n\u0002\nE\n\u0002\nT\nZ1\n\u0003 \f\f 1 ∈I\n\u0003\n≳Cf, d\n\u000e\nlog (s)d Var [T] ,\nwhich corresponds to the rate we seek to establish. Meanwhile, by standard results going\nback to Hoeﬀding [1948],\n⌈s/2⌉Var\n\u0002\nE\n\u0002\nT\nZ1\n\u0003 \f\f 1 ̸∈I\n\u0003\n≤Var\n\u0002\nE\n\u0002\nT\n{Zj : j ̸∈I}\n\u0003 \f\f I\n\u0003\n;\nthen, Lemma 2 and the argument used to establish Theorem 3 imply that\nVar\n\u0002\nE\n\u0002\nT\n{Zj : j ̸∈I}\n\u0003 \f\f I\n\u0003\n= O\ns\n−\nlog((1−α)−1)\nlog(α−1)\nπ\nd\n! ,\nand so the term arising under the 1 ̸∈I condition is negligibly small. C.3\nProperties of Subsampled Incremental Base Learners\nThe results presented in this section rely heavily on the Efron-Stein ANOVA decomposition,\nsummarized here for convenience.",
    "content_hash": "d306a074bd70b1be5d5d357a8e5a7f73cdbfa4fad0545be52a1097ea06aec695",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f0fa8b10-595d-4691-bbe9-9e77225c572e",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "where we note that the error term decays as s−(1+2ω), which will be prove to be negligible\nrelative to the main term. Finally, we can verify that\nVar\n\u0002\nE\n\u0002\nS′\n1\nX1\n\u0003\n(Y1 −µ(x))\n\u0003\n(39)\n= E\nh\nE\n\u0002\nS′\n1\nX1\n\u00032 E\nh\n(Y1 −µ (x))2 \f\f X1\nii\n−E\n\u0002\nE\n\u0002\nS′\n1\nX1\n\u0003\nE\n\u0002\nY1 −µ(x)\nX1\n\u0003\u00032 . Now, because the ﬁrst two conditional moments of Y given X are Lipschitz, and since\nE\n\u0002\nS′\n1\nX1\n\u0003\nis 0 for ∥X1 −x∥2 > s−ω thanks to our truncating argument, we see that\nE\nh\nE\n\u0002\nS′\n1\nX1\n\u00032 E\nh\n(Y1 −µ (x))2 \f\f X1\nii\n∼E\nh\nE\n\u0002\nS′\n1\nX1\n\u00032i\nVar\n\u0002\nY\nX = x\n\u0003\n∼E\nh\nE\n\u0002\nS1\nX1\n\u00032i\nVar\n\u0002\nY\nX = x\n\u0003\n. Meanwhile, the second term in the expansion (39) is of order 1/s2 and thus negligible. To\nrecap, we have shown that a version of (37) holds with T replaced by T ′; and so (37) must\nalso hold thanks to the previously established coupling result. Proof of Corollary 6.",
    "content_hash": "09f716f55a638f08e3df35ec6de38db934635ec36dbeda14c6e2862245475302",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "659d4db1-5577-4367-a7a6-bdb3fef70aab",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Now, we can also show that the H´ajek projection of ˆµ is\n˚ˆµ (x; Z1, ..., Zn) = E [T] + s\nn\nn\nX\ni=1\nT1 (Zi) . As with all projections [Van der Vaart, 2000],\nE\n\u0014\u0010\nˆµ(x) −˚ˆµ(x)\n\u00112\u0015\n= Var\nh\nˆµ(x) −˚ˆµ(x)\ni\n. Recall that the Tk (·) are all pairwise uncorrelated. Thus, using the notation sk = s ·\n(s −1) · · · (s −k) it follows that\nE\n\u0014\u0010\nˆµ(x) −˚ˆµ(x)\n\u00112\u0015\n=\ns\nX\nk=2\n\u0012 sk\nnk\n\u00132 \u0012n\nk\n\u0013\nVk,\n=\ns\nX\nk=2\nsk\nnk\n\u0012s\nk\n\u0013\nVk,\n≤s2\nn2\ns\nX\nk=2\n\u0012s\nk\n\u0013\nVk,\n≤s2\nn2\nVar [T] ,\nwhere on the last line we used (41). We recover the stated result by noticing that s2/n2 ≤\ns2/n2 for all 2 ≤s ≤n. 41",
    "content_hash": "3a45ce56c04816dd9b3635d697e912840a9b93c3aafd46d2b39401dd68e79a10",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "47453616-737b-480a-9bbc-ca32cd178db9",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "and that all 2n −1 random variables on the right side of the above expression are all mean-\nzero and uncorrelated. It immediately follows that\nVar [T] =\nn\nX\nk=1\n\u0012n\nk\n\u0013\nVk, where Vk = Var [Tk (Z1, ..., Zk)] . (41)\nFor our purposes, it is also useful to note that the H´ajek projection ˚\nT can be written as\n˚\nT (Z1, ..., Zn) = E [T] +\nn\nX\ni=1\nT1 (Zi) , and Var\nh\n˚\nT\ni\n= n V1. Thus, the ANOVA decomposition provides a convenient abstract framework for analyzing\nour quantities of interest. Proof of Lemma 7. Applying the ANOVA decomposition to the individual trees T, we see\nthat a random forest estimator ˆµ(x) of the form (12) can equivalently be written as\nˆµ (x; Z1, ..., Zn) = E [T] +\n\u0012n\ns\n\u0013−1 \u0012n −1\ns −1\n\u0013\nn\nX\ni=1\nT1 (Zi)\n+\n\u0012n −2\ns −2\n\u0013 X\ni<j\nT2 (Zi, Zj) + ... +\nX\ni1<...<is\nTs (Zi1, ..., Zis)\n! . The above formula holds because each training point Zi appears in\nn−1\ns−1\n\u0001\nout of\nn\ns\n\u0001\npossible\nsubsamples of size s, each pair (Zi, Zj) appears is\nn−2\ns−2\n\u0001\nsubsets, etc.",
    "content_hash": "a7814f89e3937d5ed9566d2930ce2e159a37f885ca07ab16212541a87155480a",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "88d461f7-9e34-4fb8-932e-ce52bc86fe99",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Thanks to honesty, we can\nverify that for any index i > 1, Yi is independent of Si conditionally on Xi and Z1, and so\nE\n\u0002\nT\nZ1\n\u0003\n−E [T] = E\n\u0002\nS1\nY1 −E\n\u0002\nY1\nX1\n\u0003\u0001 \f\f Z1\n\u0003\n+\nE\n\" n\nX\ni=1\nSiE\n\u0002\nYi\nXi\n\u0003 \f\f Z1\n#\n−E [T]\n! . Note that the two right-hand-side terms above are both mean-zero. By Jensen’s inequality,\nwe also have that\n2−(1+δ) E\nh\f\fE\n\u0002\nT\nZ1\n\u0003\n−E [T]\n2+δi\n≤E\nh\f\fE\n\u0002\nS1\nY1 −E\n\u0002\nY1\nX1\n\u0003\u0001 \f\f Z1\n\u0003\f\f2+δi\n+ E\n\n\nE\n\" n\nX\ni=1\nSiE\n\u0002\nYi\nXi\n\u0003 \f\f Z1\n#\n−E [T]\n2+δ\n. Now, again by honesty, E\n\u0002\nS1\nZ1\n\u0003\n= E\n\u0002\nS1\nX1\n\u0003\n, and so our uniform (2+δ)-moment bounds\non the distribution of Yi conditional on Xi implies that\nE\nh\f\fE\n\u0002\nS1\nY1 −E\n\u0002\nY1\nX1\n\u0003\u0001 \f\f Z1\n\u0003\f\f2+δi\n= E\nh\nE\n\u0002\nS1\nX1\n\u00032+δ \f\fY1 −E\n\u0002\nY1\nX1\n\u0003\f\f2+δi\n≤ME\nh\nE\n\u0002\nS1\nX1\n\u00032+δi\n≤ME\nh\nE\n\u0002\nS1\nX1\n\u00032i\n,\n42",
    "content_hash": "efed46cab2cbf1b02b48744434ef58133c62c302094bda1252fc1123b37f79ec",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "dd33a253-02da-4a1f-bf88-3821230c75f2",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Proof of Theorem 8. Using notation from the previous lemma, let σ2\nn = s2/n V1 be the\nvariance of ˚ˆµ. We know that\nσ2\nn = s\nn sV1 ≤s\nn Var [T] ,\nand so σn →0 as desired. Now, by Theorem 5 or Corollary 6 combined with Lemma 7, we\nﬁnd that\n1\nσ2n\nE\n\u0014\u0010\nˆµ (x) −˚ˆµ (x)\n\u00112\u0015\n≤\n\u0010 s\nn\n\u00112 Var [T]\nσ2n\n= s\nn Var [T]\n\u000e\nVar\nh\n˚\nT\ni\n≲s\nn\nlog (s)d\nCf, d/4\n→0\nby hypothesis. Thus, by Slutsky’s lemma, it suﬃces to show that (22) holds for the H´ajek\nprojection of the random forest ˚ˆµ(x). By our deﬁnition of σn, all we need to do is check that ˚ˆµ is asymptotically normal. One\nway to do so is using the Lyapunov central limit theorem [e.g., Billingsley, 2008]. Writing\n˚ˆµ(x) = s\nn\nn\nX\ni=1\nE\n\u0002\nT\nZi\n\u0003\n−E [T]\n\u0001\n,\nit suﬃces to check that\nlim\nn→∞\nn\nX\ni=1\nE\nh\f\fE\n\u0002\nT\nZi\n\u0003\n−E [T]\n2+δi \u000e\nn\nX\ni=1\nVar\n\u0002\nE\n\u0002\nT\nZi\n\u0003\u0003\n!1+δ/2\n= 0. (42)\nUsing notation from Section 3.3.1, we write T = Pn\ni=1 SiYi.",
    "content_hash": "ebbf223b925662a7b081e04b1400031b3071eac78e226e3f8c388602e9e652b0",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "67a95130-ea2d-4e70-80ea-7f46096b8414",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Then,\nthe variance σ2\nn of the H´ajek projection of ˆµ(x) is\nσ2\nn =\nn\nX\ni=1\nEZ∼F\n\u0002\nˆµ(x)\nZi\n\u0003\n−EZ∼F [ˆµ(x)]\n\u00012\n= s2\nn2\nn\nX\ni=1\nEZ∼F\n\u0002\nT\nZi\n\u0003\n−EZ∼F [T]\n\u00012 ,\nwhereas we can check that the inﬁnitesimal jackknife as deﬁned in (8) is equal to\nbVIJ = n −1\nn\n\u0012\nn\nn −s\n\u00132 s2\nn2\nn\nX\ni=1\nEZ∗⊂b\nF\n\u0002\nT\nZ∗\n1 = Zi\n\u0003\n−EZ∗⊂b\nF [T]\n\u00012 ,\nwhere bF is the empirical distribution on {Z1, ..., Zn}. Recall that we are sampling the Z∗\nfrom bF without replacement. It is useful to write our expression of interest bVIJ using the H´ajek projection ˚\nT of T:\nbVIJ = n −1\nn\n\u0012\nn\nn −s\n\u00132 s2\nn2\nn\nX\ni=1\n(Ai + Ri)2 , where\nAi = EZ∗⊂b\nF\nh\n˚\nT\nZ∗\n1 = Zi\ni\n−EZ∗⊂b\nF\nh\n˚\nT\ni\nand\nRi = EZ∗⊂b\nF\nh\nT −˚\nT\nZ∗\n1 = Zi\ni\n−EZ∗⊂b\nF\nh\nT −˚\nT\ni\n. 43",
    "content_hash": "d6daf7e1bd2e02641f0a7ebe85d01d426197316c4aba20230a67c81ccd6a702e",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3e6a34d5-4ce4-456e-ab17-c0e3a90eb14b",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "We can write\nAi = EZ∗⊂b\nF\nh\n˚\nT\nZ∗\n1 = Zi\ni\n−EZ∗⊂b\nF\nh\n˚\nT\ni\n=\n\u0010\n1 −s\nn\n\u0011\nT1 (Zi) +\n\u0012 s −1\nn −1 −s\nn\n\u0013 X\nj̸=i\nT1 (Zj) ,\nand so our sum of interest is asymptotically unbiased for σ2\nn:\nE\n\"\nn −1\nn\n\u0012\nn\nn −s\n\u00132 s2\nn2\nn\nX\ni=1\nA2\ni\n#\n= s2\nn E\nh\nT1 (Z)2i\n= s\nn Var\nh\n˚\nT (Z1, ..., Zs)\ni\n= σ2\nn. Finally, to establish concentration, we ﬁrst note that the above calculation also implies that\nσ−2\nn\ns2\nn2\nPn\ni=1 (Ai −T1(Zi))2 →p 0. Meanwhile, following the argumentation in the proof of\nTheorem 8, we can apply (2 + δ)-moment bounds on Yi −E\n\u0002\nYi\nXi\n\u0003\nto verify that\nlim\nu→∞lim\nn→∞\nE\n\u0002\nT 2\n1 (Z1)\n\u0003\n−E\n\u0002\nmin\n\b\nu, T 2\n1 (Z1)\n\u0003\u0001\n= 0,\nand so we obtain can apply a truncation-based argument to derive a weak law of large\nnumbers for triangular arrays for σ−2\nn\ns2\nn2\nPn\ni=1 T 2\n1 (Zi). 44",
    "content_hash": "1f4765321a3707ec7f62793c30b2d2270b5fa0c5ce7eee832ef82834695bbd1c",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "95a344e4-9f4e-4db1-bbbd-50b331d70766",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Next, to bound the bias itself, we start by applying unconfoundedness as in (25);\n46",
    "content_hash": "76e78cc984a9c185cc9712ff01cc1ec364ba406d4b0baff2f310b0a109c1fa9e",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2adb4749-500a-4a85-bc0f-ece247c62b1c",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Proof of Proposition 10. Let N ∗\ni denote whether or not the i-training example was used for\na subsample, as in (13). For trivial trees\nT(x; ξ, Zi1, ..., Zis) = 1\ns\ns\nX\nj=1\nYij\nwe can verify that for any i = 1, ..., n, E∗[ˆµ∗] E [N ∗\ni ] = s/n Y ,\nE∗[ˆµ∗N ∗\n1 ] = s\nn\n\u0012Yi\ns + s −1\ns\nn Y −Yi\nn −1\n\u0013\n= 1\nn\nn −s\nn −1 Yi + s −1\nn −1 Y , and\nCov∗[ˆµ∗, N ∗\ni ] = 1\nn\nn −s\nn −1 Yi +\n\u0012 s −1\nn −1 −s\nn\n\u0013\nY =\n1\nn −1\nn −s\nn\nYi −Y\n\u0001\n. Thus, we ﬁnd that\nbVIJ = n −1\nn\n\u0012\nn\nn −s\n\u00132\nn\nX\ni=1\nCov∗[ˆµ∗, N ∗\ni ]2\n=\n1\nn (n −1)\nn\nX\ni=1\nYi −Y\n\u00012\n= bVsimple,\nas we sought to verify. C.4\nExtension to Causal Forests\nProof of Theorem 11. Our argument mirrors the proof of Theorem 1. The main steps involve\nbounding the bias of causal forests with an analogue to Theorem 3 and their incrementality\nusing an analogue to Theorem 5. In general, we ﬁnd that the same arguments as used\nwith regression forests go through, but the constants in the results get worse by a factor\nε depending on the amount of overlap (6).",
    "content_hash": "404b1843b48ff7ff3144c23aeba3fcb02840068166a0a0d464568994a9cdb057",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4c91c61e-7626-4f4f-a965-feea2149bc1e",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "Given these results, the subsampling-based\nargument from Section 3.3.2 can be reproduced almost verbatim, and the ﬁnal proof of\nTheorem 11 is identical to that of Theorem 1 presented at the beginning of Appendix C. Bias. Under the conditions of Lemma 2,\nsuppose that E\n\u0002\nY (0) \f\f X = x\n\u0003\nand\nE\n\u0002\nY (1) \f\f X = x\n\u0003\nare Lipschitz continuous, that the trees Γ comprising the random forest\nare honest, and, moreover, that the overlap condition (6) holds for some ε > 0. These con-\nditions also imply that |E[Y (0) \f\f X = x]|, |E[Y (1) \f\f X = x]| ≤M for some constant M, for all\nx ∈[0, 1]d. Then, provided that α ≤0.2, the bias of the random forest at x is bounded by\n|E [ˆτ (x)] −τ (x)| ≲2M d\n\u0012\nε s\n2k −1\n\u0013−1\n2\nlog((1−α)−1)\nlog(α−1)\nπ\nd\n. To establish this claim, we ﬁrst seek with an analogue to Lemma 2, except now s in (31) is\nreplaced by smin, i.e., the minimum of the number of cases (i.e., observations with Wi = 1)\nor controls (i.e., observations with Wi = 0) in the sample. A straight-forward computation\nthen shows that smin/s ≳ε, and that a variant of (32) where we replace s with εs still holds\nfor large s.",
    "content_hash": "7cce4f20bee803b253ad49afbcfab399e0ad9fdfe95087b3e722fbbdd4fbfb07",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b1a74d42-f4eb-462b-8704-228f58841af4",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "then, the argument of Theorem 3 goes through without modiﬁcations, provided we replace\nevery instance of “s” with “εs”. Incrementality. Suppose that the conditions of Lemma 4 hold and that Γ is an hon-\nest k-regular causal tree in the sense of Deﬁnitions 2b and 4b. Suppose moreover that\nE\n\u0002\nY (0/1) \f\f X = x\n\u0003\nand Var\n\u0002\nY (0/1) \f\f X = x\n\u0003\nare all Lipschitz continuous at x, and that\nVar\n\u0002\nY\nX = x\n\u0003\n> 0. Suppose, ﬁnally, that the overlap condition (6) holds with ε > 0. Then T is ν (s)-incremental at x with\nν (s) = ε Cf, d\n\u000e\nlog (s)d,\nwhere Cf, d is the constant from Lemma 4. To prove this claim, we again focus on the case where f(x) = 1, in which case we use\nCf, d = 2−(d+1) (d −1)!. We begin by setting up notation as in the proof of Lemma 4.",
    "content_hash": "73f28e9f86e87c3c3db27f7fef27655c29dc7c2bca68fe25d076eabe0dcf9c8a",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b40e39e5-c889-4ce9-be55-857413e40803",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "We\nwrite our causal tree as Γ (x; Z) = Ps\ni=1 SiYi, where\nSi =\n\n\n\n\n\n|{i : Xi ∈L(x; Z)} , Wi = 1|−1\nif Xi ∈L(x; Z) and Wi = 1,\n−|{i : Xi ∈L(x; Z)} , Wi = 0|−1\nif Xi ∈L(x; Z) and Wi = 0,\n0\nelse,\nwhere L(x; Z) denotes the leaf containing x, and let\nP W\ni\n= 1 ({Xi is a k-PNN of x among points with treatment status Wi}) . Finally, in a break from Lemma 4, deﬁne wmin(x; Z) as the minority class within the leaf\nL(x; Z); more formally,\nwmin = 1 ({|{i : Xi ∈L(x; Z)} , Wi = 1| ≤|{i : Xi ∈L(x; Z)} , Wi = 0|}) . By regularity of Γ, we know that the leaf L(x; Z) can contain at most 2k −1 examples from\nits minority class, and so P W\ni\n= 0 and W = wmin together imply that Si = 0. Thus, we can\nverify that\nE\n\u0002\n|S1| 1 ({W1 = wmin})\nZ1\n\u0003\n≤1\nk E\n\u0002\nP W\n1\nZ1\n\u0003\n. We are now ready to use the same machinery as before.",
    "content_hash": "16e0a668861ad70ebece0911ef8efb56e127749c2b46dce2ad093662ac05c71d",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "17ddbfb5-0527-43b7-9b09-77dc70bbae31",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "The random variables P W\n1\nnow\nsatisfy\nP\n\"\nE\n\u0002\nP W\n1\nZ1\n\u0003\n≥\n1\ns2 P [W = W1]2\n#\n≲k 2d+1 log (s)d\n(d −1)! 1\ns P [W = W1];\nby the above argument and ε-overlap (6), this immediately implies that\nP\n\u0014\nE\n\u0002\n|S1| 1\n{W1 = wmin}\nZ1\n\u0001\u0003\n≥\n1\nk ε2 s2\n\u0015\n≲k 2d+1 log (s)d\n(d −1)! 1\nεs. By construction, we know that\nE\n\u0002\nE\n\u0002\n|S1| 1 ({W1 = wmin})\nZ1\n\u0003\u0003\n= E [|S1| 1 ({W1 = wmin})] = 1\ns,\nwhich by the same argument as before implies that\nE\nh\nE\n\u0002\n|S1| 1 ({W1 = wmin})\nZ1\n\u00032i\n≳\n(d −1)! 2d+1 log (s)d\nε\nk s. 47",
    "content_hash": "9fdc89c8bdd6d82427a0ffdc4e1a47e14dfd74c412426a4970e0fa211832d05c",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a7caa812-7a96-4ca4-bbc7-012a28aaf5eb",
    "source_id": "6e46afef-b380-4517-8964-d8a3cdae49ae",
    "content": "By monotonicity, we then conclude that\nE\nh\nE\n\u0002\nS1\nZ1\n\u00032i\n= E\nh\nE\n\u0002\n|S1|\nZ1\n\u00032i\n≳\n(d −1)!\n2d+1 log (s)d\nε\nk s.\nThe second part of the proof follows from a straight-forward adaptation of Theorem 5.\n48",
    "content_hash": "27301a7cd35eba9171487339734d3c8a895fd7cf376ff35e4a43126204c29da6",
    "location": null,
    "page_start": 48,
    "page_end": 48,
    "metadata": {
      "section": "The second part of the proof follows from a straight-forward adaptation of Theorem",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "61555f06-a149-4eea-b959-b91b68a9b4a0",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "MIT press,\n2010. Xindong Wu, Vipin Kumar, J Ross Quinlan, Joydeep Ghosh, Qiang Yang, Hiroshi Motoda,\nGeoﬀrey J McLachlan, Angus Ng, Bing Liu, S Yu Philip, et al. Top 10 algorithms in data\nmining. Knowledge and information systems, 14(1):1–37, 2008. Achim Zeileis, Torsten Hothorn, and Kurt Hornik. Model-based recursive partitioning. Journal of Computational and Graphical Statistics, 17(2):492–514, 2008. [60]",
    "content_hash": "6f1a3bb2ec197b7138ea05ecf065aed5cbc1f436907620b7b68929eb6275b7a1",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9f6810f2-eeb8-46b7-822c-8dba5dfde7e6",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "We can think of there being two matrices with potential outcomes,\nY(0) =\n\n\n\n\n\n\n\n? ? Y13\n. . . ? Y21\nY22\n? . . . Y2T\n? Y32\n? . . . Y3T\n... ... ... ... ... ? YN2\n? . . . YNT\n\n\n\n\n\n\n\n(potential control outcome),\nand\nY(1) =\n\n\n\n\n\n\n\nY11\nY12\n? . . . Y1T\n? ? Y23\n. . . ? Y31\n? Y33\n. . . ? ... ... ... ... ... YN1\n? YN3\n. . . ? \n\n\n\n\n\n\n(potential treated outcome). Now the problem of estimating causal eﬀects becomes one of imputing missing values in a\nmatrix. The ML literature has developed eﬀective methods for matrix completion in settings with\nboth N and T large, and a large fraction of missing data. We discuss some of these methods\nin the next section, as well as their relation to the econometrics literature. 8.2\nMatrix Completion Methods for Panel Data\nThe matrix completion literature has focused on using low rank representations for the\ncomplete data matrix. Let us consider the case without covariates, that is, no characteristics\nof the units or time periods. Let L be the matrix of expected values, and Y the observed\ndata matrix. The observed values are assumed to be equal to the corresponding values of\nthe complete data matrix, possibly with error:\nYit =\n\u001a Lit + εit\nif Wit = 1,\n0\notherwise.",
    "content_hash": "7082655252e59411e9a07484a1854f87522e21be8c88298a930d348f6047fc72",
    "location": null,
    "page_start": 1,
    "page_end": 44,
    "metadata": {
      "section": "Control Methods",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f89ec176-f321-4d5e-b359-24436b043da1",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Machine Learning Methods Economists Should Know\nAbout∗\nSusan Athey†\nGuido W. Imbens‡\nMarch 2019\nAbstract\nWe discuss the relevance of the recent Machine Learning (ML) literature for eco-\nnomics and econometrics. First we discuss the diﬀerences in goals, methods and settings\nbetween the ML literature and the traditional econometrics and statistics literatures.\nThen we discuss some speciﬁc methods from the machine learning literature that we\nview as important for empirical researchers in economics. These include supervised\nlearning methods for regression and classiﬁcation, unsupervised learning methods, as\nwell as matrix completion methods. Finally, we highlight newly developed methods\nat the intersection of ML and econometrics, methods that typically perform better\nthan either oﬀ-the-shelf ML or more traditional econometric methods when applied to\nparticular classes of problems, problems that include causal inference for average treat-\nment eﬀects, optimal policy estimation, and estimation of the counterfactual eﬀect of\nprice changes in consumer choice models.\n∗We are grateful to Sylvia Klosin for comments. This research was generously supported by\nONR grant N00014-17-1-2131 and the Sloan Foundation.\n†Professor of Economics, Graduate School of Business, Stanford University, SIEPR, and NBER,\nathey@stanford.edu.\n‡Professor of Economics, Graduate School of Business and Department of Economics, Stanford\nUniversity, SIEPR, and NBER, imbens@stanford.edu.",
    "content_hash": "eec65e000b805f34f08a4504cafd257d83e37045ff7f0e4b7ab9a290e634a411",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "Machine Learning Methods Economists Should Know",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "08237f47-bef0-4589-bbcf-325fe1922652",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "with an intercept, and a scalar Xi, we could estimate the model with only an intercept. Certainly if β = 0, that model would lead to better predictions. By the same argument,\nif the true value of β were close, but not exactly equal, to, zero, we would still do better\nleaving Xi out of the regression. Out-of-sample cross-validation can help guide such decisions. There are two components of the problem that are important for this ability. First, the goal\nis predictive power, rather than estimation of a particular structural or causal parameter. Second, the method uses out-of-sample comparisons, rather than in-sample goodness-of-ﬁt\nmeasures. This ensures that we obtain unbiased comparisons of the ﬁt. 2.4\nOver-ﬁtting, Regularization, and Tuning Parameters\nThe ML literature is much more concerned with over-ﬁtting than the standard statistics\nor econometrics literature. Researchers attempt to select ﬂexible models that ﬁt well, but\nnot so well that out-of-sample prediction is compromised. There is much less emphasis on\nformal results that particular methods are superior in large samples (asymptotically), instead\nmethods are compared on speciﬁc data sets to see “what works well.” A key concept is that\nof regularization.",
    "content_hash": "bab93f8fe9f0f71984333e60e807e31a95b4ebcaac830598566f48f52b5aaba3",
    "location": null,
    "page_start": 1,
    "page_end": 8,
    "metadata": {
      "section": "Validation and Cross-validation",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0df6a94a-19f3-4e04-9bea-69850cb3f1df",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "subject to pRF + pNN + pLASSO = 1,\nand pRF, pNN, pLASSO ≥0. One may also estimate weights based on regression of the outcomes in the test sample on\nthe predictors from the diﬀerent models without imposing that the weights sum to one and\nare non-negative. Because random forests, neural nets, and lasso have distinct strengths\nand weaknesses, in terms of how well they deal with the presence of irrelevant features,\nnonlinearities, and interactions. As a result averaging over these models may lead to out-of-\nsample predictions that are strictly better than predictions based on a single model. In a panel data context Athey et al. [2019] use ensemble methods combining various\nforms of synthetic control and matrix completion methods and ﬁnd that the combinations\noutperform the individual methods. 2.8\nInference\nThe ML literature has focused heavily on out-of-sample performance as the criterion of inter-\nest. This has come at the expense of one of the concerns that the statistics and econometrics\nliterature have traditionally focused on, namely the ability to do inference, e.g., construct\nconﬁdence intervals that are valid, at least in large samples. Efron and Hastie write:\n“Prediction, perhaps because of its model-free nature, is an area where algorith-\nmic developments have run far ahead of their inferential justiﬁcation.” (Efron\nand Hastie [2016], p.",
    "content_hash": "356bd78ca8c7418c92401bf68ec12e93acf0e779213b1601a249f3b70c097a94",
    "location": null,
    "page_start": 1,
    "page_end": 13,
    "metadata": {
      "section": "Ensemble Methods and Model Averaging",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "4aea76b5-7b0e-4ee5-898d-2a27937703d0",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "case is least squares, with\nˆβls = arg min\nβ\nN\nX\ni=1\nYi −β⊤Xi\n\u00012 . However, if the number of covariates K is large relative to the number of observations N the\nleast squares estimator ˆβls\nk does not even have particularly good repeated sampling properties\nas an estimator for βk, let alone good predictive properties. In fact, with K ≥3 the least\nsquares estimator is not even admissible and is dominated by estimators that shrink towards\nzero. With K very large, possibly even exceeding the sample size N, the least squares\nestimator has particularly poor properties, even if the conditional mean of the outcome\ngiven the covariates is in fact linear. Even with K modest in magnitude, the predictive properties of the least squares estimator\nmay be inferior to those of estimators that use some amount of regularization. One common\nform of regularization is to add a penalty term that shrinks the βk towards zero, and minimize\narg min\nβ\nN\nX\ni=1\nYi −β⊤Xi\n\u00012 + λ (∥β∥q)1/q . where ∥β∥q = PK\nk=1 |βk|q. For q = 1 this corresponds to LASSO (Tibshirani [1996]). For\nq = 2 this corresponds to ridge regression (Hoerl and Kennard [1970]). As q →0, the so-\nlution penalizes the number of non-zero covariates, leading to best subset regression (Miller\n[2002], Bertsimas et al. [2016]).",
    "content_hash": "b5308a50a75e1fb6eba62a36828b79bfca2fb9beed8418ea2358df377cb709cc",
    "location": null,
    "page_start": 1,
    "page_end": 16,
    "metadata": {
      "section": "Regularized Linear Regression: Lasso, Ridge, and Elastic Nets",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5ccca140-0ac1-4f88-a1e1-962d33863912",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Euclidean neighborhood,\nfor KNN matching. Tree-based neighborhood. One way to interpret a tree is that it is an alternative to kernel regression. Within\neach tree, the prediction for a leaf is simply the sample average outcome within the leaf. Thus, we can think of the leaf as deﬁning the set of nearest neighbors for a given target\nobservation in a leaf, and the estimator from a single regression tree is a matching estimator\nwith non-standard ways of selecting the nearest neighbor to a target point. In particular, the\nneighborhoods will prioritize some covariates over others in determining which observations\nqualify as “nearby.” The ﬁgure illustrates the diﬀerence between kernel regression and a\ntree-based matching algorithm for the case of two covariates. Kernel regression will create\na neighborhood around a target observation based on the Euclidean distance to each point,\nwhile tree-based neighborhoods will be rectangles. In addition, a target observation may\nnot be in the center of a rectangle. Thus, a single tree is generally not the best way to\npredict outcomes for any given test point x. When a prediction tailored to a speciﬁc target\nobservation is desired, generalizations of tree-based methods can be used. For better estimates of µ(x), random forests (Breiman [2001a]) build on the regression\ntree algorithm. A key issue random forests address is that the estimated regression function\ngiven a tree is discontinuous with substantial jumps, more than one might like. Random\nforests induce smoothness by averaging over a large number of trees. These trees diﬀer\nfrom each other in two ways.",
    "content_hash": "440e0f026c25c170eff6824bc13d0b585a33403451ffd1403d5910060b4d6db3",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "27a0aba8-ed37-4336-8693-b61679d21bb5",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "with classiﬁer\nsgn(ˆω⊤Xi + ˆb). Note that if there is a hyperplane with no classiﬁcation errors, a standard logit model would\nnot have a maximum likelihood estimator: the argmax of the likelihood function would\ndiverge. We can also write this problem in terms of the Lagrangian, with αi the Lagrangian\nmultiplier for the restriction Yi(ω⊤Xi + b) ≥1,\nmin\nα,ω,b\n(\n1\n2∥ω∥2 −\nN\nX\ni=1\nαi(Yi(ω⊤Xi + b) −1)\n)\n,\nsubject to 0 ≤αi. After concentrating out the weights ω this is equivalent to\nmax\nα\n( N\nX\ni=1\nαi −1\n2\nN\nX\ni=1\nN\nX\nj=1\nαiαjYiYiX⊤\ni Xj\n)\n,\nsubject to 0 ≤αi,\nN\nX\ni=1\nαiYi = 0,\nwhere ˆb solves P\ni ˆαi(Yi(X⊤\ni ω + ˆb) −1) = 0, with classiﬁer\nf(x) = sgn\nˆb +\nN\nX\ni=1\nYiˆαiX⊤\ni x\n! ,\nIn practice, of course, we are typically in a situation where there exists no hyperplane\nwithout classiﬁcation errors. In that case there is no solution as the αi diverge for some i. We can modify the classiﬁer by adding the constraint that the αi ≤C. Scholkopf and Smola\n[2001] recommend setting C = 10N. This is still a linear problem, diﬀering from a logistic regression only in terms of the\nloss function.",
    "content_hash": "e58c75ae1cd9caa5cd9be5efa896963a4cba4edc02218810636dca8c740c34c7",
    "location": null,
    "page_start": 1,
    "page_end": 29,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "54d0ba45-93c1-4d07-8c46-aacd49fc5267",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "treatment eﬀects in each element of the partition. Unlike regression trees optimized for\nprediction, the splitting rule optimizes for ﬁnding splits associated with treatment eﬀect\nheterogeneity. In addition, the method relies on sample splitting; half the data is used to\nestimate the tree structure, and the other half (the “estimation sample”) is used to estimate\ntreatment eﬀects in each leaf. The tree is pruned using cross-validation, just as in standard\nregression trees, but where the criterion for evaluating the performance of the tree in held-out\ndata is based on treatment eﬀect heterogeneity rather than predictive accuracy. Some advantages of the causal tree method are similar to advantages of regression trees. They are easy to explain; in the case of a randomized experiment, the estimate in each leaf\nis simply the sample average treatment eﬀect. A disadvantage is that the tree structure is\nsomewhat arbitrary; there may be many partitions of the data that exhibit treatment eﬀect\nheterogeneity, and taking a slightly diﬀerent subsample of the data might lead to a diﬀerent\nestimated partition. The approach of estimating simple models in the leaves of shallow trees\ncan be applied to other types of models; see [Zeileis et al., 2008] for an early version of this\nidea, although that paper did not provide theoretical guarantees or conﬁdence intervals. For some purposes, it is desirable to have a smooth estimate of τ(x).",
    "content_hash": "21a952da39d73623161ccad61588e944ba182182059ce84c0d1e2c3eb5db03ff",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "Heterogenous Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "30ab373c-bc87-49fd-b1e2-660a5ca4713d",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "[2018b] provides methods for testing this type of hypothesis. As described above in our presentation of regression forests, Athey et al. [2016b] extended\nthe framework of causal forests to analyze nonparametric parameter heterogeneity in models\nwhere the parameter of interest can be estimated by maximum likelihood or GMM. As an\napplication, the paper highlights the case of instrumental variables. Friedberg et al. [2018]\nextends local linear regression forests to the problem of heterogeneous treatment eﬀects, so\nthat regularity in the function τ(x) can be better exploited. An alternative approach to estimating parameter heterogeneity in instrumental variables\nmodels was proposed by Hartford et al. [2016], who use an approach based on neural nets,\nthough distributional theory is not available for that estimator. Other possible approaches\nto estimating conditional average treatment eﬀects can be used when the structure of the\nheterogeneity is assumed to take a simple form. Targeted maximum likelihood [van der Laan\nand Rubin, 2006] is one approach to this, while Imai et al. [2013] proposed using LASSO to\nuncover heterogeneous treatment eﬀects. K¨unzel et al. [2017] proposes an ML approach using\n“meta-learners.” Another popular alternative that takes a Bayesian approach is Bayesian\nAdditive Regression Trees (BART), developed by Chipman et al. [2010] and applied to causal\ninference by Hill [2011], Green and Kern [2012].",
    "content_hash": "3bcbe4e4d058d361f8e0b7bd5a839eda2b6d0ecd4bd1014d16dfb7b754424bab",
    "location": null,
    "page_start": 1,
    "page_end": 38,
    "metadata": {
      "section": "Heterogenous Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "7ca06b63-7a83-4a20-b094-43a47135e6d0",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "See Athey and Wager [2017] for details. 7\nExperimental Design, Reinforcement Learning, and\nMulti-Armed Bandits\nML methods have recently made substantial contributions to experimental design, with\nmulti-armed bandits becoming more popular especially in online experiments. Thompson\nsampling (Scott [2010], Thompson [1933]) and Upper Conﬁdence Bounds (UCB, Lai and\nRobbins [1985]) can be viewed as a simple example of reinforcement learning (Sutton et al. [1998]) where successful assignment decisions are rewarded by sending more units to the\ncorresponding treatment arm. 7.1\nA/B Testing versus Multi-Armed Bandits\nTraditionally much experimentation is done by assigning a predetermined number of units to\neach of a number of treatment arms. Often there would be just two treatment arms. After\n[38]",
    "content_hash": "17397b008fbcbb4c47bd20e55c80bc69e066024700c72bea22e65aca7e203d42",
    "location": null,
    "page_start": 1,
    "page_end": 39,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "00665e34-0cc1-4f15-9d6a-7e902b0a82c1",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Susan Athey, Guido Imbens, and Stefan Wager. Eﬃcient inference of average treat-\nment eﬀects in high dimensions via approximate residual balancing. arXiv preprint\narXiv:1604.07125, 2016a. Susan Athey, Julie Tibshirani, and Stefan Wager. Generalized random forests. arXiv preprint\narXiv:1610.01271, 2016b. Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi. Matrix completion methods for causal panel data models. arXiv preprint arXiv:1710.10251,\n2017a. Susan Athey, David Blei, Rob Donnelly, and Francisco Ruiz. Counterfactual inference for\nconsumer choice across many product categories. 2017b. Susan Athey, Markus M Mobius, and Jen˝o P´al. The impact of aggregators on internet news\nconsumption. 2017c. Susan Athey, Julie Tibshirani, and Stefan Wager. Generalized random forests. arXiv preprint\narXiv:1610.01271, 2017d. URL https://arxiv.org/abs/1610.01271. Susan Athey, Mohsen Bayati, Guido Imbens, and Qu Zhaonan. Ensemble methods for causal\neﬀects in panel data settings. 2019. Jushan Bai. Inferential theory for factor models of large dimensions. Econometrica, 71(1):\n135–171, 2003. Jushan Bai and Serena Ng. Determining the number of factors in approximate factor models.",
    "content_hash": "6b65353b1706feb120589a24259a1281b21a4995199c01668d97fcbe4d9235ea",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "beb52981-8e9e-41ec-b78d-434104bc88d4",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Another class of models uses supervised learning methods. These methods are used\nwhen there is a speciﬁc characteristic the researcher would like to learn from text. Examples\nmight include favorability of a review, political polarization of text spoken by legislators, or\nwhether a tweet about a company is positive or negative. Then, the outcome variable is a\nlabel that contains the characteristic of interest. A simple supervised learning model takes\nthe data matrix C, views each document i as a unit of observation, and treats the columns of\nC (each corresponding to indicators for whether a particular word is in a document) as the\ncovariates in the regression. Since T is usually much greater than N, it is important to use\nML methods that allow for regularization. Sometimes, other types of dimension reduction\ntechniques are used in advance of applying a supervised learning method (e.g. unsupervised\ntopic modeling). Another approach is to think of a generative model, where we think of the words in\nthe document as a vector of outcomes, and where the characteristics of interest about the\ndocument determine the distribution of words, as in the topic model literature. An example\nof this approach is the supervised topic model, where information about the observed char-\nacteristics in a training dataset are incorporated in the estimation of the generative model. The estimated model can then be used to predict those characteristics in a test dataset of\nunlabelled documents. See Blei and Laﬀerty [2009] for more details. 10\nConclusion\nThere is a fast growing machine learning literature that has much to oﬀer empirical re-\nsearchers in economics.",
    "content_hash": "11e807ec470c3bb2d1e39da4b34f16ea5f3374ac7483b7c946c9b858d4fb252a",
    "location": null,
    "page_start": 1,
    "page_end": 50,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "36431718-8540-4b76-94e2-af98fc3aa969",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Proceedings of the National Academy of Sciences, 113(27):7353–7360, 2016. Susan Athey and Guido W Imbens. The econometrics of randomized experiments. Handbook\nof Economic Field Experiments, 1:73–140, 2017a. Susan Athey and Guido W Imbens. The state of applied econometrics: Causality and policy\nevaluation. The Journal of Economic Perspectives, 31(2):3–32, 2017b. Susan Athey and Stefan Wager. Eﬃcient policy estimation. arXiv preprint arXiv:1702.02896,\n2017. URL https://arxiv.org/abs/1702.02896. [50]",
    "content_hash": "2e86146e57375e07129485b8ffe96bc932866b205fa99a47e717b43a1d25c7dc",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b383fdb2-423b-47e4-b183-ebb8a167c723",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Econometrica, 70(1):191–221, 2002. Jushan Bai and Serena Ng. Principal components and regularized estimation of factor mod-\nels. arXiv preprint arXiv:1708.08137, 2017. R. Bamler and S. Mandt. Dynamic word embeddings via skip-gram ﬁltering. In International\nConference in Machine Learning, 2017. O. Barkan. Bayesian neural word embedding. arXiv preprint arXiv:1603.06571, 2016. Hamsa Bastani and Mohsen Bayati. Online decision-making with high-dimensional covari-\nates. Technical report, 2015. [51]",
    "content_hash": "84a4e91b77431327682e4e11fc24e8b5c1d7a1810a03dac5adf25f11f71f8126",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9e77f4ab-811e-42f6-9cc5-a182cfacd266",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "URL https://arxiv.org/pdf/1612.09596.pdf. John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):100–108, 1979. Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical\nLearning. New York: Springer, 2009. Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning with\nSparsity: The Lasso and Generalizations. CRC Press, 2015. Trevor Hastie, Robert Tibshirani, and Ryan J Tibshirani. Extended comparisons of best sub-\nset selection, forward stepwise selection, and the lasso. arXiv preprint arXiv:1707.08692,\n2017. [55]",
    "content_hash": "60cd5dbcbba86cf3dd969899535f0406a9eeab9ac74e8ee5433cc10ff6bed418",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "29b247b7-4bf1-4fc9-b64d-3358be718da1",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of\nComputational and Graphical Statistics, 20(1):217–240, 2011. Keisuke Hirano and Jack R Porter. Asymptotics for statistical treatment rules. Econometrica, 77(5):1683–1701, 2009. Arthur E Hoerl and Robert W Kennard. Ridge regression: Biased estimation for nonorthog-\nonal problems. Technometrics, 12(1):55–67, 1970. Paul W Holland. Statistics and causal inference. Journal of the American statistical\nAssociation, 81(396):945–960, 1986. Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks\nare universal approximators. Neural networks, 2(5):359–366, 1989. Kosuke Imai, Marc Ratkovic, et al. Estimating treatment eﬀect heterogeneity in randomized\nprogram evaluation. The Annals of Applied Statistics, 7(1):443–470, 2013. Guido Imbens and Jeﬀrey Wooldridge. Recent developments in the econometrics of program\nevaluation. Journal of Economic Literature, 47(1):5–86, 2009. Guido W Imbens and Thomas Lemieux. Regression discontinuity designs: A guide to prac-\ntice. Journal of econometrics, 142(2):615–635, 2008. Guido W Imbens and Donald B Rubin. Causal Inference in Statistics, Social, and Biomedical\nSciences. Cambridge University Press, 2015. Bruno Jacobs, Bas Donkers, and Dennis Fok.",
    "content_hash": "61cebad73e6108abf389be4bf2d30dc30105c113520d1b180e9c6d5b72388948",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "6368591d-e7d8-4a84-a7e6-02bcea2e54d4",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Counterfactual estimation and optimization of\nclick metrics for search engines. CoRR, 2014. Rosa L Matzkin. Restrictions of economic theory in nonparametric methods. Handbook of\neconometrics, 4:2523–2558, 1994. Rosa L Matzkin. Nonparametric identiﬁcation. Handbook of econometrics, 6:5307–5368,\n2007. [57]",
    "content_hash": "6e98c4cc89e953b31ee538e09bd3da443c1e29e85a9bb0051efa111155a87bc5",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c4e4e135-56bb-413d-8ee8-551389c1541c",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Learning from logged implicit exploration data. Conference on Neural Information Processing Systems, 2010. Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT\npress, 1998. A. Swaminathan and T. Joachims. Batch learning from logged bandit feedback through\ncounterfactual risk minimization. Journal of Machine Learning Research, 2015. P. Thomas and E. Brunskill. Data-eﬃcient oﬀ-policy policy evaluation for reinforcement\nlearning. International Conference on Machine Learning, 2016. William R Thompson. On the likelihood that one unknown probability exceeds another in\nview of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933. [59]",
    "content_hash": "9c2384a91811afb544b3a69674e5427600df5ca49f58326cfc3735d87d06a202",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "837c3b82-b2b4-4cbe-bacc-a33cca58d8c3",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "1\nIntroduction\nIn the abstract of his provocative 2001 paper in Statistical Science the Berkeley statistician\nLeo Breiman writes about the diﬀerence between model-based versus algorithmic approaches\nto statistics:\n“There are two cultures in the use of statistical modeling to reach conclusions\nfrom data. One assumes that the data are generated by a given stochastic data\nmodel. The other uses algorithmic models and treats the data mechanism as\nunknown.” Breiman [2001b], p199. Breiman goes on to claim that:\n“The statistical community has been committed to the almost exclusive use of\ndata models. This commitment has led to irrelevant theory, questionable con-\nclusions, and has kept statisticians from working on a large range of interesting\ncurrent problems. Algorithmic modeling, both in theory and practice, has devel-\noped rapidly in ﬁelds outside statistics. It can be used both on large complex\ndata sets and as a more accurate and informative alternative to data modeling\non smaller data sets. If our goal as a ﬁeld is to use data to solve problems, then\nwe need to move away from exclusive dependence on data models and adopt a\nmore diverse set of tools.” Breiman [2001b], p199. Breiman’s characterization no longer applies to the ﬁeld of statistics. The statistics commu-\nnity has by and large accepted the Machine Learning (ML) revolution that Breiman refers to\nas the algorithm modeling culture, and many textbooks discuss ML methods alongside more\ntraditional statistical methods, e.g., Hastie et al. [2009] and Efron and Hastie [2016].",
    "content_hash": "fd8a4bc6c4f51d1567e7c22f0ba3326cc721093da9f6663033694e5bc0e08f63",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "Introduction",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a405a05a-e519-452f-a849-9123a329f440",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Al-\nthough the adoption of these methods in economics has been slower, they are now beginning\nto be widely used in empirical work, and are the topic of a rapidly increasing methodological\nliterature. In this paper we want to make the case that economists and econometricians also,\nas Breiman writes referring to the statistics community, “need to move away from exclusive\ndependence on data models and adopt a more diverse set of tools.” We discuss some of\nthe speciﬁc tools that empirical researchers would beneﬁt from, and which we feel should\nbe part of the standard graduate curriculum in econometrics if, as Breiman writes, and we\nagree with, “our goal as a ﬁeld is to use data to solve problems,” if, in other words, we view\neconometrics as in essence, decision making under uncertainty (e.g., Chamberlain [2000]),\n[1]",
    "content_hash": "acc8d7d58ff8a6e62c5cb0c790a70a28931001a22e442e3b6acfd8358d943f3d",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "Introduction",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c67632d1-a8a1-485d-ae17-a5c90c3190d5",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "and if we wish to enable students to be able to communicate eﬀectively with researchers\nin other ﬁelds where these methods are routinely being adopted. Although relevant more\ngenerally, the methods developed in the ML literature have been particularly successful in\n“big data” settings, where we observe information on a large number of units, or many pieces\nof information on each unit, or both, and often outside the simple setting with a single cross-\nsection of units. For such settings, ML tools are becoming the standard across disciplines,\nand so the economist’s toolkit needs to adapt accordingly, while preserving the traditional\nstrengths of applied econometrics. Why has the acceptance of ML methods been so much slower in economics compared to\nthe broader statistics community? A large part of it may be the culture as Breiman refers to\nit. Economics journals emphasize the use of methods with formal properties of a type that\nmany of the ML methods do not naturally deliver. This includes large sample properties of\nestimators and tests, including consistency, Normality, and eﬃciency. In contrast, the focus\nin the machine learning literature is often on working properties of algorithms in speciﬁc\nsettings, with the formal results of a diﬀerent type, e.g., guarantees of error rates. There are\ntypically fewer theoretical results of the type traditionally reported in econometrics papers,\nalthough recently there have been some major advances there (Wager and Athey [2017],\nFarrell et al. [2018]). There are no formal results that show that for supervised learning\nproblems deep learning or neural net methods are uniformly superior to regression trees or\nrandom forests, and it appears unlikely that general results for such comparisons will soon\nbe available, if ever.",
    "content_hash": "c6890567dcd66fdba80293547063fb8c9a233b63aa87700bc548cc55cc31fbbe",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "Introduction",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0e2827ed-fabb-4803-bd1b-95b5bf3fa295",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Although the ability to construct valid large sample conﬁdence intervals is important\nin many cases, one should not out-of-hand dismiss methods that cannot not deliver them\n(or possibly, that can not yet deliver them), if these methods have other advantages. The\ndemonstrated ability to outperform alternative methods on speciﬁc data sets in terms of out-\nof-sample predictive power is valuable in practice, even though such performance is rarely\nexplicitly acknowledged as a goal, or assessed, in econometrics. As Mullainathan and Spiess\n[2017] highlights, some substantive problems are naturally cast as prediction problems, and\nassessing their goodness of ﬁt on a test set may be suﬃcient for the purposes of the analysis\nin such cases. In other cases, the output of a prediction problem is an input to the primary\nanalysis of interest, and statistical analysis of the prediction component beyond convergence\nrates is not needed. On the other hand, there are also many settings where it is important to\nprovide valid conﬁdence intervals for a parameter of interest, such as an average treatment\n[2]",
    "content_hash": "1d692172e3f0d61192ed7e135e6d770acad538fddd84c6bddac73bfe359f26f3",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "Introduction",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "65f89abf-0d30-496b-a996-9e00649e4899",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "eﬀect. The degree of uncertainty captured by standard errors or conﬁdence intervals may be\na component in decisions about whether to implement the treatment. We argue that in the\nfuture, as ML tools are more widely adopted, researchers should articulate clearly the goals\nof their analysis and why certain properties of algorithms and estimators may or may not\nbe important. A major theme of this review is that even though there are cases where using simple\nof-the-shelf algorithms from the ML literature can be eﬀective (see Mullainathan and Spiess\n[2017] for a number of examples), there are also many cases where this is not the case. Often\nthe ML techniques require careful tuning and adaptation to eﬀectively address the speciﬁc\nproblems economists are interested in. Perhaps the most important type of adaptation is\nto exploit the structure of the problems, e.g., the causal nature of many estimands, the\nendogeneity of variables, the conﬁguration of data such as panel data, the nature of dis-\ncrete choice among a set of substitutable products, or the presence of credible restrictions\nmotivated by economic theory, such as monotonicity of demand in prices or other shape\nrestrictions (Matzkin [1994, 2007]). Statistics and econometrics have traditionally put much\nemphasis on these structures, and developed insights to exploit them, whereas ML has of-\nten put little emphasis on them. Exploiting these insights, both substantive and statistical,\nwhich, in a diﬀerent form, is also seen in the careful tuning of ML techniques for speciﬁc\nproblems such as image recognition, can greatly improve their performance.",
    "content_hash": "54371701da24b6336d5909e88ace87a2e8ac9373a9d33526e8d0efa3682aa20b",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Introduction",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f982fd9f-f379-46e0-b6c3-583f7089c599",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Another type\nof adaptation involves changing the optimization criteria of machine learning algorithms to\nprioritize considerations from causal inference, such as controlling for confounders or dis-\ncovering treatment eﬀect heterogeneity. Finally, techniques such as sample splitting (using\ndiﬀerent data to select models than to estimate parameters (e.g., Athey and Imbens [2016],\nWager and Athey [2017]) and orthogonalization (e.g. Chernozhukov et al. [2016a]) can be\nused to improve the performance of machine learning estimators, in some cases leading to\ndesirable properties such as asymptotic normality of machine learning estimators (e.g. Athey\net al. [2017d], Farrell et al. [2018]). In this paper, we discuss a list of tools that we feel should be be part of the empirical\neconomists’ toolkit and that we view should be covered in the core econometrics graduate\ncourses. Of course, this is a subjective list, and given the speed with which this literature is\ndeveloping, the list will rapidly evolve. Moreover, we will not give a comprehensive discussion\nof these topics, rather we aim to provide an introduction to these methods that conveys the\nmain ideas and insights, with references to more comprehensive treatments. First on our list\n[3]",
    "content_hash": "0ed51d6e696b4b32a4547d265b1357156cd327ceaf53ff0a095414b9784b16f3",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Introduction",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a001e980-f80f-4073-9856-e9315ef0f0d8",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "In the computer science\nand statistics literatures there are also a number of excellent textbooks, with diﬀerent levels\nof accessibility to researchers with a social science background, including Efron and Hastie\n[2016], Hastie et al. [2009], which is a more comprehensive text from a statistics perspective,\nand Burkov [2019] which is a very accessible introduction, Alpaydin [2009], and Knox [2018],\nwhich all take more of a computer science perspective. 2\nEconometrics and Machine Learning: Goals, Meth-\nods, and Settings\nIn this section we introduce some of the general themes of this paper. What are the diﬀerences\nin the goals and concerns of traditional econometrics and the machine learning literature,\nand how do these goals and concerns aﬀect the choices between speciﬁc methods? [4]",
    "content_hash": "7e55ce93d3c5cb89f5cd2fab087b32001e74e5bd09bf2290be05e61dd92c972f",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "Introduction",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b1e4c5cb-9e33-4748-a51b-87ce6ddcc222",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "is nonparametric regression, or in the terminology of the ML literature, supervised learning\nfor regression problems. Second, supervised learning for classiﬁcation problems, or closely\nrelated, but not quite the same, nonparametric regression for discrete response models. This\nis the area where ML methods have perhaps had their biggest successes. Third, unsupervised\nlearning, or clustering analysis and density estimation. Fourth, we analyze estimates of\nheterogeneous treatment eﬀects and optimal policies mapping from individuals’ observed\ncharacteristics to treatments. Fifth, we discuss ML approaches to experimental design,\nwhere bandit approaches are starting to revolutionize eﬀective experimentation especially in\nonline settings. Sixth, we discuss the matrix completion problem, including its application to\ncausal panel data models and problems of consumer choice among a discrete set of products. Finally, we discuss the analysis of text data. We note that there are a few other recent reviews of ML methods aimed at economists,\noften with more empirical examples and references to applications than we discuss here. Varian [2014] is an early high level discussion of a selection of important ML methods. Mullainathan and Spiess [2017] focus on the beneﬁts of supervised learning methods for\nregression, and discuss the prevalence of problems in economics where prediction methods are\nappropriate. Athey [2017] and Athey et al. [2017c] provides a broader perspective with more\nemphasis on recent developments in adapting ML methods for causal questions and general\nimplications for economics. Gentzkow et al. [2017] provide an excellent recent discussion of\nmethods for text analyses with a focus on economics applications.",
    "content_hash": "207be5b2f0dd06679c896787ee909767002b96cf85d9637247f2a75134a3a40f",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "Introduction",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e3f1869c-7f23-4cdc-81ef-bb5831f48bff",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "2.1\nGoals\nThe traditional approach in econometrics, as exempliﬁed in leading texts such as Wooldridge\n[2010], Angrist and Pischke [2008], Greene [2000] is to specify a target, an estimand, that is a\nfunctional of a joint distribution of the data. Often the target is a parameter of a statistical\nmodel that describes the distribution of a set of variables (typically conditional on some\nother variables) in terms of a set of parameters, which can be a ﬁnite or inﬁnite set. Given\na random sample from the population of interest the parameter of interest and the nuisance\nparameters are estimated by ﬁnding the parameter values that best ﬁt the full sample, using\nan objective function such as the sum of squared errors, or the likelihood function. The focus\nis on the quality of the estimators of the target, traditionally measured through large sample\neﬃciency. Often there is also interest in constructing conﬁdence intervals. Researchers\ntypically report point estimates and standard errors. In contrast, in the ML literature the focus is typically on developing algorithms (a widely\ncited paper, Wu et al. [2008], has the title “Top 10 algorithms in data mining”). The goal for\nthe algorithms is typically to make predictions about some variables given others, or classify\nunits on the basis of limited information, for example to classify handwritten digits on the\nbasis of pixel values. In a very simple example, suppose we model the conditional distribution of some outcome\nYi given a vector-valued regressor or feature Xi. Suppose we are conﬁdent that\nYi|Xi ∼N(α + β⊤Xi, σ2).",
    "content_hash": "47b8119e89e682dccea9fff78447fe70118428d3a171f2114bd6346ee931c2e8",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "Goals",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "401c11f9-2845-45d9-b8d6-6c2173e3c495",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "We could estimate θ = (α, β) by least squares, that is, as\n(ˆαls, ˆβls) = arg min\nα,β\nN\nX\ni=1\nYi −α −β⊤Xi\n\u00012 . Most introductory econometrics texts would focus on the least squares estimator without\nmuch discussion. If the model is correct, the least squares estimator has well known attrac-\ntive properties: it is unbiased, it is the best linear unbiased estimator, it is the maximum\nlikelihood estimator, and so has large sample eﬃciency properties. In ML settings the goal may be to make a prediction for the outcome for a new units on\nthe basis of their regressor values. Suppose we are interested in predicting the value of YN+1\nfor a new unit N + 1, on the basis of the regressor values for this new unit, XN+1. Suppose\nwe restrict ourselves to linear predictors, so that the prediction is\nˆYN+1 = ˆα + ˆβ⊤XN+1,\n[5]",
    "content_hash": "3775cfdd080f18873bfafc174c08a356e997c3aef1fd13a088ba772a8ed875ed",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "Goals",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a77190c4-b6aa-4a0b-952c-2cc3fc6db12f",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "for some estimator (ˆα, ˆβ). The loss associated with this decision may be the squared error\n\u0010\nYN+1 −ˆYN+1\n\u00112\n. The question now is to come up with estimators (ˆα, ˆβ) that have good properties associated\nwith this loss function. This need not be the least squares estimator. In fact, when the\ndimension of the features exceeds two, we know from decision theory that we can do better\nin terms of expected squared error than the least squares estimator. The latter is not\nadmissible, that is, there are other estimators that dominate the least squares estimator. 2.2\nTerminology\nOne source of confusion is the use of new terminology in the ML for concepts that have well-\nestablished labels in the older literatures. In the context of a regression model the sample\nused to estimate the parameters is often referred to as the training sample. Instead of\nestimating the model, it is being trained. Regressors, covariates, or predictors are referred to\nas features. Regression parameters are sometimes referred to as weights. Prediction problems\nare divided into supervised learning problems where we observe both the predictors/features\nXi and the outcome Yi, and unsupervised learning problems where we only observe the Xi\nand try to group them into clusters or otherwise estimate their joint distribution. Unordered\ndiscrete response problems are generally referred to as classiﬁcation problems. 2.3\nValidation and Cross-validation\nIn most discussions on linear regression in econometric textbooks there is little emphasis on\nmodel validation.",
    "content_hash": "fd08b2900a1104aab94dcfcd0a52e8c8c92c11a9827a6796a1bb9053f6333891",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Goals",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5710413b-d9ac-4fb5-bf34-122850d9aacc",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "The form of the regression model, be it parametric or nonparametric, and\nthe set of regressors, is assumed to be given from the outside, e.g., economic theory. Given\nthis speciﬁcation, the task of the researcher is to estimate the unknown parameters of this\nmodel. Much emphasis is on doing this estimation step eﬃciently, typically operationalized\nthrough deﬁnitions of large sample eﬃciency. If there is discussion of model selection, it is\noften in the form of testing null hypotheses concerning the validity of a particular model, with\nthe implication that there is a true model that should be selected and used for subsequent\ntasks. Consider the regression example in the previous subsection. Let us assume that we\nare interested in predicting the outcome for a new unit, randomly drawn from the same\npopulation as our sample was drawn from. As an alternative to estimating the linear model\n[6]",
    "content_hash": "294961c3db571b4abbde8fa9b85f146fe320f28e6c039cc96eee03e405e85d3e",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Validation and Cross-validation",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9b62a755-dac3-4722-87b8-2b4ea3cbe474",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "As Vapnik writes,\n“Regularization theory was one of the ﬁrst signs of the existence of intelligent\ninference” (Vapnik [1998], p.)\nConsider a setting with a large set of models that diﬀer in their complexity, measured for\nexample as the number of unknown parameters in the model, or, more subtly, through the\nthe Vapnik–Chervonenkis (VC) dimension that measures the capacity or complexity of a\nspace of models. Instead of directly optimizing an objective function, say minimizing the\nsum of squared residuals in a least squares regression setting, or maximizing the logarithm of\nthe likelihood function, a term is added to the objective function to penalize the complexity\nof the model. There are antecedents of this practice in the traditional econometrics and\nstatistics literature. One is that in likelihood settings researchers sometimes add a term to\nthe logarithm of the likelihood function equal to minus the logarithm of the sample size\ntimes the number of free parameters divided by two, leading to the Bayesian Information\nCriterion, or simply the number of free parameters, the Akaike Information Criterion. In\nBayesian analyses of regression models the use of a prior distribution on the regression\nparameters, centered at zero, independent accross parameters with a constant prior variance,\nis another way of regularizing estimation that has a long tradition. The diﬀerence with the\n[7]",
    "content_hash": "c82685adb380343ff521fda4fa7bf2bc81fc23d71ac215c5cfb480ee4f5248e5",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "Over-ﬁtting, Regularization, and Tuning Parameters",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e67a9f83-1be1-41a8-81c4-85c28c5ef521",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "modern approaches to regularization is that they are more data driven, with the amount\nof regularization determined explicitly by the out-of-sample predictive performance rather\nthan by, for example, a subjectively chosen prior distribution. Consider a linear regression model with K regressors,\nYi|Xi ∼N\nβ⊤Xi, σ2\u0001\n. Suppose we also have a prior distribution for the the slope coeﬃcients βk, with the prior for\nβk, N(0, τ 2), and independent of βk′ for any k ̸= k′. (This may be more plausible if we ﬁrst\nnormalize the features and outcome to have mean zero and unit variance. We assume this\nhas been done.) Given the value for the variance of the prior distribution, τ 2, the posterior\nmean for β is the solution to\narg min\nβ\nN\nX\ni=1\nYi −β⊤Xi\n\u00012 + σ2\nτ 2 ∥β∥2\n2,\nwhere ∥β∥2 =\n\u0010PK\nk=1 β2\nk\n\u00111/2\n. One version of an ML approach to this problem is to estimate\nβ by minimizing\narg min\nβ\nN\nX\ni=1\nYi −β⊤Xi\n\u00012 + λ∥β∥2\n2. The only diﬀerence is in the way the penalty parameter λ is chosen. In a formal Bayesian\napproach this reﬂects the (subjective) prior distribution on the parameters, and it would be\nchosen a priori. In an ML approach λ would be chosen through out-of-sample cross-validation\nto optimize the out-of-sample predictive performance. This is closer to an Empirical Bayes\napproach where the data are used to estimate the prior distribution (e.g., Morris [1983]).",
    "content_hash": "65a1d10f26c22142eb701d5aa20cf10f93ea59449b23887f757a65847c267f9a",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Over-ﬁtting, Regularization, and Tuning Parameters",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "87658230-27ef-49de-a239-32e3a2dc7a2b",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "2.5\nSparsity\nIn many settings in the ML literature the number of features is substantial, both in absolute\nterms and relative to the number of units in the sample. However, there is often a sense that\nmany of the features are of minor importance, if not completely irrelevant. The problem is\nthat we may not know ex ante which of the features matter, and which can be dropped from\nthe analysis without substantially hurting the predictive power. Hastie et al. [2009, 2015] discuss what they call the sparsity principle:\n[8]",
    "content_hash": "0f1597e13f658e0758acb081c79e17df1dfdc2898d9a6937c0f354bbbcaa757b",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Sparsity",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0d784697-4bea-4db7-9692-52951d5dca6e",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "“Assume that the underlying true signal is sparse and we use an ℓ1 penalty to\ntry to recover it. If our assumption is correct, we can do a good job in recovering\nthe true signal. ... But if we are wrong—the underlying truth is not sparse in the\nchosen bases—then the ℓ1 penalty will not work well. However, in that instance,\nno method can do well, relative to the Bayes error.” (Hastie et al. [2015], page\n24). Exact sparsity is in fact stronger than is necessary, in many cases it is suﬃcient to have\napproximate sparsity where most of the explanatory variables have very limited explanatory\npower, even if not zero, and only a few of the features are of substantial importance (see, for\nexample, Belloni et al. [2014]). Traditionally in the empirical literature in social sciences researchers limited the number\nof explanatory variables by hand, rather than choosing them in a data-dependent manner. Allowing the data to play a bigger role in the variable selection process appears a clear\nimprovement, even if the assumption that the underlying process is at least approximately\nsparse is still a very strong one, and even if inference in the presence of data-dependent\nmodel selection can be challenging. 2.6\nComputational Issues and Scalability\nCompared to the traditional statistics and econometrics literatures the ML literature is much\nmore concerned with computational issues and the ability to implement estimation methods\nwith large data sets. Solutions that may have attractive theoretical properties in terms of\nstatistical eﬃciency but that do not scale well to large data sets are often discarded in favor\nof methods that can be implemented easily in very large data sets.",
    "content_hash": "5454592f31ae29fd65a9e79dc8047617356399f4551950e4ec428efa70a0ff9f",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Sparsity",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0bd3f518-0603-419a-8f3a-4624b86e2006",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "This can be seen in\nthe discussion of the relative merits of LASSO versus subset selection in linear regression\nsettings. In a setting with a large number of features that might be included in the analysis,\nsubset selection methods focus on selecting a subset of the regressors and then estimate the\nparameters of the regression function by least squares. However, LASSO has computational\nadvantages. It can be implemented by adding a penalty term that is proportional to the\nsum of the absolute values of the parameters. A major attraction of LASSO is that there are\neﬀective methods for calculating the LASSO estimates with the number of regressors in the\nmillions. Best subset selection regression, on the other hand, is an NP-hard problem. Until\nrecently it was thought that this was only feasible in settings with the number of regressors in\n[9]",
    "content_hash": "c3325062a48984050e2b3baf1a932b5e7caff5dd7e691add78f4a54658bf5934",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Computational Issues and Scalability",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "58ce967d-452b-4a2f-9cee-a57cce4cec8a",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Classic gradient decscent methods involve an\niterative approach, where ˆθk is updated from ˆθk−1 as follows:\nθk = θk−1 −ηk\n1\nN\nX\ni\n∇Qi(ˆθ),\nwhere ηk is the learning rate, often chosen optimally through line search. More sophisticated\noptimization methods multiply the ﬁrst derivative with the inverse of the matrix of second\nderivatives or estimates thereof. The challenge with this approach is that it can be computationally expensive. The\ncomputational cost is in evaluating the full derivative P\ni ∇Qi, and even more in optimizing\nthe learning rate ηk. The idea behind SGD is that it is better to take many small steps that\nare noisy but on average in the right direction, than it is to spend equivalent computational\ncost in very accurately ﬁguring out in what direction to take a single small step. More\nspeciﬁcally, SGD, uses the fact that the average of ∇Qi for a random subset of the sample is\nan unbiased (but noisy) estimate of the gradient. For example, dividing the data randomly\ninto ten subsets or batches, with Bi ∈{1, 10} denoting the subset unit i belongs to, one\ncould do ten steps of the type\nθk = θk−1 −ηk\n1\nN/10\nX\ni:Bi=k\n∇Qi(ˆθk),\n[10]",
    "content_hash": "d32908738c62bed8425bac9b975acc29a366914c2c2f070d0e7ba2c3bd990fb2",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Computational Issues and Scalability",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f5d9bafb-3ad4-4654-84eb-24d316198fc7",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "the 30s, although current research (Bertsimas et al. [2016]) suggests it may be feasible with\nthe number of regressors in the 1000s. This has reopened a new, still unresolved, debate\non the relative merits of LASSO versus best subset selection (see Hastie et al. [2017]) in\nsettings where both are feasible. There are some indications that in settings with a low\nsignal to noise ratio, as is common in many social science applications, LASSO may have\nbetter performance, although there remain many open questions. In many social science\napplications the scale of the problems is such that best subset selection is also feasible,\nand the computational issues may be less important than these substantive aspects of the\nproblems. A key computational optimization tool used in many ML methods is Stochastic Gradient\nDescent (SGD, Friedman [2002], Bottou [1998, 2012]). It is used in a wide variety of settings,\nincluding in optimizing neural networks and estimating models with many latent variables\n(e.g., Ruiz et al. [2017]). The idea is very simple. Suppose that the goal is to estimate\na parameter θ, and the estimation approach entails ﬁnding the value ˆθ that minimizes an\nempirical loss function, where Qi(θ) is the loss for observation i, and the overall loss is the\nsum P\ni Qi(ˆθk), with derivative P\ni ∇Qi(ˆθ).",
    "content_hash": "393e834c9936a8fb2593f225a1f769c5f5d5e4c0b1f02f5a873853d087591068",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Computational Issues and Scalability",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d29e5a2b-7841-4c62-b58f-13c1a9e89e27",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Mixture models are\nalso used to combine diﬀerent parameter values in a single prediction. However, in both\ncases this model averaging involves averaging over similar models, typically with the same\nspeciﬁcation, and only diﬀerent in terms of parameter values. In the modern literature, and\nin the top entries in the Netﬂix competition, the models that are averaged over can be quite\ndiﬀerent, and the weights are obtained by optimizing out-of-sample predictive power, rather\nthan in-sample ﬁt. For example, one may have three predictive models, one based on a random forest, leading\nto predictions ˆY RF\ni\n, one based on a neural net, with predictions ˆY NN\ni\n, and one based on a\nlinear model estimated by LASSO, leading to ˆY LASSO\ni\n. Then, using a test sample, one can\nchoose weights pRF, pNN, and pLASSO, by minimizing the sum of squared residuals in the test\nsample:\n(ˆpRF, ˆpNN, ˆpLASSO) = arg\nmin\npRF,pNN,pLASSO\nNtest\nX\ni=1\n\u0010\nYi −pRF ˆY RF\ni\n−pNN ˆY NN\ni\n−pLASSO ˆY LASSO\ni\n\u00112\n,\n[11]",
    "content_hash": "be72d0bf358706c53dbd7a50a43f17bf5f4dfaa55816ec56589ed203a15308ba",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Ensemble Methods and Model Averaging",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5d5a7b57-cb2e-47ba-8dc8-0e71f1921f00",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "with a deterministic learning rate ηk. After the ten iteractions one could reshuﬄe the dataset\nand then repeat. If the learning rate ηk decreases at an appropriate rate, under relatively mild\nassumptions, SGD converges almost surely to a global minimum when the objective function\nis convex or pseudoconvex, and otherwise converges almost surely to a local minimum. See\nBottou [2012] for an overview and practical tips for implementation. The idea can be pushed even further in the case where ∇Qi(θ) is itself an expectation. We can consider evaluating ∇Qi using Monte Carlo integration. But, rather than taking\nmany Monte Carlo draws to get an accurate approximation to the integral, we can instead\ntake a small number of draws, or even a single draw. This type of approximation is used in\neconomic applications in Ruiz et al. [2017] and Hartford et al. [2016]. 2.7\nEnsemble Methods and Model Averaging\nAnother key feature of the machine learning literature is the use of model averaging and en-\nsemble methods (e.g., Dietterich [2000]). In many cases a single model or algorithm does not\nperform as well as a combination of possibly quite diﬀerent models, averaged using weights\n(sometimes called votes) obtained by optimizing out-of-sample performance. A striking ex-\nample is the Netﬂix Prize Competition (Bennett et al. [2007]), where all the top contenders\nuse combinations of models, often averages of many models (Bell and Koren [2007]). There\nare two related ideas in the traditional econometrics literature. Obviously Bayesian analysis\nimplicitly averages over the posterior distribution of the parameters.",
    "content_hash": "533e0b63474c6fcd1f07422534f646e1a2e81f78d6f00536a656ac57aa2f52da",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Computational Issues and Scalability",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "04dc6801-b882-4578-a4a0-e25ee7e274a4",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "209)\nAlthough there has recently been substantial progress in the development of methods for\ninference for low-dimensional functionals in speciﬁc settings (e.g., Wager and Athey [2017]\nin the context of random forests, and Farrell et al. [2018] in the context of neural networks),\nit remains the case that for many methods it is currently impossible to construct conﬁdence\nintervals that are valid, even if only asymptotically. A question is whether this ability\nto construct conﬁdence intervals is as important as the traditional emphasis on it in the\neconometric literature suggests. For many decision problems it may be that prediction is of\nprimary importance, and inference is at best of secondary importance. Even in cases where it\nis possible to do inference, it is important to keep in mind that the requirements that ensure\nthis ability often come at the expense of predictive performance. One can see this tradeoﬀ\nin traditional kernel regression, where the bandwidth that optimizes expected squared error\nbalances the tradeoﬀbetween the square of the bias and the variance, so that the optimal\nestimators have an asymptotic bias that invalidates the use of standard conﬁdence intervals. [12]",
    "content_hash": "7608ee8873d9c58c08ffc435d649515817e7458e825b86b369f7ddab5b4513a9",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "Inference",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a297e339-3a89-4461-a4b6-e7567b86cedb",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "This can be ﬁxed by using a bandwidth that is smaller than the optimal one, so that the\nasymptotic bias vanishes, but it does so explicitly at the expense of increasing the variance. 3\nSupervised Learning for Regression Problems\nOne of the canonical problems in both the ML and econometric literatures is that of esti-\nmating the conditional mean of a scalar outcome given a set of of covariates or features. Let\nYi denote the outcome for unit i, and let Xi denote the K-component vector of covariates\nor features. The conditional expectation is\ng(x) = E[Yi|Xi = x]. Compared to the traditional econometric textbooks (e.g., Angrist and Pischke [2008], Greene\n[2000], Wooldridge [2010]) there are some conceptual diﬀerences with the ML literature\n(see the discussion in Mullainathan and Spiess [2017]). In the settings considered in the\nML literature there are often many covariates, sometimes more than there are units in the\nsample. There is no presumption in the ML literature that the conditional distribution of\nthe outcomes given the covariates follows a particular parametric model. The derivatives of\nthe conditional expectation for each of the covariates, which in the linear regression model\ncorrespond to the parameters, are not of intrinsic interest. Instead the focus is on out-\nof-sample predictions and their accuracy. Furthermore, there is less of a sense that the\nconditional expectation is monotone in each of the covariates compared to many economic\napplications.",
    "content_hash": "309290c221f6b5481e8eb996b86ff0de8c19176cf2ae8331892b65e104e4d9b6",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "Supervised Learning for Regression Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ba975a3e-a711-4219-8c6e-76fd8b83bcc6",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Often there is concern that the conditional expectation may be an extremely\nnon-monotone function with some higher order interactions of substantial importance. The econometric literature on estimating the conditional expectation is also huge. Para-\nmetric methods for estimating g(·) often used least squares. Since the work by Bierens\n[1987], kernel regression methods have become a popular alternative when more ﬂexibil-\nity is required, with subsequently series or sieve methods gaining interest (see Chen [2007]\nfor a survey). These methods have well established large sample properties, allowing for\nthe construction of conﬁdence intervals. Simple non-negative kernel methods are viewed\nas performing very poorly in settings with high-dimensional covariates, with the diﬀerence\nˆg(x) −g(x) of order Op(N −1/K). This rate can be improved by using higher order ker-\nnels and assuming the existence of many derivatives of g(·), but practical experience with\nhigh-dimensional covariates has not been satisfactory for these methods, and applications of\nkernel methods in econometrics are generally limited to low-dimensional settings. [13]",
    "content_hash": "4658dba1a3c801abee4ce952f764044ae0f1c1ecfe419ca363e3094469f4fcc6",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "Supervised Learning for Regression Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "18ba0ccb-546e-4ba5-800d-7b2d48486c93",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "First, we discuss methods where the class of models considered\nis linear in the covariates, and the question is solely about regularization. Next we discuss\nmethods based on partitioning the covariate space using regression trees and random forests. In the third subsection we discuss neural nets, which were the focus on of a small econo-\nmetrics literature in the 1990s (White [1992], Hornik et al. [1989]), but more recently has\nbecome a very prominent literature in ML in various subtle reincarnations. Then we discuss\nboosting as a general principle. 3.1\nRegularized Linear Regression: Lasso, Ridge, and Elastic Nets\nSuppose we consider approximations to the conditional expectation that have a linear form\ng(x) = β⊤x =\nK\nX\nk=1\nβkxk,\nafter the covariates and the outcome are demeaned, and the covariates are normalized to\nhave unit variance. The traditional method for estimating the regression function in this\n[14]",
    "content_hash": "1441d823560b0d522a3c2e4f46723caf1bd5c40afbeb5421bd2b8f14db4ca79e",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "Regularized Linear Regression: Lasso, Ridge, and Elastic Nets",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "4a3c0bd4-5b88-4caf-80c7-2c2e3fb5f10e",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "The diﬀerences in performance between some of the traditional methods such as kernel\nregression and the modern methods such as random forests are particularly pronounced in\nsparse settings with a large number of more or less irrelevant covariates. Random forests\nare eﬀective at picking up on the sparsity and ignoring the irrelevant features, even if there\nare many of them, while the traditional implementations of kernel methods essentially waste\ndegrees of freedom on accounting for these covariates. Although it may be possible to adapt\nkernel methods for the presence of irrelevant covariates by allowing for covariate speciﬁc\nbandwidths, in practice there has been little eﬀort in this direction. A second issue is that\nthe modern methods are particularly good at detecting severe nonlinearities and high-order\ninteractions. The presence of such high-order interactions in some of the success stories of\nthese methods should not blind us to the fact that with many economic data we expect\nhigh-order interactions to be of limited importance. If we try to predicting earnings for\nindividuals, we expect the regression function to be monotone in many of the important\npredictors such as education and prior earnings variables, even for homogenous subgroups. This means that models based on linearizations may do well in such cases relative to other\nmethods, compared to settings where monotonicity is fundamentally less plausible, as, for\nexample, in an image recognition problem. This is also a reason for the superior performance\nof locally linear random forests (Friedberg et al. [2018]) relative to standard random forests. We discuss four speciﬁc sets of methods, although there are many more, including varia-\ntions on the basic methods.",
    "content_hash": "b013a97089610e74b3922fb17a61f16b64359225b3d4ade5b2d1a838cee5f282",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "Supervised Learning for Regression Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "507602ec-fede-4202-8bfd-9aab5d3e0f4e",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "In addition there are many hybrid methods and modiﬁ-\ncations, including elastic nets which combines penalty terms from LASSO and ridge (Zou\nand Hastie [2005]), the relaxed lasso, which combines least squares estimates from the sub-\nset selected by LASSO and the LASSO estimates themselves (Meinshausen [2007]), Least\nAngle Regression (Efron et al. [2004]), the Dantzig Selector (Cand`es and Tao [2007]), the\nNon-negative Garrotte (Breiman [1993]) and others. There are a couple of important conceptual diﬀerences between these three special cases,\nsubset selection, LASSO, and ridge regression. See for a recent discussion Hastie et al. [2017]. Both best subset and LASSO lead to solutions with a number of the regression\ncoeﬃcients exactly equal to zero, a sparse solution. For the ridge estimator on the other\nhand all the estimated regression coeﬃcients will generally diﬀer from zero. It is not always\nimportant to have a sparse solution, and often the variable selection that is implicit in\nthese solutions is over-interpreted. Second, best subset regression is computationally hard\n(NP-hard), and as a result not feasible in settings with N and K large, although recently\n[15]",
    "content_hash": "60ee844d4f74c9b527d3343765c887ee621c622f828e05562b9bb969df6153ae",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "Regularized Linear Regression: Lasso, Ridge, and Elastic Nets",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b45b0b03-54b9-4456-a81a-d532bd32155b",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "progress has been made in this regard (Bertsimas et al. [2016]). LASSO and ridge have a\nBayesian interpretation. Ridge regression gives the posterior mean and mode under a Normal\nmodel for the conditional distribution of Yi given Xi, and Normal prior distributions for the\nparameters. LASSO gives the posterior mode given Laplace prior distributions. However,\nin contrast to formal Bayesian approaches, the coeﬃcient λ on the penalty term is in the\nmodern literature choosen through out-of-sample crossvalidation rather than subjectively\nthrough the choice of prior distribution. 3.2\nRegression Trees and Forests\nRegression trees (Breiman et al. [1984]), and their extension random forests (Breiman [2001a])\nhave become very popular and eﬀective methods for ﬂexibly estimating regression func-\ntions in settings where out-of-sample predictive power is important. They are considered\nto have great out-of-the-box performance without requiring subtle tuning. Given a sample\n(Xi1, . . . , XiK, Yi), for i = 1, . . . , N, the idea is to split the sample into subsamples, and\nestimate the regression function within the subsamples simply as the average outcome. The\nsplits are sequential and based on a single covariate Xik at a time exceeding a threshold c. Starting with the full training sample, consider a split based on feature or covariate k, and\nthreshold c. The sum of in-sample squared errors before the split was\nQ =\nN\nX\ni=1\nYi −Y\n\u00012 ,\nwhere Y = 1\nN\nN\nX\ni=1\nYi.",
    "content_hash": "433ae37be27e2ae030cddc14104389039f8e6dc593e3925408127651693621a6",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "4771f2b7-bf86-4b44-b93f-bae1503a46c7",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "After a split based on covariate k and threshold c the sum of in-sample squared errors is\nQ(k, c) =\nX\ni:Xik≤c\nYi −Y k,c,l\n\u00012 +\nX\ni:Xik>c\nYi −Y k,c,r\n\u00012 ,\nwhere (with “l” and “r” denoting “left” and “right”),\nY k,c,l =\nX\ni:Xik≤c\nYi\n. X\ni:Xik≤c\n1,\nand Y k,c,r =\nX\ni:Xik>c\nYi\n. X\ni:Xik>c\n1,\nare the average outcomes in the two subsamples. We split the sample using the covariate\nk and threshold c that minimize the average squared error Q(k, c) over all covariates k =\n1, . . . , K and all thresholds c ∈(−∞, ∞). We then repeat this, now optimizing also over the\nsubsamples or leaves. At each split the average squared error is further reduced (or stays\nthe same). We therefore need some regularization to avoid the overﬁtting that would result\nfrom splitting the sample too many times. One approach is to add a penalty term to the sum\n[16]",
    "content_hash": "b06d3289a241e2434da1e9d92a3a9d025b8acc8ef2674ca5ecbcdaf2a5beefc1",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "71e67f0e-438f-4c81-a3ea-9a603d7d3e9a",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Although trees are easy to interpret, it is important not to go too far in interpreting\nthe structure of the tree, including the selection of variables used for the splits. Standard\nintuitions from econometrics about “omitted variable bias” can be useful here. Particular\ncovariates that have strong associations with the outcome may not show up in splits because\nthe tree splits on covariates highly correlated with those covariates. [17]",
    "content_hash": "50292ba01056ab1de62db0cad05e6bb1f22e87d03833de18794c8c74b5753d16",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "809f3b61-7911-49c6-acce-02e084088178",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "of squared residuals that is linear in the number of subsamples (the leaves). The coeﬃcient\non this penalty term is then chosen through cross-validation. In practice, a very deep tree\nis estimated, and then pruned to a more shallow tree using cross-validation to select the\noptimal tree depth. The sequence of ﬁrst growing followed by pruning the tree avoids splits\nthat may be missed because their beneﬁts rely on subtle interactions. An advantage of a single tree is that it is easy to explain and interpret results. Once\nthe tree structure is deﬁned, then the prediction in each leaf is a sample average, and the\nstandard error of that sample average is easy to compute. However, it is not in general true\nthat the sample average of the mean within a leaf is an unbiased estimate of what the mean\nwould be within that same leaf in a new test set. Since the leaves were selected using the\ndata, the leaf sample means in the training data will tend to be more extreme (in the sense\nof being diﬀerent from the overall sample mean) than in an independent test set. Athey and\nImbens [2016] suggest sample splitting as a way to avoid this issue. If a conﬁdence interval\nfor the prediction is desired, then the analyst can simply split the data in half. One half of\nthe data is used to construct a regression tree. Then, the partition implied by this tree is\ntaken to the other half of the data where the sample mean within a given leaf is an unbiased\nestimate of the true mean value for the leaf.",
    "content_hash": "be35bd61511d4b5a006139ef799c12ead06e8fa172eefdf7febd2424b6da44fb",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "069ae1e0-9097-423a-b282-49b3a0377200",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "First, each tree is based not on the original sample, but on\na bootstrap sample (known as bagging (Breiman [1996])) or alternatively on a subsample\nof the data. Second, the splits at each stage are not optimized over all possible covariates,\n[18]",
    "content_hash": "ef827cb60705c4b72f27a124366f18abd6ebea5540130310d35e052a9c36ade5",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "22241624-9104-49bc-b370-a3660bea020a",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "but over a random subset of the covariates, changing every split. These two modiﬁcations\nlead to suﬃcient variation in the trees that the average is relatively smooth (although still\ndiscontinuous), and, more importantly, has better predictive power than a single tree. Random forests have become very popular methods. A key attraction is that they require\nrelatively little tuning and have great performance out-of-the-box compared to more complex\nmethods such as deep learning neural networks. Random forests and regression trees are\nparticularly eﬀective in settings with a large number of features that are not related to the\noutcome, that is, settings with sparsity. The splits will generally ignore those covariates,\nand as a result the performance will remain strong even in settings with a large number of\nfeatures. Indeed, when comparing forests to kernel regression, a reliable way to improve the\nrelative performance of random forests perform is to add irrelevant covariates that have no\npredictive power. These will rapidly degrade the performance of kernel regression, but will\nnot aﬀect random forest nearly as severely because it will largely ignore them [Wager and\nAthey, 2017]. Although the statistical analysis of forests had proved elusive since Breiman’s original\nwork, Wager and Athey [2017] show that a particular variant of random forests can produce\nestimates ˆµ(x) with an asymptotically normal distribution centered on the true value µ(x),\nand further, they provide an estimate of the variance of the estimator so that centered\nconﬁdence intervals can be constructed.",
    "content_hash": "73fc52a518c4ec486161e5461a4ccafda3cccbf75d65d3db2f26586cf951d9f0",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "7ead6f44-5f53-4b50-9f3b-940bd51df997",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "The variant they study uses subsampling rather\nthan bagging; and further, each tree is built using two disjoint subsamples, one used to\ndeﬁne the tree, and the second used to estimate sample means for each leaf. This honest\nestimation is crucial for the asymptotic analysis. Random forests can be connected to traditional econometric methods in several ways. Returning to the kernel regression comparison, since each tree is a form of matching estima-\ntor, the forest is an average of matching estimators. By averaging over trees, the prediction\nfor each point will be centered on the test point (except near boundaries of the covariate\nspace). However, the forest prioritizes more important covariates for selecting matches in a\ndata-driven way. Another way to interpret random forests (e.g. Athey et al. [2017d]), is that\nthey generate weighting functions analogous to kernel weighting functions. For example, a\nkernel regression makes a prediction at a point x by averaging nearby points, but weighting\ncloser points more heavily. A random forest, by averaging over many trees, will include\nnearby points more often than distant points. We can formally derive a weighting function\nfor a given test point by counting the share of trees where a particular observation is in the\n[19]",
    "content_hash": "f1b36eb2f6ca3f1fb43050f1a17ffdecc36279929d25afc546915d00d852c05c",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "767b663b-1909-45a3-94bf-d85dd7f454b0",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "same leaf as a test point. Then, random forest predictions can be written as\nˆµrf(x) =\nn\nX\ni=1\nαi(x) Yi,\nn\nX\ni=1\nαi(x) = 1,\nαi(x) ≥0,\n(3.1)\nwhere the weights αi(x) encode the weight given by the forest to the i-th training example\nwhen predicting at x. The diﬀerence between typical kernel weighting functions and forest-\nbased weighting functions is that the forest weights are adaptive; if a covariate has little\neﬀect, it will not be used in splitting leaves, and thus the weighting function will not be very\nsensitive to distance along that covariate.\nDiﬀerent\nTrees in Random Forest Generating Weights for Test Point X\nThe Kernel Based on Share of Trees in Same Leaf as Test Point X\nRecently random forests have been extended to settings where the interest is in causal\neﬀects, either average or unit-level causal eﬀects (Wager and Athey [2017]), as well as for\nestimating parameters in general economic models that can be estimated with maximum\nlikelihood or Generalized Method of Moments (GMM, Athey et al. [2017d]). In the latter\ncase, the interpretation of the forest as creating a weighting function is operationalized; the\nnew generalized random forest algorithm operates in two steps. First, a forest is constructed,\n[20]",
    "content_hash": "e1dedecb4bd49eff71fbdbec9cee515ade03d7238fde63a9f16c842d36880321",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "de08b7d9-ce7f-4e84-81b2-3fdccf20b299",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "and second, a GMM model is estimated for each test point, where points that are nearby in\nthe sense of frequently occuring in the same leaf as the test point are weighted more heavily in\nestimation. With an appropriate version of honest estimation, these forests produce param-\neter estimates with an asymptotically normal distribution. Generalized random forests can\nbe thought of as a generalization of local maximum likelihood, introduced by Tibshirani and\nHastie [1987], but where kernel weighting functions are used to weight nearby observations\nmore heavily than observations distant from a particular test point. A weakness of forests is that they are not very eﬃcient at capturing linear or quadratic\neﬀects, or at exploiting smoothness of the underlying data generating process. In addition,\nnear the boundaries of the covariate space, they are likely to have bias, because the leaves of\nthe component trees of the random forest cannot be centered on points near the boundary. Traditional econometrics encounters this boundary bias problem in analyses of regression\ndiscontinuity designs where, for example, geographical boundaries of school districts or test\nscore cutoﬀs determine eligibility for schools or programs (Imbens and Lemieux [2008]). The solution proposed in the econometrics literature, for example in the matching literature\n(Abadie and Imbens [2011]) is to use local linear regression, which is a regression with nearby\npoints weighted more heavily. Suppose that the conditional mean function is increasing as\nit approaches the boundary. Then the local linear regression corrects for the fact that at a\ntest point near the boundary, most sample points lie in a region with lower conditional mean\nthan the conditional mean at the boundary. Friedberg et al.",
    "content_hash": "3ffe7816537b5915b39f5564de0931c2259fe554d6f5b0c80d87e4ae7f5fc315",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "8f2140e4-56ac-4653-96bd-d4f66aa4aeac",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "[2018] extends the generalized\nrandom forest framework to local linear forests, which are constructed by running a regression\nweighted by the weighting function derived from a forest. In their simplest form, local linear\nforests just take the forest weights αi(x), and use them for local regression:\n(ˆµ(x), ˆθ(x)) = argminµ,θ\n( n\nX\ni=1\nαi(x)(Yi −µ(x) −(Xi −x)θ(x))2 + λ||θ(x)||2\n2\n)\n. (3.2)\nPerformance can be improved by modifying the tree construction to incorporate a regression\ncorrection; in essence, splits are optimized for predicting residuals from a local regression. This algorithm performs better than traditional forests in settings where a regression can\ncapture broad patterns in the conditional mean function such as monotonicity or a quadratic\nstructure, and again, asymptotic normality is established. Figure 1, from Friedberg et al. [2018], illustrates how local linear forests can improve on regular random forests; by ﬁtting\nlocal linear regressions with a random-forest estimated kernel, the resulting predictions can\nmatch a simple polynomial function even in relatively small data sets. In contrast, a forest\n[21]",
    "content_hash": "1ca043bde5474452685601766c0338f63a29accc3e76a976aafeb3d2d4a7b346",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f9f27808-6cdc-4296-bfc0-38b38a0093ce",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "G\nG\nGGGGG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nGG\nG\nG\nGG\nG\nGG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\n0\n2\n4\n6\n−1.0\n−0.5\n0.0\n0.5\n1.0\nx\ny\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nGG\nGG\nGG\nGG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nG\nG\nG\nG\nGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nGGG\nG\nG\nG\nG\nG\nG\nG\nGG\nG\nGG\nG\nG\nG\nGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n2\n4\n6\n−1.0\n−0.5\n0.0\n0.5\n1.0\nx\ny\nRandom forest\nLocal linear forest\nFigure 1: Predictions from random forests and local linear forests on 600 test points.",
    "content_hash": "16f52a7255f2a4ce0944290a152fa26520d95ce71bd446b7c99bd041707d47e5",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "90931835-9a3d-4e12-8d52-7f88686ea308",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "tends to have bias, particularly near boundaries, and in small data sets will have more\nof a step function shape. Although the ﬁgure shows the impact in a single dimension,\nan advantage of the forest over a kernel is that these corrections can occur in multiple\ndimensions, while still allowing the traditional advantages of a forest in uncovering more\ncomplex interactions among covariates.",
    "content_hash": "40014c577b6dce13fdf5118b30a2c8cba9bf9e174ba123e6a97d5af00f11f139",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "Regression Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c6f4c518-bd6e-4f9e-96f4-927aa2cff5d1",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Training\nand test data were simulated from Yi = log\n1 + e6Xi1\u0001\n+ ϵi, ϵi ∼N(0, 20) with X having\ndimension d = 20 (19 covariates are irrelevant) and errors ϵ ∼N(0, 20). Forests were trained\non n = 600 training points using the R package GRF, and tuned via cross-validation. Here\nthe true conditional mean signal µ(x) is in black, and predictions are shown in red. 3.3\nDeep Learning and Neural Nets\nNeural networks and related deep learning methods are another general and ﬂexible approach\nto estimating regression functions. They have been found to be very succesful in complex\nsettings, with extremely large number of features. However, in practice these methods require\na substantial amount of tuning in order to work well for a given application, relative to\nmethods such as random forests. Neural networks were studied in the econometric literature\nin the 1990s, but did not catch on at the time (see White [1992], Hornik et al. [1989], White\n[1992]). Let us consider a simple example. Given K covariates/features Xik, we model K1 la-\n[22]",
    "content_hash": "3277342eb4c26ccfbfbe593a39d654c32cccee55ed81037e9e0d5d419211bac6",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "Deep Learning and Neural Nets",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "bb5421cd-9cfd-4158-8a74-f70495681b8d",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "tent/unobserved variables Zik (hidden nodes) that are linear in the original covariates:\nZ(1)\nik =\nK\nX\nj=1\nβ(1)\nkj Xij,\nfor k = 1, . . . , K1. We then modify these linear combinations using a simple nonlinear transformation, e.g., a\nsigmoid function\ng(z) = (1 + exp(−z))−1,\nor a rectiﬁed linear function\ng(z) = z1z>0,\nand then model the outcome as a linear function of this nonlinear transformation of these\nhidden nodes plus noise:\nYi =\nK1\nX\nk=1\nβ(2)\nk g\n\u0010\nZ(1)\nik\n\u0011\n+ εi. This is a neural network with a single hidden layer with K1 hidden nodes. The transformation\ng(·) introduces nonlinearities in the model. Even with this single layer, with many nodes\none can approximate arbitrarily well a rich set of smooth functions. It may be tempting to ﬁt this into a standard framework and interpret this model simply\nas a complex, but fully parametric, speciﬁcation for the potentially nonlinear conditional\nexpectation of Yi given Xi:\nE[Yi|Xi = x] =\nK1\nX\nk′=1\nβ(2)\nk′ g\nK\nX\nk=1\nβ(1)\nk′kXik\n! . Given this interpretation, we can estimate the unknown parameters using nonlinear least\nsquares. We could then derive the properties of the least squares estimators, and functions\nthereof, under standard regularity conditions.",
    "content_hash": "92f9a675eed72c76d585fd5431047e1cd33c9a881b7822aeefedfd0e9480d336",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "Deep Learning and Neural Nets",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "81e10113-0c73-47e6-822f-c766cad1b648",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "However, this interpretation of a neural net\nas a standard nonlinear model would be missing the point, for four reasons. First, it is likely\nthat the asymptotic distributions for the parameter estimates would be poor approximations\nto the actual sampling distributions. Second, the estimators for the parameters would be\npoorly behaved, with likely substantial collinearity without careful regularization. Third,\nand more important, these properties are not of intrinsic interest. We are interested in the\nproperties of the predictions from these speciﬁcations, and these can be quite attractive\n[23]",
    "content_hash": "f6cdf334a5ce24695d92b35f25f827478e5981dc4153dd73a82df2ee405a41b5",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "Deep Learning and Neural Nets",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d89fcac1-5e5e-4eaa-b369-d1f52cc0aed0",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "[2015])\nIn cases with multiple hidden layers and many hidden nodes one needs to carefully reg-\nularize the parameter estimation, possibly through a penalty term that is proportional to\nthe sum of the squared coeﬃcients in the linear parts of the model. The architecture of the\nnetworks is also important. It is possible, as in the speciﬁcation above, to have the hidden\nnodes at a particular layer be a linear function of all the hidden nodes of the previous layer,\nor restrict them to a subset based on substantive considerations (e.g., proximity of covariates\nin some metric, such as location of pixels in a picture). Such convolutional networks have\nbeen very succesful, but require even more careful tuning (Krizhevsky et al. [2012]). Estimation of the parameters of the network is based on approximately minimizing the\nsum of the squared residuals, plus a penalty term that depends on the complexity of the\nmodel. This minimization problem is challenging, especially in settings with multiple hidden\n[24]",
    "content_hash": "22279c23996cd4204d4d2d194381cddf9986104001b6d064456f4b692ca14a45",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "Boosting",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ac8945dd-82a7-4c4c-9d46-ccd8ecd54d62",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "even if the properties of the paramater estimates are not. Fourth, we can make these models\nmuch more ﬂexible, and at the same time, make the properties of the corresponding least\nsquares estimators of the parameters substantially less tractable and attractive, by adding\nlayers to the neural network. A second layer of hidden nodes would have representations\nthat are linear in the same transformation g(·) of linear combinations of the ﬁrst layer of\nhidden nodes:\nZ(2)\nik =\nK1\nX\nj=1\nβ(2)\nkj g\n\u0010\nZ(1)\nij\n\u0011\n,\nfor k = 1, . . . , K2,\nwith the outcome now a function of the second layer of hidden nodes,\nYi =\nK2\nX\nk=1\nβ(3)\nk g\n\u0010\nZ(2)\nik\n\u0011\n+ εi. The depth of the network substantially increases the ﬂexibility in practice, even if with\na single layer and many nodes we can already approximate a very rich set of functions. Asymptotic properties for multilayer networks have recently been established in Farrell et al. [2018]. In applications researchers have used models with many layers, e.g., ten or more,\nand millions of parameters:\n“We observe that shallow models [models with few layers] in this context overﬁt\nat around 20 millions parameters while deep ones can beneﬁt from having over\n60 million. This suggests that using a deep model expresses a useful preference\nover the space of functions the model can learn.” LeCun et al.",
    "content_hash": "e2a46c7991eb6ac1017c47ac1dadd68ca0079bbbde51fab0a3f22bed6082d5a6",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "Deep Learning and Neural Nets",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ed443c26-5c24-49b8-94ad-bf3a407abd5f",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "layers. The algorithms of choice use the back-propagation algorithm and variations thereon\n(Rumelhart et al. [1986]) to calculate the exact derivatives with respect to the parameters\nof the unit-level terms in the objective function. This algorithm exploits in a clever way\nthe hierarchical structure of the layers, and the fact that each parameter enters only in a\nsingle layer. The algorithms then use stochastic gradient descent (Friedman [2002], Bottou\n[1998, 2012]) described in Section 2.6 as a computationally eﬃcient method for ﬁnding the\napproximate optimum. 3.4\nBoosting\nBoosting is a general purpose technique to improve the performance of simple supervised\nlearning methods. See Schapire and Freund [2012] for a detailed discussion. Let us say we\nare interested in prediction of an outcome given a substantial number of features. Suppose\nwe have a very simple algorithm for prediction, a simple base learner. For example, we could\nhave a regression tree with three leaves, that is, a regression tree based on a two splits, where\nwe estimate the regression function as the average outcome in the corresponding leaf. Such\nan algorithm on its own would not lead to a very attractive predictor in terms of predictive\nperformance because it uses at most two of the many possible features. Boosting improves\nthis base learner in the following way. Take for all units in the training sample the residual\nfrom the prediction based on the simple three-leaf tree model, Yi −ˆY (1)\ni\n. Now we apply the\nsame base learner (here the two split regression tree) with the residuals as the outcome of\ninterest (and with the same set of original features).",
    "content_hash": "690f0b64b8ac9bc032e366f1cd141d5372e5fd1960dd395b59459c7b676ece64",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "Boosting",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5baaa79e-9c61-4637-9332-b5b85fb675a3",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Let ˆY (2)\ni\ndenote the prediction from\ncombining the ﬁrst and second steps. Given this new tree we can calculate the new residual,\nYi −ˆY (2)\ni\n. We can then repeat these step, using the new residual as the outcome and again\nconstructing a two split regression tree. We can do this many times, and get a prediction\nbased on re-estimating the basic model many times on the updated residuals. If we base our boosting algorithm on a regression tree with L splits, it turns out that the\nresulting predictor can approximate any regression function that can be written as the sum\nof functions of L of the original features at a time. So, with L = 1, we can approximate any\nfunction that is additive in the features, and with L = 2 we can approximate any function\nthat is additive in functions of the original features that allow for general second order eﬀects. Boosting can also be applied using base learners other than regression trees. The key is to\nchoose a base learner that is easy to apply many times without running into computational\nproblems. [25]",
    "content_hash": "c0f8c61e31fbf132d359cf63ee3b558131d55b54b576c4845f7f686c1bd2e85c",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "Supervised Learning for Classiﬁcation Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f183eff5-1236-4fc0-9c4c-899baec04d87",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Here ML methods have been spectacularly successful. Support\nVector Machines (SVMs Cortes and Vapnik [1995]) greatly outperformed other methods in\nthe nineties. More recently deep convolutional neural networks (Krizhevsky et al. [2012])\nhave improved error rates even further. 4.1\nClassiﬁcation Trees and Forests\nTrees and random forests are easily modiﬁed from a focus on estimation of regression func-\ntions to classiﬁcation tasks. See Breiman et al. [1984] for a general discussion. Again we\nstart by splitting the sample into two leaves, based on a single covariate exceeding or not a\nthreshold. We optimize the split over the choice of covariate and the threshold. The diﬀer-\nence between the regression case and the classiﬁcation case is in the objective function that\nmeasures the improvement from a particular split. In classiﬁcation problems this is called\nthe impurity function. It measures, as a function of the shares of units in a given leaf with\n[26]",
    "content_hash": "1f12ca1a78da7110ce1ee96552ca6236cc7cb6d133b7a51203e2d31b90fb98dc",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "Support Vector Machines and Kernels",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "7c03d63c-8ac0-4d15-a2eb-ad23ea607944",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "4\nSupervised Learning for Classiﬁcation Problems\nClassiﬁcation problems are the focus of the other main branch of the supervised learning\nliterature. The problem is, given a set of observations on a vector of features Xi, and a label\nYi (an unordered discrete outcome), the goal is a function that assigns new units, on the\nbasis of their features, to one of the labels. This is very closely related to discrete choice\nanalysis in econometrics, where researchers specify statistical models that imply a probability\nthat the outcome takes on a particular value, conditional on the covariates/features. Given\nsuch an probability it is of course straightforward to predict a unique label, namely the one\nwith the highest probability. However, there are diﬀerences between the two approaches. An important one is that in the classiﬁcation literature the focus is often solely on the\nclassiﬁcation, the choice of a single label. One can classify given a probability for each label,\nbut one does not need such a probability to do the classiﬁcation. Many of the classiﬁcation\nmethods do not, in fact, ﬁrst estimate a probability for each label, and so are not directly\nrelevant in settings where such a probability is required. A practical diﬀerence is that the\nclassiﬁcation literature has often focused on settings where ultimately the covariates allow\none to assign the label with almost complete certainty, as opposed to settings where even\nthe best methods have high error rates. The classic example is that of digit recognition. Based on a picture, coded as a set of say\n16 or 256 black and white pixels, the challenge is to classify the image as corresponding to one\nof the ten digits from 0 to 9.",
    "content_hash": "d184c60cf4bf0ba7eca176e1f66d5bb68164f0aabde780043f47ff47bde7a148",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "Classiﬁcation Trees and Forests",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "cb102b20-0d73-483a-b5c9-d5d6daee29b4",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "a particular label, how impure that particular leaf is. If there are only two labels, we could\nsimply assign the labels the numbers zero and one, interpret the problem as one of esti-\nmating the conditional mean and use the average squared residual as the impurity function. That does not generalize naturally to the multi-label case. Instead a more common impurity\nfunction, as a function of the M shares p1, . . . , pM is the Gini impurity,\nI(p1, . . . , pM) = −\nM\nX\nm=1\npm ln(pm). This impurity function is minimized if the leaf is pure, meaning that all units in that leaf\nhave the same label, and is maximized if the shares are all equal to 1/M. The regularization\ntypically works again through a penalty term on the number of leaves in the tree. The same\nextension from a single tree to a random forest that was discussed for the regression case\nworks for the classiﬁcation case. 4.2\nSupport Vector Machines and Kernels\nSupport Vector Machines (SVMs, Vapnik [1998], Scholkopf and Smola [2001]) are another\nﬂexible set of methods for classiﬁcation analyses. SVMs can also extended to regression\nsettings, but are more naturally introduced in a classiﬁcation context, and for simplicity we\nfocus on the case with two possible labels. Suppose we have a set with N observations on a\nK-dimensional vector of features Xi and a binary label Yi ∈{−1, 1} (we could use 0/1 labels\nbut using -1/1 is more convenient).",
    "content_hash": "aa197bfb26e06e57b475dcd78c724cffaa03767bc20a62c63ac37b2b45a30fa5",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "Support Vector Machines and Kernels",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e185fe36-653f-4c62-ad59-3aa86d69beb0",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Given a K-vector of weights ω (what we would typically\ncall the parameters) and a constant b (often called the bias in the SVM literature), deﬁne\nthe hyperplane x ∈R such that ω⊤x + b = 0. We can think of this hyperplane deﬁning a\nbinary classiﬁer sgn(ω⊤Xi + b), with units i with ω⊤x + b ≥0 classiﬁed as 1 and units with\nω⊤x + b < 0 classiﬁed as -1. Now consider for each hyperplane (that is, for each pair (ω, b))\nthe number of classiﬁcation errors in the sample. If we are very fortunate there would be\nsome hyperplanes with no classiﬁcation errors. In that case there are typically many such\nhyperplanes, and we choose the one that maximizes the distance to the closest units. There\nwill typically be a small set of units that have the same distance to the hyperplane (the same\nmargin). These are called the support vectors. We can write this as an optimization problem as\n(ˆω,ˆb) = arg min\nω,b ∥ω∥2 ,\nsubject to Yi(ω⊤Xi + b) ≥1,\nfor all i = 1, . . . , N. [27]",
    "content_hash": "b178b9003ed7532dad517778e0a4a180c6f725d1964311d275e65f22ec5e7307",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "Support Vector Machines and Kernels",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2af320a1-ee1a-4ed5-a154-d521afe44d45",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Units far away from the hyperplane do not aﬀect the estimator as much in\nthe SVM approach as they do in a logistic regression, leading to more robust estimates. However, the real power from the SVM approach is in the nonlinear case. We can think of\nthat in terms of constructing a number of functions of the original covariates, φ(Xi), and\nthen ﬁnding the optimal hyperplane in the transformed feature space. However, because\nthe features enter only through the inner product X⊤\ni Xj, it is possible to skip the step of\nspecifying the transformations φ(·), and instead directly write the classiﬁer in terms of a\nkernel K(x, z), through\nmax\nα\nN\nX\ni=1\n(\nαi −1\n2\nN\nX\ni=1\nN\nX\nj=1\nαiαjYiYiK(Xi, Xj)\n)\n,\nsubject to 0 ≤αi ≤C,\nN\nX\ni=1\nαiYi = 0,\n[28]",
    "content_hash": "e21e36b1d6a4b1a3d93d0d79bc45f7f29886f0313a9d1b92d7ca771232f04b8e",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "K-means Clustering",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ce3e5efc-d970-4117-a908-eb8609634b40",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "This is an unusual problem, in the sense that there is no natural benchmark\nto assess whether a particular solution is a good one relative to some other one. A closely\nrelated approach that is more traditional in the econometrics and statistics literatures is\nmixture models, where the distribution that generated the sample is modelled as a mixture\nof diﬀerent distributions. The mixture components are similar in nature to the clusters. A key method is the k-means algorithm (Hartigan and Wong [1979], Alpaydin [2009]). Consider the case where we wish to partition the feature space into K subspaces or clusters. We wish to choose centroids b1, . . . , bK, and then assign units to the cluster based on their\nproximity to the centroids. The basic algorithm works as follows. We start with a set of\nK centroids, b1, . . . , bK, elements of the feature space, and suﬃciently spread out over this\nspace. Given a set of centroids, assign each unit to the cluster that minimizes the distance\nbetween the unit and the centroid of the cluster:\nCi = arg\nmin\nc∈{1,...,K} ∥Xi −bc∥2 . [29]",
    "content_hash": "8f7cb113a9b993f0df648106d26e91b91def8cb32045d9eb9eb998078ad05f20",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "Generative Adverserial Networks",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "69c7d61d-41ae-4a2b-b379-d01ecbcaf851",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "where ˆb solves P\ni ˆαi(Yi(X⊤\ni ω + ˆb) −1) = 0, with classiﬁer\nf(x) = sgn\nN\nX\ni=1\nYiαiK(Xi, x) + b\n! . Common choices for the kernel are kh(x, z) = exp(−(x −z)⊤(x −z)/h), or kκ,Θ(x, z) =\ntanh(κ(x−z)⊤(x−z)+Θ). The parameters of the kernel, capturing the amount of smoothing,\nare typically chosen through crossvalidation. 5\nUnsupervised Learning\nA second major topic in the ML literature is unsupervised learning. In this case we have a\nnumber of cases without labels. We can think of that as having a number of observations\non covariates without an outcome. We may be interested in partitioning the sample into a\nnumber of subsamples or clusters, or in estimating the joint distribution of these variables. 5.1\nK-means Clustering\nHere the goal is given a set of observations on features Xi, to partition the feature space\ninto a number of subspaces. These clusters may be used to to create new features, based on\nsubspace membership. For example, we may wish to use the partioning to estimate parsi-\nmonious models within each of the subspaces. We may also wish to use cluster membership\nas a way to organize the sample into types of units that may receive diﬀerent exposures to\ntreatments.",
    "content_hash": "cb52ad0a2f3cbaf6b7bd56c172410cfe065fdc171e8fdaac9b87103fc0ef03a8",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "K-means Clustering",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "8e13d2d8-5b14-48ea-9264-b2fff7bac3e1",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Then update the centroids as the average of the Xi in each of the clusters:\nbc =\nX\ni:Ci=c\nXi\nX\ni:Ci=c\n1. Repeatedly interate between the two steps. Choosing the number of clusters K is diﬃcult\nbecause there is no direct cross-validation method to assess the performance of one value\nversus the other. Often this number is chosen on substantive grounds rather than in a\ndata-driven way. There are a large number of alternative unsupervised methods, including topic models,\nwhich we discuss further below in the section about text. Unsupervised variants of neural\nnets are particularly popular for images and videos. 5.2\nGenerative Adverserial Networks\nNow let us consider the problem of estimation of a joint distribution, given observations on\nXi for a random sample of units. A recent ML approach to this is Generative Adverserial\nNetworks (GANs, Arjovsky and Bottou [2017], Goodfellow et al. [2014]). The idea is to\nﬁnd an algorithm to generate data that look like the sample X1, . . . , XN. A key insight is\nthat there is an eﬀective way of assessing whether the algorithm is succesful that is like a\nTuring test. If we have a succesful algorithm we should not be able to tell whether data\nwere generated by the algorithm, or came from the original sample. Hence we can assess\nthe algorithm by training a classiﬁer on data from the algorithm and a subsample from the\noriginal data. If the algorithm is succesful the classiﬁer cannot be succesfully classifying the\ndata as coming from the original data or the algorithm.",
    "content_hash": "ea92257a98a4d8a743ed44d53dbbd591247cc3bcdf2c2cca77896fa8c19ea763",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "Average Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "077c1c8d-ee3c-4ace-b010-65d54094e2be",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "The GAN then uses the relative\nsuccess of the classiﬁcation algorithm to improve the algorithm that generates the data, in\neﬀect pitting the classiﬁcation algorithm against the generating algorithm. This type of algorithm may also be an eﬀective way of choosing simulation designs in-\ntended to mimic real world data. 6\nMachine Learning and Causal Inference\nAn important diﬀerence between much of the econometrics literature and the machine learn-\ning literature is that the econometrics literature is often focused on questions beyond simple\nprediction. In many, arguably most, cases, researchers are interested in average treatment\neﬀects or other causal or structural parameters (see Abadie and Cattaneo [2018] and Imbens\n[30]",
    "content_hash": "89c9f83bcab454ca2c2cc23cd3e35fbdcc50e30acb4b57af0b12853a79063a3d",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "Average Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "95e2398e-3b85-49c9-a229-acdd65e6c6dc",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "and Wooldridge [2009] for surveys). Covariates that are of limited importance for prediction\nmay still play an important role in estimating such structural parameters. 6.1\nAverage Treatment Eﬀects\nA canonical problem is that of estimating average treatment eﬀects under unconfoundedness\n(Rosenbaum and Rubin [1983], Imbens and Rubin [2015]). Given data on an outcome Yi,\na binary treatment Wi, and a vector of covariates or features Xi, a common estimand, the\nAverage Treatment Eﬀect (ATE) is deﬁned as τ = E[Yi(1) −Yi(0)], where Yi(w) is the\npotential outcome unit i would have experienced if their treatment assignment had been\nw. Under the unconfoundedness assumption, which ensures that potential outcomes are\nindependent of the treatment assignment conditional on covariates\nWi ⊥⊥\n\u0010\nYi(0), Yi(1)\n\u0011 \f\f\f Xi,\nthe ATE is identiﬁed. The ATE can be characterized in an number of diﬀerent ways as\na functional of the joint distribution of (Wi, Xi, Yi). Three important ones are (i) as the\ncovariate-adjusted diﬀerence between the two treatment groups, (ii) as a weighted average\nof the outcomes, and (iii) in terms of the inﬂuence or eﬃcient score function.",
    "content_hash": "1016620c26d253f4675f59cf364cc4112a9e1e61be8d1f3372570354cc6ac7b4",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "Average Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2619ccc7-7538-45ba-83bf-ee52c8002eea",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "τ = E [µ(1, Xi) −µ(0, Xi)]\n= E\n\u0014 YiWi\ne(Xi) −Yi(1 −Wi)\n1 −e(Xi)\n\u0015\n= E\n\u0014(Yi −µ(1, Xi))Wi\ne(Xi)\n−(Yi −µ(0, Xi))(1 −Wi)\n1 −e(Xi)\n\u0015\n+ E [µ(1, Xi) −µ(0, Xi)] ,\n(6.3)\nwhere\nµ(w, x) = E[Yi|Wi = w, Xi = x],\nand\ne(x) = E[Wi|Xi = x]. One can estimate the average treatment eﬀect by using the ﬁrst representation by estimating\nthe conditional outcome expectations µ(·), using the second representation by estimating the\npropensity score e(·), or using the third representation and estimating both the conditional\noutcome expectation and the propensity score. Given a particular choice of representation,\nthere is the question of the appropriate estimator for the particular conditional expectations\nthat enter into that representation. For example, if we wish to use the ﬁrst representation,\nand want to consider linear models, it may seem natural to use LASSO or subset selection. However, as illustrated in Belloni et al. [2014], such a strategy could have very poor proper-\nties. The set of features that is optimal for inclusion when the objective is estimating µ(·)\n[31]",
    "content_hash": "cb561db31a9cd61b8efcf5004a333d2561fa9c2cb5d572fa094a8df561e0b078",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "Average Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f13d06e3-1960-4b49-bfcb-180fe3a04d99",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "is not necessarily optimal for estimating τ. The reason is that omitting from the regression\ncovariates that are highly correlated with the treatment Wi can introduce substantial biases\neven if their correlation with the outcome is only modest. Thus, optimizing model selection\nsolely for predicting outcomes is not the best approach. Belloni et al. [2014] propose using\na covariate selection method that selects both covariates that are predictive of the outcome\nand covariates that are predictive of the treatment, and show that this substantially improves\nthe properties of the corresponding estimator for τ. More recent methods focus on combinations of estimating both the conditional outcome\nexpectations µ(·) and the propensity score e(·) ﬂexibly and combining them in doubly ro-\nbust methods (Robins and Rotnitzky [1995], Chernozhukov et al. [2016a,b]), and methods\nthat combine estimating the conditional outcome expectations µ(·) with covariate balancing\n(Athey et al. [2016a]). Covariate balancing is inspired by another common approach in ML,\nwhich frame data analysis as an optimization problem. Here, instead of trying to estimate\na primitive object, the propensity score e(·), the optimization procedure directly optimizes\nweights for the observations that lead to the same mean values of covariates in the treat-\nment and control group (Zubizarreta [2015]). This approach allows for eﬃcient estimation\nof average treatment eﬀects even when the propensity score is too complex to estimate well.",
    "content_hash": "0c5a52e617ed608f352b8875be9c8c4fdb30141e53983b7ae3ee703dc3cda33d",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "Orthogonalization and Cross-Fitting",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "490049f8-a2e6-4d2a-bc23-cff0c94fc44a",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Because traditional propensity score weighting entails dividing by the estimated propensity\nscore, instability in propensity score estimation can lead to high variability in estimates for\nthe average treatment eﬀect. Further, in an environment with many potential confounders,\nestimating the propensity score using regularization may lead to the omission of weak con-\nfounders that still contribute to bias. Directly optimizing for balancing weights can be more\neﬀective in environments with many weak confounders. The case of estimating average treatment eﬀects under unconfoundedness is an example\nof a more general theme from econometrics; typically, economists prioritize precise estimates\nof causal eﬀects above predictive power (see Athey [2017, 2018] for further elaboration of\nthis point). In instrumental variables models, it is common that goodness of ﬁt falls by a\nsubstantial amount between an ordinary least squares regression and the second stage of a\ntwo-stage least squares model. However, the instrumental variables estimate of causal eﬀects\ncan be used to answer questions of economic interest, and so the loss of predictive power is\nviewed as less important. [32]",
    "content_hash": "504a7af8d949cc3c559ce90e8afae81faf2a1b209c309ce1ffba9e7d19e861ca",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "Orthogonalization and Cross-Fitting",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b4355f62-61d7-4f50-8647-d3b7d4fccfae",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "6.2\nOrthogonalization and Cross-Fitting\nA theme that has emerged across multiple distinct applications of machine learning to pa-\nrameter estimation is that both practical performance and theoretical guarantees can be\nimproved by using two simple techniques, both involving nuisance parameters that are es-\ntimated using machine learning. These can be illustrated through the lens of estimation of\naverage treatment eﬀects. Building from the third representation in (6.3), we can deﬁne the\ninﬂuence function of each observation as follows:\nψ(y, w, x) = µ(1, x) −µ(0, x) +\nw\ne(x)(y −µ(1, x) +\n1 −w\n1 −e(x)(y −µ(0, x)),\nwith Ψi = ψ(Yi, Wi, Xi). An estimate of the ATE can be constructed by ﬁrst constructing\nestimates ˆµ(w, x) and ˆe(x), and plugging those in to get an estimate ˆΨi = ˆψ(Yi, Wi, Xi) for\neach observation. Then, the sample average of ˆΨi is an estimator for the ATE. This approach\nis analyzed in Bickel et al. [1998] and Van der Vaart [2000] for the general semiparametric\ncase, and in Chernozhukov et al. [2017] for the average treatment eﬀect case.",
    "content_hash": "d1311d3fe106cba1bc20f47c3b3b190a67a5bebc9480b05fc841cc84a631ce4a",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "Orthogonalization and Cross-Fitting",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "1f57a2b9-782a-4a4d-be15-79e36f4b7028",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "When random forests are used to estimate the nuisance parameters, this is straightforward,\nsince “out-of-bag” predictions (standard in random forest statistical packages) provide the\npredictions obtained using trees that were constructed without using observation i. When\n[33]",
    "content_hash": "51066855aaabbdbdd4bdf5cf0b6dd7bdc90a161246a5b73cc1d50b668b77708f",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "Heterogenous Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "201f8587-9535-4322-ae32-3fd8f68a5709",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "A key result is\nthat an estimator based on this approach is eﬃcient if the estimators are suﬃciently accurate\nin the following sense:\nE[(ˆµ(w, Xi) −µ(w, Xi))2]\n1\n2E[(ˆe(Xi) −e(Xi))2]\n1\n2 = oP\nN −1/2\u0001\n. For example, each nuisance component, ˆµ(·) and ˆe(·), could converge at rate close to N −1/4,\nan order of magnitude slower than the ATE estimate. This works because Ψi makes use of\northogonalization; by construction, errors in estimating the nuisance components are orthog-\nonal to errors in Ψi. This idea is more general, and has been exploited in a series of papers,\nwith theoretical analysis in Chernozhukov et al. [2018a,c], and other applications including\nAthey et al. [2017d] for estimating heterogeneous eﬀects in models with unconfoundedness\nor those that make use of instrumental variables. A second idea, also exploited in the same series of papers, is that performance can be\nimproved using techniques such as sample splitting, cross-ﬁtting, out-of-bag prediction, and\nleave-one-out estimation. All of these techniques have the same ﬁnal goal: nuisance param-\neters estimated to construct the inﬂuence function ˆΨi for observation i (for the ATE case,\nˆµ(w, Xi) and ˆe(Xi)) should be estimated without using outcome data about observation i.",
    "content_hash": "b134a24b89c2fa6e44e43287d4d25978fa76ca0fa6d434e6d044c382355e0f09",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "Heterogenous Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "521b94e8-3005-4c5d-98d7-2cb5942e204e",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "other types of ML models are used to estimate the nuisance parameters, “cross-ﬁtting” or\nsample splitting advocates splitting the data into folds and estimating the nuisance parame-\nters separately on all data except a left-out fold, and then predicting the nuisance paramters\nin the left-out fold. When there are as many folds as observations, this is known as leave-\none-out estimation. Although these two issues are helpful in traditional small–data applications, when ML\nis used to estimate nuisance parameters (because there are many covariates), these issues\nbecome much more salient. Overﬁtting is more of a concern, and in particular, a single\nobservation i can have a strong eﬀect on the predictions made for covariates Xi when the\nmodel is very ﬂexible. Cross-ﬁtting can solve this problem. Second, we should expect that\nwith many covariates relative to the number of observations, accurate estimation of nuisance\nparameters is harder to achieve. Thus, orthogonalization makes estimation more robust to\nthese errors. 6.3\nHeterogenous Treatment Eﬀects\nAnother place where machine learning can be very useful is in uncovering treatment eﬀect\nheterogeneity, where we focus on heterogeneity with respect to observable covariates. Ex-\namples of questions include, which individuals beneﬁt most from a treatment? For which\nindividuals is the treatment eﬀect positive? How do treatment eﬀects change with covariates?",
    "content_hash": "ad152bde37025603cde2e7a0896e8e144a112b10ea4308a26a30e210accf072a",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "Heterogenous Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0fef355d-12ba-4391-8cb7-6f572f4e0f07",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Understanding treatment eﬀect heterogeneity can be useful for basic scientiﬁc understand-\ning, or for estimating optimal policy assignments; see Athey and Imbens [2017b] for further\ndiscussion. Continuing with the potential outcome notation from the last subsection, we deﬁne the\nconditional average treatment eﬀect (CATE) as τ(x) = E[τi|Xi = x], where τi = Yi(1)−Yi(0)\nis the treatment eﬀect for individual i. The CATE is identiﬁed under the unconfoundedness\nassumption introduced in the last subsection. Note that τi can not be observed for any unit;\nthis “fundamental problem of causal inference” (Holland [1986]) is the source of an appar-\nent diﬀerence between estimating heterogeneous treatment eﬀects and predicting outcomes,\nwhich are typically observed for each unit. We focus here on three types of questions: (i) learning a low-dimensional representation\nof treatment eﬀect heterogeneity, and conducting hypothesis tests about this heterogeneity;\n(ii) learning a ﬂexible (nonparametric) estimate of τ(x); and (iii) estimating an optimal\npolicy allocating units to either treatment or control on the basis of covariates x. [34]",
    "content_hash": "422217ae3d7af529bed44a78953e1c19b91ee595ba20961d6ef4e90360c4f251",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "Heterogenous Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f65bc1b9-9b82-4ba1-ad43-0aa0e7e9794c",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Athey and Imbens [2016] proposes several diﬀerent\npossible criterion to use for optimizing splits as well as for cross-validation. A ﬁrst insight\nis that when conducting model selection, it is only necessary to compare models. The τ 2\ni\nterm (which would be diﬃcult to estimate) cancels out when comparing two estimators, say\nˆτ ′(x) and ˆτ ′′(x). The remaining terms are linear in τi, and the expected value of τi can be\nestimated. If we deﬁne what Athey and Imbens [2016] call the transformed outcome\nY ∗\ni = Wi\nYi\ne(Xi) −(1 −Wi)\nYi\n1 −e(Xi),\nthen E[Y ∗\ni |Xi] = E[τi|Xi]. When the propensity score is unknown, it must be estimated,\nwhich implies that a criterion based on an estimate of the mean squared error of the CATE\nwill depend on modeling choices. Athey and Imbens [2016] build on this insight and propose several diﬀerent estimators\nfor the relative MSE of estimators for the CATE. They develop a method, which they call\ncausal tree, for learning a low-dimensional representation of treatment eﬀect heterogeneity,\nwhich provides reliable conﬁdence intervals for the parameters it estimates. The paper builds\non regression tree methods, creating a partition of the covariate space and then estimating\n[35]",
    "content_hash": "2455cb0849f936bf08b997de13b1b871710923765639c8412c532b304631b707",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "Heterogenous Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "8235d913-8f5c-432e-9e72-03d704cd329f",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "An important issue in adapting machine learning methods to focus on causal parameters\nrelates to the criterion function used in model selection. Predictive models typically use a\nmean squared error (MSE) criterion, P\ni(Yi −ˆµ(Xi))2/N to evaluate performance. Although\nthe MSE in a held-out test set is a noisy estimate to the population expectation of the MSE\nin an independent set, the sample average MSE is a good, that is, unbiased, approximation\nthat does not rely on further assumptions (beyond independence of observations), and the\nstandard error of the squared errors in the test set accurately captures the uncertainty in\nthe estimate. In contrast, consider the problem of estimating the CATE in observational\nstudies. It would be natural to use as a criterion function the mean squared error of treatment\neﬀects, P\ni(τi −ˆτ(Xi))2/N, where ˆτ(x) is the estimate of the CATE. However, this criterion\nis infeasible, since we do not observe unit-level causal eﬀects. Further, there is no simple,\nmodel-free unbiased estimate of this criterion in observational studies. For this reason,\ncomparing estimators, and as a result developing regularization strategies, is a substantially\nharder challenge in settings where we are interested in structural or causal parameters than\nin settings where we are interested in predictive performance. These diﬃculties in ﬁnding eﬀective cross-validation strategies are not always unsur-\nmountable, but they lead to a need to carefully adapt and modify basic regularization meth-\nods to address the questions of interest.",
    "content_hash": "4f5bf3664ee5312507d848daa52b6b2f29e4ef19b936ed2b779abdc716b6a803",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "Heterogenous Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e0e47efd-087c-4964-bbc6-bdfa9400dbe5",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "For example, if a\ntreatment decision must be made for a particular individual with covariates x, a regression\ntree may give a biased estimate for that individual given that the individual may not be in the\ncenter of the leaf, and that the leaf may contain other units that are distant in covariate space. In the traditional econometrics literature, non-parametric estimation could be accomplished\nthrough kernel estimation or matching techniques. However, their theoretical and practical\nproperties are poor with many covariates. Wager and Athey [2017] introduces causal forests. Essentially, a causal forest is the average of a large number of causal trees, where trees diﬀer\nfrom one another due to subsampling. Similar to prediction forests, a causal forest can be\nthought of as a version of a nearest neighbor matching method, but one where there is a\ndata-driven approach to determine which dimensions of the covariate space are important\nto match on. The paper establishes asymptotic normality of the estimator (so long as\ntree estimation is “honest,” making use of sample splitting for each tree) and provides an\nestimator for the variance of estimates so that conﬁdence intervals can be constructed. A challenge with forests is that it is diﬃcult to describe the output, since the estimated\nCATE function ˆτ(x) may be quite complex. However, in some cases one might wish to test\nsimpler hypotheses, such as the hypothesis that the top 10% of individuals ranked by their\nCATE have a diﬀerent average CATE than the rest of the population. Chernozhukov et al. [36]",
    "content_hash": "6a9c835fdc53854670a6f16c81fcf075d932317d9c02e0f8ed2402a80bdbaf49",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "Heterogenous Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b15cb6fb-c884-4416-966b-449bc2580c5d",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "A main motivation for understanding treatment eﬀect heterogeneity is that the CATE\ncan be used to deﬁne policy assignment functions, that is, functions that map from the\nobservable covariates of individuals to policy assignments. A simple way to deﬁne a policy\nis to estimate ˆτ(x) and to assign the treatment to all individuals with positive values of\nˆτ(x), where the estimate should be augmented with any costs of being in the treatment or\ncontrol group. Hirano and Porter [2009] shows that this is optimal under some conditions. A concern with this approach, depending on the method used to estimate ˆτ(x), is that the\npolicy may be very complex and is not guaranteed to be smooth. Kitagawa and Tetenov [2015] focus on estimating the optimal policy from a class of\npotential policies of limited complexity in an observational study with known propensity\nscores. The goal is to select a policy function to minimize the loss from failing to use the\n(infeasible) ideal policy, referred to as the “regret” of the policy. Athey and Wager [2017] also\nstudies policies with limited complexity and accomodates other constraints, such as budget\nconstraints on the treatment, and proposes an algorthm for estimating optimal policies. The\npaper provides bounds on the performance of its algorithm for the case where the data come\nfrom an observational study under confoundedness and the propensity score is unknown. [37]",
    "content_hash": "7386a1dbfe2fc2f444501f5c79a2490c2ba022ad6babbad802e837cf7ca26dc8",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "Multi-Armed Bandits",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "01c756c0-6abc-4b8e-b453-3291e0e980ab",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "The paper also extends the analysis to settings that do not satisfy unconfoundedness, for\nexample, to settings where there is an instrumental variable. Athey and Wager [2017] shows how bringing in insights from semi-parametric eﬃciency\ntheory enables tighter bounds on performance than the ML literature, thus narrowing down\nsubstantially the set of algorithms that might achieve the regret bound. For the case of\nunconfoundedness, the policy estimation procedure recommended by Athey and Wager [2017]\ncan be written as follows, where Π is the set of functions π : X →0, 1, and ˆΨi is deﬁned\nabove and makes use of cross-ﬁtting as well as orthogonalization:\nmaxπ∈Π\nX\ni\n(2π(Xi) −1) · ˆΨi\n(6.4)\nThe topic of optimal policy estimation has received some attention in the ML literature,\nfocusing on data from observational studies with unconfoundedness, including Strehl et al. [2010], Dudik et al. [2011], Li et al. [2012], Dudik et al. [2014], Li et al. [2014], Swaminathan\nand Joachims [2015], Jiang and Li [2016], Thomas and Brunskill [2016], Kallus [2017]. Zhou\net al. [2018] analyzes the case with more than two arms, extending the eﬃciency results of\nAthey and Wager [2017]. One insight that comes out of the ML approach to this problem is that the optimization\nproblem 6.4 can be reframed as a classiﬁcation problem and thus solved with oﬀ-the-shelf\nclassiﬁcation tools.",
    "content_hash": "93576c00eabf0fe741d03621e7d1e8bb361a7605074a6bc6716f0c63a13cf9b7",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "A/B Testing versus Multi-Armed Bandits",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "49474f06-0817-4660-8eb7-d20ab6d633f6",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "This means that the probability of assign-\nment to a treatment arm for which we are conﬁdent that it is inferior to some of the other\ntreatments is low, and eventually all new units will be assigned to the optimal treatment\nwith probability close to one. To provide some more intuition, consider a case with K treatments where the outcome\nis binary, so the model is a binomial distribution with treatment-arm-speciﬁc success proba-\nbility pk, k = 1, . . . , K. If the prior distribution for all probabilities is uniform, the posterior\ndistribution for the success probability of arm k, given Mk succeses in the Nk trials concluded\nsofar, is Beta with parameters Mk + 1 and Nk −Mk + 1. Given that the Beta distribution\nis simple to approximate by simulation the probability that treatment arm k is the optimal\none (the one with the highest success probability), pr(pk = maxK\nm=1 pm). We can simplify the calculations by updating the assignment probabilities only after\n[39]",
    "content_hash": "162badb62d1fa5153873f7b34ccb06a24d27810f0647ca262cea0ca552124e53",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "Contextual Bandits",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "532d317e-a490-40f7-a50e-6c8bc8b67a4e",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "the outcomes are measured the average eﬀect of the treatment would be estimated using\nthe diﬀerence in average outcomes by treatment arm. This is a potentially very ineﬃcient\nway of experimentation where we waste units by assigning them to treatment arms that\nwe already know with a high degreee of uncertainty to be inferior to some of the other\narms. Modern methods for experimentation focus on balancing exploration of new treatments\nwith exploition of treatments currently assessed to be of high quality. Suppose what we\nare interested in is primarly ﬁnding a treatment that is good among the set of treatments\nconsidered, rather than in estimation of expected outcomes for the full set of treatments. Moreover, suppose that we measure the outcomes quickly after the treatments have been\nassigned, and suppose the units arrive sequentially for assignment to a treatment. After\noutcomes for half the units have been observed, we may have a pretty good idea which of the\ntreatments are still candidates for the optimal treatment. Exposing more units to treatments\nthat are no longer competitive is suboptimal for both exploration and exploitation purposes:\nit does not help us distinguish between the remaining candidate optimal treatments, and it\nexposes those units to inferior treatments. Multi-armed bandit approaches (Scott [2010], Thompson [1933]) attempt to improve over\nthis static design. In the extreme case, the assignment for each unit depends on all the in-\nformation learned up to that point. Given that information, and given a parametric model\nfor the outcomes for each treatment, and a prior for the parameters of these models, we\ncan estimate the probability of each treatment being the optimal one. Thompson sampling\nsuggests assigning the next unit to each treatment with probability equal to the probability\nthat that particular treatment is the optimal one.",
    "content_hash": "ce7108340681e8c9f7a7e338e2b4832c970777583320d07dee46d1de92e5d0ce",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "A/B Testing versus Multi-Armed Bandits",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "89b1789b-8c06-4345-aac9-ba168360b6d7",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "seeing a number of new observations. That is, we re-evaluate the assignment probabilities\nafter a batch of new observations has come in, all based on the same assignment probabilities. From this perspective we can view a standard A/B experiment as one where the batch is\nthe full set of observations. From that perspective it is clear that to, at least occasionally,\nupdate the assignment probabilities to avoid sending units to inferior treatments, is a superior\nstrategy. An alternative approach is to use the Upper Conﬁdence Bounds (UCB, Lai and Robbins\n[1985]) approach. In that case we construct a 100(1-p)% conﬁdence interval for the popula-\ntion average outcome µk for each treatment arm. We then collect the upper bounds of these\nconﬁdence intervals for each treatment arm and assign the next unit to the treatment arm\nwith the highest value for the upper conﬁdence bounds. As we get more and more data, we\nlet one minus the level of the conﬁdence intervals p go to zero slowly. With UCB methods\nwe need to be more careful with if we wish to update assignments only after batches of units\nhave come in. If two treatment arms have very similar UCBs, assigning a large number of\nunits to the one that has a slightly higher upper conﬁdence bound may not be satisfactory:\nhere Thompson sampling would assign similar numbers of units to both those treatment\narms.",
    "content_hash": "ced6c5aa4f5ecc2f3fb45a6f0043572b2f20c1f26f47f07db961a4d47a389594",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "Contextual Bandits",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "acc0a041-7b1b-4dc4-80ba-c18403b1cffc",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "More generally, the stochastic nature of the assignment under Thompson sampling,\ncompared to the deterministic assignment in the UCB approach, has conceptual advantages\nfor the ability to do randomization inference (e.g., Athey and Imbens [2017a]). 7.2\nContextual Bandits\nThe most important extension of multi-armed bandits is to settings where we observe fea-\ntures of the units that can be used in the assignment mechanism. If treatment eﬀects are\nheterogeneous, and that heterogeneity is associated with observed characteristics of the units,\nthere may be substantial gains from assigning units to diﬀerent treatments based on these\ncharacteristics. See Dimakopoulou et al. [2018] for detials. A simple way to incorporate covariates would be to build a parametric model for the\nexpected outcomes in each treatment arm (the reward function, estimate that given the\ncurrent data and infer from there the probability that a particular arm is optimal for a new\nunit conditional on the characteristics of that unit. This is conceptually a straightforward\nway to incorporate characteristics, but it has some drawbacks. The main concern is that\nsuch methods may implicitly rely substantially on the model being correctly speciﬁed. It\nmay be the case that the data for one treatment arm come in with a particular distribution\n[40]",
    "content_hash": "04e54c71ccd5669c9ee36a30676cbed14280a38a2085efcd4c58397c69ad7675",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "Contextual Bandits",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "07638432-f1f8-4b13-a306-edb572cbdd65",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "of the characteristics, but it gets used to predict outcomes for units with very diﬀerent\ncharacteristics. See Bastani and Bayati [2015] for some discussion. A risk is that if the\nalgorithm estimates a simple linear model mapping characteristics to outcomes, then the\nalgorithm may suggest a great deal of certainty about outcomes for an arm in a region of\ncharacteristic space where that treatment arm has never been observed. This can lead the\nalgorithm to never experiment with the arm in that region, allowing for the possibility that\nthe algorithm never corrects its mistake and fails to learn the true optimal policy even in\nlarge samples. As a result one should be careful in building a ﬂexible model relating the characteristics\nto the outcomes. Dimakopoulou et al. [2017] highlight the beneﬁts of using random forests\nas a way to avoid making functional form assumptions. Beyond this issue, a number of novel considerations that arise in contextual bandits. Because the assignment rules as a function of the features changes as more units arrive,\nand tend to assign more units to a given arm in regions of the covariate space where it\nhas performed well in the past, particular care has to be taken to eliminate biases in the\nestimation of the reward function. Thus, although there is formal randomization, the issues\nconcerning robust estimation of conditional average causal eﬀects in observational studies\nbecome relevant here. One solution, motivated by the literature on causal inference, is\nto use propensity score weighting of outcome models. Dimakopoulou et al.",
    "content_hash": "7b7bf821eeb27dee9b7f6e2040a8fcf1b799cd148bbecea720a70e0f5f8645fc",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "The Netﬂix Problem",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2f8436cd-fac5-4576-8754-3587bbb5ddfb",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "[2018] studies\nbounds on the performance of contextual bandits using doubly robust estimation (propensity-\nweighted outcome modeling), and also demonstrates on a number of real-world datasets that\npropensity weighting improves performance. Another insight is that it can be useful to make use of simple assignment rules, particu-\nlarly in early stages of bandits, because complex assignment rules can lead to confounding\nlater. In particular, if a covariate is related to outcomes and is used in assignment, then\nlater estimation much control for this covariate to eliminate bias. For this reason, LASSO,\nwhich selects a sparse model, can perform better than Ridge, which places weights on more\ncovariates, when estimating an outcome model that will be used to determine the assignment\nof units in subsequent batches. Finally, ﬂexible outcome models can be important in certain\nsettings; random forests can be a good alternative in those cases. [41]",
    "content_hash": "6f9bc774f6b51403504e99716a4d4337db382fe38c149a8fb929058920a8fa02",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "The Netﬂix Problem",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c8bc3983-3a20-49e0-b9c9-6f268452afb5",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "8\nMatrix Completion and Recommender Systems\nSo far the methods we have discussed are primarily for settings where we observe information\non a number of units in the form of a single outcome and a set of covariates or features,\nwhat is known in the econometrics literature as a cross-section setting. There are also many\ninteresting new methods for settings that resemble what are in the econometric literature\nreferred to as longitudinal or panel data settings. Here we discuss a canonical version of that\nproblem, and then consider some speciﬁc methods. 8.1\nThe Netﬂix Problem\nThe Netﬂix Prize Competition was set up in 2006 (Bennett et al. [2007]), and asked re-\nsearchers to use a training data set to develop an algorithm that improved on the Netﬂix\nalgorithm for recommending movies by providing predictions for movie ratings. Researchers\nwere given a training data set that contained movie and individual characteristics, as well\nas movie ratings, and were asked to predict ratings for movie/individual pairs for which\nthey were not given the ratings. Because of the magnitude of the prize, $1,000,000, this\ncompetition and the associated problem generated a lot of attention, and the development\nof new methods for this type of setting accelarated substantially as a result. The winning\nsolutions, and those that were competitive with the winners, had some key features. One is\nthat they relied heavily on model averaging. Second, many of the models included matrix\nfactorization and nearest neighbor methods. Although it may appear at ﬁrst as a problem that is very distinct from the type of problem\nstudied in econometrics, one can cast many econometric panel data in a similar form.",
    "content_hash": "15863dfa1c81aa861114bc1bc3164e66a76716f5774dc97a4f1f53491cb82448",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "Matrix Completion Methods for Panel Data",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "fd9d2d0e-17f7-47ea-967b-64a14dcc01ca",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "In\nsettings where researchers are interested in causal eﬀects of a binary treatment, one can think\nof the realized data as consisting of two incomplete potential outcome matrices, one for the\noutcomes given the treatment, and one for the outcomes given the control treatment. Hence\nthe problem of estimating the average treatment eﬀects can be cast as a matrix completion\nproblem. Suppose we observe outcomes on N units, over T time periods, with the outcome\nfor unit i at time period t denoted by Yit, and a binary treatment, denoted by Wit, with\nY =\n\n\n\n\n\n\n\nY11\nY12\nY13\n. . . Y1T\nY21\nY22\nY23\n. . . Y2T\nY31\nY32\nY33\n. . . Y3T\n... ... ... ... ... YN1\nYN2\nYN3\n. . . YNT\n\n\n\n\n\n\n\nW =\n\n\n\n\n\n\n\n1\n1\n0\n. . . 1\n0\n0\n1\n. . . 0\n1\n0\n1\n. . . 0\n... ... ... ... ... 1\n0\n1\n. . . 0\n\n\n\n\n\n\n\n. [42]",
    "content_hash": "ce51f048a76dc901cd442a944a260e3ebfa9db3ce21d0ae18a2f87e132f5e3ee",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "Control Methods",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "309e1320-494c-4359-bfab-c20f687a4533",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Using the singular value decomposition, L = USV⊤, where U is an N × N matrix, V is a\nT × T matrix, and S is an N × T matrix with rank R, with the only non-zero elements on\nthe diagonal (the singular values). We are not interested in estimating the matrices U and\nV, only in the product USV⊤, and possibly in the singular values, the diagonal elements\nof S. Obviously some regularization is required, and an eﬀective one is to use the nuclear\nnorm ∥· ∥∗, which is equal to the sum of the singular values. Building on the ML literature\n(Cand`es and Recht [2009], Mazumder et al. [2010]), Athey et al. [2017a] focus on estimating\n[43]",
    "content_hash": "f4177a45352bb647097693bcb49704f12e9e8897c815c32edc68121c68bc7fb7",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "Demand Estimation in Panel Data",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "cb0ddf55-8d85-4062-ba78-4594696acecd",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "[2010, 2015] propose\nimputing these using a weighted average of the outcomes for other units in the same period. Doudchenko and Imbens [2016] show that the Abadie et al. [2015] methods can be viewed\nas regression the outcomes for the last row on outcomes for the other units, and using the\nregression estimates from that to impute the missing values, in what Athey et al. [2017a] call\nthe vertical regression. This contrasts with a horizontal regression, common in the program\nevaluation literature where outcomes in the last period are regressed on outcomes in earlier\nperiods, and those estimates are used to impute the missing values. In contrast to both the\nhorizontal and vertical regression approaches the matrix completion approach in principle\nattempts to exploit both stable patterns over time and stable patterns between units in\nimputing the missing values, and also can deal directly with more complex missing data\npatterns. [44]",
    "content_hash": "600051085e147d7a621e93e906c4df69cb2d474b88fd31ef415c98e169b6504b",
    "location": null,
    "page_start": 45,
    "page_end": 45,
    "metadata": {
      "section": "Demand Estimation in Panel Data",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "bae6a268-18a2-4c1f-a741-844ab72f2eba",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "L by minimizing\nmin\nL\n\n\n\nX\n(i,t)∈O\n(Yit −Lit)2 + λ ∥L∥∗\n\n\n,\nwhere λ is a penalty parameter chosen through cross-validation. Using the nuclear norm\nhere, rather than the rank of the matrix L is important for computational reasons. Using\nthe Frobenius norm, equal to the sum of the squares of the singular values would not work\nbecause it is equal to the sum of the squared values of the matrix, and thus would lead to\nimputing all missing values as zeros. For the nuclear norm case there are eﬀective algorithms\nthat can deal with large N and large T. See Cand`es and Recht [2009], Mazumder et al. [2010]. 8.3\nThe Econometrics Literature on Panel Data and Synthetic\nControl Methods\nThe econometrics literature has studied these problems from a number of diﬀerent perspec-\ntives. The panel data literature traditionally focused on ﬁxed eﬀect methods, and has gener-\nalized those to models with multiple latent factors (Bai and Ng [2017, 2002], Bai [2003]) that\nare essentially the same as the low rank factorizations in the ML literature. The diﬀerence\nis that in the econometrics literature there has been more focus on actually estimating the\nfactors, and using normalizations that allow for their identiﬁcation. Typically it is assumed\nthat there is a ﬁxed number of factors. The synthetic control literature studied similar settings, but focused on the case with\nonly missing values for a single row of the matrix Y. Abadie et al.",
    "content_hash": "9294d7996c522bb287ca70a5178e14a8e3ce9e499a94782c395c72f5f9672794",
    "location": null,
    "page_start": 45,
    "page_end": 45,
    "metadata": {
      "section": "Demand Estimation in Panel Data",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "377fb6db-282d-4359-8964-9245c6f052a7",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "If the\nconsumer selects the item with highest utility, then\nPr(Yijt = j) =\nexpµij−φipjt\nP\nj′ expµij−φipjt\nFrom the machine learning perspective, a panel dataset with consumer choices might\nbe studied using techniques from matrix completion, as described above. The model would\ndraw inferences from products that had similar purchase patterns across consumers, as well as\nconsumers who had similar purchase patterns across products. However, such models would\ntypically not be well-suited to analyze the extent to which two products are substitutes, or\nto analyze counterfactuals. For example, Jacobs et al. [2014] propose using a related latent factorization approach\nin order to ﬂexibly model consumer heterogeneity in the context of online shopping with\na large assortment of products. They use data from medium sized online retailer. They\nconsider 3,226 products, and aggregate up to the category x brand level to reduce to 440\n“products.” They do not model responses to price changes or substitution between similar\n[45]",
    "content_hash": "9d49d07112bea265a740203a6284ba2d264f1962e8bcfc5019cf7e23d3011628",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "Demand Estimation in Panel Data",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b73a6903-179e-4cd1-8352-486d3205b8fc",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "8.4\nDemand Estimation in Panel Data\nA large literature in economics and marketing focuses on estimating consumer preferences\nusing data about their choices. A typical paper analyzes the discrete choice of a consumer\nwho selects a single product from a set of prespeciﬁed imperfect substitutes, e.g. laundry\ndetergent, personal computers, or cars (see, e.g., Keane et al. [2013] for a review). The lit-\nerature typically focuses on one product category at the time, and typically models choices\namong a small number of products. Often this literature focuses on estimating cross-price\nelasticities, so that counterfactuals about ﬁrm mergers or price changes can be analyzed. Although it is common to incorporate individual-speciﬁc preferences for observable charac-\nteristics, such as prices and other product characteristics, there are typically a small number\nof latent variables in the models. A standard set-up starts with consumer i’s utility for\nproduct j at time t, where\nUijt = µij −φipjt + ϵijt,\nwhere ϵijt has an extreme value distribution and is independently and identically dis-\ntributed across consumers, products, and time. µij is consumer i’s mean utility for product\nj, φi is consumer i’s price sensitivity, and pjt is the price of product j at time t.",
    "content_hash": "dabdf2a8bcad116317311639c4a5d57cf423709b19f1e185d70f615585b5ddce",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "Demand Estimation in Panel Data",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "efe5af9e-e324-4dec-b93e-f559a8663bab",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "To see how matrix factorization can augment a standard consumer choice model, we can\nwrite the utility of consumer i for product j at time t as\nUijt = β′\niθj −ρ′\niαjpjt + ϵijt,\nwhere βi, θj, ρi, and αj are each vectors of latent variables. The vector θj, for example,\ncan be interpreted as a vector of latent product characteristics for product j, while βi rep-\nresents consumer i’s latent preferences for those characteristics. The basic functional form\nfor choice probabilities is unchanged, except that the utilities are now functions of the latent\ncharacteristics. Such models had not been studied in the machine learning literature until recently, in\npart because the functional form for choice probabilities, which is nonlinear in a large num-\n[46]",
    "content_hash": "8d1a6a85c86856d946869a07be440eb346a085dcdf4fe676942d60602bb6557c",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "Text Analysis",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9cdd66c5-42db-40e0-aebc-5855605a7a80",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "products; instead, in the spirit of the machine learning literature, they evaluate performance\nin terms of predicting which new products a customer will buy. In contrast to this “oﬀ-the-shelf” application of machine learning to product choice, a\nrecent literature has emerged that attempts to combine machine learning methods with\ninsights from the economics literature on consumer choice, typically in panel data settings. A theme of this literature is that models that take advantage of some of the structure of\nthe problem will outperform models that will not. For example, the functional form implied\nby the consumer choice model from economics places a lot of structure on how products\nwithin a category interact with one another. An increase in the price of one product aﬀects\nother products in a particular way, implied by the functional form. To the extent that the\nrestrictions implied by the functional form are good approximations to reality, they can\ngreatly improve the eﬃciency of estimation. Incorporating the functional forms that have\nbeen established to be eﬀective across decades of economic research can improve performance. On the other hand, economic models have typically failed to incorporate all of the infor-\nmation that is available in a panel dataset, the type of information that matrix completion\nmethods typically exploit. In addition, computational issues have prevented economists from\nstudying consumer choices across multiple product categories, even though in practice data\nabout a consumer’s purchases in one category is informative about the consumer’s purchases\nin other categories; and further, the data can also reveal which products tend to have sim-\nilar purchase patterns. Thus, the best-performing models from this new hybrid literature\ntend to exploit techniques from the matrix completion literature, and in particular, matrix\nfactorization.",
    "content_hash": "02b9b8b5d6722b50f1db9bf4575a780e05c9b3b3fb4c5fbd5548c475adc857d0",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "Demand Estimation in Panel Data",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "507889f6-fb89-405c-885b-d71c15885ba8",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "A complementary approach to one based\non latent product characteristics is the work by Semenova et al. [2018], who considers ob-\nservational high-dimensional product attributes (e.g., text descriptions and images) rather\nthan latent features. 9\nText Analysis\nThere is a large machine learning literature on analyzing text data. It is beyond the scope\nof this paper to fully describe this literature; Gentzkow et al. [2017] provides an excellent\nrecent review. Here, we provide a high-level overview. To start, we consider a dataset consisting of N documents, indexed by i = 1, .., N. Each\ndocument contains a set of words. One way to represent the data is as a N × T matrix,\ndenoted C, where T is the number of words in the language, where each element of the matrix\n[47]",
    "content_hash": "8d48faa347a4715676add3df92b5deb5a86fc3ca937a81f9ed48109adbfd1aef",
    "location": null,
    "page_start": 48,
    "page_end": 48,
    "metadata": {
      "section": "Text Analysis",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f846ad4b-b7da-4abc-a4cd-e441276338e3",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "ber of latent parameters, makes computation challenging. In contrast, traditional machine\nlearning models might treat all products as independently chosen (e.g. Gopalan et al. [2015]),\nmaking computation much easier. Ruiz et al. [2017] applies state-of-the-art computational\ntechniques from machine learning (in particular, stochastic gradient descent and variational\ninference) together with a number of approximations in order to make the method scalable\nto thousands of consumers making choices over thousands of items in dozens or hundreds of\nshopping trips per consumer. Ruiz et al. [2017] does not make use of any data about the\ncategories of products; it attempts to learn from the data (which incorporates substantial\nprice variation) which products are substitutes or complements. In contrast, Athey et al. [2017b] incorporates information about product categories and imposes the assumption that\nconsumers buy only one product per category on a given trip; the paper also introduces a\nnested logit structure, which allows utilities to be correlated across products within a cat-\negory, thus better accounting for consumers’ choices about whether to purchase a category\nat all. A closely related approach is taken in Wan et al. [2017]. They use a latent factorization\napproach that incorporates price variation. They model consumer choice as a three stage\nprocess: (i) Choose whether to buy the category, (ii) Choose which item in category, and\n(iii) Choose number of the item to purchase. The paper uses customer loyalty transaction\ndata from two diﬀerent datasets. In all of these approaches, using the utility maximization\napproach from economics makes it possible to perform traditional analyses such as analyzing\nthe impact of price changes on consumer welfare.",
    "content_hash": "b0e1d701f337cbfdb3c56dd0b16ec9aafe1ab58c4462a4c60aa92f8c9c632c8a",
    "location": null,
    "page_start": 48,
    "page_end": 48,
    "metadata": {
      "section": "Text Analysis",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2be3bb02-f9ea-470d-ac94-6d27f943a475",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "is an indicator for whether word t appears in document i. Such a representation would los\ninformation by ignoring the ordering of the words in the text. Richer reprsentations might\nlet T be the number of bigrams, where a bigram is a pair of words that appear adjacent to\none another in the document, or sequences of three of more words. There are two types of exercises we can do with this type of data. One is unsupervised\nlearning, and the other is supervised. For the unsupervised case, the goal would be to ﬁnd\na lower-rank representation of the matrix C. Given that a low-rank matrix can be well\napproximated by a factor structure, as discussed above, this is equivalent to ﬁnding a set\nof k latent characteristics of documents (denoted β) and a set of latent weights on these\ntopics, denoted θ, such that the probability that word t appears in document i is a function\nof θ′\niβj). This view of the problem basically turns the problem into a matrix completion\nproblem; we would say that a particular representation performs well if we hold out a test\nset of randomly selected elements of C, and the model predicts well those held-out elements. All of the methods described above for matrix completion can be applied here. One implementation of these ideas are referred to as topic models; see Blei and Laﬀerty\n[2009] for a review. These models specify a particular generative model of the data. In the\nmodel, there are a number of topics, which are latent variables. Each topic is associated\nwith a distribution of words. An article is characterized by weights on each topic.",
    "content_hash": "0b60f83838f1a483959a6b79ea31f272063d007875f962be50c123afeeaca601",
    "location": null,
    "page_start": 49,
    "page_end": 49,
    "metadata": {
      "section": "Text Analysis",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "bcf81cd4-ea2c-4e89-ac77-be313f44ab16",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "The goal\nof a topic model is to estimate the latent topics, the distribution over words for each topic,\nand the weights for each article. A popular model that does this is known as the Latent\nDirichlet Allocation model. More recently, more complex models of language have emerged, following the theme that,\nalthough simple machine learning models perform quite well, incorporating problem-speciﬁc\nstructure is often helpful and is typically incorporated in state-of-the art machine learning\nin popular application areas. Broadly, these are known as word embedding methods. These\nattempt to capture latent semantic structure in language; see [Mnih and Hinton, 2007, Mnih\nand Teh, 2012, Mikolov et al., 2013a,b,c, Mnih and Kavukcuoglu, 2013, Pennington et al.,\n2014, Levy and Goldberg, 2014, Vilnis and McCallum, 2015, Arora et al., 2016, Barkan, 2016,\nBamler and Mandt, 2017]. Consider the neural probabilistic language model of Bengio et al. [2003, 2006]. That model speciﬁes a joint probability of sequences of words, parameterized\nby a vector representation of the vocabulary. Vector representations of words (also known as\n“distributed representations”) can incorporate ideas about word usage and meaning [Harris,\n1954, Firth, 1957, Bengio et al., 2003, Mikolov et al., 2013b]. [48]",
    "content_hash": "a2e7781dd2d7575d7c9379f1f9768909e15fb180b0b71614c8af97eae1a7b34b",
    "location": null,
    "page_start": 49,
    "page_end": 49,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "8b0bbfe4-836d-4266-9d5a-6e6cdeb2a2b6",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "In this review we describe some of the methods we view as most\nuseful for economists, and that we view as important to include in the core graduate econo-\nmetrics sequences. Being familar with these methods will allow researchers to do more\nsophisticated empirical work, and to communicate more eﬀectively with researchers in other\nﬁelds. References\nAlberto Abadie and Matias D Cattaneo. Econometric methods for program evaluation. Annual Review of Economics, 10:465–503, 2018. [49]",
    "content_hash": "a45f17ee478d7736196748b68033d51672188b53ae0cd56137c5b8db25078e6f",
    "location": null,
    "page_start": 50,
    "page_end": 50,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "85b3a3bd-fb0a-4ea9-91ca-b8ce15d9e152",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Alberto Abadie and Guido W Imbens. Bias-corrected matching estimators for average treat-\nment eﬀects. Journal of Business & Economic Statistics, 29(1):1–11, 2011. Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Synthetic control methods for\ncomparative case studies: Estimating the eﬀect of California’s tobacco control program. Journal of the American Statistical Association, 105(490):493–505, 2010. Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Comparative politics and the\nsynthetic control method. American Journal of Political Science, pages 495–510, 2015. Ethem Alpaydin. Introduction to machine learning. MIT press, 2009. Joshua D Angrist and J¨orn-Steﬀen Pischke. Mostly harmless econometrics: An empiricist’s\ncompanion. Princeton University Press, 2008. Martin Arjovsky and L´eon Bottou. Towards principled methods for training generative\nadversarial networks. arXiv preprint arXiv:1701.04862, 2017. S. Arora, Y. Li, Y. Liang, and T. Ma. RAND-WALK: A latent variable model approach to\nword embeddings. Transactions of the Association for Computational Linguistics, 4, 2016. Susan Athey. Beyond prediction: Using big data for policy problems. Science, 355(6324):\n483–485, 2017. Susan Athey. The impact of machine learning on economics. The Economics of Artiﬁcial\nIntelligence, 2018. Susan Athey and Guido Imbens. Recursive partitioning for heterogeneous causal eﬀects.",
    "content_hash": "6e57567062eb9f8596e308ac44b968f1b8e0db5c755f9c1aa1a09e01293fd2f8",
    "location": null,
    "page_start": 51,
    "page_end": 51,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "591e5448-c3e9-485c-af25-22a161b6a662",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "David M Blei and John D Laﬀerty. Topic models. In Text Mining, pages 101–124. Chapman\nand Hall/CRC, 2009. L´eon Bottou. Online learning and stochastic approximations. On-line learning in neural\nnetworks, 17(9):142, 1998. L´eon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade,\npages 421–436. Springer, 2012. L Breiman. Better subset selection using the non-negative garotte. Technical report, Tech-\nnical Report. University of California, Berkeley, 1993. Leo Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996. Leo Breiman. Random forests. Machine Learning, 45(1):5–32, 2001a. [52]",
    "content_hash": "5ab43a9776fc5d6e555fe63c26e2f87dad46c4c3ca6e57aecfbb29187f501f29",
    "location": null,
    "page_start": 53,
    "page_end": 53,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e420a6fc-867c-4a3c-8d8b-0125e745d7b7",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Robert M Bell and Yehuda Koren. Lessons from the netﬂix prize challenge. Acm Sigkdd\nExplorations Newsletter, 9(2):75–79, 2007. Alexandre Belloni, Victor Chernozhukov, and Christian Hansen. High-dimensional methods\nand inference on structural and treatment eﬀects. Journal of Economic Perspectives, 28\n(2):29–50, 2014. Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155, 2003. Y. Bengio, H. Schwenk, J.-S. Sen´ecal, F. Morin, and J.-L. Gauvain. Neural probabilistic\nlanguage models. In Innovations in Machine Learning. Springer, 2006. James Bennett, Stan Lanning, et al. The netﬂix prize. In Proceedings of KDD cup and\nworkshop, volume 2007, page 35. New York, NY, USA, 2007. Dimitris Bertsimas, Angela King, Rahul Mazumder, et al. Best subset selection via a modern\noptimization lens. The Annals of Statistics, 44(2):813–852, 2016. Peter Bickel, Chris Klaassen, Yakov Ritov, and Jon Wellner. Eﬃcient and adaptive estima-\ntion for semiparametric models. 1998. Herman J Bierens. Kernel estimators of regression functions. In Advances in econometrics:\nFifth world congress, volume 1, pages 99–144, 1987.",
    "content_hash": "fef105da5e4e8472b706bede6fcde494b5e16613fa9c2de7419e524c6bd6b13e",
    "location": null,
    "page_start": 53,
    "page_end": 53,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "fb9349e8-73ed-42bc-be04-2746dc4fc692",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duﬂo, Christian Hansen,\nand Whitney Newey. Double/debiased/neyman machine learning of treatment eﬀects. American Economic Review, 107(5):261–65, 2017. Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duﬂo, Christian Hansen,\nWhitney Newey, and James Robins. Double/debiased machine learning for treatment and\nstructural parameters. The Econometrics Journal, 21(1):C1–C68, 2018a. Victor Chernozhukov, Mert Demirer, Esther Duﬂo, and Ivan Fernandez-Val. Generic machine\nlearning inference on heterogenous treatment eﬀects in randomized experiments. Technical\nreport, National Bureau of Economic Research, 2018b. [53]",
    "content_hash": "3bb0b0822f71a06810a61e6f03045d2ed46fd3adf3ba681f19427c723dc27ca3",
    "location": null,
    "page_start": 54,
    "page_end": 54,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b89dfc28-c7e7-4a2e-ba2a-4193b2d67b24",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Leo Breiman. Statistical modeling: The two cultures (with comments and a rejoinder by the\nauthor). Statistical Science, 16(3):199–231, 2001b. Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen. Classiﬁcation and\nRegression Trees. CRC press, 1984. Andriy Burkov. The Hundred-page Machine Learning Book. Burkov, 2019. Emmanuel Cand`es and Terence Tao. The dantzig selector: Statistical estimation when p is\nmuch larger than n. The Annals of Statistics, pages 2313–2351, 2007. Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717, 2009. Gary Chamberlain. Econometrics and decision theory. Journal of Econometrics, 95(2):\n255–283, 2000. Xiaohong Chen. Large sample sieve estimation of semi-nonparametric models. Handbook of\neconometrics, 6:5549–5632, 2007. Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duﬂo, Christian Hansen,\nWhitney K Newey, et al. Double machine learning for treatment and causal parameters. Technical report, Centre for Microdata Methods and Practice, Institute for Fiscal Studies,\n2016a. Victor Chernozhukov, Juan Carlos Escanciano, Hidehiko Ichimura, and Whitney K Newey. Locally robust semiparametric estimation. arXiv preprint arXiv:1608.00033, 2016b.",
    "content_hash": "7c0b9aab8ba983a3ef06d055925d0ab34438da7f608fd7869b070470844a1cc4",
    "location": null,
    "page_start": 54,
    "page_end": 54,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "37d85024-ee79-462d-a1df-084d00518be4",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Victor Chernozhukov, Whitney Newey, and James Robins. Double/de-biased machine learn-\ning using regularized riesz representers. arXiv preprint arXiv:1802.08667, 2018c. Hugh A Chipman, Edward I George, Robert E McCulloch, et al. Bart: Bayesian additive\nregression trees. The Annals of Applied Statistics, 4(1):266–298, 2010. Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):\n273–297, 1995. Thomas G Dietterich. Ensemble methods in machine learning. In International workshop\non multiple classiﬁer systems, pages 1–15. Springer, 2000. M. Dimakopoulou, S. Athey, and G. Imbens. Estimation considerations in contextual bandits. arXiv, 2017. Maria Dimakopoulou, Zhengyuan Zhou, Susan Athey, and Guido Imbens. Balanced linear\ncontextual bandits. arXiv preprint arXiv:1812.06227, 2018. Nikolay Doudchenko and Guido W Imbens. Balancing, regression, diﬀerence-in-diﬀerences\nand synthetic control methods: A synthesis. Technical report, National Bureau of Eco-\nnomic Research, 2016. M. Dudik, J. Langford, and L. Li. Doubly robust policy evaluation and learning. International\nConference on Machine Learning, 2011. M. Dudik, D. Erhan, J. Langford, and L. Li. Doubly robust policy evaluation and optimiza-\ntion. Statistical Science, 2014.",
    "content_hash": "7d0ddd09dc889700fd6c18fd85deeb532691ca8577c71d9616927d85bc34e4ab",
    "location": null,
    "page_start": 55,
    "page_end": 55,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "1d293c2a-9a0b-4a0b-b399-13526ea1d84f",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Bradley Efron and Trevor Hastie. Computer Age Statistical Inference, volume 5. Cambridge\nUniversity Press, 2016. Bradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et al. Least angle regres-\nsion. The Annals of statistics, 32(2):407–499, 2004. Max H Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation\nand inference: Application to causal eﬀects and other semiparametric estimands. arXiv\npreprint arXiv:1809.09953, 2018. J. R. Firth. A synopsis of linguistic theory 1930-1955. In Studies in Linguistic Analysis\n(special volume of the Philological Society), volume 1952–1959, 1957. [54]",
    "content_hash": "a10a50368200bd907106c6731a7d57924c23e14d67c8768070294ab46a56f919",
    "location": null,
    "page_start": 55,
    "page_end": 55,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "fe5c36a6-49c0-4d68-87e1-1ea4ba27d830",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Rina Friedberg, Julie Tibshirani, Susan Athey, and Stefan Wager. Local linear forests. arXiv\npreprint arXiv:1807.11408, 2018. Jerome H Friedman. Stochastic gradient boosting. Computational Statistics & Data\nAnalysis, 38(4):367–378, 2002. Matthew Gentzkow, Bryan T Kelly, and Matt Taddy. Text as data. Technical report,\nNational Bureau of Economic Research, 2017. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in\nneural information processing systems, pages 2672–2680, 2014. P. Gopalan, J. Hofman, and D. M. Blei. Scalable recommendation with hierarchical Poisson\nfactorization. In Uncertainty in Artiﬁcial Intelligence, 2015. Donald P Green and Holger L Kern. Modeling heterogeneous treatment eﬀects in survey\nexperiments with bayesian additive regression trees. Public opinion quarterly, 76(3):491–\n511, 2012. William H Greene. Econometric analysis 4th edition. International edition, New Jersey:\nPrentice Hall, pages 201–215, 2000. Z. S. Harris. Distributional structure. Word, 10(2–3):146–162, 1954. Jason Hartford, Greg Lewis, and Matt Taddy. Counterfactual Prediction with Deep Instru-\nmental Variables Networks. 2016.",
    "content_hash": "ad2bf2a1e1a03c826b850e3c15b4933c9dbd6357ac693d88f3c3e87a039d0f2b",
    "location": null,
    "page_start": 56,
    "page_end": 56,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "356d494f-864b-4d40-838b-98530498d66b",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Product Recommendations Based on Latent\nPurchase Motivations. 2014. URL http://www.ssrn.com/abstract=2443455. N. Jiang and L. Li. Doubly robust oﬀ-policy value evaluation for reinforcement learning. International Conference on Machine Learning, 2016. N. Kallus. Balanced policy evaluation and learning. arXiv, 2017. Michael P Keane et al. Panel data discrete choice models of consumer demand. Prepared\nfor The Oxford Handbooks: Panel Data, 2013. [56]",
    "content_hash": "1a114e023a6e8d0892b5ccdfc0c8616df5640c02824b935d7165eb7cf63507b8",
    "location": null,
    "page_start": 57,
    "page_end": 57,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "cb432c0f-7d96-4e50-8ba6-459a52f69d48",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Toru Kitagawa and Aleksey Tetenov. Who should be treated? Empirical welfare maximiza-\ntion methods for treatment choice. Technical report, Centre for Microdata Methods and\nPractice, Institute for Fiscal Studies, 2015. Steven W Knox. Machine learning: a concise introduction, volume 285. John Wiley & Sons,\n2018. Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in neural information processing systems,\npages 1097–1105, 2012. S¨oren K¨unzel, Jasjeet Sekhon, Peter Bickel, and Bin Yu. Meta-learners for estimating het-\nerogeneous treatment eﬀects using machine learning. arXiv preprint arXiv:1706.03461,\n2017. Tze Leung Lai and Herbert Robbins. Asymptotically eﬃcient adaptive allocation rules. Advances in applied mathematics, 6(1):4–22, 1985. Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. nature, 521(7553):436,\n2015. O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In\nAdvances in Neural Information Processing Systems, 2014. L. Li, W. Chu, J. Langford, T. Moon, and X. Wang. An unbiased oﬄine evaluation of\ncontextual bandit algorithms with generalized linear models. Journal of Machine Learning\nResearch Workshop and Conference Proceedings, 2012. L. Li, S. Chen, J. Kleban, and A. Gupta.",
    "content_hash": "37ea16a48cf0066a9c2b78a4acff10e27f8213e65e40815fd4984da87c62df24",
    "location": null,
    "page_start": 58,
    "page_end": 58,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e600c12f-eea3-4e38-816c-8ab53c5de7cd",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Parametric empirical bayes inference: theory and applications. Journal of\nthe American Statistical Association, 78(381):47–55, 1983. Sendhil Mullainathan and Jann Spiess. Machine learning: an applied econometric approach. Journal of Economic Perspectives, 31(2):87–106, 2017. J. Pennington, R. Socher, and C. D. Manning. GloVe: Global vectors for word representation. In Conference on Empirical Methods on Natural Language Processing, 2014. [58]",
    "content_hash": "d16b683e2cee770e61a019af3d3542e6df2f4b75b0cea8dff67537535005bbc5",
    "location": null,
    "page_start": 59,
    "page_end": 59,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "783c948e-696e-4e8b-b639-34b8b20a4ea2",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms\nfor learning large incomplete matrices. Journal of machine learning research, 11(Aug):\n2287–2322, 2010. Nicolai Meinshausen. Relaxed lasso. Computational Statistics & Data Analysis, 52(1):\n374–393, 2007. T. Mikolov, K. Chen, G. S. Corrado, and J. Dean. Eﬃcient estimation of word representations\nin vector space. International Conference on Learning Representations, 2013a. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations\nof words and phrases and their compositionality. In Advances in Neural Information\nProcessing Systems, 2013b. T. Mikolov, W.-T. au Yih, and G. Zweig. Linguistic regularities in continuous space word\nrepresentations. In Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, 2013c. Alan Miller. Subset selection in regression. Chapman and Hall/CRC, 2002. A. Mnih and G. E. Hinton. Three new graphical models for statistical language modelling. In International Conference on Machine Learning, 2007. A. Mnih and K. Kavukcuoglu. Learning word embeddings eﬃciently with noise-contrastive\nestimation. In Advances in Neural Information Processing Systems, 2013. A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural probabilistic\nlanguage models. In International Conference on Machine Learning, 2012. Carl N Morris.",
    "content_hash": "76f8cded86cdec2a760f8e907d8c423824f929b70d40ee5cccf306919f819847",
    "location": null,
    "page_start": 59,
    "page_end": 59,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "7832aa71-5aef-4567-ba48-ebced6821ef1",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "James Robins and Andrea Rotnitzky. Semiparametric eﬃciency in multivariate regression\nmodels with missing data. Journal of the American Statistical Association, 90(1):122–129,\n1995. Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in obser-\nvational studies for causal eﬀects. Biometrika, 70(1):41–55, 1983. Francisco JR Ruiz, Susan Athey, and David M Blei. Shopper: A probabilistic model of\nconsumer choice with substitutes and complements. arXiv preprint arXiv:1711.03560,\n2017. David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning representations\nby back-propagating errors. nature, 323(6088):533, 1986. Robert E Schapire and Yoav Freund. Boosting: Foundations and algorithms. MIT press,\n2012. Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines,\nregularization, optimization, and beyond. MIT press, 2001. Steven L Scott. A modern bayesian look at the multi-armed bandit. Applied Stochastic\nModels in Business and Industry, 26(6):639–658, 2010. V. Semenova, M. Goldman, V. Chernozhukov, and M. Taddy. Orthogonal ML for demand\nestimation: High dimensional causal inference in dynamic panels. arXiv:1712.09988, 2018. A. Strehl, J. Langford, L. Li, and S. Kakade.",
    "content_hash": "cb7789bbafbdabff8f066ac6c99b1ff8cc542e6007c908b328d10e25616c0607",
    "location": null,
    "page_start": 60,
    "page_end": 60,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "461cb0e0-6ef2-43ba-a649-c6845223c802",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal\nStatistical Society. Series B (Methodological), pages 267–288, 1996. Robert Tibshirani and Trevor Hastie. Local likelihood estimation. Journal of the American\nStatistical Association, 82(398):559–567, 1987. Mark J van der Laan and Daniel Rubin. Targeted maximum likelihood learning. The\nInternational Journal of Biostatistics, 2(1), 2006. Aad W Van der Vaart. Asymptotic Statistics. Cambridge University Press, 2000. Vladimir Naumovich Vapnik. Statistical learning theory, volume 1. Wiley New York, 1998. Hal R Varian. Big data: New tricks for econometrics. Journal of Economic Perspectives, 28\n(2):3–28, 2014. L. Vilnis and A. McCallum. Word representations via Gaussian embedding. In International\nConference on Learning Representations, 2015. Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment eﬀects\nusing random forests. Journal of the American Statistical Association, (just-accepted),\n2017. M. Wan, D. Wang, M. Goldman, M. Taddy, J. Rao, J. Liu, D. Lymberopoulos, and\nJ. McAuley. Modeling consumer preferences and price sensitivities from large-scale grocery\nshopping transaction logs. In International World Wide Web Conference, 2017. Halbert White. Artiﬁcial neural networks: approximation and learning theory. Blackwell\nPublishers, Inc., 1992. Jeﬀrey M Wooldridge. Econometric analysis of cross section and panel data.",
    "content_hash": "cbda75bac53eb6a391fceb64646c1486f21eb0baf782f2b35e7ea2a29b79c69c",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9880008c-f172-4de0-b99b-77b27e670b2b",
    "source_id": "8baf995f-455d-44ae-a8bf-8b8be0f0c5cc",
    "content": "Zhengyuan Zhou, Susan Athey, and Stefan Wager.\nOﬄine multi-action policy learning:\nGeneralization and optimization. arXiv preprint arXiv:1810.04778, 2018.\nHui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal\nof the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005.\nJose R. Zubizarreta. Stable weights that balance covariates for estimation with incomplete\noutcome data. Journal of the American Statistical Association, 110(511):910–922, 2015.\ndoi: 10.1080/01621459.2015.1023805.\n[61]",
    "content_hash": "757f830125816ffd5b8b9dc9e6c062de74a7a1a94543f545da25e64cae06de19",
    "location": null,
    "page_start": 62,
    "page_end": 62,
    "metadata": {
      "section": "References",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "07f0a769-b781-476f-90e6-2e290a659b66",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "(2016), “Dynamic treatment eﬀects,” Journal of Economet-\nrics, 191(2), 276–292. Heckman, J. J., Ichimura, H., Smith, J., and Todd, P. (1998), “Characterizing selection bias using experimental\ndata,” Econometrica, 66(5), 1017–1098. Heckman, J. J., Ichimura, H., and Todd, P. (1997), “Matching as an econometric evaluation estimator: Evidence\nfrom evaluating a job training programme,” The Review of Economic Studies, 64(4), 605–654. Imai, K., Kim, I. S., and Wang, E. (2018), “Matching methods for causal inference with time-series cross-section\ndata,” Working Paper. Khan, S., and Tamer, E. (2010), “Irregular identiﬁcation, support conditions, and inverse weight estimation,”\nEconometrica, 78(6), 2021–2042. Kline, P., and Santos, A. (2012), “A score based approach to wild bootstrap inference,” Journal of Econometric\nMethods, 1(1), 1–40. Kosorok, M. R. (2008), Introduction to Empirical Processes and Semiparametric Inference, New York, NY: Springer. Laporte, A., and Windmeijer, F.",
    "content_hash": "4d416871d3762224f7045a4df4b6cd2460663115c65cd36f6f385949c07209d2",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ccfb713e-8565-44f4-866a-b06ecf424daa",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "treatment at diﬀerent lengths of exposure to the treatment. Despite the popularity of these speciﬁcations, recent research has shown that one must be very careful\nin attaching a causal interpretation to these aggregated parameters. For instance, Borusyak and Jaravel\n(2017), Goodman-Bacon (2019), de Chaisemartin and D’Haultfœuille (2020), and Athey and Imbens\n(2018) have shown that, in general, β recovers a weighted average of some underlying treatment eﬀect\nparameters but some of the weights on these parameters can be negative. This can potentially lead to\nparticularly problematic cases such as the eﬀect of the treatment being positive for all units, but the\nTWFE estimation resulting in estimates of β that are negative. Even in cases where the weights are\nnot negative, the weights on underlying treatment eﬀect parameters are entirely driven by the TWFE\nestimation strategy and are sensitive to the size of each group, the timing of treatment, and the total\nnumber of time periods (see Theorem 1 in Goodman-Bacon (2019)). The results in this section can be\nused in exactly the same setup to identify a single interpretable average treatment eﬀect parameter and,\nthus, provide a way to circumvent the issues with the more common approach. As discussed by Goodman-Bacon (2019), the “negative weight problem” associated with β arises when\ntreatment eﬀects evolve over time. Thus, one may wonder if such problems would still be present when\nconsidering more general, dynamic speciﬁcations such as (3.3).",
    "content_hash": "f5ce65c8d1c10dc36fd91585ab56f5af13fac4556f7f5d0afea773eee85925be",
    "location": null,
    "page_start": 1,
    "page_end": 14,
    "metadata": {
      "section": ") do not recover easy-to-interpret causal parameters and",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2e21d3ad-b9d2-48f0-9df2-7db2199a7d02",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "where bC (g, t) is as deﬁned in Algorithm 1. Remark 10. In DiD applications, it is common to use “cluster-robust” inference procedures; see, e.g.,\nWooldridge (2003) and Bertrand et al. (2004). However, we note that the choice of whether to cluster\nor not is usually not obvious, and depends on the kind of uncertainty one is trying to reﬂect; see, e.g.,\nAbadie et al. (2017) for a discussion in a cross-sectional setup.15 In the case that one wishes to account\nfor clustering to reﬂect “cluster-based” sampling uncertainty, we note that this can be done in a straight-\nforward manner using a small modiﬁcation of the multiplier bootstrap described above, provided that the\nnumber of cluster is “large.” More precisely, instead of drawing observation-speciﬁc V ’s, one simply needs\nto draw cluster-speciﬁc V ’s; see, e.g., Sherman and Le Cessie (2007), Kline and Santos (2012), Cheng\net al. (2013), and MacKinnon and Webb (2018, 2020). If the number of clusters is “small,” however,\nthe application of the aforementioned bootstrap procedure is not warranted.16\nRemark 11. In Algorithm 1 we have required an estimator for the main diagonal of Σ. However, we\nnote that if one takes bΣ (g, t) = 1 for all (g, t), the result in Corollary 1 continues to hold.",
    "content_hash": "afb0f9c0251d487db2b61d81adaeda3bb226d2800601fe8b98bf1afafb2390b4",
    "location": null,
    "page_start": 1,
    "page_end": 25,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "1ba24e2d-285b-4be5-9742-9f1790c688b2",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": ", T } such that t ≥g −δ,\nE[Yt(0) −Yt−1(0)|X, Gg = 1] = E[Yt(0) −Yt−1(0)|X, C = 1] a.s.. Assumption 5 (Conditional Parallel Trends based on “Not-Yet-Treated” Groups). Let δ be as deﬁned\nin Assumption 3. For each g ∈G and each (s, t) ∈{2, . . . , T } × {2, . . . , T } such that t ≥g −δ and\nt + δ ≤s < ¯g,\nE[Yt(0) −Yt−1(0)|X, Gg = 1] = E[Yt(0) −Yt−1(0)|X, Ds = 0, Gg = 0] a.s.. Assumptions 4 and 5 are two diﬀerent conditional parallel trends assumptions that generalize the\ntwo-period parallel trends assumption to the case where there are multiple time periods and multiple\ntreatment groups; see, e.g., Heckman et al. (1997, 1998), Abadie (2005) and Sant’Anna and Zhao (2020). Both assumptions hold after conditioning on covariates X. This can be important in many applications in\neconomics particularly in cases where there are covariate speciﬁc trends in outcomes over time and when\nthe distribution of covariates is diﬀerent across groups. For example, Heckman et al. (1997) motivates\nconditional parallel trends assumptions in the context of evaluating a job training program.",
    "content_hash": "cf36bab9bee3b29c93740e50dfebdb5e105fe81466ddadc4c55a7f97f512edc0",
    "location": null,
    "page_start": 1,
    "page_end": 8,
    "metadata": {
      "section": "Both assumptions hold after conditioning on covariates",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d3f98339-fab0-4628-87f5-12ad491334cd",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Diﬀerence-in-Diﬀerences with Multiple Time Periods∗\nBrantly Callaway†\nPedro H. C. Sant’Anna‡\nDecember 1, 2020\nAbstract\nIn this article, we consider identiﬁcation, estimation, and inference procedures for treatment eﬀect\nparameters using Diﬀerence-in-Diﬀerences (DiD) with (i) multiple time periods, (ii) variation in treat-\nment timing, and (iii) when the “parallel trends assumption” holds potentially only after conditioning\non observed covariates. We show that a family of causal eﬀect parameters are identiﬁed in stag-\ngered DiD setups, even if diﬀerences in observed characteristics create non-parallel outcome dynamics\nbetween groups. Our identiﬁcation results allow one to use outcome regression, inverse probability\nweighting, or doubly-robust estimands. We also propose diﬀerent aggregation schemes that can be\nused to highlight treatment eﬀect heterogeneity across diﬀerent dimensions as well as to summarize\nthe overall eﬀect of participating in the treatment. We establish the asymptotic properties of the\nproposed estimators and prove the validity of a computationally convenient bootstrap procedure to\nconduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the\nrelevance of our proposed tools by analyzing the eﬀect of the minimum wage on teen employment from\n2001–2007. Open-source software is available for implementing the proposed methods. JEL: C14, C21, C23, J23, J38.",
    "content_hash": "e6d5bf02b63e76047a9e040b7b4595a838560e5f7294bb1398f38ff43d7768a6",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "Diﬀerence-in-Diﬀerences with Multiple Time Periods",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2705725b-a988-451d-aff1-01e46bc1e218",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Keywords: Diﬀerence-in-Diﬀerences, Dynamic Treatment Eﬀects, Doubly Robust, Event Study, Vari-\nation in Treatment Timing, Treatment Eﬀect Heterogeneity, Semi-Parametric. ∗First complete version: March 23, 2018. A previous version of this paper has been circulated with the title “Diﬀerence-\nin-Diﬀerences with Multiple Time Periods and an Application on the Minimum Wage and Employment”. We thank the\nEditor, the Associate Editor, two anonymous referees, St´ephane Bonhomme, Carol Caetano, Sebastian Calonico, Xiaohong\nChen, Cl´ement de Chaisemartin, Xavier D’Haultfoeuille, Bruno Ferman, John Gardner, Andrew Goodman-Bacon, Federico\nGutierrez, Sukjin Han, Hugo Jales, Andrew Johnston, Vishal Kamat, Qi Li, Tong Li, Jason Lindo, Catherine Maclean, Matt\nMasten, Magne Mogstad, Tom Mroz, Aureo de Paula, Jonathan Roth, Donald Rubin, Bernhard Schmidpeter, Yuya Sasaki,\nNa’Ama Shenhav, Tymon S loczy´nski, Sebastian Tello-Trillo, Alex Torgovitsky, Jeﬀrey Wooldridge, Haiqing Xu and several\nseminar and conference audiences for comments and suggestions. Code to implement the methods proposed in the paper is\navailable in the R package did which is available on CRAN. †Department of Economics, University of Georgia. Email: brantly.callaway@uga.edu\n‡Department of Economics, Vanderbilt University.",
    "content_hash": "efa62e1ab7ddf38437ab710646392e2241f722ee503789ca9128e38b456c4b6d",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "December 1, 2020",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "3e99f924-4110-438a-92af-ec0fa9d765b9",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Email: pedro.h.santanna@vanderbilt.edu\n1\narXiv:1803.09015v4  [econ.EM]  1 Dec 2020",
    "content_hash": "f3ec8243f0c8095b692fe58fc20b5422d5dc080e4245d9246859d456d549dcf4",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "December 1, 2020",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "728b9ccb-533d-4760-83d9-d4ddb6dec5d8",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": ", T ., with (G2, . . . , GT , C, X) being invariant to T. Assumption B.1 implies that our sample consists of random draws from the mixture distribution\nFM(y, g2, . . . , gT , c, t, x) =\nT\nX\nt=1\nλt · FY,G2,...,GT ,C,X|T (y, g2, . . . , gT , c, x|t),\nwhere λt = P(Tt = 1). It also rules-out compositional changes across time. This assumption is related to the\nsampling assumption imposed by Abadie (2005) and Sant’Anna and Zhao (2020) in the two periods, two groups\nDiD setup. Notice that, once one conditions on the time period, then expectations under the mixture distribution\ncorrespond to population expectations. Also, because X, Gg, and C are observed for all units, by the stationarity\ncondition one can use draws from the mixture distribution to estimate the generalized propensity score. With some\nabuse of notation, we then use pg,s (X) as a short notation for EM [Gg|X, Gg + (1 −Ds) (1 −Gg) = 1], where EM [·]\ndenotes expectations with respect to FM (·). Also, we use pg(X) = pg,T (X) = EM [Gg|X, Gg + C = 1]. Before formalizing all the results, we need to introduce some additional notation.",
    "content_hash": "a9b275d427a147728fc28ef6de4afaf17441bc583d82b24f2ce7a99877d9de25",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9ae0700c-3347-4e37-b541-0269628816bc",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "= E [Gg · E [(Yt −Yg−δ−1)| X, C = 1]] . Thus, combined this with (A.3), we establish (A.1), implying that ATT(g, t) = ATT nev\nipw (g, t; δ). Finally, notice that\nATT nev\ndr\n(g, t; δ) = E\n\n\n\n\n\n\nGg\nE [Gg] −\npg (X) C\n1 −pg (X)\nE\n\u0014 pg (X) C\n1 −pg (X)\n\u0015\n\n\n\n\nYt −Yg−δ−1 −mnev\ng,t,δ (X)\n\u0001\n\n\n= E\n\n\n\n\n\n\nGg\nE [Gg] −\npg (X) C\n1 −pg (X)\nE\n\u0014 pg (X) C\n1 −pg (X)\n\u0015\n\n\n\n(Yt −Yg−δ−1)\n\n\n|\n{z\n}\n≡AT T nev\nipw (g,t;δ)\n−E\n\n\n\n\n\n\nGg\nE [Gg] −\npg (X) C\n1 −pg (X)\nE\n\u0014 pg (X) C\n1 −pg (X)\n\u0015\n\n\n\nmnev\ng,t,δ (X)\n\n\n= ATT (g, t) −\n1\nE [Gg]E\n\u0014\u0012\nGg −E [Gg|X] C\nE [C|X]\n\u0013\nmnev\ng,t,δ (X)\n\u0015\n= ATT (g, t) −\n1\nE [Gg]E\n\u0002\n(E [Gg|X] −E [Gg|X]) · mnev\ng,t,δ (X)\n\u0003\n= ATT (g, t) .",
    "content_hash": "f6a086ca42d244d5b9e637bdcd27df006ac98cb54efded8ea0f14d586e68fe7e",
    "location": null,
    "page_start": 1,
    "page_end": 35,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "22cebd3c-ee0e-4e09-b6ba-92adb46e6c54",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Table 2: Summary Statistics for Main Dataset\nTreated Counties\nUntreated Counties\nDiﬀ. P-val on Diﬀ. Midwest\n0.59\n0.34\n0.25\n0.00\nSouth\n0.27\n0.59\n-0.32\n0.00\nWest\n0.14\n0.07\n0.07\n0.00\nPopulation (1000s)\n94.32\n53.43\n40.89\n0.00\nWhite\n0.89\n0.83\n0.06\n0.00\nHS Graduates\n0.59\n0.55\n0.04\n0.00\nPoverty Rate\n0.13\n0.16\n-0.03\n0.00\nMedian Inc. (1000s)\n33.91\n31.89\n2.02\n0.00\nNotes: Summary statistics for counties located in states that raised their minimum wage between Q2 of\n2003 and Q1 of 2007 (treated) and states whose minimum wage was eﬀectively set at the federal minimum\nwage for the entire period (untreated). The sample consists of 2284 counties. Sources:\nQuarterly Workforce\nIndicators and 2000 County Data Book\neﬀect of raising the minimum wage on teen employment. The results for group-time average treatment\neﬀects are reported in Panel (a) of Figure 1 along with a uniform 95% conﬁdence band. All inference\nprocedures use clustered bootstrapped standard errors at the county level, and account for the autocorre-\nlation of the data. The plot contains pre-treatment estimates that can be used to “pre-test” the parallel\ntrends assumption as well as treatment eﬀect estimates in post-treatment periods. The group-time average treatment eﬀect estimates provide support for the view that increasing the\nminimum wage led to a reduction in teen employment.",
    "content_hash": "fe876b6d1343d9df936f33d7024b8dff306d529def08c51424f97a84c024c6d1",
    "location": null,
    "page_start": 1,
    "page_end": 28,
    "metadata": {
      "section": "Figure 1: Minimum Wage Group-Time Average Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9d6d653c-ba09-44af-9a45-50ffcc067057",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "to estimate the summarized average treatment eﬀect parameters discussed in Section 3. Remark 9. In applications with limited covariate overlap (i.e., with propensity scores suﬃciently close\nto one), IPW and DR estimators may lead to imprecise (irregular) inference procedures, see, e.g., Khan\nand Tamer (2010). In such cases, provided that one is comfortable with (parametric) extrapolation and\nis suﬃciently conﬁdent that the outcome regression working models are correctly speciﬁed, relying on the\nOR estimation approach may lead to more informative inferences. Alternatively, one may choose to trim\nextreme propensity score estimates though proceeding in this manner would change the target parameter;\ni.e., we would not be recovering the ATT (g, t)’s; see, e.g., Crump et al. (2009) and Yang and Ding (2018)\nfor related discussion in other contexts. In the rest of the paper, we abstract from these points. 4.1\nAsymptotic Theory for Group-Time Average Treatment Eﬀects\nNext, we derive the asymptotic properties of our DR DiD estimators for the ATT (g, t)’s. To simplify\nexposition, we focus on the case with a never-treated comparison group as in (4.1); results that come from\nusing the not-yet-treated group as the comparison group as in (4.2) follow from symmetric arguments\nand are therefore omitted. We also note that the theoretical results in this section are justiﬁed within\nthe large n, ﬁxed T paradigm.",
    "content_hash": "2ce14a0d3acdc067c2ebe7cb9b02d7948718d8699f98e193917f794e2081acc4",
    "location": null,
    "page_start": 1,
    "page_end": 21,
    "metadata": {
      "section": "int",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5c0e01a3-9e89-43aa-af67-2e2ca1056242",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Theorem 1. Let Assumptions 1, 2, 3 and 6 hold. (i) If Assumption 4 holds, then, for all g and t such that g ∈Gδ, t ∈{2, . . . T −δ} and t ≥g −δ,\nATT (g, t) = ATT nev\nipw (g, t; δ) = ATT nev\nor\n(g, t; δ) = ATT nev\ndr\n(g, t; δ) . (ii) If Assumption 5 holds, then, for all g and t such that g ∈Gδ, t ∈{2, . . . T −δ} and g−δ ≤t < ¯g−δ,\nATT (g, t) = ATT ny\nipw (g, t; δ) = ATT ny\nor (g, t; δ) = ATT ny\ndr (g, t; δ) . Theorem 1 is the ﬁrst main result of this paper. It provides powerful identiﬁcation results that\nextend the DiD identiﬁcation results based on the outcome regression approach of Heckman et al. (1997,\n1998), the IPW approach of Abadie (2005), and the DR approach of Sant’Anna and Zhao (2020) to\nthe multiple-periods, multiple groups setup.",
    "content_hash": "f662453832fa435da9ab13abbe8505410e9c80a286dedbed6ddd5a8beed5159f",
    "location": null,
    "page_start": 1,
    "page_end": 11,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "7f597582-1cfb-4cd6-9bad-f31528a2c1a0",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Finally, let ˙hdr,nev\ng,t\nW; κnev\ng,t\n\u0001\n= ∂hdr,nev\ng,t\nW; κnev\ng,t\n\u0001. ∂κnev\ng,t . Assumption 7. (i) g (x; γ) is a parametric model for g (x), where γ ∈Θ ⊂Rk, Θ being compact; (ii)\ng (X; γ) is a.s. continuous at each γ ∈Θ; (iii) there exists a unique pseudo-true parameter γ∗∈int (Θ);\n(iv) g (X; γ) is a.s. twice continuously diﬀerentiable in a neighborhood of γ∗, Θ∗⊂Θ; (v) the estimator\nbγ is strongly consistent for γ∗and satisﬁes the following linear expansion:\n√n (bγ −γ∗) =\n1\n√n\nn\nX\ni=1\nlg,t (Wi; γ∗) + op (1) ,\nwhere lg,t (·; γ) is a k × 1 vector such that E [lg,t (W; γ∗)] = 0, E\n\u0002\nlg,t (W; γ∗) lg,t (W; γ∗)′\u0003\nexists and is\npositive deﬁnite and lims→0 E\nh\nsupγ∈Θ∗:∥γ−γ∗∥≤s ∥lg (W; γ) −lg (W; γ∗)∥2i\n= 0.",
    "content_hash": "1b59dbeafa849dbf23240f1f9aa1e57f582a0d49067027cbc14d442f60ffc858",
    "location": null,
    "page_start": 1,
    "page_end": 21,
    "metadata": {
      "section": "propensity score, and",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "044c9a88-d124-4ec6-8e93-cdf71d7ba211",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "1\nIntroduction\nDiﬀerence-in-Diﬀerences (DiD) has become one of the most popular research designs used to evaluate\ncausal eﬀects of policy interventions. In its canonical format, there are two time periods and two groups:\nin the ﬁrst period no one is treated, and in the second period some units are treated (the treated group),\nand some units are not (the comparison group). If, in the absence of treatment, the average outcomes\nfor treated and comparison groups would have followed parallel paths over time (which is the so-called\nparallel trends assumption), one can estimate the average treatment eﬀect for the treated subpopulation\n(ATT) by comparing the average change in outcomes experienced by the treated group to the average\nchange in outcomes experienced by the comparison group. Methodological extensions of DiD methods\noften focus on this standard two periods, two groups setup; see, e.g., Heckman et al. (1997, 1998), Abadie\n(2005), Athey and Imbens (2006), Qin and Zhang (2008), Bonhomme and Sauder (2011), de Chaisemartin\nand D’Haultfœuille (2017), Botosaru and Gutierrez (2018), Callaway et al. (2018), and Sant’Anna and\nZhao (2020).1\nMany DiD empirical applications, however, deviate from the canonical DiD setup and have more than\ntwo time periods and variation in treatment timing. In this article, we provide a uniﬁed framework for\naverage treatment eﬀects in DiD setups with multiple time periods, variation in treatment timing, and\nwhen the parallel trends assumption holds potentially only after conditioning on observed covariates.",
    "content_hash": "374c467c014b62738d69e3b782cde651dc3fc13972743ddc7d3379f8f27c19f3",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "Diﬀerence-in-Diﬀerences (DiD) has become one of the most popular research designs used to evaluate",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "3ccbf0a2-b51f-4732-a8a1-69ef3736cd2f",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "In the canonical DiD setup with two periods and two groups, these parameters\nreduce to the ATT which is typically the parameter of interest in that setup. An attractive feature of the\ngroup-time average treatment eﬀect parameters is that they do not directly restrict heterogeneity with\nrespect to observed covariates, the period in which units are ﬁrst treated, or the evolution of treatment\neﬀects over time. As a consequence, these easy-to-interpret causal parameters can be directly used for\nlearning about treatment eﬀect heterogeneity, and/or to construct many other more aggregated causal\n1See Section 6 of Athey and Imbens (2006) and Theorem S1 in de Chaisemartin and D’Haultfœuille (2017) for notable\nexceptions that cover multiple periods and multiple groups. 2",
    "content_hash": "0560523caf47221dba7c24a2cac8fc3b36cd1ea17f70880710f29e2072462721",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": ", where a “group” is deﬁned by the time period",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "6126f2d6-8053-4adb-90f0-a069279e7b6f",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "We\nconcentrate our attention on DiD with staggered adoption, i.e., to DiD setups such that once units are\ntreated, they remain treated in the following periods. The core of our proposal relies on separating the DiD analysis into three separate steps: (i) identiﬁ-\ncation of policy-relevant disaggregated causal parameters; (ii) aggregation of these parameters to form\nsummary measures of the causal eﬀects; and (iii) estimation and inference about these diﬀerent target\nparameters. Our approach allows for estimation and inference on interpretable causal parameters al-\nlowing for arbitrary treatment eﬀect heterogeneity and dynamic eﬀects, thereby completely avoiding the\nissues of interpreting results of standard two-way ﬁxed eﬀects (TWFE) regressions as causal eﬀects in\nDiD setups as pointed out by Borusyak and Jaravel (2017), de Chaisemartin and D’Haultfœuille (2020),\nGoodman-Bacon (2019), Sun and Abraham (2020), and Athey and Imbens (2018). In addition, it adds\ntransparency and objectivity to the analysis (Rubin (2007, 2008)), and allows researchers to exploit a\nvariety of estimation methods to answer diﬀerent questions of interest. The identiﬁcation step of the analysis provides a blueprint for the other steps. In this paper, we pay\nparticular attention to the disaggregated causal parameter that we call the group-time average treatment\neﬀect, i.e., the average treatment eﬀect for group g at time t, where a “group” is deﬁned by the time period\nwhen units are ﬁrst treated.",
    "content_hash": "9860b656d1577ac3bd1080deae83e34ad51941fba07b465cf882061a4d4a5d9f",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "Many DiD empirical applications, however, deviate from the canonical DiD setup and have more than",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "4911edfe-0767-4c69-b9ca-d386bf72c9c6",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Here, we note that using doubly-robust\nestimators can be particularly attractive as they rely on less stringent modeling conditions than the\noutcome regression and the inverse probability weighting procedures. In order to conduct asymptotically valid inference, we justify the use of a computationally conve-\nnient multiplier-type bootstrap procedure. This approach can be used to obtain simultaneous conﬁdence\nbands for the group-time average treatment eﬀects. Unlike commonly used pointwise conﬁdence bands,\nour simultaneous conﬁdence bands asymptotically cover the entire path of the group-time average treat-\nment eﬀects with ﬁxed probability and take into account the dependency across diﬀerent group-time\n3",
    "content_hash": "582919e1b7234d608d6ae23e66e89ffa20a8305c1e67db591f3d34cfd7823e1e",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "outcome regression and the inverse probability weighting procedures.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "af456484-9fec-4042-bb18-707864f6be02",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "parameters. We view this level of generality and ﬂexibility as one of the main advantages of our proposal. We provide suﬃcient conditions related to treatment anticipation behavior and conditional parallel\ntrends under which these group-time average treatment eﬀects are nonparametrically point-identiﬁed. A unique feature of our framework is that it shows how researchers can ﬂexibly incorporate covariates\ninto the staggered DiD setup with multiple groups and multiple periods. This is particularly important\nin applications in which diﬀerences in observed characteristics create non-parallel outcome dynamics\nbetween diﬀerent groups – in this case, unconditional DiD strategies are generally not appropriate to\nrecover sensible causal parameters of interest (Heckman et al., 1997, 1998; Abadie, 2005). We propose\nthree diﬀerent types of DiD estimands in staggered treatment adoption setups: one based on outcome\nregressions (Heckman et al., 1997, 1998), one based on inverse probability weighting (Abadie, 2005),\nand one based on doubly-robust methods (Sant’Anna and Zhao, 2020). We provide versions of these\nestimands both for the case with panel data and for the case with repeated cross sections data. To the\nbest of our knowledge, this paper is the ﬁrst to show how one can allow for covariate-speciﬁc trends across\ngroups in DiD setups with variation in treatment timing. Our results also highlight that, in practice, one\ncan rely on diﬀerent types of parallel trends assumptions and allow some types of treatment anticipation\nbehavior; our proposed estimands explicitly reﬂect these assumptions.",
    "content_hash": "7c4e67b5a10ff2c3bed6f5d8201a72d0b1d02f7a4ab9a4e83e9e813d494ff76a",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "We provide suﬃcient conditions related to treatment anticipation behavior and conditional parallel",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "30677dd6-4960-4168-b436-ed5ad582cf06",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Our framework acknowledges that in some applications there may be many group-time average treat-\nment eﬀects and researchers may want to aggregate them into diﬀerent summary causal eﬀect measures. This characterizes the aggregation step of the analysis. We provide ways to aggregate the potentially\nlarge number of group-time average treatment eﬀects into a variety of intuitive summary parameters and\ndiscuss speciﬁc aggregation schemes that can be used to highlight diﬀerent sources of treatment eﬀect\nheterogeneity across groups and time periods. In particular, we consider aggregation schemes that deliver\na single overall treatment eﬀect parameter with similarities to the ATT in the two period and two group\ncase as well as partial aggregations that highlight heterogeneity along certain dimensions such as (a) how\naverage treatment eﬀects vary with length of exposure to the treatment (event-study-type estimands); (b)\nhow average treatment eﬀects vary across treatment groups; and (c) how cumulative average treatment\neﬀects evolve over calendar time. We also provide a formal discussion of the costs and beneﬁts of balanc-\ning the sample in “event time” when analyzing dynamic treatment eﬀects. Overall, our setup makes it\nclear that, in general, the “best” aggregation scheme is application-speciﬁc as it depends on the type of\nquestion one wants to answer. Given that our identiﬁcation results are constructive, we propose easy-to-use plug-in type (parametric)\nestimators for the causal parameters of interest. Although the outcome regression, inverse probability\nweighting and doubly-robust estimands are equivalent from the identiﬁcation point of view, they suggest\ndiﬀerent types of DiD estimators one can use in practice.",
    "content_hash": "774190175d7f0ed61c0c86a05ed69881a88e8b2f2442e1349f553b20df5c3938",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "ment eﬀects and researchers may want to aggregate them into diﬀerent summary causal eﬀect measures.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0b7c0dd3-08fb-444e-a21f-17fb452a1962",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Nonetheless, we note that the unconditional versions of our\nparallel trends assumptions are weaker than the one in de Chaisemartin and D’Haultfœuille (2020), even\nif one were to specialize their setup to staggered adoption designs. Sun and Abraham (2020) proposes a parameter, cohort-speciﬁc average treatment eﬀects, that trans-\nlates our group-time average treatment eﬀects from calendar time into event time. Sun and Abraham\n(2020) proposes regression-based estimators of these parameters that have similar properties to our esti-\nmators in the speciﬁc case of staggered treatment adoption under an unconditional version of the parallel\ntrends assumption. However, our approach is more general in several respects. First, we allow for parallel\ntrends assumptions to hold after conditioning on covariates, and it is not clear how to adapt the regression\nbased estimators in Sun and Abraham (2020) to this case. Second, we consider a wide variety of possible\naggregations of group-time average treatment eﬀects where Sun and Abraham (2020) focuses particularly\n4",
    "content_hash": "64773e81758e58da904c1fa73460b347d00646e6263f8eb16a1f3129d513429a",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Sun and Abraham",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "28954b1f-d0c6-40ba-944e-e084ebe42610",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "average treatment eﬀect estimators. Thus, our proposed conﬁdence bands are arguably more suitable for\nvisualizing the overall estimation uncertainty than more traditional pointwise conﬁdence intervals. We illustrate the practical relevance of our proposal by analyzing the eﬀect of the minimum wage on\nteen employment. Here, we follow much empirical work on the eﬀects of the minimum wage and exploit\nhaving access to panel data and variation in treatment timing across states (e.g., Card and Krueger\n(1994); Neumark and Wascher (2000, 2008); Dube et al. (2010), among many others) in order to estimate\nthe eﬀect of the minimum wage on employment. Interestingly, in our setup, using our approach leads\nto qualitatively diﬀerent results than results from the TWFE estimator. This suggests that, at least\nin certain applications, using methods that are robust to treatment eﬀect heterogeneity can lead to\nmeaningful diﬀerences relative to more standard TWFE regressions. Recent Related Literature: This paper is related to the recent and emerging literature on het-\nerogeneous treatment eﬀects in DiD and/or event studies with variation in treatment timing; see, e.g.,\nde Chaisemartin and D’Haultfœuille (2020), Goodman-Bacon (2019), Imai et al. (2018), Borusyak and\nJaravel (2017), Athey and Imbens (2018) and Sun and Abraham (2020).",
    "content_hash": "cae7392109fe704f5783cab79280314e86d0a80c8c09da36e231ae45fe01a28f",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "visualizing the overall estimation uncertainty than more traditional pointwise conﬁdence intervals.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "aaf58e7a-3980-48ce-9deb-05b4e6285ea1",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "All these papers present, among\nother things, some negative results about the interpretation of parameters associated with standard\nTWFE linear regression speciﬁcations; see also Laporte and Windmeijer (2005), Wooldridge (2005a),\nChernozhukov et al. (2013), and Gibbons et al. (2018) for earlier related results based on (one-way) ﬁxed-\neﬀect estimators. Our proposed procedure completely bypasses the pitfalls highlighted in these papers as\nwe clearly separate the identiﬁcation, aggregation and estimation/inference steps of the analysis. These aforementioned papers also propose alternative DiD estimators that do not suﬀer from the\npitfalls associated with TWFE. Among these, perhaps the closest to our proposal are those of de Chaise-\nmartin and D’Haultfœuille (2020), and Sun and Abraham (2020), though several major diﬀerences are\nworth stressing. de Chaisemartin and D’Haultfœuille (2020) is focused on recovering an instantaneous treatment eﬀect\nmeasure, while we pay particular attention to treatment eﬀect dynamics. In fact, our framework allows one\nto form families of diﬀerent aggregate parameters in a uniﬁed manner. Second, while we pay particular\nattention to the role played by pre-treatment covariates, de Chaisemartin and D’Haultfœuille (2020)\nmainly focus on unconditional DiD designs. On the other hand, the setup in de Chaisemartin and\nD’Haultfœuille (2020) is more general than ours as we consider staggered adoption designs and they\nallow for more general treatment selection.",
    "content_hash": "72c1f62725b2dbd97d65d47d99355757be7a3b929ab81bc02d22ea7e5e159238",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Wooldridge",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2f923056-cf6a-4f9c-b02f-528ad151262b",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "In the Supplementary Appendix, we present proofs\nfor the results when only repeated cross-sections data is available, provide additional details about the\nempirical application, and present a small scale Monte Carlo simulation to illustrate the ﬁnite sample\nproperties of our proposed estimators.2\n2\nIdentiﬁcation\n2.1\nSetup\nWe ﬁrst introduce the notation we use throughout the article. We consider the case with T periods and\ndenote a particular time period by t where t = 1, . . . , T . In a canonical DiD setup, T = 2 and no one\nis treated in period t = 1. Let Di,t be a binary variable equal to one if unit i is treated in period t and\nequal to zero otherwise. We make the following assumption about the treatment process:\nAssumption 1 (Irreversibility of Treatment). D1 = 0 almost surely (a.s.). For t = 2, . . . , T ,\nDt−1 = 1 implies that Dt = 1 a.s.. Assumption 1 states that no one is treated at time t = 1, and that once a unit becomes treated,\nthat unit will remain treated in the next period.3 This assumption is also called staggered treatment\n2Supplementary Appendix is available at https://pedrohcgs.github.io/files/Callaway_SantAnna_2020_supp.pdf. 3In applications, it can be the case that some units are already treated by the ﬁrst time period. In our case, we would\ndrop these units; this is analogous to the case with two time periods.",
    "content_hash": "1350f87542e2e51ebc73f78b6b0475a39a23a382a453bae2d5218a0f9490152b",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "properties of our proposed estimators.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a941f0ac-8388-4381-99cc-19fa34c9ae8f",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "on the event study type of aggregation. Third, we make use of simultaneous inference procedures that\nexplicitly account for potential multiple-testing problems; Sun and Abraham (2020) focuses on pointwise\ninference. On the other hand, we do not have any results highlighting the pitfalls associated with using\nTWFE speciﬁcations with leads and lags of treatment indicators to conduct causal inference; these are\nunique to Sun and Abraham (2020). We also note that Athey and Imbens (2018) considers a staggered treatment adoption setup similar\nto ours. However, the starting point of Athey and Imbens (2018) is an assumption that the treatment\nadoption date is fully randomized which is stronger than our parallel trends assumptions. We also note\nthat Athey and Imbens (2018) abstracts away from the important role played by covariates in the DiD\nanalysis and does not consider aggregation schemes to summarize treatment eﬀect heterogeneity like\nwe do. On the other hand, we stress that the main focus of their paper is on providing design-based\ninference procedures for staggered DiD setups with random treatment dates. Their design-based inference\nprocedures complement our sampling-based inference procedures. Organization of the paper: The remainder of this article is organized as follows. Section 2 presents\nour main identiﬁcation results. We discuss our diﬀerent aggregation schemes in Section 3. Estimation\nand inference procedures for the treatment eﬀects of interest are presented in Section 4. We revisit the\neﬀect of minimum wage on employment in Section 5. Section 6 concludes. Proofs as well as additional\nmethodological results are reported in the Appendix.",
    "content_hash": "312cffc8963faa84df941d7c820bc51721a91db5abe41eaa0afef062937ae2f1",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": ") focuses on pointwise",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "1e623d7b-ae82-4e25-bfe4-ad946c98e60d",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "The reason to drop these units is that untreated\npotential outcomes are never observed for this group which will imply that treatment eﬀects are not identiﬁed for this group\nnor are they useful as a comparison group under a parallel trends assumption. 5",
    "content_hash": "6132bc996e4619be1f5d6f245a18f1fad3e47e291a0310e47d71d04eba68be66",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "This assumption is also called staggered treatment",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "988e571e-df12-4fc0-ada6-cd1a2a6b18ef",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Many of our results use a specialized version of this generalized\npropensity score, and, henceforth, we deﬁne pg(X) = pg,T (X) = P(Gg = 1|X, Gg + C = 1) which is the\nprobability of being ﬁrst treated in period g conditional on covariates and either being a member of group\ng or not participating in the treatment in any time period. Let G = supp(G)\\ {¯g} ⊆{2, 3, . . . , T } denote\nthe support of G excluding ¯g.5 Likewise, let X = supp(X) ⊆Rk denote the support of the pre-treatment\ncovariates. Finally, for a generic δ ≥0, let Gδ = G∩{2 + δ, 3 + δ, . . . , T }. Next, we set up the potential outcomes framework. Here, we combine the dynamic potential outcomes\nframework of Robins (1986, 1987) with the multi-stage treatment adoption setup discussed by Heckman\net al. (2016); see also Sianesi (2004). Let Yi,t(0) denote unit i’s untreated potential outcome at time t\nif they remain untreated through time period T ; i.e., if they were not to participate in the treatment\nacross all available time periods. For g = 2, . . . , T , let Yi,t(g) denote the potential outcome that unit i\nwould experience at time t if they were to ﬁrst become treated in time period g.",
    "content_hash": "10e53fa8a7f4b2bfc0d6db4223d796fb98f1d2660388b8ce9a7ddfa0f829d6da",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "= supp(",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "3e587f8d-0b3a-4296-beae-51d0e344ff34",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "When such “never-treated”\ngroup is not available, we exclude the latest-treated group as there will be no valid untreated comparison group for them. 6",
    "content_hash": "04edbf15a04e4817bc565e3656974760bf0f524361d17ce6181308583bb95568",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "imposes that each",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a72ea0cd-52e2-4f7c-aa29-9620dd9272d0",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "adoption in the literature. We interpret this assumption as if units do not “forget” about the treatment\nexperience.4\nDeﬁne G as the time period when a unit ﬁrst becomes treated. Under Assumption 1, for all units\nthat eventually participate in the treatment, G deﬁnes which “group” they belong to. If a unit does not\nparticipate in any time period, we arbitrarily set G = ∞. We deﬁne Gg to be a binary variable that is\nequal to one if a unit is ﬁrst treated in period g (i.e., Gi,g = 1{Gi = g}) and deﬁne C to be a binary\nvariable that is equal to one for units that do not participate in the treatment in any time period (i.e.,\nCi = 1{Gi = ∞} = 1−Di,T ). Let ¯g = maxi=1,··· ,n Gi be the maximum G in the dataset. Next, denote the\ngeneralized propensity score as pg,s(X) = P(Gg = 1|X, Gg + (1 −Ds) (1 −Gg) = 1). Note that pg,s(X)\nindicates the probability of being ﬁrst treated at time g, conditional on pre-treatment covariates X and\non either being a member of group g (in this case, Gg = 1) or a member of the “not-yet-treated” group by\ntime s (in this case, (1−Ds)(1−Gg) = 1).",
    "content_hash": "3cdda40c7439e869b0a16c43c25e643d0eb0449daf1c6f3dc34134f8364be39c",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "participate in any time period, we arbitrarily set",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "45f4e04e-3b0e-4377-9a6d-beed9c136430",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Note that our potential\noutcomes notation accounts for potential dynamic treatment selection, though it also accommodates (pre-\nspeciﬁed) treatment regimes (Murphy et al., 2001; Murphy, 2003). The observed and potential outcomes\nfor each unit i are related through\nYi,t = Yi,t (0) +\nT\nX\ng=2\n(Yi,t (g) −Yi,t (0)) · Gi,g\n(2.1)\nIn other words, we only observe one potential outcome path for each unit. For those that do not participate\nin the treatment in any time period, observed outcomes are untreated potential outcomes in all periods. For units that do participate in the treatment, observed outcomes are the unit-speciﬁc potential outcomes\ncorresponding to the particular time period when that unit adopts the treatment. We also impose the following random sampling assumption. Assumption 2 (Random Sampling). {Yi,1, Y,i2, . . . Yi,T , Xi, Di,1, Di,2, . . . , Di,T }n\ni=1 is independent and\nidentically distributed (iid). Assumption 2 implies that we have access to panel data; our results extend essentially immediately\n4See Han (2020), de Chaisemartin and D’Haultfœuille (2020) and Bojinov et al. (2020) for alternative setups where\ntreatment can “turn oﬀ”. 5When there is a “never treated” set of units with G = ∞, G only excludes this group.",
    "content_hash": "b1e181ab58cc75621eb3542dbdbaf5debf647538db77a5ad801ada9191801bde",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "(2.1)",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "91358d7d-62ed-457b-9401-6f9090364637",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Note that the ATT (g, t) does not impose any restriction on treatment eﬀect heterogeneity across\ngroups or across time. Thus, focusing on the family of ATT(g, t)’s allow us to analyze how average\ntreatment eﬀects vary across diﬀerent dimensions in a uniﬁed manner. For instance, by ﬁxing a group g\nand varying time t, one is able to highlight how average treatment eﬀects evolve over time for that speciﬁc\ngroup. By doing this for diﬀerent groups, we can have a better understanding about how treatment eﬀect\ndynamics vary across groups. In addition, as we discuss in Section 3, one can build on the ATT(g, t)’s\nto form more aggregated causal parameters that are constructed to answer speciﬁc questions like: (a)\nWhat was the average eﬀect of participating in the treatment across all groups that participated in the\ntreatment by time period T ? (b) Are average treatment eﬀects heterogeneous across groups? (c) How do\naverage treatment eﬀects vary by length of exposure to the treatment? (d) How do cumulative average\ntreatment eﬀects evolve over calendar time? We view this level of generality and ﬂexibility as one of the\nmain advantages of our framework that ﬁrst focuses on the family of ATT(g, t)’s. 2.3\nIdentifying Assumptions\nIn order to identify the ATT(g, t) and their functionals, we impose the following assumptions. 6Existence of expectations is assumed throughout. 7",
    "content_hash": "93440e0861d2f58a557345dc7bd0d5fb1ca6e7403c801d746c7465c1ea34a7d4",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": ", one can build on the",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "41ac514d-91cf-4e4c-a27f-ea06919a5719",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "to the case with repeated cross sections data and this case is developed in Appendix B. Here, we note\nthat Assumption 2 allows us to view all potential outcomes as random. Furthermore, it does not impose\nrestrictions between potential outcomes and treatment allocation, nor does it restrict the time series\ndependence of the observed random variables. On the other hand, Assumption 2 imposes that each\nunit i is randomly drawn from a large population of interest. For an alternative design-based inference\napproach, see Athey and Imbens (2018). Henceforth, to keep the notation more concise, we will suppress the unit index i in our notation. 2.2\nThe Group-Time Average Treatment Eﬀect Parameter\nGiven that diﬀerent potential outcomes cannot be observed for the same unit at the same time, researchers\noften focus on identifying and estimating some average causal eﬀects. For instance, in the canonical DiD\nsetup with two time periods, the most popular treatment eﬀect parameter of interest is the average\ntreatment eﬀect on the treated, denoted by6\nATT = E[Y2(2) −Y2(0)|G2 = 1]. In this paper, we consider a natural generalization of the ATT that is suitable to setups with multiple\ntreatment groups and multiple time periods. More precisely, we use the average treatment eﬀect for units\nwho are members of a particular group g at a particular time period t, denoted by\nATT(g, t) = E[Yt(g) −Yt(0)|Gg = 1],\nas the main building block of our framework. We call this causal parameter the group-time average\ntreatment eﬀect.",
    "content_hash": "0993c4ce082a56d52392aae1934633c3883725e8552d09199b4ff5605b4bf1b4",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Henceforth, to keep the notation more concise, we will suppress the unit index",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "02e90ea0-db5d-42b2-8f48-138a30d2904c",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "For evaluating\njob training programs, the distribution of observed covariates such as age, employment history, and years\nof education is often quite diﬀerent between individuals who participate in job training and those that do\nnot. When the path of labor market outcomes (in the absence of participating in job training) depends\non these covariates, a conditional parallel trends becomes more plausible than an unconditional parallel\ntrends assumption. In fact, ignoring the presence of covariate-speciﬁc trends can result in important\nbiases when evaluating causal eﬀects of policy interventions using unconditional DiD methods. Assumptions 4 and 5 diﬀer from each other depending on the comparison group one is willing to use\nin a given application. More speciﬁcally, Assumption 4 states that, conditional on covariates, the average\noutcomes for the group ﬁrst treated in period g and for the “never-treated” group would have followed\nparallel paths in the absence of treatment. Assumption 5 imposes conditional parallel trends between\n8",
    "content_hash": "ea6d9b13c15558b002ae8e62a9d12f7829a8aeaf78181bacfd133ffaccaa4327",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "in a given application. More speciﬁcally, Assumption",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e2678580-db1f-4bfc-bb1f-da26e11323a7",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Assumption 3 (Limited Treatment Anticipation). There is a known δ ≥0 such that\nE[Yt(g)|X, Gg = 1] = E[Yt(0)|X, Gg = 1] a.s. for all g ∈G, t ∈{1, . . . , T } such that t < g −δ. Assumption 3 restricts anticipation of the treatment for all “eventually treated” groups. When δ = 0,\nit imposes a “no-anticipation” assumption, see, e.g., Abbring and van den Berg (2003) and Sianesi (2004). This is likely to be the case when the treatment path is not a priori known and/or when units are not\nthe ones who “choose” treatment status. However, Assumption 3 also allows for anticipation behavior, as\nlong as we have a good understanding about the anticipation horizon δ. For instance, if units anticipate\ntreatment by one period, Assumption 3 would hold with δ = 1; see, e.g., Laporte and Windmeijer (2005)\nand Malani and Reif (2015) for the importance of accounting for potential anticipation behavior. Note\nthat, under Assumption 3, ATT (g, t) = 0 for all pre-treatment periods such that t < g −δ. Next, we consider two alternative assumptions that impose restrictions on the evolution of untreated\npotential outcomes. Assumption 4 (Conditional Parallel Trends based on a “Never-Treated” Group). Let δ be as deﬁned in\nAssumption 3. For each g ∈G and t ∈{2, . . .",
    "content_hash": "b408cea27cdfd267b0a68e861adfa877ee4dff774bf4c008ea8fd35332fb4b67",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "long as we have a good understanding about the anticipation horizon",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a7577e6d-2375-4545-8b44-983ce5a21508",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "group g and groups that are “not-yet-treated” by time t + δ.7 Importantly, both of these assumptions\nallow for covariate-speciﬁc trends and do not restrict the relationship between treatment timing and the\npotential outcomes, Yt (g)’s. Thus, they are weaker than the randomization-based assumption made by\nAthey and Imbens (2018). We also note that the unconditional versions of Assumptions 4 and 5 are\nweaker than the parallel trends assumption imposed by de Chaisemartin and D’Haultfœuille (2020) and\nSun and Abraham (2020) as they impose fewer restrictions on the evolution of Yt (0) in pre-treatment\nperiods; see, e.g., Marcus and Sant’Anna (2020) for a comparison. In our view, practitioners may favor Assumption 4 with respect to Assumption 5 when there is a\nsizable group of units that do not participate in the treatment in any period, and, at the same time,\nthese units are similar enough to the “eventually treated” units. When a “never-treated” group of units\nis not available or “too small”, researchers may favor Assumption 5 as it allows one to use more groups\nas valid comparison units, which potentially leads to more informative inference procedures. However, it\nis important to stress that favoring Assumption 5 with respect to Assumption 4 also involves potential\ndrawbacks. For instance, in the absence of treatment anticipation (δ = 0), Assumption 4 does not restrict\nobserved pre-treatment trends across groups, whereas Assumption 5 does; see, e.g., Marcus and Sant’Anna\n(2020).",
    "content_hash": "1aa85961a7c7a9fe9c3ea3400c8974fbff59bacaf925da13848353522e6f542b",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "when there is a",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "83f714cd-30a0-4ffc-8cf2-1adb619d5381",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Not restricting pre-treatment trends may be particularly meaningful in applications where the\neconomic environment during the “early-periods” was potentially diﬀerent from the “later-periods.” In\nthese cases, the outcomes of diﬀerent groups may evolve in a non-parallel manner during “early-periods”,\nperhaps because the groups were exposed to diﬀerent shocks, while trends become parallel in the “later-\nperiods.” We recommend taking these trade-oﬀs into account when deciding which conditional parallel\ntrends assumption is more appropriate for a given application.8\nThe ﬁnal identifying assumption we impose is an overlap condition. Assumption 6 (Overlap). For each t ∈{2, . . . , T }, g ∈G, there exist some ε > 0 such that P (Gg = 1) >\nε and pg,t(X) < 1 −ε a.s.. Assumption 6 extends the overlap assumption in Heckman et al. (1997, 1998), Abadie (2005), and\nSant’Anna and Zhao (2020) to the multiple groups and multiple periods setup. It states that a positive\nfraction of the population starts treatment in period g, and that, for all g and t, the generalized propensity\nscore is uniformly bounded away from one. Assumption 6 rules out “irregular identiﬁcation”, see, e.g.,\nKhan and Tamer (2010). Remark 1. Note that Assumption 3 and Assumption 4 (Assumption 5) are intrinsically connected. For\ninstance, when one imposes the “no-anticipation” condition (so that δ = 0), Assumption 4 would then\nimpose conditional parallel trends only for post-treatment periods t ≥g.",
    "content_hash": "06ed59070f4db3ff8f1418f988de16b8ecdc16b7d1e4f48b5f2efd39ff7f7b64",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Abadie",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "68a4fcd7-f449-4a2a-96da-c7f1970138c8",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "If one allows for anticipation\nbehavior (so that δ > 0), Assumption 4 would then impose conditional parallel trends in some pre-\ntreatment periods, too. In fact, the parallel trends assumptions become stronger as one increases δ. To\n7Athey and Imbens (2006) and de Chaisemartin and D’Haultfœuille (2017) also consider using “not-yet-treated” units\nas comparison groups in related DiD procedures. 8It may be tempting to use statistical pre-tests to select between diﬀerent versions of the parallel trends assumption. However, the results of Roth (2020) show that such a practice can lead to important distortions when conducting inference. Thus, we do not recommend following this path, but instead recommend taking the context of the application into account\nin order to choose the appropriate parallel trends assumption. 9",
    "content_hash": "1334bdb4f34217fa126018ffe937c43a0b497cb716c18157f97186208fcd9c44",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": ". To",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "bd5ed4f5-b2ae-4599-9d0f-d0bec0cedaa7",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "(2.4)\nAnalogously, let\nATT ny\nipw (g, t; δ) = E\n\n\n\n\n\n\nGg\nE [Gg] −\npg,t+δ (X) (1 −Dt+δ) (1 −Gg)\n1 −pg,t+δ (X)\nE\n\u0014pg,t+δ (X) (1 −Dt+δ) (1 −Gg)\n1 −pg,t+δ (X)\n\u0015\n\n\n\n(Yt −Yg−δ−1)\n\n,\n(2.5)\nATT ny\nor (g, t; δ) = E\n\u0014\nGg\nE [Gg]\n\u0010\nYt −Yg−δ−1 −mny\ng,t,δ (X)\n\u0011\u0015\n,\n(2.6)\nATT ny\ndr (g, t; δ) = E\n\n\n\n\n\n\nGg\nE [Gg] −\npg,t+δ (X) (1 −Dt+δ) (1 −Gg)\n1 −pg,t+δ (X)\nE\n\u0014pg,t+δ (X) (1 −Dt+δ) (1 −Gg)\n1 −pg,t+δ (X)\n\u0015\n\n\n\n\n\u0010\nYt −Yg−δ−1 −mny\ng,t,δ (X)\n\u0011\n\n. (2.7)\nWith some abuse of notation, we write ¯g −δ = ∞for any non-negative δ whenever ¯g = ∞. 10",
    "content_hash": "8a7cf7e24041df5dc1d4ee47ec6934a53446f05ba1dd6795672fe36c7ef6dcaf",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Sant’Anna and Zhao",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d0df402e-40f7-4fda-8e82-226918db47ef",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "the best of our knowledge, this trade-oﬀbetween the strength of these assumptions has not been noticed\nbefore. Remark 2. In some applications, practitioners may not be comfortable with using “never-treated” units\nas part of the comparison group because they behave very diﬀerently from the other “eventually treated”\nunits. In these cases, practitioners could drop all “never-treated” units from the analysis and proceed with\nAssumption 5. 2.4\nNonparametric Identiﬁcation of the Group-Time Average Treatment Eﬀects\nIn this section, we show that the family of group-time average treatment eﬀects are nonparametrically\npoint-identiﬁed under the aforementioned assumptions. Furthermore, we show that one can use outcome\nregression (OR), inverse probability weighting (IPW), or doubly robust (DR) estimands to recover the\nATT (g, t)’s. In addition, we also highlight the roles played by Assumption 3 and by Assumptions 4 and\n5 when forming these diﬀerent estimands. Before formalizing all the results, we need to introduce some additional notation. Let mnev\ng,t,δ (X) =\nE [Yt −Yg−δ−1|X, C = 1] and mny\ng,t,δ (X) = E [Yt −Yg−δ−1|X, Dt+δ = 0, Gg = 0]. These are population\noutcome regressions for the never-treated group and for the “not-yet-treated” by time t + δ group.",
    "content_hash": "2adb2e842dd565cf4982c3b46be08fd3c98aea1107dffe1815a3342d6bfeabb6",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "and by Assumptions",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "14d53c4f-8309-4376-91ef-067ec3aab029",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Let\nATT nev\nipw (g, t; δ) = E\n\n\n\n\n\n\nGg\nE [Gg] −\npg (X) C\n1 −pg (X)\nE\n\u0014 pg (X) C\n1 −pg (X)\n\u0015\n\n\n\n(Yt −Yg−δ−1)\n\n,\n(2.2)\nATT nev\nor\n(g, t; δ) = E\n\u0014\nGg\nE [Gg]\nYt −Yg−δ−1 −mnev\ng,t,δ (X)\n\u0001\u0015\n,\n(2.3)\nATT nev\ndr\n(g, t; δ) = E\n\n\n\n\n\n\nGg\nE [Gg] −\npg (X) C\n1 −pg (X)\nE\n\u0014 pg (X) C\n1 −pg (X)\n\u0015\n\n\n\n\nYt −Yg−δ−1 −mnev\ng,t,δ (X)\n\u0001\n\n.",
    "content_hash": "b4700e2caf144e2bf8c5ed6da130da9d12bd84f92f2d2db3e33a8f9b1c7f7865",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": ", . . .",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b396151f-e737-477d-9165-39ea87c8bb69",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Interestingly, the more treatment anticipation is allowed (i.e., the higher\nδ is), the further back in time one needs to go.9 Theorem 1 also suggests that the choice of comparison\ngroup is directly tied to the conditional parallel trends assumption one makes: under Assumption 4, one\ncan use “never treated” units as a ﬁxed comparison group for all “eventually treated” units; whereas,\nunder Assumption 5, one can use the “not-yet-treated by time t + δ” units as a valid comparison group\nfor those who are ﬁrst treated at time g. In this latter case, Theorem 1 also highlights that when all units\neventually gets treated (¯g < ∞), one is only able to identify the ATT(g, t)’s for time periods before the\nlast treated group “eﬀectively” starts their treatment, i.e., t < ¯g −δ. In this case, one can not identify\nthe ATT(g, t) for the last treated cohort, too.",
    "content_hash": "508eb1de18a4092833a1fecd974ccfe5452c243308794966a385ff35cafb3943",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": ") clearly resemble the one for",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e0a0377d-ddb0-46be-a568-f66b49407eec",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "In other words, Theorem 1 says that, from an identiﬁcation\npoint of view, one can recover the ATT (g, t)’s by exploiting diﬀerent parts of the data generating process:\nthe OR approach only relies on modeling the conditional expectation of the outcome evolution for the\ncomparison groups, the IPW approach relies on modeling the conditional probability of being in group\ng, whereas the DR approach exploits both OR and IPW components. In order to extend the results of Heckman et al. (1997, 1998), Abadie (2005), and Sant’Anna and Zhao\n(2020) to the multiple groups, multiple periods framework, we have to address two diﬀerent challenges: one\nassociated with an appropriate reference time period and one associated with an appropriate comparison\ngroup. Theorem 1 highlights how a solution to these challenges is directly connected to the limited\nanticipation and the conditional parallel trends assumptions. More speciﬁcally, Theorem 1 says that we\ncan use the time period t = g −δ −1 as an appropriate reference time period under Assumption 3 and\neither Assumption 4 or 5. This is the most recent time period when untreated potential outcomes are\nobserved for units in group g.",
    "content_hash": "b234337710137a2e5c0af1c324796e0f5fc632619e92d60c74bde4beb2b25b4f",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "is), the further back in time one needs to go.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0942db8d-4d4c-43b5-baa5-e3d436515674",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Finally, we note that when pre-treatment covariates play no role in identiﬁcation (i.e., Assumptions\n3, 4, and 5 hold unconditionally on X), (2.2)-(2.4) collapse to\nATT nev\nunc (g, t; δ) = E[Yt −Yg−δ−1|Gg = 1] −E[Yt −Yg−δ−1|C = 1],\n(2.8)\nand (2.5)-(2.7) collapse to\nATT ny\nunc(g, t; δ) = E[Yt −Yg−δ−1|Gg = 1] −E[Yt −Yg−δ−1|Dt+δ = 0]. (2.9)\n9As mentioned in Remark 1, as one allows δ to increase, Assumptions 4 and 5 becomes more restrictive. 11",
    "content_hash": "8a907fd8eea085a786512b7c8a329d27946d73c6e25cae8585e17f926642fc42",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": ", and then, using only the",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "aca55abe-2329-4354-9f4e-189d45537e26",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "As we discuss in Section 4, DiD estimators based on the DR estimands (2.4) and (2.7)\nusually enjoy additional robustness against model-misspeciﬁcations when compared to the IPW and OR\nestimands. Remark 6. Theorem 1 suggests that we can identify ATT(g, t) only for groups in Gδ ⊆G which can\ninvolve dropping some “early treated” groups due to anticipation eﬀects. When δ = 0, i.e. when there is\nno anticipation, Gδ = G. Theorem 1 also suggests that we can identify ATT (g, t) only until t = T −δ\nbecause of potential treatment anticipation behavior. In applications where some units are known to never\nparticipate in the treatment (including periods after time period T ), however, we can identify ATT(g, t)\nup to t = T by using these units as a valid comparison group for all time periods t = T −δ + 1, . . . , T ,\nprovided that an appropriate parallel trends assumption is satisﬁed. Remark 7. From Theorem 1 it is clear that pre-treatment covariates play a prominent role in our anal-\nysis. Importantly, Assumptions 4 and 5 suggest that researchers should include pre-treatment covariates\n12",
    "content_hash": "e362472b2aa47854226be1d0e6bdd61aea2b6103637a90d81e7f3ba830d924f2",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "behavior and imposing a conditional parallel trends assumption. In many applications, the",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "8d7a76ee-246f-4e06-834e-095e77ff7871",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Alternatively, one could\nuse the interacted two-way ﬁxed eﬀects regression proposed by Sun and Abraham (2020). Remark 4. When covariates are available, the ˜βg,t coeﬃcient of the population linear regression\nY = ˜αg,t\n1 + ˜αg,t\n2 · Gg + ˜αg,t\n3 · 1 {T = t} + ˜βg,t · (Gg × 1 {T = t}) + ˜γ · X + ˜ϵg,t\nthat uses the same subset of data as in Remark 3 is, in general, not equal to ATT (g, t) unless one is\nwilling to assume (i) homogeneous (in X) treatment eﬀects, i.e., E[Yt(g) −Yt(0)|Gg = 1, X] = E[Yt(g) −\nYt(0)|Gg = 1] a.s., and (ii) rule-out covariate-speciﬁc trends, i.e., for E [Yt −Yt−1|X, G] = E[Yt −Yt−1|G]\na.s. for all groups and time periods; see, e.g., S loczy´nski (2018) for a related discussion. The characteri-\nzations of ATT (g, t) discussed in Theorem 1 do not rely on these restrictive conditions. Remark 5. Although the IPW, OR, and DR based estimands presented in Theorem 1 are identical from\nan identiﬁcation standpoint, this is not the case when one wants to estimate and make inference about\nthe ATT (g, t).",
    "content_hash": "972b99b09d64d89816f4e40a27f514df62c35c31da4d45a20519d4e55b635d46",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "T −",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "84c3aa43-0d9c-4f2d-8706-8b47577a3503",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "These expressions for ATT(g, t) clearly resemble the one for ATT in the canonical two-periods and two-\ngroups case. As in that case, the average eﬀect of participating in the treatment for units in group g is\nidentiﬁed by taking the path of outcomes (i.e., the change in outcomes between the most recent period\nbefore they were aﬀected by the treatment and the current period) actually experienced by that group\nand adjusting it by the path of outcomes experienced by a comparison group. Under the parallel trends\nassumption, this latter path is the path of outcomes that units in group g would have experienced if they\nhad not participated in the treatment. Remark 3. From (2.8) one can see that when Assumptions 3 and 4 hold unconditionally and there is\nno-anticipation, the ATT (g, t) parameter can be obtained by ﬁrst subsetting the data to only contain\nobservations at time t and g −1, from units with either Gg = 1 or C = 1, and then, using only the\nobservations of this subset, running the (population) linear regression\nY = αg,t\n1 + αg,t\n2 · Gg + αg,t\n3 · 1 {T = t} + βg,t · (Gg × 1 {T = t}) + ϵg,t. (2.10)\nIt is then easy to verify that βg,t = ATT (g, t). Note that one would need to consider diﬀerent partitions\nof the data to characterize diﬀerent ATT (g, t) in terms of regression parameters.",
    "content_hash": "ecf686d193243847f83bf7e763303e27ac8f9fa921cf68c275998ad0bfbd34b4",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Remark 4.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "06caec51-78a9-4819-9c8d-0012e1c0ad88",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "These are based on the “static” and “dynamic” two-way ﬁxed eﬀects (TWFE) linear regression\nspeciﬁcations\nYi,t = αt + αg + βDi,t + ϵi,t,\n(3.2)\nYi,t = αt + αg +\n−2\nX\ne=−K\nδanticip\ne\n· De\ni,t +\nL\nX\ne=0\nβe · De\ni,t + vi,t,\n(3.3)\nrespectively, where αt is a time ﬁxed eﬀect, αg is a group ﬁxed eﬀect, ϵi,t and vi,t are error terms,\nDe\ni,t = 1 {t −Gi = e} is an an indicator for unit i being e periods away from initial treatment at time\nt, and K and L are positive constants. The parameter of interest in the static TWFE speciﬁcation is\nβ, which, in applications, is typically interpreted as an overall eﬀect of participating in the treatment\nacross groups and time periods. In the dynamic TWFE speciﬁcation, practitioners usually focus on the\nβe, e ≥0, and these parameters are typically interpreted as measuring the eﬀect of participating in the\n13",
    "content_hash": "f0161837724b3fbd7dc66f923a3596021173856b22a5fc582fc549874e16d7d5",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "not negative, the weights on underlying treatment eﬀect parameters are entirely driven by the TWFE",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "995b130e-6119-440c-b2ec-3133b7fa54be",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "that are potentially associated with the outcome evolution of Y (0) during post-treatment periods. We ex-\nplicitly rule out incorporating post-treatment covariates as they can potentially be aﬀected by the treatment;\nsee, e.g., Wooldridge (2005b), for a related discussion under the unconfoundedness setup. 3\nSummarizing Group-Time Average Treatment Eﬀects\nThe previous section shows that we can identify the ATT(g, t)’s by restricting treatment anticipation\nbehavior and imposing a conditional parallel trends assumption. In many applications, the ATT(g, t)’s\ncan be the ultimate causal parameters of interest. They can be used to highlight treatment eﬀect hetero-\ngeneity across diﬀerent groups g, at diﬀerent points in time t, and across diﬀerent lengths of treatment\nexposure, e = t −g. In other situations, however, researchers may want to combine these diﬀerent\nATT (g, t)’s to form more aggregated causal parameters. For instance, if the number of groups and time\nperiods is relatively large, it may be challenging to interpret many group-time average treatment eﬀects. In this section, we consider diﬀerent aggregation schemes for the ATT(g, t)’s that allow researchers\nto form a variety of summary measures of the causal eﬀects of a given policy.",
    "content_hash": "c7d155024dfc61330669a5893f033807adb71cba3bf574b658a9e24539a520e8",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "g, t",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b67c508d-0286-4746-9d2f-75c7d1df3e26",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Our aggregation schemes\nare of the form\nθ =\nX\ng∈G\nT\nX\nt=2\nw (g, t) · ATT(g, t),\n(3.1)\nwhere w (g, t) are carefully-chosen (known or estimable) weighting functions speciﬁed by the researcher\nsuch that θ can be used to address a well-posed empirical/policy question. Diﬀerence choices of w (g, t)\nallows researchers to highlight diﬀerent types of treatment eﬀect heterogeneity. We pay particular at-\ntention to aggregations that result in a single overall treatment eﬀect summary parameter as well as to\naggregations related to understanding dynamic eﬀects as is commonly done in event-study analysis. Of\ncourse, many other aggregated parameters of the type (3.1) can be easily constructed following our frame-\nwork. We illustrate this point by also summarizing heterogeneity with respect to group or by calendar\ntime. Before proceeding with the discussion on how to construct these diﬀerent aggregated parameters, it is\nworth revisiting the two most popular treatment eﬀect summary measures used by practitioners in DiD\nsetups.",
    "content_hash": "4a4b943f50730f3cc154f96b81c5e5a2e3a1249ea632f2794d100b05f05af9c1",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "across groups and time periods. In the dynamic TWFE speciﬁcation, practitioners usually focus on the",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2e8684ce-4cf4-405a-af0b-e4540b3938cb",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Indeed, answering this type of question is often the main motivation for using the event study regression\nin (3.3), though, as we mentioned above, that sort of regression may not be suitable for such a task. In\nthis section, we propose an aggregation scheme that is suitable to highlight treatment eﬀect heterogeneity\nwith respect to length of exposure to the treatment that does not suﬀer from the drawbacks associated\n14",
    "content_hash": "3a65a21a2ea3f55c7122f46321ca48a18bffa9d2807d5e577f16a84ef136c98e",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "across all groups that are ever observed to have participated in the treatment for exactly",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "72cf36c6-9bc6-4b31-9ed5-638eb3b099af",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Sun and Abraham (2020) shows that this\nis still the case as the β′\nes associated with (3.3) do not recover easy-to-interpret causal parameters and\nstill generally suﬀer from the same sorts of “negative weighting problems.” In contrast to this, we provide\na simple way to directly aggregate our group-time average treatment eﬀects into average treatment eﬀects\nacross diﬀerent lengths of exposure to the treatment. 3.1\nAggregations to Highlight Treatment Eﬀect Heterogeneity\nNext, we discuss several partial aggregations of the group-time average treatment eﬀects in order to sum-\nmarize diﬀerent dimensions of treatment eﬀect heterogeneity. Although there are additional possibilities,\nwe focus our discussion below on how to answer three particular questions: (a) How does the eﬀect of\nparticipating in the treatment vary with length of exposure to the treatment? (b) Do groups that are\ntreated earlier have, on average, higher/lower average treatment eﬀects than groups that are treated\nlater? (c) What is the cumulative average treatment eﬀect of the policy across all groups until some\nparticular point in time? Throughout this section, to avoid notation clutter, we assume that units do not\nanticipate treatment, i.e., we consider the case where Assumption 3 holds with δ = 0. We also assume\nthat a “never treated” group is available. 3.1.1\nHow do average treatment eﬀects vary with length of exposure to the treatment? One of the most popular questions that arises in DiD setups with multiple time periods concerns treatment\neﬀect dynamics: How does the eﬀect of participating in the treatment vary with length of exposure to the\ntreatment? For instance, do average treatment eﬀects increase/decrease with elapsed treatment time?",
    "content_hash": "aae237686eb10bdffb1fa4d572e2fb2bde24c68e76618a52a8afed7624db0a80",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "treatment? For instance, do average treatment eﬀects increase/decrease with elapsed treatment time?",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ca1680bb-46e4-41fa-81c3-e45f8c70d130",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Similarly, one can plot θes(e) across diﬀerent\ne’s to better understand treatment eﬀect dynamics. When doing so, it is important to be aware that\nthese comparisons may include compositional changes that can complicate the interpretation of these\nparameters (note that the same complications arise for event study regressions as well). To see this, for\n10Many of the parameters in this section involve expressions that have similar components as the one for θes(e) in (3.4),\nand it is worth mentioning a few extra details for this case that are common to the other expressions below. The term\ninvolving the indicator function, 1{g + e ≤T }, limits consideration to identiﬁed group-time average treatment eﬀects. The\nsummation over groups with group speciﬁc weights, in this case given by P(G = g|G + e ≤T ), calculates an average,\nweighted by group size, of ATT(g, t)’s that are involved in a particular aggregation. In addition, it is straightforward to\nshow that θes(e) can be written in the form of θ in Equation (3.1). Throughout this section, we have written each parameter\nof interest in its most intuitive form. Weights for each parameter in this section corresponding to the form of the weights in\nEquation (3.1) are provided in Table 1. 15",
    "content_hash": "ecdf6f93960e9b8671a0f8328bc5bf174379c2ab8f638e5fc77838ce677f5bc1",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "The ﬁrst term arises because the weights",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "531d3be1-93e5-4dfb-b8b9-dd9b71502856",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Table 1: Weights on ATT(g, t) for Aggregated Parameters\nParameter\nw(g, t)\nθes(e)\nθbal\nes (e, e′)\nθsel(˜g)\nθc(˜t)\nθcumu\nc\n(˜t)\nθO\nW\nθO\nsel\nwes\ne (g, t) = 1{g + e ≤T }1{t −g = e}P(G = g|G + e ≤T )\nwes,bal\ne\n(g, t) = 1{g + e′ ≤T }1{t −g = e}P(G = g|G + e′ ≤T )\nws\n˜g (g, t) = 1{t ≥g}1 {g = ˜g}/ (T −g + 1)\nwc\n˜t (g, t) = 1{t ≥g}1\n\b\nt = ˜t\nP(G = g|G ≤t)\nwc,cumu\n˜t\n(g, t) = 1{t ≥g}1\n\b\nt ≤˜t\nP(G = g|G ≤t)\nwO\nW (g, t) = 1{t ≥g}P(G = g|G ≤T )/ P\ng∈G\nPT\nt=2 1{t ≥g}P(G = g|G ≤T )\nwO\nsel (g, t) = 1{t ≥g}P(G = g|G ≤T )/ (T −g + 1)\nNotes: This table provides expressions for the weights on each ATT(g, t) (as in Equation 3.1) for each\nparameter discussed in this section.",
    "content_hash": "d9e6f74144832f22b2e62b7c6c977e7615e610e98acfba8d579efc9dda5dd404",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "diﬀerences as being due to treatment eﬀect dynamics.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "69021dad-627b-4238-ad68-587d63eccd06",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "In all cases except for θcumu\nc\n(˜t), the weights are all non-negative and sum to\none. For θcumu\nc\n(˜t), the weights are all non-negative but sum up to ˜t −1 (rather than one), but this is just a\nreﬂection of θcumu\nc\n˜t\n\u0001\nbeing a cumulative treatment eﬀect measure. with the event study regression in (3.3). Let e denote event-time, i.e., e = t −g denotes the time elapsed since treatment was adopted. Recall\nthat G denotes the time period that a unit is ﬁrst treated. Thus, a way to aggregate the ATT(g, t)’s to\nhighlight treatment eﬀect heterogeneity with respect to e is\nθes(e) =\nX\ng∈G\n1{g + e ≤T }P(G = g|G + e ≤T )ATT(g, g + e). (3.4)\nThis is the average eﬀect of participating in the treatment e time periods after the treatment was adopted\nacross all groups that are ever observed to have participated in the treatment for exactly e time periods. Here, the “on impact” average eﬀect of participating in the treatment occurs for e = 0. θes(e) is the\nnatural target for event study regressions that are common in applied work, though it completely avoids\nthe pitfalls associated with the dynamic TWFE speciﬁcation in (3.3).10\nIn event study regressions, it is common to plot βe across diﬀerent values of e and to interpret\ndiﬀerences as being due to treatment eﬀect dynamics.",
    "content_hash": "809bc8f9a06b9be5c69aefa826b8a0d63b9872015accf03f865f99f7fd7eba7e",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "these comparisons may include compositional changes that can complicate the interpretation of these",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "43039f2d-3dfa-4bd2-9393-f1303d7a881a",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "In this case, since the composition of groups is the same across\nall values of e, the additional terms in (3.5) do not show up at all and diﬀerences in θbal\nes (e; e′) across\ndiﬀerent values of e cannot be due to diﬀerences in the composition of groups at diﬀerent values of e. As\nan example, when one is interested in analyzing the evolution of treatment eﬀects up to 5 periods after\ntreatment was implemented, one can set e′ = 5 and, this way, the same groups of units will be used when\ncomputing θbal\nes (0; 5), θbal\nes (1; 5), . . . , θbal\nes (5; 5). The price one pays for “balancing” the groups with respect to event time is that fewer groups are\nused to compute these event-study-type estimands, which can lead to less informative inference. Thus, in\n11The composition changes mentioned here arise due to the staggered adoption of the treatment. For example, when\nT = 3, groups 2 and 3 both show up in the expression for θes(0), but only group 2 shows up in the expression for θes(1). 12If ATT(g, g + e) does not vary with g for any e ≥0, it is straightforward to show that the last two terms of (3.5) sum\nup to 0. 16",
    "content_hash": "d23c0719d5a2e9949c60ac355feb6acd0b350c63ea9f3c0c59975e4f16819f84",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "How do average treatment eﬀects vary across groups?",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "aefd4d40-df98-43c0-97e2-05011141c134",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "These two\nadditional terms may prevent one from interpreting the diﬀerences in θes(e) across diﬀerent values of e\nas being actual dynamic eﬀects of participating in the treatment unless one is willing to impose that\nATT(g, g + e) does not vary with g for any e ≥0; i.e., that dynamic eﬀects are common across groups.12\nHowever, this sort of homogeneity condition may be deemed too strong in many applications. A simple alternative causal parameter that can be used to highlight treatment eﬀect dynamics with\nrespect to length of exposure to the treatment and does not suﬀer from the issue of compositional changes\nhighlighted in (3.5) arises from “balancing” the groups with respect to event time, i.e., to only aggregate\nthe ATT (g, t)’s for a ﬁxed set of groups that are exposed to the treatment for at least some particular\nnumber of time periods and thereby circumvent the issue of compositional changes across diﬀerent values\nof e. In particular, for some event time e′ with 0 ≤e ≤e′ ≤T −2, let\nθbal\nes (e; e′) =\nX\ng∈G\n1{g + e′ ≤T }ATT(g, g + e)P(G = g|G + e′ ≤T ). (3.6)\nNotice that the deﬁnition of θbal\nes (e; e′) is very similar to θes(e) except that it calculates the average group-\ntime average treatment eﬀect for units whose event time is equal to e and who are observed to participate\nin the treatment for at least e′ periods.",
    "content_hash": "4b4f959e9fc19bb17f0e17ec70ba272b655094d3a58a6697a16549e4271276ca",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "(5; 5).",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c80cda36-8c36-43b3-a3b8-a3a0b517f23c",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "0 ≤e1 < e2 ≤T −2, consider the diﬀerence between θes(e2) and θes(e1) which is given by\nθes(e2) −θes(e1) =\nX\ng∈G\n1{g + e1 ≤T }P(G = g|G + e1 ≤T )(ATT(g, g + e2) −ATT(g, g + e1))\n|\n{z\n}\ndynamic eﬀect for group g\n(3.5)\n+\nX\ng∈G\n1{g + e2 ≤T }(P(G = g|G + e2 ≤T ) −P(G = g|G + e1 ≤T ))\n|\n{z\n}\ndiﬀerences in weights\nATT(g, g + e2)\n−\nX\ng∈G\n1{T −e2 < g ≤T −e1}\n|\n{z\n}\ndiﬀerent composition of groups\nP(G = g|G + e1 ≤T )ATT(g, g + e2). From the above decomposition it becomes clear that comparing θes(e) at two diﬀerent values of e provides\na weighted average of the dynamic eﬀect of participating in the treatment – the ﬁrst component on the\nright-hand side of (3.5) – plus two extra undesirable terms. Both of these undesirable terms are due\nto diﬀerent compositions of groups at diﬀerent event times.11 The ﬁrst term arises because the weights\nat each length of exposure diﬀer due to the changing composition of groups at each event time. The\nsecond term comes directly from diﬀerent compositions of groups at each length of exposure.",
    "content_hash": "493740425cd0929eb9329b09dbd3b065f4290a6b21a6735bcae97a642b3f3f52",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "g, g",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ea9729a4-9f84-4d22-9a2c-79789b17bb32",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "In addition, in the next section, these parameters\nwill be the building block for our main measure of the overall eﬀect of participating in the treatment. To\nconsider heterogeneous eﬀects across groups, we consider the following parameter\nθsel(˜g) =\n1\nT −˜g + 1\nT\nX\nt=˜g\nATT(˜g, t). (3.7)\nθsel(˜g) is the average eﬀect of participating in the treatment among units in group ˜g, across all their\npost-treatment periods. 3.1.3\nWhat is the cumulative average treatment eﬀect of the policy across all groups until\ntime ˜t? In some applications, researchers may want to construct an aggregated target parameter to highlight\ntreatment eﬀect heterogeneity with respect to calendar time. In economics, for example, researchers\nmight wish to study heterogeneous treatment eﬀects across the business cycle. The average eﬀect of\nparticipating in the treatment in time period t (across groups that have adopted the treatment by period\nt) is given by\nθc(˜t) =\nX\ng∈G\n1{˜t ≥g}P(G = g|G ≤˜t)ATT(g, t)\n(3.8)\nAn extension to this parameter is to think about the cumulative eﬀect of participating in the treatment\nup to some particular time period. For instance, in active labor market applications, policy makers may\nwant to know the cumulative average eﬀect of a given training program on earnings from the year that\nthe ﬁrst group of people were trained until year ˜t. This would provide a measure of the cumulative\n17",
    "content_hash": "948aa760e130468a6b361540be4e068a7479b778988eec475d664433dd40f8a5",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "where",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "34604693-7486-4a84-a536-9063816d03c4",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "practice, one should consider this “robustness” versus “eﬃciency” trade-oﬀwhen choosing between θbal\nes\nand θes. Remark 8. We note that θbal\nes (e; e′) closely resembles the empirical practice of only reporting event-study-\ntype coeﬃcients for the event periods that do not suﬀer from compositional changes, see, e.g., McCrary\n(2007) and Bailey and Goodman-Bacon (2015). An important caveat is that our proposed event-study-type\nestimands θbal\nes (e; e′) are not based on dynamic TWFE speciﬁcations akin to (3.3), and therefore bypass\nthe pitfalls associated with (3.3) highlighted by Sun and Abraham (2020). 3.1.2\nHow do average treatment eﬀects vary across groups? It is also straightforward to aggregate our group-time average treatment eﬀects to understand hetero-\ngeneity in the eﬀect of participating in the treatment across groups. Although understanding this sort\nof heterogeneity is relatively less common in applied work than trying to understand dynamic eﬀects as\ndiscussed above, there are still a number of cases in economics where understanding this sort of hetero-\ngeneity may be of interest. For example, work on the eﬀect of graduating during a recession on labor\nmarket outcomes (Oreopoulos et al. (2012)) or the eﬀect of job displacement across the business cycle\n(Farber (2017)) are related to heterogeneous eﬀects across groups. More generally, these parameters are\nuseful for understanding if the eﬀect of participating in the treatment was larger for groups that are\ntreated earlier relative to groups that are treated later.",
    "content_hash": "cf225337d60aa6d621bdd21a2d276384c566538f0f664e1837d0f03909225707",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "treatment eﬀect heterogeneity with respect to calendar time.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "68630b71-5c22-4044-83ed-1a8064cd5117",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "earnings gains induced by the training program. Alternatively, in health applications, researchers may\nwant to measure how many COVID-19 cases have been averted by shelter-in-place orders up to day ˜t. To consider the cumulative eﬀect, consider the following parameter\nθcumu\nc\n˜t\n\u0001\n=\n˜t\nX\nt=2\nθc(t). (3.9)\nθcumu\nc\n˜t\n\u0001\ncan be interpreted as the cumulative average treatment eﬀect among the units that have been\ntreated by time ˜t. 3.2\nAggregations into Overall Treatment Eﬀect Parameters\nFinally in this section, we consider some ideas for aggregating group time average treatment eﬀects into\nan overall eﬀect of participating in the treatment. One very simple idea is to just average all of the\nidentiﬁed group-time average treatment eﬀects together; i.e., to consider the parameter\nθO\nW = 1\nκ\nX\ng∈G\nT\nX\nt=2\n1{t ≥g}ATT(g, t)P(G = g|G ≤T )\n(3.10)\nwhere κ = P\ng∈G\nPT\nt=2 1{t ≥g}P(G = g|G ≤T ) (which ensures that the weights on ATT(g, t) in the sec-\nond term sum up to one). θO\nW is a weighted average of each ATT(g, t) putting more weight on ATT(g, t)’s\nwith larger group sizes.",
    "content_hash": "78c426f3734df660337e72ed8e10f78208b2e445fa328465106e191b3dc02bb9",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "is the average eﬀect of participating in the treatment experienced by all",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "bf236fee-f890-47e1-838d-332996d46890",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Unlike β in the TWFE regression speciﬁcation (3.2), this simple combination of\nATT(g, t)’s immediately rules out troubling issues due to negative weights; as a particular example, when\nthe eﬀect of participating in the treatment is positive for all units, this aggregated parameter cannot be\nnegative. That being said, just requiring positive weights is a very minimal requirement of a reasonable overall\ntreatment eﬀect parameter. For example, one drawback of θO\nW is that it systematically puts more weight\non groups that participate in the treatment for longer. Instead, we suggest the following parameter as a\ngeneral-purpose summary of the average eﬀect of participating in the treatment\nθO\nsel =\nX\ng∈G\nθsel(g)P(G = g|G ≤T )\n(3.11)\nwhere θsel(g) is the average eﬀect of participating in the treatment for units in group g as deﬁned in\nEquation (3.7) above. θO\nsel ﬁrst computes the average eﬀect for each group (across all time periods) and\nthen averages these eﬀects together across groups to summarize the overall average eﬀect of participating\nin the treatment. Thus, θO\nsel is the average eﬀect of participating in the treatment experienced by all\nunits that ever participated in the treatment. In this respect, its interpretation is the same as the ATT in\nthe canonical DiD setup with two periods and two groups. This is an attractive property for a summary\nmeasure of the overall eﬀect of participating in the treatment in the context of multiple time periods and\nvariation in treatment timing. Working by analogy, one can also deﬁne overall treatment eﬀect parameters by averaging θes(e) across\n18",
    "content_hash": "512ebf2753bf5e8fbbab3d6addb7036b60ddc66da923e784fb9b6332febf4398",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "periods of exposure to the",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "efbf5b93-885d-4060-bdb9-8eb4848b02ba",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "4\nEstimation and Inference\nSo far we have focused on the identiﬁcation and aggregation stages of the analysis. In this section,\nwe show how one can build on these results to form estimators for and conduct inference about the\ngroup-time average treatment eﬀects and their summary measures described in Section 3. Given that the\nATT (g, t)’s are the main building blocks of our analysis, we start with them. First, it is important to notice that our identiﬁcation results in Theorem 1 are constructive and\nsuggest a simple and intuitive two-step estimation strategy to estimate the ATT (g, t)’s. In the ﬁrst step,\none estimates the nuisance functions for each group g and time period t — pg(x) and/or mnev\ng,t,δ (X) if\none relies on Assumption 4, and pg,t+δ(x) and/or mny\ng,t,δ (X) if one relies on Assumption 5. In the second\nstep, one plugs the ﬁtted values of these estimated nuisance functions into the sample analogue of the\nconsidered ATT (g, t) estimand to obtain estimates of the group-time average treatment eﬀect. A natural question that then arises is which type of approach one should use in practice: the out-\ncome regression, inverse probability weighting, or the doubly-robust one. Although these three diﬀer-\nent approaches are equivalent from the identiﬁcation perspective, this is not the case from the esti-\nmation/inference perspective. The OR approach requires researchers to correctly model the outcome\nevolution of the comparison group to estimate the group-time average treatment eﬀects.",
    "content_hash": "a5b59579f0a6a7e6947a279011bb53cfc88edbca3f208c2965eb5c85d7e4a66f",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "Thus, the DR approach enjoys additional robustness against model misspeciﬁcations when compared to",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e394b620-d592-4cac-b8c7-9c3d819339a7",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "all event times or θc(t) across all time periods, i.e.,\nθO\nes =\n1\nT −1\nT −2\nX\ne=0\nθes(e)\nθO\nc =\n1\nT −1\nT\nX\nt=2\nθc(t)\n(3.12)\nIn our view, the appeal of these aggregations is likely to be somewhat more limited than that of θO\nsel\nin most applications. For example, the interpretation of θO\nes is complicated by the issue of the changing\ncomposition of groups across diﬀerent values of e discussed above (similar arguments apply to θO\nc as well). As before, one can circumvent the issue of the changing composition of groups by balancing the sample\nwith respect to event time. A (local) single summary parameter is given by\nθO,bal\nes\n(e′) =\n1\ne′ + 1\ne′\nX\ne=0\nθbal\nes (e, e′)\n(3.13)\nThis is the average eﬀect of participating in the treatment over the ﬁrst e′ periods of exposure to the\ntreatment. This is also a reasonable alternative overall treatment eﬀect parameter, but it should also be\nnoted that it is local to groups that participated in the treatment for at least e′ periods. As a ﬁnal comment, in general, none of the overall eﬀect parameters considered in this section are\nequal to each other except in the special case where ATT(g, t) is the same for all groups and all time\nperiods. In that case, all of the aggregated parameters, including β from the TWFE regression, are equal\nto each other.",
    "content_hash": "b610d66fec4dc93f39feebf89f759b842bbbfd9d7b75553271d75888c2020912",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": ") estimand to obtain estimates of the group-time average treatment eﬀect.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "7375477e-a624-4524-9ded-6f47d7d6971b",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "This approach\nis explicitly connected with the conditional parallel trends assumption required in DiD analysis as this\ncondition is usually expressed in terms of conditional expectations. The IPW approach, on the other\nhand, avoids explicitly modeling the outcome evolution of the comparison group and therefore does not\n19",
    "content_hash": "32ef6dc000496be8a019c148468cfa69bbcc660582422541995752e0bbb90921",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": ") when one invokes Assumption",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "8e9063f6-7b2e-4683-bf93-433695abff2f",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "We\nconsider this case mainly for its practical appeal which is especially true in applications where the number\nof covariates is fairly large and the number of observations is only moderate.13\nMore concisely, let\n[\nATT\nnev\ndr (g, t; δ) = En\nhbwtreat\ng\n−bwcomp,nev\ng\n\u0001 \u0010\nYt −Yg−δ−1 −bmnev\ng,t,δ\n\u0010\nX; bβnev\ng,t,δ\n\u0011\u0011i\n,\n(4.1)\n[\nATT\nny\ndr (g, t; δ) = En\nhbwtreat\ng\n−bwcomp,ny\ng\n\u0001 \u0010\nYt −Yg−δ−1 −bmny\ng,t,δ\n\u0010\nX; bβny\ng,t,δ\n\u0011\u0011i\n,\n(4.2)\nwhere\nbwtreat\ng\n=\nGg\nEn [Gg], bwcomp,nev\ng\n=\nbpg (X; bπg) C\n1 −bpg (X; bπg)\nEn\n\u0014 bpg (X; bπg) C\n1 −bpg (X; bπg)\n\u0015, bwcomp,ny\ng\n=\nbpg,t+δ (X; bπg,t+δ) (1 −Dt+δ) (1 −Gg)\n1 −bpg,t+δ (X; bπg,t+δ)\nEn\n\u0014 bpg,t+δ (X; bπg,t+δ) (1 −Dt+δ) (1 −Gg)\n1 −bpg,t+δ (X; bπg,t+δ)\n\u0015,\nwith bpg (·; bπg), bpg,t+δ(·; bπg,t+δ), bmnev\ng,t,δ(·; bβnev\ng,t,δ) and bmny\ng,t,δ(·; bβny\ng,t,δ) being (parametric) estimators of pg(·),\npg,t+δ(·), mnev\ng,t,δ(·) and mny\ng,t,δ(·), respectively, and for a generic Z, En [Z] = n−1 Pn\ni=1 Zi.",
    "content_hash": "3561eeefdbbcd269a3975bf135c11d754d6fb5951efd04b663a4052de79c0ac3",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "). In such cases, provided that one is comfortable with (parametric) extrapolation and",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "841290ca-5fa4-4240-9104-40634d0154a7",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "[\nATT\nnev\ndr (g, t; δ)\nand [\nATT\nnev\ndr (g, t; δ) are our proposed DR DiD estimators for ATT (g, t) when one invokes Assumption\n4 and Assumption 5, respectively. These estimators extend the DR DiD estimators of Sant’Anna and\nZhao (2020) from the two periods, two groups setup to the multiple groups, multiple periods setup while\nallowing for possible treatment anticipation. In addition, these estimators are of the H´ajek (1971)-type\nand their associated weights are guaranteed to sum up to one in ﬁnite samples. As illustrated by Busso\net al. (2014), this usually leads to improved ﬁnite sample properties. With the estimators for the ATT (g, t)’s in hand, one can use the analogy principle and combine these\n13Alternatively, one could adopt a fully nonparametric approach. Let f (x) be a generic notation for the nuisance\nfunctions. From Newey (1994), Chen et al. (2003), Ai and Chen (2003, 2007, 2012), and Chen et al. (2008), one can see\nthat the use of nonparametric ﬁrst-step estimators bg (x) of g (x) is warranted provided that ∥bg (x) −g (x)∥H = op\n\u0010\nn−1/4\u0011\nfor a pseudo-metric ∥·∥H, H being a vector space of functions.",
    "content_hash": "14e30721dcbd188434e49c76d76544f22580a9dfe1951eb81b3c5bfe0b4689f1",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": ") follow from symmetric arguments",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "1e079cd1-f70e-4d9d-963b-b75273edcfcb",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "However, when the dimension of X is moderate or large, as\nis often the case in empirical applications, conditions ensuring that ∥bg (x) −g (x)∥H = op\n\u0010\nn−1/4\u0011\ncan be rather stringent\ndue to the so-called “curse of dimensionality”. 20",
    "content_hash": "f456a07b4eb54b5ca158e47e9de594c5425d5d01c7fa8bb2ffdd300c5ddcaf31",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "where",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5e566a83-c1a7-4391-a4a1-73e7fb736194",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "rely on putative model restrictions directly tied to the parameter of interest. Instead, the IPW approach\nrequires one to correctly model the conditional probability of unit i being in group g given their covari-\nates X and that they are either in group g or in an appropriate comparison group. The DR approach\ncombines both the OR and IPW approaches as it relies on modeling both the outcome evolution and the\npropensity score. However, it only requires one to correctly specify either (but not necessarily both) the\noutcome evolution for the comparison group or the propensity score model (Sant’Anna and Zhao, 2020). Thus, the DR approach enjoys additional robustness against model misspeciﬁcations when compared to\nthe OR and IPW approaches. In addition, the DR approach potentially allows one to use a broader set\nof estimation methods such as those that involve penalization and some types of model selection, see, e.g. Belloni et al. (2017). Given these attractive robustness features associated with the DR approach, in this section we consider\nestimators of the DR form; the discussion on how to proceed with the OR and IPW approaches is\nanalogous and therefore omitted. We also focus on parametric estimators for the nuisance functions.",
    "content_hash": "7116c876e9cd038b3b06d9245eae40f1314ffa3a94cac11d978eed0aa88df1be",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": ")-type",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "24cba15a-4518-4661-a549-7e1559de11a0",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "In addition, (vi) for some\nε > 0 and all g ∈G, 0 ≤pg (X; πg) ≤1 −ε a.s., for all π ∈int (Θps), where Θps denotes the parameter\nspace of πg. 21",
    "content_hash": "7e3a02e93b1e46a567bca66876a826dae9eb4e0b5730d7c11f9ec13b7ced309f",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": ") says that either the working parametric model for the generalized propensity score is correctly",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "65dbbf7e-0e24-49fa-8fba-b2cd822b563e",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Let ∥Z∥=\np\ntrace (Z′Z) denote the Euclidean norm of Z and set W = (Y1, . . . , YT , X, D1, . . . , DT ). For a generic κnev\ng,t =\n\u0010\nπ′\ng, βnev\ng,t,δ\n\u0011′\n, let\nhdr,nev\ng,t\nW; κnev\ng,t , δ\n\u0001\n=\nwtreat\ng\n(W) −wcomp,nev\ng\n(W; πg)\n\u0001 Yt −Yg−δ−1 −mnev\ng,t,δ\nX; βnev\ng,t,δ\n\u0001\u0001\n,\nwhere the normalized weights wtreat\ng\n(W) and wcomp,nev\ng\n(W; πg) are given by\nwtreat\ng\n(W) =\nGg\nE [Gg],\nwcomp,nev\ng\n(W; πg) =\npg (X; πg) C\n1 −pg (X; πg)\nE\n\u0014 pg (X; πg) C\n1 −pg (X; πg)\n\u0015\n. (4.3)\nLet g (·) be a generic notation for pg (·) and mnev\ng,t,δ (·). With some abuse of notation, let g (·; γ) be a\ngeneric notation for pg (·; πg) and mnev\ng,t,δ\n\u0010\n·; βnev\ng,t,δ\n\u0011\n. The vector of pseudo-true parameters is given by\nκ∗,nev\ng,t\n=\n\u0010\nπ∗′\ng , β∗,nev ′\ng,t,δ\n\u0011′\n.",
    "content_hash": "409e10ac1cd8482b2148a5c0b4bdcfe146899524918b61015f78746d25f45bfa",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": ", β",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "4f5b345e-7c49-49bc-b88a-1fa572a8683b",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "For a generic κnev\ng,t =\n\u0010\nπ′\ng, βnev′\ng,t,δ\n\u0011′\n, deﬁne\nψdr,nev\ng,t,δ\n(Wi; κnev\ng,t ) = ψtreat,nev\ng,t,δ\n(Wi; βnev\ng,t,δ) −ψcomp,nev\ng,t,δ\n(Wi; πg, βnev\ng,t,δ) −ψest,nev\ng,t,δ\n(Wi; πg, βnev\ng,t,δ),\n(4.4)\nwith\nψtreat,nev\ng,t,δ\n(W; βnev\ng,t,δ) = wtreat\ng\n·\nYt −Yg−δ−1 −mnev\ng,t,δ\nβnev\ng,t,δ\n\u0001\u0001\n−wtreat\ng\n· E\n\u0002\nwtreat\ng\n·\nYt −Yg−δ−1 −mnev\ng,t,δ\nβnev\ng,t,δ\n\u0001\u0001\u0003\n,\nψcomp,nev\ng,t,δ\n(W; πg, βnev\ng,t,δ) = wcomp\ng\n(πg) ·\nYt −Yg−δ−1 −mnev\ng,t,δ\nβnev\ng,t,δ\n\u0001\u0001\n−wcomp\ng\n(πg) · E\n\u0002\nwcomp\ng\n(πg) ·\nYt −Yg−δ−1 −mnev\ng,t,δ\nβnev\ng,t,δ\n\u0001\u0001\u0003\n,\nand\nψest,nev\ng,t\n(W; πg, βnev\ng,t,δ) = lor,nev\ng,t\nβnev\ng,t,δ\n\u0001′ · Mdr,nev,1\ng,t,δ\n+ lps,nev\ng\n(πg)′ · Mdr,nev,2\ng,t,δ\n,\nwhere lor,nev\ng,t\n(·) is the asymptotic linear representation of the estimator for the outcome evolution of the\ncomparison groups as described in Assumption 7(iv), lps,nev\ng\n(·) is deﬁned analogously for the generalized\npropensity score, and\nMdr,nev,1\ng,t,δ\n= E\n\u0002wtreat\ng\n−wcomp\ng\n(πg)\n\u0001\n· ˙mnev\ng,t,δ\nβnev\ng,t,δ\n\u0001\u0003\n,\nMdr,nev,2\ng,t,δ\n= E\n\u0002\nαps,nev\ng\n(πg) ·\nYt −Yg−δ−1 −mnev\ng,t,δ\nβnev\ng,t,δ\n\u0001\u0001\n· ˙pg (πg)\n\u0003\n−E\n\u0002\nαps,nev\ng\n(πg) · wcomp\ng\n(πg) ·\nYt −Yg−δ−1 −mnev\ng,t,δ\nβnev\ng,t,δ\n\u0001\u0001\n· ˙pg (πg)\n\u0003\n,\nwith ˙mnev\ng,t,δ\n\u0010\nβnev\ng,t,δ\n\u0011\n= ∂mnev\ng,t,δ\n\u0010\nX; βnev\ng,t,δ\n\u0011.",
    "content_hash": "633f47423eb528c8d4d7f8f17fe56f5ac3e7119b0865cfdd25e8e15e765197e0",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "simply amounts to “perturbing” the inﬂuence function by a random weight",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2afc320b-9ac1-4a30-9fb1-727843b82270",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Assumption 8. For each g ∈G and t = {2, . . . , T −δ}, assume that E\nh\r\rhnev\ng,t\nW; κ∗,nev\ng,t\n, δ\n\u0001\r\r2i\n< ∞\nand E\nh\nsupκ∈Γ∗\n˙hnev\ng,t (W; κ)\ni\n< ∞, where Γ∗is a small neighborhood of κ∗,nev\ng,t\n. Assumptions 7-8 are standard in the literature, see e.g. Abadie (2005), Wooldridge (2007), Bonhomme\nand Sauder (2011), Graham et al. (2012), and Sant’Anna and Zhao (2020). Assumption 7 requires that the\nﬁrst-step estimators are based on smooth parametric models and that the estimated parameters admit √n-\nasymptotically linear representations, whereas Assumption 8 imposes some weak integrability conditions. Under mild moment conditions, these requirements are fulﬁlled when one adopts linear/nonlinear outcome\nregressions or logit/probit models, for example, and estimates the unknown parameters by (nonlinear)\nleast squares, quasi-maximum likelihood, or other alternative estimation methods, see e.g. Chapter 5 in\nvan der Vaart (1998), Wooldridge (2007), Graham et al. (2012) and Sant’Anna and Zhao (2020). In\nother words, Assumptions 7-8 allow for ﬂexible parametric speciﬁcations of the nuisance functions and\naccommodate diﬀerent estimation methods.",
    "content_hash": "9f36c7fb0e541dddd994e0ec868dd4b5d7077cd343b9f744ba597930fd11aa63",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "speciﬁed, or the working outcome regression model for the comparison group is correctly speciﬁed.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "511075b2-ee46-4b0b-8d42-b5f2fbd3f023",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "In what follows, we write wtreat\ng\n= wtreat\ng\n(W), wcomp\ng\n(πg) = wcomp,nev\ng\n(W; πg), and mnev\ng,t,δ\n\u0010\nβnev\ng,t,δ\n\u0011\n=\nmnev\ng,t,δ\n\u0010\nX; βnev\ng,t,δ\n\u0011\nto minimize notation.",
    "content_hash": "50a66bf279036f5860b54d2af758acab3f4735c983d4dfb14df98bd96b95f738",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "Our proposed bootstrap leverages the asymptotic linear representations derived in Theorem",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "38b7d9a9-7d2a-4535-adb7-958e21333ade",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "∂βnev\ng,t,δ, ˙pg (πg) = ∂pg (X; πg)/ ∂πg, and\nαps,nev\ng\n(πg) =\nC\n(1 −pg (X; πg))2\n,\nE\n\u0014 pg (X; πg) C\n1 −pg (X; πg)\n\u0015\n. Finally, let ATTt≥(g−δ) and [\nATT\ndr,nev\nt≥(g−δ) denote the vector of ATT(g, t) and [\nATT\nnev\ndr (g, t; δ), respec-\n22",
    "content_hash": "c4bcafed0b3f6bcbae233576371d8d1c2c6a34b8b5d8780ebf03ac26fe16bedf",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "unit variance, and ﬁnite third moment, independent of the original sample",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0dfe38e0-f83d-41e0-821b-89dbdcc0e3c5",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "tively, for all g ∈Gδ, t ∈{2, . . . T −δ} such that t ≥g −δ. Analogously, let Ψdr,nev\nt≥(g−δ) denote the collection\nof ψdr,nev\ng,t,δ\nacross all g ∈Gδ, t ∈{2, . . . T −δ} such that t ≥g −δ. Consider the following claim:\nFor each g ∈Gδ, t ∈{2, . . . T −δ} such that t ≥g −δ,\n∃π∗\ng ∈Θps : P\npg(X; π∗\ng) = pg (X)\n\u0001\n= 1\nor\n(4.5)\n∃β∗,nev\ng,t,δ ∈Θreg : P\n\u0010\nmnev\ng,t,δ\n\u0010\nX; β∗,nev\ng,t,δ\n\u0011\n= mnev\ng,t,δ (X)\n\u0011\n= 1. Claim (4.5) says that either the working parametric model for the generalized propensity score is correctly\nspeciﬁed, or the working outcome regression model for the comparison group is correctly speciﬁed. The next theorem establishes the joint limiting distribution of [\nATT\ndr,nev\nt≥(g−δ). Theorem 2. Under Assumptions 1-4, 6-8, for each g and t such that g ∈Gδ, t ∈{2, . . .",
    "content_hash": "2c03255d8f5638b06b5c6c1b4d52363be4e3242fc7906cefabd49e7d23ba46af",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "ATT",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "229c298c-c680-4b02-bb36-1034e245b11c",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Let {Vi}n\ni=1 be a sequence of iid random variables with zero mean,\n23",
    "content_hash": "92e64e7a1d60ab36bd697da1343095c60e30aa61ade9b16b5ff6eca93d1a7377",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "the application of the aforementioned bootstrap procedure is not warranted.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c24e64f3-8e8e-4c5a-aa99-02eb5c38385d",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Our proposed bootstrap leverages the asymptotic linear representations derived in Theorem 2 and inherits\nimportant advantages. First, it is easy to implement and very fast to compute. Each bootstrap iteration\nsimply amounts to “perturbing” the inﬂuence function by a random weight V , and it does not require\nre-estimating the propensity score in each bootstrap draw. Second, in each bootstrap iteration, there are\nalways observations from each group. This can be a real problem with the traditional empirical bootstrap\nwhere there may be no observations from a particular group in some particular bootstrap iteration. Third, computation of simultaneously (in g and t) valid conﬁdence bands is relatively straightforward. This is particularly important since researchers are likely to use conﬁdence bands to visualize estimation\nuncertainty about ATT (g, t) . Unlike pointwise conﬁdence bands, simultaneous conﬁdences bands do not\nsuﬀer from multiple-testing problems and are guaranteed to cover all ATT (g, t)’s with a probability at\nleast 1 −α. Finally, we note that our proposed bootstrap procedure can be readily modiﬁed to account\nfor clustering, see Remark 10 below. To proceed, let bΨdr,nev\nt≥(g−δ)(W) denote the sample-analogue of Ψdr,nev\nt≥(g−δ)(W), where population expec-\ntations are replaced by their empirical analogue, and the true nuisance functions and their derivatives\nare replaced by their estimators.",
    "content_hash": "4c8ca620f3a1adc77dbdc47f8ded31ac1b030ae3c831b464d6480defba679cdf",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "g, t",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f5c6224c-82db-423b-8b06-f3c438105d42",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "T −δ} and\nt ≥g −δ, provided that (4.5) is true,\n√n( [\nATT\nnev\ndr (g, t; δ) −ATT (g, t)) =\n1\n√n\nn\nX\ni=1\nψdr,nev\ng,t,δ\n(Wi; κ∗,nev\ng,t\n) + op(1). Furthermore, as n →∞,\n√n( [\nATT\ndr,nev\nt≥(g−δ) −ATTt≥(g−δ)) d−→N(0, Σ)\nwhere Σ = E[Ψdr,nev\nt≥(g−δ)(W)Ψdr,nev\nt≥(g−δ)(W)′]. Theorem 2 provides the inﬂuence function for estimating the vector of group-time average treatment\neﬀects, ATTt≥(g−δ), as well as its limiting distribution. Importantly, Theorem 2 emphasizes the DR\nproperty of [\nATT\nnev\ndr (g, t; δ): it recovers the ATT (g, t) provided that either the propensity score working\nmodel or outcome regression working model for the “never treated” is correctly speciﬁed. In order to conduct inference, one can show that the sample analogue of Σ is a consistent estimator\nfor Σ, which leads directly to standard errors and pointwise conﬁdence intervals. Instead of following this\nroute, we propose to use a simple multiplier bootstrap procedure to conduct asymptotically valid inference.",
    "content_hash": "2e7014c4cd91997ce987b5be6590feaa1224c800eb36f44e0902c2cdc0f74d16",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "This is similar to the bootstrap procedures used in",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a98200e1-1326-48c4-9aab-57dcda977ef8",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "unit variance, and ﬁnite third moment, independent of the original sample {Wi}n\ni=1. A popular exam-\nple involves iid Bernoulli variates {Vi} with P (V = 1 −κ) = κ/\n√\n5 and P (V = κ) = 1 −κ/\n√\n5, where\nκ =\n√\n5 + 1\n\u0001\n/2, as suggested by Mammen (1993). We deﬁne [\nATT\n∗,dr,nev\nt≥(g−δ), a bootstrap draw of [\nATT\ndr,nev\nt≥(g−δ), via\n[\nATT\n∗,dr,nev\nt≥(g−δ) = [\nATT\ndr,nev\nt≥(g−δ) + En\nh\nV · bΨdr,nev\nt≥(g−δ)(W)\ni\n. (4.6)\nThe next theorem establishes the asymptotic validity of the multiplier bootstrap procedure proposed\nabove. Theorem 3. Under the assumptions of Theorem 2\n√n\n\u0012\n[\nATT\n∗,dr,nev\nt≥(g−δ) −[\nATT\ndr,nev\nt≥(g−δ)\n\u0013\nd→\n∗N(0, Σ),\nwhere Σ is as in Theorem 2, and d→\n∗denotes weak convergence (convergence in distribution) of the bootstrap\nlaw in probability, i.e., conditional on the original sample {Wi}n\ni=1.",
    "content_hash": "e87d09b7bb655ad6cc5adcb9303ada900406e37a48763cc22c74b4742ac8781b",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "we have required an estimator for the main diagonal of",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "08f3c4c2-177b-436d-9ca6-da05db4570a0",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Additionally, for any continuous\nfunctional Γ(·),14\nΓ\n\u0012√n\n\u0012\n[\nATT\n∗,dr,nev\nt≥(g−δ) −[\nATT\ndr,nev\nt≥(g−δ)\n\u0013\u0013\nd→\n∗Γ (N(0, Σ)) . We now describe a practical bootstrap algorithm to compute studentized conﬁdence bands that cover\nATT (g, t) simultaneously over all t ≥g −δ with a pre-speciﬁed probability 1 −α in large samples. This is similar to the bootstrap procedures used in Kline and Santos (2012), Belloni et al. (2017) and\nChernozhukov et al. (2018) in diﬀerent contexts. Algorithm 1. 1) Draw a realization of {Vi}n\ni=1. 2) Compute [\nATT\n∗,dr,nev\nt≥(g−δ) as in (4.6), denote its (g, t)-\nelement as [\nATT\n∗(g, t) , and form a bootstrap draw of its limiting distribution as\nˆR∗(g, t) = √n\n\u0010\n[\nATT\n∗(g, t) −[\nATT (g, t)\n\u0011\n. 3) Repeat steps 1-2 B times.",
    "content_hash": "f07d28fe6b9ba071ce5d9ef131da41ddcd213d5d3175c519025fbcaafb6dd81b",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": ") can be easily adjusted to include these by simply replacing the “long",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "57cfcdea-3a02-42bf-b378-db50362844b2",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "4) Compute a bootstrap estimator of the main diagonal of Σ1/2 such as\nthe bootstrap interquartile range normalized by the interquartile range of the standard normal distribu-\ntion, bΣ1/2 (g, t) = (q0.75 (g, t) −q0.25 (g, t)) / (z0.75 −z0.25) , where qp (g, t) is the pth sample quantile of the\nˆR∗(g, t) in the B draws, and zp is the pth quantile of the standard normal distribution. 5) For each boot-\nstrap draw, compute t−testt≥(g−δ) = max(g,t)\nˆR∗(g, t)\nbΣ (g, t)−1/2 . 5) Construct bc1−α as the empirical\n(1 −a)-quantile of the B bootstrap draws of t−testt≥(g−δ). 6) Construct the bootstrapped simultaneous\nconﬁdence band for ATT (g, t), t ≥(g −δ) , as bC (g, t) = [ [\nATT\nnev\ndr (g, t; δ) ± bc1−αbΣ (g, t)−1/2 /√n]. The next corollary to Theorem 3 states that the simultaneous conﬁdence band for ATT (g, t) described\nin Algorithm 1 has correct asymptotic coverage. Corollary 1.",
    "content_hash": "4360703f2f453ed53d15b2fe4630867123e5efb606f28c641fce2dfba383e70a",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "; 0)",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e577001e-8258-46be-8d22-265af47686b9",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Under the assumptions of Theorem 2, for any 0 < α < 1, as n →∞,\nP\n\u0010\nATT (g, t) ∈bC (g, t)\n∀t ∈{2, . . . , T } , g ∈Gδ : t ≥g −δ\n\u0011\n→1 −α,\n14Since the number of periods T is ﬁxed, Γ(·) should be interpreted as a continuous functional between Euclidean spaces. 24",
    "content_hash": "e2a158011b2d55eb65a39a1acd27689e25552fa800ccb2d40e9e8fef4739ac33",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "g, t",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "276fc6ca-8b9b-47f9-9b35-b7ddcdc1c8ce",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "However, the\nresulting “constant width” simultaneous conﬁdence band may be of larger length; see, e.g., Montiel Olea\nand Plagborg-Møller (2018) and Freyberger and Rai (2018). Remark 12. The above results focus on making inference about ATT(g, t)’s in (eﬀective) post-treatment\nperiods t ≥g −δ. Although the limited anticipation condition in Assumption 3 implies that ATT(g, t) = 0\nfor all t < g −δ regardless of the group g, it is common practice to also estimate these pre-treatment\nparameters and use them to assess the credibility of the underlying identifying assumptions. Note that\nour DiD estimands (2.2) - (2.7) can be easily adjusted to include these by simply replacing the “long\ndiﬀerences” (Yt −Yg−δ−1) with the “short diﬀerences” (Yt −Yt−1) for all t < g −δ. All our results\ncontinues to hold when one augments [\nATT\ndr,nev\nt≥(g−δ) to also include these estimates for the ATT(g, t)’s in\nthe pre-treatment periods t < g −δ. 4.2\nAsymptotic Theory for Summary Parameters\nAssume, for simplicity, that Assumption 3 holds with δ = 0. In this section, we discuss how one can\nestimate and make inference about the summary measures of the casual eﬀects discussed in Section 3. More concisely, we consider parameters of the form of θ as deﬁned in (3.1), which covers all of the\naggregated parameters discussed in Section 3.",
    "content_hash": "b9b0632d89543f3f7b9d640f3b7b7e8b0df5a71b4a0a63be40d9c865956836a9",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "The Eﬀect of Minimum Wage Policy on Teen Employment",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a27e1d52-4a9c-4219-b9d8-0083f11f0c25",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Given the discussion in Section 4.1, a natural way to estimate θ is to use the plug-in type estimators\nˆθ =\nX\ng∈G\nT\nX\nt=2\nbw (g, t) [\nATT\nnev\ndr (g, t; 0) ,\n15The formal results in Abadie et al. (2017) focus on the cross section case and rely on additional functional form\nrestrictions that we do not impose in this paper. Fully extending the results of Abadie et al. (2017) to the semiparametric\npanel data case is beyond the scope of our paper. 16In such cases, provided that one is comfortable imposing additional functional form assumptions, one could use alter-\nnative procedures such as Conley and Taber (2011) and Ferman and Pinto (2019). Extending these proposals to our setup\nis beyond the scope of this paper though. 25",
    "content_hash": "3d0c94c4d38120eee278ddc7f7a562485417fb76222002224b6ab5c1e542a296",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "5.15 per hour. We focus on county level teen employment in states whose",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "49689378-00a1-4f25-9061-70be445ce018",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "We think that this comparison is important in order to get\na sense of whether the theoretical limitations of TWFE discussed in recent work end up translating into\nmeaningful diﬀerences in applications. Moreover, one might expect that understanding the eﬀect of a\nminimum wage change on employment is a challenging case for TWFE as the eﬀect of the minimum wage\nmay be dynamic (Meer and West (2016)) and the timing of minimum wage changes varies across states. Unlike TWFE, the approach that we have proposed in the current paper is robust to these challenges. 26",
    "content_hash": "fc41968eeee49c75aa073472ea3a47454f3903368e2bf477445b149bc9ca939c",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "comparison group and allowing for one year anticipation in the Supplementary Appendix; results from",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "80b84a03-de7c-47df-8b15-e1d9b92562ba",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "where bw (g, t) are estimators for w (g, t) such that for all g ∈G and t = 2, . . . , T ,\n√n ( bw (g, t) −w (g, t)) =\n1\n√n\nn\nX\ni=1\nξw\ng,t(Wi) + op (1) ,\nwith E\n\u0002\nξw\ngt(W)\n\u0003\n= 0 and E\n\u0002\nξw\ngt(W)ξw\ngt(W)′\u0003\nﬁnite and positive deﬁnite. Estimators based on the sample\nanalogue of the weights discussed in Section 3 satisfy this condition. Let\nlw (Wi) =\nX\ng∈G\nT\nX\nt=2\n\u0010\nw (g, t) · ψdr,nev\ng,t,0 (Wi; κ∗,nev\ng,t\n) + ξw\ng,t(Wi) · ATT(g, t)\n\u0011\n,\nwhere ψdr,nev\ng,t,δ\nare as deﬁned in (4.4). The following result follows immediately from Theorem 2, and can be used to conduct asymptotically\nvalid inference for the summary causal parameters θ. Corollary 2.",
    "content_hash": "fa88349c9772142686f33ffbf6b016ada8576d7d5c0ad61135ecbe0aadd76a15",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "employment comes from the Quarterly Workforce Indicators (QWI), as in",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "6bc681f7-bed7-48eb-a29f-33208fbffef9",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Under the assumptions of Theorem 2,\n√n(ˆθ −θ) =\n1\n√n\nn\nX\ni=1\nlw (Wi) + op(1)\nd−→N\n\u0010\n0, E\nh\nlw (W)2i\u0011\nCorollary 2 implies that one can construct standard errors and conﬁdence intervals for summary\ntreatment eﬀect parameters based on a consistent estimator of E\nh\nlw (W)2i\nor by using a bootstrap\nprocedure like the one in Algorithm 1. The main advantage of using the bootstrap procedure akin\nto Algorithm 1 is that inference procedures would be robust against multiple-testing problems. This\nis particularly attractive when considering θes(e), θbal\nes (e; e′), θsel(˜g), and θc\n˜t\n\u0001\n, as practitioners would\nprobably analyze how these parameters diﬀer across event-times e, groups ˜g, and calendar-time ˜t. Remark 13. As discussed in Remark 10, the validity of the “cluster-robust” multiplier bootstrap procedure\nrelies on the number of clusters being “large.” In some applications such a condition may be more plausible\nwhen analyzing the aggregated parameter θ than when analyzing the ATT(g, t) themselves. 5\nThe Eﬀect of Minimum Wage Policy on Teen Employment\nIn this section, we illustrate the empirical relevance of our proposed methods. To do this, we apply our\nmethods to study the eﬀect of the minimum wage on teen employment. The main goal of this section\nis to compare results arising from using a TWFE speciﬁcation (as is most common in applications) to\nresults coming from our proposed method.",
    "content_hash": "3b01dff8f71e3415c58190e645061fa5e2a78939bc23477eb8c46c59719872cc",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": ") for a detailed discussion of this dataset. Other pre-treatment county characteristics come from",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9abf632a-7f12-4cc1-8d97-d2936a5332c9",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "By far the most common approach to trying to understand the eﬀect of the minimum wage on em-\nployment is to exploit variation in the timing of minimum wage increases across states. Our identiﬁcation\nstrategy follows this approach. In particular, we consider a time period from 2001-2007 where the federal\nminimum wage was ﬂat at $5.15 per hour. We focus on county level teen employment in states whose\nminimum wage was equal to the federal minimum wage at the beginning of the period. Some of these\nstates increased their minimum wage over this period – these become treated groups. In particular, we\ndeﬁne groups by the time period when a state ﬁrst increased its minimum wage. Others did not increase\ntheir minimum wage – these are the untreated group. This setup allows us to have more data than local\ncase study approaches. On the other hand, it also allows us to have cleaner identiﬁcation (state-level\nminimum wage policy changes) than in studies with more periods; the latter setup is more complicated\nthan ours particularly because of the variation in the federal minimum wage over time. It also allows\nus to check for internal consistency of identifying assumptions – namely whether or not the identifying\nassumptions hold in periods before particular states raised their minimum wages. We use county level data on teen employment and other county characteristics. County level teen\nemployment comes from the Quarterly Workforce Indicators (QWI), as in Dube et al. (2016); see Dube\net al. (2016) for a detailed discussion of this dataset. Other pre-treatment county characteristics come from\nthe 2000 County Data Book.",
    "content_hash": "1a83787ac493a2ffa74e2bc46905496446de010ad4ea3cdbde6266be52cc0879",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "0.00",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d5c40d5d-73fb-49e7-a91f-cb2b944602b7",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "In the main text,\nwe consider the case where never-treated counties are the comparison group and where we do not allow\nfor any anticipation eﬀects (i.e., δ = 0). We provide results using the not-yet-treated counties as the\ncomparison group and allowing for one year anticipation in the Supplementary Appendix; results from\nthose cases are quite similar to the ones presented here. The ﬁrst set of results comes from using the unconditional parallel trends assumption to estimate the\n27",
    "content_hash": "cffc1b580c474d7f353a47fdcc17a946cd8627938c03bb67e53ae5eb089a76ba",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "Figure 1: Minimum Wage Group-Time Average Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "aa4393da-5305-4f98-95de-eceeebd31fec",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "These include county population in 2000, the fraction of the population that\nis white, educational characteristics from 1990, median income in 1997, and the fraction of the population\nbelow the poverty level in 1997. After dropping ten states due to their minimum wage being higher than\nthe federal minimum wage in 2000, seven other states for lack of data on teen employment, and four\nother states in the Northern census region, our ﬁnal sample includes county-level data from 29 states. We provide additional details on constructing the data in the Supplementary Appendix. Summary statistics for county characteristics are provided in Table 2. There are some notable dif-\nferences in county characteristics between counties in states that increased their minimum wage and in\nstates that did not increase their minimum wage. Treated counties are much less likely to be in the South. They also have much higher population (on average 94,000 compared to 53,000 for untreated counties). The proportion of white residents is higher in treated counties (on average, 89% compared to 83% for\nuntreated counties). There are smaller diﬀerences in the fraction with high school degrees and the poverty\nrate though the diﬀerences are both statistically signiﬁcant. Treated counties have a somewhat higher\nfraction of high school graduates and a somewhat lower poverty rate. 5.1\nResults\nIn the following we discuss diﬀerent sets of results using diﬀerent identiﬁcation strategies. In particular,\nwe consider the cases in which one would assume that the parallel trends assumption would hold un-\nconditionally, and when it holds only after controlling on observed characteristics X.",
    "content_hash": "120699b5bf71111c2dcf220e9d9da2b341586940ac5bde53f053cb76edb5f5cd",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "). A two-way ﬁxed eﬀects model with a post treatment dummy variable also",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "72bda148-239f-4ad0-8baf-0145f7b694e1",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "For states ﬁrst treated in 2006, there is a small eﬀect in 2006: 2.3% lower teen\nemployment; however, it is larger in 2007: 7.1% lower teen employment. Panel (a) of Table 3 reports aggregated treatment eﬀect measures. First, we consider how the eﬀect\nof increasing the minimum changes by the amount of time that the policy has been in place. These\nparameters paint largely the same picture as the group-time average treatment eﬀects. The eﬀect of\n28",
    "content_hash": "7c3e6752ef16eb45e6e118db4b0e6af173a722ac51e6a3084db7536263abd2ea",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "Table 3: Minimum Wage Aggregated Treatment Eﬀect Estimates",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a141646a-f6b1-4bae-bc48-667325a40c47",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "For 5 out of 7 group-time average treatment ef-\nfects, there is a clear statistically signiﬁcant negative eﬀect on employment. The other two are marginally\ninsigniﬁcant (and negative). The group-time average treatment eﬀects range from 2.3% lower teen em-\nployment to 13.6% lower teen employment. The simple average (weighted only by group size) is 5.2%\nlower teen employment, and the average eﬀect of a minimum wage increase across all groups that in-\ncreased their minimum wage (corresponding to an estimate of θO\nsel above) is 3.9% lower teen employment\n(see Panel (a) of Table 3). A two-way ﬁxed eﬀects model with a post treatment dummy variable also\nprovides similar results, indicating 3.7% lower teen employment due to increasing the minimum wage. In\nlight of the literature on the minimum wage these results are not surprising as they correspond to the\ntypes of regressions that tend to ﬁnd that increasing the minimum wage decreases employment; see the\ndiscussion in Dube et al. (2010). As in Meer and West (2016), there also appears to be a dynamic eﬀect of increasing the minimum wage. For Illinois (the only state in the group that ﬁrst raised its minimum wage in 2004), teen employment\nis estimated to be 3.4% lower on average in 2004 than it would have been if the minimum wage had\nnot been increased. In 2005, teen employment is estimated to be 7.1% lower; in 2006, 12.5% lower; and\nin 2007, 13.6% lower.",
    "content_hash": "dd330cb22137dce720372036fe20e9e0a0f39f3fab5df4aa89ee33e4f228d0b6",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "labeled ‘Event Study w/ Balanced Groups’). When",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "84f8f8e7-f24e-4b32-be0c-232c8ca4088a",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Figure 1: Minimum Wage Group-Time Average Treatment Eﬀects\nG\nG\nPre−Treatment\nPost−Treatment\nG\nG\nG\nG\nG\nG\n−0.2\n−0.1\n0.0\n0.1\n0.2\n2002\n2003\n2004\n2005\n2006\n2007\nGroup 2004\nG\nG\nG\nG\nG\nG\n−0.2\n−0.1\n0.0\n0.1\n0.2\n2002\n2003\n2004\n2005\n2006\n2007\nGroup 2006\nG\nG\nG\nG\nG\nG\n−0.2\n−0.1\n0.0\n0.1\n0.2\n2002\n2003\n2004\n2005\n2006\n2007\nGroup 2007\n(a) Unconditional Parallel Trends\nG\nG\nG\nG\nG\nG\n−0.2\n−0.1\n0.0\n0.1\n0.2\n2002\n2003\n2004\n2005\n2006\n2007\nGroup 2004\nG\nG\nG\nG\nG\nG\n−0.2\n−0.1\n0.0\n0.1\n0.2\n2002\n2003\n2004\n2005\n2006\n2007\nGroup 2006\nG\nG\nG\nG\nG\nG\n−0.2\n−0.1\n0.0\n0.1\n0.2\n2002\n2003\n2004\n2005\n2006\n2007\nGroup 2007\n(b) Conditional Parallel Trends\nNotes: The eﬀect of the minimum wage on teen employment estimated under the unconditional parallel trends assumption\n(Panel (a)) and the conditional parallel trends assumption (Panel (b)). Red lines give point estimates and uniform 95%\nconﬁdence bands for pre-treatment periods allowing for clustering at the county level. Under the null hypothesis of the\nparallel trends assumption holding in all periods, these should be equal to 0. Blue lines provide point estimates and uniform\n95% conﬁdence bands for the treatment eﬀect of increasing the minimum wage allowing for clustering at the county level.",
    "content_hash": "88d36f9af9194f80822004b0a3333240a67c57561d3c244b663865e6ce1e1718",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "Table 3: Minimum Wage Aggregated Treatment Eﬀect Estimates",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "3a04fa07-4d36-4207-9b20-b6dc7efab6cf",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "The top row includes states that increased their minimum wage in 2004, the middle row includes states that increased their\nminimum wage in 2006, and the bottom row includes states that increased their minimum wage in 2007. The estimates in\nPanel (b) use the the doubly robust estimator discussed in the text. increasing the minimum wage on teen employment appears to be negative and increasing in magnitude\nthe longer states are exposed to the higher minimum wage. In particular, in the ﬁrst year that a state\nincreases its minimum wage, teen employment is estimated to decrease by 2.7%, in the second year it\nis estimated to decrease by 7.1%, in the third year by 12.5%, and in the fourth year by 13.6%. Notice\nthat the last two dynamic treatment eﬀect estimates are exactly the same as the estimates coming from\nIllinois alone because Illinois is the only state that is treated for at least two years. These results are\nrobust to keeping the composition of groups constant by “balancing” the groups across diﬀerent lengths\nof exposure to the treatment (see the row in Table 3 labeled ‘Event Study w/ Balanced Groups’). When\nwe restrict the sample to only include groups that had a minimum wage increase for at least one full\nyear (i.e., we keep groups 2004 and 2006 but not 2007), we estimate that the eﬀect of increasing the\nminimum wage on impact is 2.7% lower teen employment and 7.1% lower teen employment one year after\n29",
    "content_hash": "1d2c8ac8d854cac9484484b34d071644f666da34f9fd39085bf20b82c5252078",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "Table 3: Minimum Wage Aggregated Treatment Eﬀect Estimates",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c619fcef-37da-4998-998b-92b8335e20fb",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "the increase.17\nTable 3: Minimum Wage Aggregated Treatment Eﬀect Estimates\n(a) Unconditional Parallel Trends\nPartially Aggregated\nSingle Parameters\nTWFE\n-0.037\n(0.006)\nSimple Weighted Average\n-0.052\n(0.006)\nGroup-Speciﬁc Eﬀects\ng=2004\ng=2006\ng=2007\n-0.091\n-0.047\n-0.028\n-0.039\n(0.019)\n(0.008)\n(0.007)\n(0.007)\nEvent Study\ne=0\ne=1\ne=2\ne=3\n-0.027\n-0.071\n-0.125\n-0.136\n-0.090\n(0.006)\n(0.009)\n(0.021)\n(0.023)\n(0.013)\nCalendar Time Eﬀects\nt=2004\nt=2005\nt=2006\nt=2007\n-0.034\n-0.071\n-0.055\n-0.050\n-0.052\n(0.019)\n(0.02)\n(0.009)\n(0.006)\n(0.013)\nEvent Study\ne=0\ne=1\nw/ Balanced Groups\n-0.027\n-0.071\n-0.049\n(0.009)\n(0.009)\n(0.008)\n(b) Conditional Parallel Trends\nPartially Aggregated\nSingle Parameters\nTWFE\n-0.008\n(0.006)\nSimple Weighted Average\n-0.033\n(0.007)\nGroup-Speciﬁc Eﬀects\ng=2004\ng=2006\ng=2007\n-0.044\n-0.029\n-0.029\n-0.031\n(0.020)\n(0.008)\n(0.008)\n(0.007)\nEvent Study\ne=0\ne=1\ne=2\ne=3\n-0.024\n-0.041\n-0.050\n-0.071\n-0.046\n(0.006)\n(0.009)\n(0.022)\n(0.026)\n(0.013)\nCalendar Time Eﬀects\nt=2004\nt=2005\nt=2006\nt=2007\n-0.030\n-0.025\n-0.030\n-0.049\n-0.033\n(0.022)\n(0.021)\n(0.009)\n(0.007)\n(0.012)\nEvent Study\ne=0\ne=1\nw/ Balanced Groups\n-0.016\n-0.041\n-0.028\n(0.010)\n(0.009)\n(0.008)\nNotes:\nThe table reports aggregated treatment eﬀect parameters under the unconditional and conditional parallel trends\nassumptions and with clustering at the county level.",
    "content_hash": "dc2707a6e8f6fa9df0ef83258b44de9f360ed99c6a854305cf5a1040f7b6f8fd",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "Before presenting these results, we note that our doubly robust estimation procedure is not compu-",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b6eca84f-e5b9-42e0-a20e-90a7b275053b",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "The reason that they are the same\nfor initial exposure is coincidental as the results holding group composition constant do not include the group ﬁrst treated\nin 2007 (the estimated eﬀect of the minimum wage in 2007 for the group of states ﬁrst treated in 2007 is 2.76% lower teen\nemployment which just happens to correspond to the estimated eﬀect for the balanced groups). On the other hand, for the\nsecond period, they correspond by construction because both estimates only include the groups ﬁrst treated in 2004 and\n2006. 30",
    "content_hash": "fc79139fb23033ea845b274ce2467116a1661a8234f06afe6af6f50146acdf32",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "tended to reduce teen employment. The estimated group-time average treatment eﬀects range from 0.9%",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "505f55ae-6318-42ba-b44b-74742e3270d4",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "The row ‘TWFE’ reports the coeﬃcient on a post-treatment dummy\nvariable from a two-way ﬁxed eﬀects regression. The row ‘Simple Weighted Average’ reports the weighted average (by group\nsize) of all available group-time average treatment eﬀects as in Equation (3.10). The row ‘Group-Speciﬁc Eﬀects’ summarizes\naverage treatment eﬀects by the timing of the minimum wage increase; here, g indexes the year that a county is ﬁrst treated. The\nrow ‘Event Study’ reports average treatment eﬀects by the length of exposure to the minimum wage increase; here, e indexes the\nlength of exposure to the treatment. The row ‘Calendar Time Eﬀects’ reports average treatment eﬀects by year; here, t indexes\nthe year. The row ‘Event Study w/ Balanced Groups’ reports average treatment eﬀects by length of exposure using a ﬁxed set\nof groups at all lengths of exposure; here, e indexes the length of exposure and the sample consists of counties that have at least\ntwo years of exposure to minimum wage increases. The column ‘Single Parameters’ represents a further aggregation of each type\nof parameter, as discussed in the text. The estimates in Panel (b) use the the doubly robust estimator discussed in the text. 17Notice that these estimates are exactly the same as in the ﬁrst two periods for the dynamic treatment eﬀect estimates\nthat do not hold the composition of groups constant across diﬀerent lengths of exposure.",
    "content_hash": "43a4cdd71d04df517f28565fab72ee47d280e34d4bb0ed359bb7cb46579ebbc6",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "Goodman-Bacon",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "4e440bd8-f6f0-488b-a017-1f0c2b350d65",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Our summary parameters aggregated by group and by calendar time are also consistent with the idea\nthat increasing the minimum wage had a negative eﬀect on county level teen employment relative to what\nwould have happened in the absence of the minimum wage increase. The second set of results comes from using the conditional parallel trends assumption; that is, we\nassume only that counties with the same characteristics would follow the same trend in teen employment\nin the absence of treatment. The county characteristics that we use are region of the country, county\npopulation, county median income, the fraction of the population that is white, the fraction of the\npopulation with a high school education, and the county’s poverty rate. We use the doubly robust\nestimation procedure discussed above. Thus, estimation requires a ﬁrst step estimation of the generalized\npropensity score and outcome regression discussed above. For each generalized propensity score, we\nestimate a logit model that includes each county characteristic along with quadratic terms for population\nand median income.18 For the outcome regressions, we use the same speciﬁcation for the covariates. Before presenting these results, we note that our doubly robust estimation procedure is not compu-\ntationally demanding. Our estimates of group-time average treatment eﬀects in this section (across all\ngroups and time periods and including our multiplier bootstrap with 1000 iterations) run in 3.0 seconds on\na laptop with a 2.80-GHz Intel i5 processor with 8GB of RAM and without using any parallel processing. For comparison’s sake, we ﬁrst estimate the coeﬃcient on a post-treatment dummy variable in a model\nwith unit ﬁxed eﬀects and region-year ﬁxed eﬀects.",
    "content_hash": "ab4412a64ee64754370cd1220e8c3469373b0f0ecb4841e38609df0740c457a0",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "Overall, our results suggest that increasing the minimum wage decreased teen employment relative to",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b2faabd0-1d1b-4963-add5-a44512d23b26",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "The estimated group-time average treatment eﬀects range from 0.9%\nlower teen employment (not statistically diﬀerent from 0) in 2006 for the group of states ﬁrst treated\nin 2006 to 7.1% lower teen employment in 2007 for states ﬁrst treated in 2004. Now, 3 of 7 group-time\naverage treatment eﬀects are statistically signiﬁcant. The average eﬀect of increasing the minimum wage\n18Using the propensity score speciﬁcation tests proposed by Sant’Anna and Song (2019), we fail to reject the null hy-\npothesis that these models are correctly speciﬁed at the usual signiﬁcance levels. 19Our approach is also diﬀerent from that of Dube et al. (2010) in several other ways that are worth mentioning. We focus\non teen employment; Dube et al. (2010) considers employment in the restaurant industry. Their most similar speciﬁcation to\nthe one mentioned above includes census division-time ﬁxed eﬀects rather than region-time ﬁxed eﬀects though the results\nare similar. Finally, our period of analysis is diﬀerent from theirs; in particular, there are no federal minimum wage changes\nover the periods we analyze. 31",
    "content_hash": "42a4d5b33dc382d195b5538f457877052a6768098e7cfc16e28215dcd724f45a",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "package.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ed83db60-1d8c-4b02-9f8c-6a6b18ef6e52",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "This is very similar to one of the sorts of models\nthat Dube et al. (2010) ﬁnds to eliminate the correlation between the minimum wage and employment. Like Dube et al. (2010), using this speciﬁcation, we ﬁnd that the estimated coeﬃcient is small and not\nstatistically diﬀerent from 0. However, one must have in mind that the approach we proposed in this\narticle is diﬀerent from the two-way ﬁxed eﬀects regression. In particular, we explicitly identify group-\ntime average treatment eﬀects for diﬀerent groups and diﬀerent times, allowing for arbitrary treatment\neﬀect heterogeneity as long as the conditional parallel trends assumption is satisﬁed. Thus, our causal\nparameters have a clear interpretation. As pointed out by Wooldridge (2005a), Chernozhukov et al. (2013), de Chaisemartin and D’Haultfœuille (2020), Borusyak and Jaravel (2017), Goodman-Bacon (2019)\nand S loczy´nski (2018), the same may not be true for two-way ﬁxed eﬀects regressions in the presence of\ntreatment eﬀect heterogeneity.19\nThe results using our approach are available in Panel (b) in Figure 1 and Panel (b) in Table 3. Interestingly, we ﬁnd quite diﬀerent results using our approach than are suggested by the two-way ﬁxed\neﬀects regression approach. In particular, we continue to ﬁnd evidence that increasing the minimum wage\ntended to reduce teen employment.",
    "content_hash": "c33f0c936a4041b4cb9a5c327d156e189ba0340aef5b0ba23e10f812f0cb89fb",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": ") has been obtained for diﬀerent values of",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "99783568-75f2-4178-9fc6-aa761eae222e",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "on teen employment across all groups that increased their minimum wage is a 3.1% reduction in teen\nemployment. This estimate is much diﬀerent from the TWFE estimate. In addition, the pattern of\ndynamic treatment eﬀects where the magnitude of the eﬀect of increasing the minimum wage tends to\nincrease with length of exposure is the same as in the unconditional case. Overall, our results suggest that increasing the minimum wage decreased teen employment relative to\nwhat it would have been without the policy change. However, there are some important limitations of our\napplication. First, some of the estimates of pseudo group-time average treatment eﬀects in pre-treatment\nperiods in Figure 1 are signiﬁcantly diﬀerent from zero which provides some suggestive evidence against\nthe parallel trends assumption. Second, as discussed in the Supplementary Appendix, there is some\nheterogeneity in the size of the minimum wage increase itself across states which could complicate the\ninterpretation of our results. Together, these suggest that our results should be interpreted with some\ncaution. That being said, we think that the key takeaway from the application is that, (implicitly) holding\nthe main identifying assumptions constant, in a prominent application in economics that has many very\ncommon features (treatment eﬀect heterogeneity, dynamic eﬀects, and staggered treatment adoption) the\nchoice of estimation method can potentially lead to qualitatively diﬀerent conclusions. 6\nConclusion\nThis paper has considered Diﬀerence-in-Diﬀerences methods in the case where there are more than two\ntime periods and units can become treated at diﬀerent points in time – a commonly encountered setup\nin empirical work in economics.",
    "content_hash": "bc99b1f49f5dc1c802a6031d28470c4f4d9bcf5f35fb912ec30b82f8b89ad63b",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "Lemma A.1.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "04272363-471a-41ce-a9e2-bd92f4855807",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "We established consistency\nand asymptotic normality of the proposed estimators, and proved the validity of a powerful, but easy\nto implement, multiplier bootstrap procedure to construct simultaneous conﬁdence bands for ATT(g, t). The computational costs of our approach are generally low, and code for implementing our approach is\navailable in the R did package. Finally, we applied our approach to study the eﬀect of minimum wage increases on teen employment. We found some evidence that increasing the minimum wage led to reductions in teen employment. More\n32",
    "content_hash": "d9e7ae79fd5f5b1f31cb3770765c04261ed85b3029cd6e333fc5f5a43a17cbec",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "34c60d42-e890-40d9-9283-26b0beee3e37",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "In this setup, we have proposed group-time average treatment eﬀects,\nATT(g, t), that are the average treatment eﬀect in period t for the group of units ﬁrst treated in period\ng. Unlike the more common approach of including a post-treatment dummy variable in a two-way ﬁxed\neﬀects regression, ATT(g, t) corresponds to a well deﬁned treatment eﬀect parameter. We also showed\nthat once ATT(g, t) has been obtained for diﬀerent values of g and t, they can be aggregated into other\nparameters to more concisely summarize heterogeneity with respect to some particular dimension of\ninterest (such as length of exposure to the treatment) or, alternatively, into a single overall treatment\neﬀect parameter. In addition, our approach is suitable (i) for cases where the parallel trends assumption\nholds only after conditioning on covariates, (ii) using diﬀerent comparison groups such as the never-\ntreated or not-yet-treated, and (iii) when units can anticipate participating in the treatment and may\nadjust their behavior before the treatment is implemented. We view such ﬂexibility as an important\ncomponent of our proposed methodology. We also provided nonparametric identiﬁcation results leading to outcome regression, inverse proba-\nbility weighting, and doubly robust estimands. Given that our nonparametric identiﬁcation results are\nconstructive, we proposed to estimate ATT(g, t) using its sample analogue.",
    "content_hash": "62709e751032a8aa43bd1f57331f4a73db8f6cbdbdd2fc3f3c3398104e4ac8df",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ce19fccf-cff0-401c-b756-461b5034f909",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "interestingly though, in some cases we found notable diﬀerences between the results coming from our\napproach relative to the more common two-way ﬁxed eﬀects approach. These diﬀerences suggest that\nusing an approach that is robust to treatment eﬀect heterogeneity and dynamics should be strongly\nconsidered by applied researchers. Appendix A: Proofs of Main Results\nWe provide the proofs of our results in this appendix. Before proceeding, we ﬁrst state and prove several auxiliary\nlemmas that help us to prove our main theorems. Let\nATTX(g, t) = E[Yt(g) −Yt(0)|X, Gg = 1]. Lemma A.1. Let Assumptions 1, 2, 3, 4, and 6 hold. Then, for all g and t such that g ∈Gδ, t ∈{2, . . . T −δ}\nand t ≥g −δ,\nATTX(g, t) = E[Yt −Yg−δ−1|X, Gg = 1] −E[Yt −Yg−δ−1|X, C = 1] a.s.. Proof of Lemma A.1: In what follows, take all equalities to hold almost surely (a.s.).",
    "content_hash": "13198a7da637a6fbcfcbb046f33b2b48e2c98fb32305f716777fdf07e4115104",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ff19e37a-3fb4-4ac4-a896-857fd5a61a87",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Then, for all g and t such that g ∈Gδ, t ∈{2, . . . T −δ}\nwith g −δ ≤t < ¯g\nATTX(g, t) = E[Yt −Yg−δ−1|X, Gg = 1] −E[Yt −Yg−δ−1|X, Dt+δ = 0, Gg = 0] a.s.. Proof of Lemma A.2: The proof follows similar steps as the proof of Lemma A.1. Taking all equalities to hold\nalmost surely (a.s.), we have that\nATTX(g, t) = E[Yt (g) −Yg−δ−1 (0) |X, Gg = 1] −E[Yt (0) −Yg−δ−1 (0) |X, Gg = 1]\n= E[Yt (g) −Yg−δ−1 (0) |X, Gg = 1] −\nt−g−δ\nX\nℓ=0\nE[∆Yt−ℓ(0) |X, Gg = 1]\n= E[Yt (g) −Yg−δ−1 (0) |X, Gg = 1] −\nt−g−δ\nX\nℓ=0\nE[∆Yt−ℓ(0) |X, Dt+δ = 0, Gg = 0]\n33",
    "content_hash": "b421e0418657e849306b5b7b6f6c7aea40fc2ace877d3e5cded84d2c91a52afe",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e5646dc3-a0d1-4842-a4e9-fe7399003838",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Then, we have that\nATTX(g, t) = E[Yt (g) −Yg−δ−1 (0) |X, Gg = 1] −E[Yt (0) −Yg−δ−1 (0) |X, Gg = 1]\n= E[Yt (g) −Yg−δ−1 (0) |X, Gg = 1] −\nt−g−δ\nX\nℓ=0\nE[∆Yt−ℓ(0) |X, Gg = 1]\n= E[Yt (g) −Yg−δ−1 (0) |X, Gg = 1] −\nt−g−δ\nX\nℓ=0\nE[∆Yt−ℓ(0) |X, C = 1]\n= E[Yt (g) −Yg−δ−1 (0) |X, Gg = 1] −E[Yt (0) −Yg−δ−1 (0) |X, C = 1]\n= E[Yt −Yg−δ−1|X, Gg = 1] −E[Yt −Yg−δ−1|X, C = 1\nwhere the ﬁrst equality follows from adding and subtracting E[Yg−δ−1 (0) |X, Gg = 1], the second equality from\nsimple algebra, the third equality by Assumption 4, the fourth equality by simple algebra, and the last equality\nfrom (2.1) and Assumption 3. □\nLemma A.2. Let Assumptions 1, 2, 3, 5 and 6 hold.",
    "content_hash": "0006ade1c2ee42ad7527dba61bc748da62fc1f848a50d47b62f7c59d355ae6a8",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "57ee6980-6ccc-4a7e-bad0-415a5c9b40aa",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "(A.1)\nTowards this end, by noticing that\npg (X) =\nE [Gg|X]\nE [Gg + C|X], 1 −pg (X) =\nE [C|X]\nE [Gg + C|X],\n(A.2)\nit follows that\nE\n\u0014 pg (X) C\n1 −pg(X)\n\u0015\n= E\n\u0014E [Gg|X] C\nE [C|X]\n\u0015\n= E\n\u0014E [Gg|X] E [C|X]\nE [C|X]\n\u0015\n= E [E [Gg|X]]\n= E [Gg] . (A.3)\nNext, by exploiting (A.2) and applying the law of iterated expectations, we have that\nE\n\u0014\npg (X) C\n(1 −pg(X))(Yt −Yg−δ−1)\n\u0015\n= E\n\u0014E [Gg|X] C\nE [C|X]\n(Yt −Yg−δ−1)\n\u0015\n= E\n\u0014E [Gg|X]\nE [C|X] E [C · (Yt −Yg−δ−1)| X]\n\u0015\n= E [E [Gg|X] · E [(Yt −Yg−δ−1)| X, C = 1]]\n34",
    "content_hash": "90ffb6e19b94d7df7cfa27588b2c56c90aa2ddf5cfd046862bc04a7136f19d0b",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "208e2ed4-8caa-4c2f-a9b7-7de297a157e8",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "= E[Yt (g) −Yg−δ−1 (0) |X, Gg = 1] −E[Yt (0) −Yg−δ−1 (0) |X, Dt+δ = 0, Gg = 0]\n= E[Yt −Yg−δ−1|X, Gg = 1] −E[Yt −Yg−δ−1|X, Dt+δ = 0, Gg = 0]\nwhere the ﬁrst equality follows from adding and subtracting E[Yg−δ−1 (0) |X, Gg = 1], the second equality from\nsimple algebra, the third equality by Assumption 5 with s = t + δ, the fourth equality by simple algebra, and the\nlast equality from (2.1) and Assumption 3. □\nNow, we are ready to proceed with the proofs of our main theorems. Proof of Theorem 1:\nPart 1: Identiﬁcation when Assumption 4 is invoked.",
    "content_hash": "ce5cd71e192b4f43aad6d797cd032de897784a9f3abccb24d2315c4c737f9f93",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "cb6482c0-9dc8-4511-97e2-721ae809d4b3",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "In this case, given the result in Lemma A.1,\nATT(g, t) = E[ATTX(g, t)|Gg = 1]\n= E [(E[Yt −Yg−δ−1|X, Gg = 1] −E[Yt −Yg−δ−1|X, C = 1])| Gg = 1]\n= E[Yt −Yg−δ−1|Gg = 1] −E\n\nE[Yt −Yg−δ−1|X, C = 1]\n|\n{z\n}\n=mnev\ng,t,δ(X)\n|Gg = 1\n\n\n= E\n\u0014\nGg\nE [Gg]\nYt −Yg−δ−1 −mnev\ng,t,δ (X)\n\u0001\u0015\n. Hence, we have that ATT(g, t) = ATT nev\nor\n(g, t; δ) . Next, to show that ATT(g, t) = ATT nev\nipw (g, t; δ), it suﬃces to show that\nE\n\u0014\npg (X) C\n(1 −pg(X))(Yt −Yg−δ−1)\n\u0015\nE\n\u0014\npg (X) C\n(1 −pg(X))\n\u0015\n= E [Gg · E[Yt −Yg−δ−1|X, C = 1]]\nE [Gg]\n.",
    "content_hash": "0b2560797903d9a4b866e0729efbb16f59d58c0080d5ec73fa592621fe132572",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "469ce55e-3e7c-4e94-b846-664a505a7600",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "where the third equality follows from ATT nev\nipw (g, t; δ) = ATT (g, t), (A.2) and (A.3), and the fourth equality from\nthe law of iterated expectations. Part 2: Identiﬁcation when Assumption 5 is invoked. In this case, given the result in Lemma A.2,\nATT(g, t) = E[ATTX(g, t)|Gg = 1]\n= E [(E[Yt −Yg−δ−1|X, Gg = 1] −E[Yt −Yg−δ−1|X, Dt+δ = 0, Gg = 0])| Gg = 1]\n= E[Yt −Yg−δ−1|Gg = 1] −E\n\nE[Yt −Yg−δ−1|X, Dt+δ = 0, Gg = 0]\n|\n{z\n}\n=mny\ng,t,δ(X)\n|Gg = 1\n\n\n= E\n\u0014\nGg\nE [Gg]\n\u0010\nYt −Yg−1−a −mny\ng,t,δ (X)\n\u0011\u0015\n. Hence, we have that ATT(g, t) = ATT ny\nor (g, t; δ) .",
    "content_hash": "ed877d8b826fe37bd9f8d0f48b559326f31b71dca06f9c48c150fce26e449328",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "47ff79ad-f623-4668-a323-ac969b6dfa35",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Next, to show that ATT(g, t) = ATT ny\nipw (g, t; δ), it suﬃces to show that\nE\n\u0014pg,t+δ (X) (1 −Dt+δ) (1 −Gg)\n1 −pg,t+δ (X)\n(Yt −Yg−δ−1)\n\u0015\nE\n\u0014pg,t+δ (X) (1 −Dt+δ) (1 −Gg)\n1 −pg,t+δ (X)\n\u0015\n= E [Gg · E[Yt −Yg−δ−1|X, Dt+δ = 0, Gg = 0]]\nE [Gg]\n. (A.4)\nTowards this end, recall that pg,t+δ(X) = P(Gg = 1|X, Gg + (1 −Dt+δ) (1 −Gg) = 1) and also notice that\npg,t+δ (X) =\nE [Gg|X]\nE [Gg + (1 −Dt+δ) (1 −Gg) |X], 1 −pg (X) =\nE [(1 −Dt+δ) (1 −Gg) |X]\nE [Gg + (1 −Dt+δ) (1 −Gg) |X],\n(A.5)\n35",
    "content_hash": "ef3e9c2698d06260ca8c3098c9b2b347b56550211aca0dabb39a2a4d01af5ebd",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "3e42732b-c930-4417-85aa-8dcc7d322590",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "where the second equality follows from (A.5) and (A.6), and the third equality from ATT nev\nipw (g, t; δ) = ATT (g, t)\nand the law of iterated expectations.□\nProof of Theorem 2: From Theorem 1 it follows that ATT (g, t)’s are point-identiﬁed for all groups g and\ntime periods t such that g ∈Gδ, t ∈{2, . . . T −δ} and t ≥g −δ.",
    "content_hash": "0e77d0d0568562fae4f31fd3832252d2b101429792ee6a2fd1eaf8fa61a47148",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "06cebcc8-f726-4cbb-8e0e-8ee61e155b32",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Thus, combined this with (A.6), we establish (A.4), implying that ATT(g, t) = ATT ny\nipw (g, t; δ). Finally, notice that\nATT ny\ndr (g, t; δ) = E\n\n\n\n\n\n\nGg\nE [Gg] −\npg,t+δ (X) (1 −Dt+δ) (1 −Gg)\n1 −pg,t+δ (X)\nE\n\u0014pg,t+δ (X) (1 −Dt+δ) (1 −Gg)\n1 −pg,t+δ (X)\n\u0015\n\n\n\n\n\u0010\nYt −Yg−δ−1 −mny\ng,t,δ (X)\n\u0011\n\n\n= ATT ny\nipw (g, t; δ) −\n1\nE [Gg]E\n\u0014\u0012\nGg −E [Gg|X] (1 −Dt+δ) (1 −Gg)\nE [(1 −Dt+δ) (1 −Gg) |X]\n\u0013\nmny\ng,t,δ (X)\n\u0015\n= ATT (g, t) −\n1\nE [Gg]E\n\u0002\n(E [Gg|X] −E [Gg|X]) · mnev\ng,t,δ (X)\n\u0003\n= ATT (g, t) .",
    "content_hash": "12f7c6daf9c6aca60d93f19a619f1e85b630d4d34de4dc415b51acff4783156f",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "dce4838d-149d-48a0-aa64-474e7e9363b2",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "it follows that by the law of iterated expectations,\nE\n\u0014pg,t+δ (X) (1 −Dt+δ) (1 −Gg)\n1 −pg,t+δ (X)\n\u0015\n= E\n\u0014E [Gg|X] (1 −Dt+δ) (1 −Gg)\nE [(1 −Dt+δ) (1 −Gg) |X]\n\u0015\n= E [Gg] . (A.6)\nNext, by exploiting (A.5) and applying the law of iterated expectations, we have that\nE\n\u0014pg,t+δ (X) (1 −Dt+δ) (1 −Gg)\n1 −pg,t+δ (X)\n(Yt −Yg−δ−1)\n\u0015\n= E\n\u0014E [Gg|X] (1 −Dt+δ) (1 −Gg)\nE [(1 −Dt+δ) (1 −Gg) |X] (Yt −Yg−δ−1)\n\u0015\n= E\n\u0014\nE [Gg|X]\nE [(1 −Dt+δ) (1 −Gg) |X]E [(1 −Dt+δ) (1 −Gg) · (Yt −Yg−δ−1)| X]\n\u0015\n= E [E [Gg|X] · E [(Yt −Yg−δ−1)| X, Dt+δ = 0, Gg = 0]]\n= E [Gg · E [(Yt −Yg−δ−1)| X, Dt+δ = 0, Gg = 0]] .",
    "content_hash": "fc6137ca3f722912f9211f627d4bf3a9ed0cc22911ac20f449673edf69b9988b",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "Lemma A.2.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d14f4af3-1862-4432-b46d-6dab873e84da",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "For each (g, t) pair, the asymptotic linear\nrepresentation of √n( [\nATT\nnev\ndr (g, t; δ) −ATT (g, t)) follows from Theorem A.1(a) of Sant’Anna and Zhao (2020),\nwhereas\n√n( [\nATT\ndr,nev\nt≥(g−δ) −ATTt≥(g−δ))\nd−→N(0, E[Ψdr,nev\nt≥(g−δ)(W)Ψdr,nev\nt≥(g−δ)(W)′])\nfollows from the Lindeberg–L´evy central limit theorem.□\nProof of Theorem 3: Note that, by the conditional multiplier central limit theorem, see Lemma 2.9.5 in van der\nVaart and Wellner (1996), as n →∞,\n1\n√n\nn\nX\ni=1\nVi · Ψdr,nev\nt≥(g−δ)(Wi)\nd→\n∗N(0, Σ),\n(A.7)\nwhere Σ = E[Ψdr,nev\nt≥(g−δ)(W)Ψdr,nev\nt≥(g−δ)(W)′]. Thus, to conclude the proof that\n√n\n\u0012\n[\nATT\n∗,dr,nev\nt≥(g−δ) −[\nATT\ndr,nev\nt≥(g−δ)\n\u0013\nd→\n∗N(0, Σ),\n36",
    "content_hash": "0b8974789419c9990bebcb002f58554ee6885e3f233d661002b4aec3ac38867a",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "Appendix B: Additional Results for Repeated Cross Sections",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5c1f8619-38eb-4cd6-a161-58005d91cb7e",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "it suﬃces to show that, for all g and t such that g ∈Gδ, t ∈{2, . . . T −δ} and t ≥g −δ,\n1\n√n\nn\nX\ni=1\nVi ·\nh\nbψdr,nev\ng,t,δ\n(Wi; bκnev\ng,t ) −ψdr,nev\ng,t,δ\n(Wi; κ∗,nev\ng,t\n)\ni\n= op∗(1) ,\n(A.8)\nwhere κ∗,nev\ng,t\n=\n\u0010\nπ∗′\ng , β∗,nev ′\ng,t,δ\n\u0011′\nis the vector of pseudo-true ﬁnite-dimensional parameters.",
    "content_hash": "ed75b5cf19478d818ed3b7720c414f9a1a4fa8651a54e88769e376a5fcb8e951",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "Assumption B.1.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "6b1ad540-9c47-4be1-9c55-033b1ce297be",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Towards this end, recall that\nbψdr,nev\ng,t,δ\n(Wi; bκnev\ng,t ) = bψtreat,nev\ng,t,δ\n(Wi; bβnev\ng,t,δ) −bψcomp,nev\ng,t,δ\n(Wi; bπg, bβnev\ng,t,δ) −bψest,nev\ng,t,δ\n(Wi; bπg, bβnev\ng,t,δ)\nwhere\nbψtreat,nev\ng,t,δ\n(W; bβnev\ng,t,δ) = bwtreat\ng\n·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nbβnev\ng,t,δ\n\u0011\u0011\n−bwtreat\ng\n· En\nh\nbwtreat\ng\n·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nbβnev\ng,t,δ\n\u0011\u0011i\n,\nbψcomp,nev\ng,t,δ\n(W; bπg, bβnev\ng,t,δ) = bwcomp,nev\ng\n(bπg) ·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nbβnev\ng,t,δ\n\u0011\u0011\n−bwcomp\ng\n(bπg) · En\nh\nwcomp\ng\n(bπg) ·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nbβnev\ng,t,δ\n\u0011\u0011i\n,\nbψest,nev\ng,t\n(W; bπg, bβnev\ng,t,δ) = lor,nev\ng,t\n\u0010\nbβnev\ng,t,δ\n\u0011′\n· c\nM dr,nev,1\ng,t,δ\n+ lps,nev\ng\n(bπg)′ · c\nM dr,nev,2\ng,t,δ\n,\nwith\nbwtreat\ng\n=\nGg\nEn [Gg],\nbwcomp,nev\ng\n(bπg) =\nbpg (X; bπg) C\n1 −bpg (X; bπg)\nEn\n\u0014 bpg (X; bπg) C\n1 −bpg (X; bπg)\n\u0015,\nand\nc\nM dr,nev,1\ng,t,δ\n= En\nhbwtreat\ng\n−bwcomp,nev\ng\n(bπg)\n\u0001\n· ˙mnev\ng,t,δ\n\u0010\nbβnev\ng,t,δ\n\u0011i\n,\nc\nM dr,nev,2\ng,t,δ\n= En\nh\nbαps,nev\ng\n(bπg) ·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nbβnev\ng,t,δ\n\u0011\u0011\n· ˙pg (bπg)\ni\n−En\nh\nbαps,nev\ng\n(bπg) · bwcomp,nev\ng\n(bπg) ·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nbβnev\ng,t,δ\n\u0011\u0011\n· ˙pg (bπg)\ni\n,\nbαps,nev\ng\n(bπg) =\nC\n(1 −pg (X; bπg))2\n,\nEn\n\u0014 pg (X; bπg) C\n1 −pg (X; bπg)\n\u0015\n.",
    "content_hash": "06baead677933db12d578dcccfc1530e4cedfb076370c3470b6573513b865f49",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "Assumption B.1.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "84416797-bd7e-467f-9d33-5f4838973803",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "We ﬁrst show that\n1\n√n\nn\nX\ni=1\nVi ·\n\u0010\nbψtreat,nev\ng,t,δ\n(Wi; bβnev\ng,t,δ) −ψtreat,nev\ng,t,δ\n(Wi; β∗,nev\ng,t,δ )\n\u0011\n= op∗(1) . (A.9)\nUsing the mean-value theorem, we write\n1\n√n\nn\nX\ni=1\nVi · bψtreat,nev\ng,t,δ\n(Wi; bβnev\ng,t,δ)\n=\n1\n√n\nn\nX\ni=1\nVi · bwtreat\ng,i\n·\n\u0010\nYi,t −Yi,g−δ−1 −mnev\ng,t,δ\n\u0010\nWi; β∗,nev\ng,t,δ\n\u0011\u0011\n−√n\n\u0010\nbβnev\ng,t,δ −β∗,nev\ng,t,δ\n\u0011′ 1\nn\nn\nX\ni=1\nVi · bwtreat\ng,i\n· ˙mnev\ng,t,δ\nWi; ¯βnev\ng,t,δ\n\u0001\n37",
    "content_hash": "2699dfb53f2e409cdd329bb3e8df482dea46e7bd63662402458d558e35ed199e",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "Assumption B.1.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "16888867-6248-4a4c-9a9d-9d4eec344552",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "(A.10)\nAgain, by the mean value theorem, we write\n1\n√n\nn\nX\ni=1\nVi · bψcomp,nev\ng,t,δ\n(Wi; bπg, bβnev\ng,t,δ)\n=\n1\n√n\nn\nX\ni=1\nVi · bwcomp,nev\ng\n(Wi; bπg) ·\n\u0010\nYi,t −Yi,g−δ−1 −mnev\ng,t,δ\n\u0010\nWi; β∗,nev\ng,t,δ\n\u0011\u0011\n−√n\n\u0010\nbβnev\ng,t,δ −β∗,nev\ng,t,δ\n\u0011′ 1\nn\nn\nX\ni=1\nVi · bwcomp,nev\ng\n(Wi; bπg) · ˙mnev\ng,t,δ\nWi; ¯βnev\ng,t,δ\n\u0001\n−En\nh\nbwcomp,nev\ng\n(bπg) ·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nbβnev\ng,t,δ\n\u0011\u0011i 1\n√n\nn\nX\ni=1\nVi · bwcomp,nev\ng\n(Wi; bπg)\n=\n1\n√n\nn\nX\ni=1\nVi ·\npg\nXi; π∗\ng\n\u0001\nCi\n1 −pg\nX; π∗g\n\u0001\nEn\n\u0014 bpg (X; bπg) C\n1 −bpg (X; bπg)\n\u0015 ·\n\u0010\nYi,t −Yi,g−δ−1 −mnev\ng,t,δ\n\u0010\nWi; β∗,nev\ng,t,δ\n\u0011\u0011\n+ √n\nbπg −π∗\ng\n\u0001′ 1\nn\nn\nX\ni=1\nVi ·\nCi\n(1 −pg (Xi; ¯πg))2\nEn\n\u0014 bpg (X; bπg) C\n1 −bpg (X; bπg)\n\u0015 ·\n\u0010\nYi,t −Yi,g−δ−1 −mnev\ng,t,δ\n\u0010\nWi; β∗,nev\ng,t,δ\n\u0011\u0011\n· ˙pg (Xi; ¯πg)\n−√n\n\u0010\nbβnev\ng,t,δ −β∗,nev\ng,t,δ\n\u0011′ 1\nn\nn\nX\ni=1\nVi · bwcomp,nev\ng\n(Wi; bπg) · ˙mnev\ng,t,δ\nWi; ¯βnev\ng,t,δ\n\u0001\n−c\nM comp\ng,t,δ\n1\n√n\nn\nX\ni=1\nVi ·\npg\nXi; π∗\ng\n\u0001\nCi\n1 −pg\nXi; π∗g\n\u0001\nEn\n\u0014 bpg (X; bπg) C\n1 −bpg (X; bπg)\n\u0015\n−c\nM comp\ng,t,δ\n√n\nbπg −π∗\ng\n\u0001′ 1\nn\nn\nX\ni=1\nVi ·\nCi\n(1 −pg (Xi; ¯πg))2\nEn\n\u0014 bpg (X; bπg) C\n1 −bpg (X; bπg)\n\u0015 · ˙pg (Xi; ¯πg)\n38",
    "content_hash": "58b7c4bd12b0de718eedb42f80d00d128ad4eabcd6fd6c45cec2fb6881d370ef",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "Theorem B.1.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "bbedb731-e77c-4224-a5c1-92295f4306a5",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Next, we show\n1\n√n\nn\nX\ni=1\nVi ·\n\u0010\nbψcomp,nev\ng,t,δ\n(Wi; bπg, bβnev\ng,t,δ) −ψcomp,nev\ng,t,δ\n(Wi; π∗\ng, β∗,nev\ng,t,δ )\n\u0011\n= op∗(1) .",
    "content_hash": "88aa93efe753405aba243c9535844f3564e462554536fd28fe50c4eec97c9528",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "Theorem B.1.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "70e57e1d-690d-41c9-9e69-7a2289630142",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "−En\nh\nbwtreat\ng\n·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nbβnev\ng,t,δ\n\u0011\u0011i 1\n√n\nn\nX\ni=1\nVi · bwtreat\ng,i\n= bI1,treat\ng,t,δ\n−bI2,treat\ng,t,δ\n−bI3,treat\ng,t,δ\n,\nwhere ¯βnev\ng,t,δ is an intermediate point that satisﬁes\n¯βnev\ng,t,δ −β∗,nev\ng,t,δ\n≤\nbβnev\ng,t,δ −β∗,nev\ng,t,δ\na.s.. From the strong law of\nlarge numbers and the fact that V is mean zero, independent of W, it follows that\nbI1,treat\ng,t,δ\n=\n1\n√n\nn\nX\ni=1\nVi · wtreat\ng,i\n·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nWi; β∗,nev\ng,t,δ\n\u0011\u0011\n+ op∗(1)\nbI3,treat\ng,t,δ\n=\n1\n√n\nn\nX\ni=1\nVi · wtreat\ng,i\n· E\nh\nwtreat\ng\n·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nβ∗,nev\ng,t,δ\n\u0011\u0011i\n+ op∗(1)\nSimilarly, from Assumptions 7 and 8, and the strong law of large numbers, we conclude that bI2,treat\ng,t,δ\n= op∗(1). Now\n(A.9) follows from combining these results.",
    "content_hash": "cf002545215a4afbe8a6b9c5cd21e70c9584ebd2a50f4231377b834937aba206",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "Assumption B.1.",
      "heading_level": 3
    },
    "domain_id": "econometrics"
  },
  {
    "id": "504edc46-b9ad-4a1a-8bf3-9aab4b5e0ed8",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "= 1\nn\nn\nX\ni=1\n\u0010\nlest\ng,t\nWi; bκnev\ng,t,δ\n\u0001\n−lest\ng,t\n\u0010\nWi; κ∗,nev\ng,t,δ\n\u0011\u00112\n= op (1) ,\nwhich, in turn, implies (A.11). 39",
    "content_hash": "a2ad50f02e7d7f763fda76d34fd5ad7531324d86a6ee6a0268b9e7a7b16f4e64",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "69b5bdce-cf9c-499d-8e9e-cc129069cc86",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "= bI1,comp\ng,t,δ\n+ bI2,comp\ng,t,δ\n−bI3,comp\ng,t,δ\n−bI4,comp\ng,t,δ\n−bI5,comp\ng,t,δ\n,\nwhere ¯βnev\ng,t,δ and ¯πg are intermediate points that satisﬁes\n¯βnev\ng,t,δ −β∗,nev\ng,t,δ\n≤\nbβnev\ng,t,δ −β∗,nev\ng,t,δ\na.s. and\n¯πg −π∗\ng\n≤\nbπg −π∗\ng\na.s., respectively, and\nc\nM comp\ng,t,δ = En\nh\nbwcomp,nev\ng\n(bπg) ·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nbβnev\ng,t,δ\n\u0011\u0011i\n.",
    "content_hash": "e51229ad4ec1b00854fda309969aa2011ee1f8be3be4c0868b3e9b36d43e0d5c",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e45082f7-c6e8-4da4-8527-8e5ba1a394fb",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "From the strong law of large numbers and the fact that V is mean zero, has variance one, and is independent of\nW, it follows that\nbI1,comp\ng,t,δ\n=\n1\n√n\nn\nX\ni=1\nVi · wcomp,nev\ng\nWi; π∗\ng\n\u0001\n·\n\u0010\nYi,t −Yi,g−δ−1 −mnev\ng,t,δ\n\u0010\nWi; β∗,nev\ng,t,δ\n\u0011\u0011\n+ op∗(1) ,\nbI3,treat\ng,t,δ\n=\n1\n√n\nn\nX\ni=1\nVi · wcomp,nev\ng\nWi; π∗\ng\n\u0001\n· E\nh\nwcomp\ng\n·\n\u0010\nYt −Yg−δ−1 −mnev\ng,t,δ\n\u0010\nβ∗,nev\ng,t,δ\n\u0011\u0011i\n+ op∗(1) . Similarly, from Assumptions 7 and 8, and the strong law of large numbers, we conclude that\nbI2,comp\ng,t,δ\n= bI4,comp\ng,t,δ\n= bI5,comp\ng,t,δ\n= op∗(1) . Now (A.10) follows from combining these results. Next, we show that\n1\n√n\nn\nX\ni=1\nVi ·\n\u0010\nbψest,nev\ng,t\n(Wi; bπg, bβnev\ng,t,δ) −ψest,nev\ng,t\n(Wi; π∗\ng, β∗,nev\ng,t,δ )\n\u0011\n= op∗(1) .",
    "content_hash": "d275ae2b364bf4c993bc0e4f84639c81e21d0bd227a2c88b536f5bfb33a2bce9",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "033a4b1d-8f10-4250-b94b-f206f84fbc80",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "(A.11)\nFrom the strong law of large numbers and Assumptions 7 and 8,\n1\n√n\nn\nX\ni=1\nVi · bψest,nev\ng,t\n(Wi; bπg, bβnev\ng,t,δ)\n=\n1\n√n\nn\nX\ni=1\nVi ·\n\u0012\nlor,nev\ng,t\n\u0010\nWi; bβnev\ng,t,δ\n\u0011′\n· M dr,nev,1\ng,t,δ\n+ lps,nev\ng\n(Wi; bπg)′ · M dr,nev,2\ng,t,δ\n\u0013\n+ op∗(1)\n=\n1\n√n\nn\nX\ni=1\nVi · lest\ng,t\nWi; bκnev\ng,t\n\u0001\n+ op∗(1) ,\nwhere, for a generic κnev\ng,t =\n\u0010\nπ′\ng, βnev′\ng,t,δ\n\u0011′\n,\nlest\ng,t\nW; κnev\ng,t\n\u0001\n= lor,nev\ng,t\nW; βnev\ng,t,δ\n\u0001′ · M dr,nev,1\ng,t,δ\n+ lps,nev\ng\n(Wi; πg)′ · M dr,nev,2\ng,t,δ\nThus, from Lemma 4.3 in Newey and McFadden (1994) and Assumption 7, it follows that\nV ar∗\n1\n√n\nn\nX\ni=1\nVi ·\n\u0010\nlest\ng,t\nWi; bκnev\ng,t,δ\n\u0001\n−lest\ng,t\n\u0010\nWi; κ∗,nev\ng,t,δ\n\u0011\u0011!",
    "content_hash": "9d489c47e639586ef29e28c82e9c184994dc3380ef938bc89466265e606d59df",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2c437f63-9fdd-46bf-9c5b-23c038cef228",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Taking (A.9), (A.10), and (A.11) together, we then establish (A.8). Thus, by (A.7), we have\n√n\n\u0012\n[\nATT\n∗,dr,nev\nt≥(g−δ) −[\nATT\ndr,nev\nt≥(g−δ)\n\u0013\nd→\n∗N(0, Σ). Finally, by the continuous mapping theorem, see e.g. Theorem 10.8 in Kosorok (2008), for any continuous\nfunctional Γ(·)\nΓ\n\u0012√n\n\u0012\n[\nATT\n∗,dr,nev\nt≥(g−δ) −[\nATT\ndr,nev\nt≥(g−δ)\n\u0013\u0013\nd→\n∗Γ (N(0, Σ)) ,\nconcluding our proof. □\nAppendix B: Additional Results for Repeated Cross Sections\nIn this section we extend our identiﬁcation results to the case where one has access to repeated cross sections data\ninstead of panel data. Here we assume that for each unit in the pooled sample, we observe (Y, G2, . . . , GT , C, T, X)\nwhere T ∈{1, . . . , T } denotes the time period when that unit is observed. Let Tt = 1 if an observation is observed\nat time t, and zero otherwise. We assume that random samples are available for each time period. Assumption B.1. Conditional of T = t, the data are independent and identically distributed from the distribution\nof (Yt, G2, . . . , GT , C, X) , for all t = 1, . . .",
    "content_hash": "8766dfb13d6dc07fa592703d039ae79ce7c76356b7017619725d2e8f0545255b",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b717bdf4-0131-432b-bc56-e6bd3b5f9018",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Let mrc,nev\nc,t\n(x) ≡EM[Y |X =\nx, C = 1, T = t], mrc,treat\ng,t\n(x) ≡EM[Y |X = x, Gg = 1, T = t] and mrc,ny\ns,t\n(x) ≡EM[Y |X = x, Ds = 0, Gg = 0, T =\nt]. Consider the weights\nwtreat (a, b) = Tb · Ga/ EM [Tb · Ga] ,\nwcomp\nnev\n(a, b) = Tb · pa (X) C\n1 −pa (X)\nEM\n\u0014Tb · pa (X) C\n1 −pa (X)\n\u0015\n,\nwcomp\nny\n(a, b, s) = Tb · pa,s (X) (1 −Db) (1 −Ga)\n1 −pa,s (X)\nEM\n\u0014Tb · pa,s (X) (1 −Db) (1 −Ga)\n1 −pa,s (X)\n\u0015\n. Finally, consider the outcome regression (OR) estimands,\nATT nev\nor,rc (g, t; δ) = EM\n\u0014\nGg\nEM [Gg]\n\u0010\u0010\nmrc,treat\ng,t\n(X) −mrc,treat\ng,g−δ−1 (X)\n\u0011\n−\n\u0010\nmrc,nev\nc,t\n(X) −mrc,nev\nc,g−δ−1 (X)\n\u0011\u0011\u0015\n,\n40",
    "content_hash": "accb4abdeb1ebd806ca04d421d344badfcb3321872af265d6be3b83084364495",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "52915ad3-eead-46a4-a4b1-4100c05482be",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "The OR, IPW and DR estimands respectively generalize Heckman et al. (1997), Abadie (2005) and Sant’Anna\nand Zhao (2020) estimands for the two groups, two periods DiD setup to the staggered DiD setup with multiple\nperiods and multiple groups. Theorem B.1. Let Assumptions 1, 3, 6, and B.1 hold. (i) If Assumption 4 in the main text holds, then, for all g and t such that g ∈Gδ, t ∈{2, . . . T −δ} and t ≥g−δ,\nATT (g, t) = ATT nev\nipw,rc (g, t; δ) = ATT nev\nor,rc (g, t; δ) = ATT nev\ndr,rc (g, t; δ) . (ii) If Assumption 5 in the main text holds, then, for all g and t such that g ∈Gδ, t ∈{2, . . . T −δ} and\nt ≥g −δ,\nATT (g, t) = ATT ny\nipw,rc (g, t; δ) = ATT ny\nor,rc (g, t; δ) = ATT ny\ndr,rc (g, t; δ) . We defer the proof of Theorem B.1 to the Supplementary Appendix. The identiﬁcation results in Theorem B.1\nsuggest a simple two-step estimation procedure for the ATT(g, t) with repeated cross-section data that is analogous\nto the panel data case discussed in Section 4.",
    "content_hash": "ef4ea2b2e357c51dc83b0675305da1bcec8ab4c8aff536f1b6eece4e217c068a",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "41eafa28-5e9e-45ce-86ac-767e8726fdf4",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "The asymptotic properties of such two-step estimators follow from\nanalogous arguments; the details are omitted for brevity. Likewise, we can aggregate these estimators to provide\nsummary measures of the causal eﬀects like those discussed in Section 3. References\nAbadie, A. (2005), “Semiparametric diﬀerence-in-diﬀerence estimators,” Review of Economic Studies, 72, 1–19. 41",
    "content_hash": "e49c3ca0e09417d5de473c64eed25e01580b0cb98fbad779e3176796aec0acf1",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "91389411-439c-4469-b555-350b9ff5d1d6",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "ATT ny\nor,rc (g, t; δ) = EM\n\u0014\nGg\nEM [Gg]\n\u0010\u0010\nmrc,treat\ng,t\n(X) −mrc,treat\ng,g−δ−1 (X)\n\u0011\n−\n\u0010\nmrc,ny\nt+δ,t (X) −mrc,ny\nt+δ,g−δ−1 (X)\n\u0011\u0011\u0015\n,\nthe inverse probability weighted (IPW) estimands\nATT nev\nipw,rc (g, t; δ) = EM\n\u0002wtreat (g, t) −wtreat (g, g −δ −1)\n\u0001\n· Y\n\u0003\n−EM [(wcomp\nnev\n(g, t) −wcomp\nnev\n(g, g −δ −1)) · Y ] ,\nATT ny\nipw,rc (g, t; δ) = EM\n\u0002wtreat (g, t) −wtreat (g, g −δ −1)\n\u0001\n· Y\n\u0003\n−EM\n\u0002wcomp\nny\n(g, t, t + δ) −wcomp\nny\n(g, g −δ −1, t + δ)\n\u0001\n· Y\n\u0003\n,\nand the doubly-robust (DR) estimands\nATT nev\ndr,rc (g, t; δ) = EM\n\u0014\nGg\nEM [Gg]\n\u0010\u0010\nmrc,treat\ng,t\n(X) −mrc,treat\ng,g−δ−1 (X)\n\u0011\n−\n\u0010\nmrc,nev\nc,t\n(X) −mrc,nev\nc,g−δ−1 (X)\n\u0011\u0011\u0015\n+ EM\nh\nwtreat (g, t)\nY −mrc,treat\ng,t\n(X)\n\u0001\n−wtreat (g, g −δ −1)\n\u0010\nY −mrc,treat\ng,g−δ−1 (X)\n\u0011i\n−EM\nh\nwcomp\nnev\n(g, t)\nY −mrc,nev\nc,t\n(X)\n\u0001\n−wcomp\nnev\n(g, g −δ −1)\n\u0010\nY −mrc,nev\nc,g−δ−1 (X)\n\u0011i\n,\nATT ny\ndr,rc (g, t; δ) = EM\n\u0014\nGg\nEM [Gg]\n\u0010\u0010\nmrc,treat\ng,t\n(X) −mrc,treat\ng,g−δ−1 (X)\n\u0011\n−\n\u0010\nmrc,ny\nt+δ,t (X) −mrc,ny\nt+δ,g−δ−1 (X)\n\u0011\u0011\u0015\n+ EM\nh\nwtreat (g, t)\nY −mrc,treat\ng,t\n(X)\n\u0001\n−wtreat (g, g −δ −1)\n\u0010\nY −mrc,treat\ng,g−δ−1 (X)\n\u0011i\n−EM\nh\nwcomp\nny\n(g, t, t + δ)\n\u0010\nY −mrc,ny\nt+δ,t (X)\n\u0011\n−wcomp\nny\n(g, g −δ −1, t + δ)\n\u0010\nY −mrc,ny\nt+δ,g−δ−1 (X)\n\u0011i\n.",
    "content_hash": "d2cd1fea915bd1ca1cdd0993aead8110492591792a7d84e1f817892e1d02ff5e",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e5aff19a-54fe-4f93-b58e-185c80071cef",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Abadie, A., Athey, S., Imbens, G., and Wooldridge, J. (2017), “When should you adjust standard errors for\nclustering?,” Working Paper. Abbring, J. H., and van den Berg, G. J. (2003), “The nonparametric identiﬁcation of treatment eﬀects in duration\nmodels,” Econometrica, 71(5), 1491–1517. Ai, C., and Chen, X. (2003), “Eﬃcient estimation of models with conditional moment restrictions containin un-\nknown functions,” Econometrica, 71(6), 1795–1843. Ai, C., and Chen, X. (2007), “Estimation of possibly misspeciﬁed semiparametric conditional moment restriction\nmodels with diﬀerent conditioning variables,” Journal of Econometrics, 141(1), 5–43. Ai, C., and Chen, X. (2012), “The semiparametric eﬃciency bound for models of sequential moment restrictions\ncontaining unknown functions,” Journal of Econometrics, 170(2), 442–457. Athey, S., and Imbens, G. W. (2006), “Identiﬁcation and inference in nonlinear diﬀerence in diﬀerences models,”\nEconometrica, 74(2), 431–497. Athey, S., and Imbens, G. W. (2018), “Design-based analysis in diﬀerence-in-diﬀerences settings with staggered\nadoption,” Working Paper. Bailey, M.",
    "content_hash": "bfe01b6bce6aa54b4db4e3e2c06462e8cbc7183c46f8dec3c67a25818dbd5a52",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0c0c36cc-fe70-4f9c-b198-96c9dc7a31f3",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "J., and Goodman-Bacon, A. (2015), “The war on poverty’s experiment in public medicine: Community\nhealth centers and the mortality of older Americans,” American Economic Review, 105(3), 1067–1104. Belloni, A., Chernozhukov, V., Fern´andez-Val, I., and Hansen, C. (2017), “Program evaluation and causal inference\nwith high-dimensional data,” Econometrica, 85(1), 233–298. Bertrand, M., Duﬂo, E., and Mullainathan, S. (2004), “How much should we trust diﬀerences-in-diﬀerences esti-\nmates?,” The Quarterly Journal of Economics, 119(1), 249–275. Bojinov, I., Rambachan, A., and Shephard, N. (2020), “Panel Experiments and Dynamic Causal Eﬀects: A Finite\nPopulation Perspective,” Working Paper. Bonhomme, S., and Sauder, U. (2011), “Recovering distributions in diﬀerence-in-diﬀerences models: A comparison\nof selective and comprehensive schooling,” Review of Economics and Statistics, 93(May), 479–494. Borusyak, K., and Jaravel, X. (2017), “Revisiting event study designs,” Working Paper. Botosaru, I., and Gutierrez, F. H. (2018), “Diﬀerence-in-diﬀerences when the treatment status is observed in only\none period,” Journal of Applied Econometrics, 33(1), 73–90.",
    "content_hash": "c4bfde396b8292c8369097be08cef049977097a57a827260946c98fc8cdabc43",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "94cee133-7edc-4f65-a57b-0d51063154b0",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Busso, M., Dinardo, J., and McCrary, J. (2014), “New evidence on the ﬁnite sample properties of propensity score\nreweighting and matching estimators,” The Review of Economics and Statistics, 96(5), 885–895. Callaway, B., Li, T., and Oka, T. (2018), “Quantile treatment eﬀects in diﬀerence in diﬀerences models under\ndependence restrictions and with only two time periods,” Journal of Econometrics, 206(2), 395–413. Card, D., and Krueger, A. B. (1994), “Minimum wages and employment: A case study of the fast-food industry in\nNew Jersey and Pennsylvania,” American Economic Review, 84(4), 772–793. Chen, X., Hong, H., and Tarozzi, A. (2008), “Semiparametric eﬃciency in GMM models with auxiliary data,” The\nAnnals of Statistics, 36(2), 808–843. Chen, X., Linton, O., and Van Keilegom, I. (2003), “Estimation of semiparametric models when the criterion\nfunction is not smooth,” Econometrica, 71(5), 1591–1608. Cheng, G., Yu, Z., and Huang, J. Z. (2013), “The cluster bootstrap consistency in generalized estimating equations,”\nJournal of Multivariate Analysis, 115, 33–47.",
    "content_hash": "84b47b9d1399ea133752803e7509082cafed45df26b7c9f4ac9ad03f07aa0b1e",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "42bb9764-cc93-4537-855d-5ccb9f8fbcd2",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Chernozhukov, V., Fern´andez-Val, I., Hahn, J., and Newey, W. (2013), “Average and quantile eﬀects in nonseparable\npanel models,” Econometrica, 81(2), 535–580. Chernozhukov, V., Fern´andez-Val, I., and Luo, Y. (2018), “The sorted eﬀects method: Discovering heterogeneous\neﬀects beyond their averages,” Econometrica, 86(6), 1911–1938. Conley, T., and Taber, C. (2011), “Inference with “diﬀerence in diﬀerences” with a small number of policy changes,”\nReview of Economics and Statistics, 93(1), 113–125. 42",
    "content_hash": "45c384498b72ed49c89bf127d1763614ad6464ec2ebb55ac2b83473bc4aeb3eb",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "3a91fd8a-7778-47b1-907a-1b68d1624361",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Crump, R. K., Hotz, V. J., Imbens, G. W., and Mitnik, O. A. (2009), “Dealing with limited overlap in estimation\nof average treatment eﬀects,” Biometrika, 96(1), 187–199. de Chaisemartin, C., and D’Haultfœuille, X. (2017), “Fuzzy diﬀerences-in-diﬀerences,” The Review of Economic\nStudies, 85, 999–1028. de Chaisemartin, C., and D’Haultfœuille, X. (2020), “Two-way ﬁxed eﬀects estimators with heterogeneous treat-\nment eﬀects,” American Economic Review, 110(9), 2964–2996. Dube, A., Lester, T. W., and Reich, M. (2010), “Minimum wage eﬀects across state borders: Estimates using\ncontiguous counties,” Review of Economics and Statistics, 92(4), 945–964. Dube, A., Lester, T. W., and Reich, M. (2016), “Minimum wage shocks, employment ﬂows, and labor market\nfrictions,” Journal of Labor Economics, 34(3), 663–704. Farber, H. S. (2017), “Employment, hours, and earnings consequences of job loss: US evidence from the displaced\nworkers survey,” Journal of Labor Economics, 35(S1), S235–S272. Ferman, B., and Pinto, C.",
    "content_hash": "e23e0e8aff3b3ba1c5dd4a116e9590c3b0d2230cc8f239233bea7dbec71a55bc",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9648e7be-7d75-4819-95c9-187e8024ff5d",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "(2019), “Inference in diﬀerences-in-diﬀerences with few treated groups and heteroskedas-\nticity,” Review of Economics and Statistics, 101(3), 452–467. Freyberger, J., and Rai, Y. (2018), “Uniform conﬁdence bands: Characterization and optimality,” Journal of\nEconometrics, 204(1), 119–130. Gibbons, C. E., Su´arez Serrato, J. C., and Urbancic, M. B. (2018), “Broken or Fixed Eﬀects?,” Journal of Econo-\nmetric Methods, 8(1). Goodman-Bacon, A. (2019), “Diﬀerence-in-diﬀerences with variation in treatment timing,” Working Paper. Graham, B., Pinto, C., and Egel, D. (2012), “Inverse probability tilting for moment condition models with missing\ndata,” The Review of Economic Studies, 79(3), 1053–1079. H´ajek, J. (1971), “Discussion of ‘An essay on the logical foundations of survey sampling, Part I’, by D. Basu,”\nin Foundations of Statistical Inference, eds. V. P. Godambe, and D. A. Sprott, Toronto: Holt, Rinehart, and\nWinston. Han, S. (2020), “Identiﬁcation in nonparametric models for dynamic treatment eﬀects,” Journal of Econometrics,\nForthcoming. Heckman, J. J., Humphries, J. E., and Veramendi, G.",
    "content_hash": "9b70a36bbcbcb2d5069003c59de7d954133af15ec65ece2ee03aee4278f7ca6c",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "6cd03d49-280a-4d1a-ac6d-e2965eb55e51",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "(2005), “Estimation of panel data models with binary indicators when treatment\neﬀects are not constant over time,” Economics Letters, 88(3), 389–396. MacKinnon, J. G., and Webb, M. D. (2018), “The wild bootstrap for few (treated) clusters,” The Econometrics\nJournal, 21(2), 114–135. MacKinnon, J. G., and Webb, M. D. (2020), “Randomization inference for diﬀerence-in-diﬀerences with few treated\nclusters,” Journal of Econometrics, 218(2), 435–450. 43",
    "content_hash": "e73e5346a35f9b96ba89aa3c9dd06ba7ec65e451d54a2e7a63a9e98f187074f4",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9f2e7af6-98f6-44b9-b2e0-2b98de25d095",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "(2020), “Doubly robust diﬀerence-in-diﬀerences estimators,” Journal of Economet-\nrics, 219(1), 101–122. Sherman, M., and Le Cessie, S. (2007), “A comparison between bootstrap methods and generalized estimating\nequations for correlated outcomes in generalized linear models,” Communications in Statistics - Simulation and\nComputation, 26(3), 901–925. 44",
    "content_hash": "1c4b9ad5c334c1e3c3c6385df50c504a0de7687ed0a802d9bb4f082f7e524882",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "88248f8d-9605-416e-b3ad-2a8678ddbdd3",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "(2008), “Empirical-likelihood-based diﬀerence-in-diﬀerences estimators,” Journal of the\nRoyal Statistical Society. Series B (Methodological), 75(8), 329–349. Robins, J. M. (1986), “A new approach to causal inference in mortality studies with a sustained exposure period -\nApplication to control of the healthy worker survivor eﬀect,” Mathematical Modelling, 7, 1393–1512. Robins, J. M. (1987), “Addendum to ‘A new approach to causal inference in mortality studies with a sustained\nexposure period - Application to control of the healthy worker survivor eﬀect’,” Computers & Mathematics with\nApplications, 14(9-12), 923–945. Roth, J. (2020), “Pre-test with caution: Event-study estimates after testing for parallel trends,” Working Paper,\npp. 1–84. Rubin, D. B. (2007), “The design versus the analysis of observational studies for causal eﬀects: Parallels with the\ndesign of randomized trials,” Statistics in Medicine, 26(1), 20–36. Rubin, D. B. (2008), “For objective causal inference, design trumps analysis,” Annals of Applied Statistics,\n2(3), 808–840. Sant’Anna, P. H. C., and Song, X. (2019), “Speciﬁcation tests for the propensity score,” Journal of Econometrics,\n210, 379–404. Sant’Anna, P. H., and Zhao, J.",
    "content_hash": "13723305b7809c7900651eaf4a31bcdb0b383eedfe85846a6d2b1789987a123b",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "37628a48-3d9c-4f18-9ee4-ba051631fbc6",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "L., Coie, J. D., Greenberg, M. T., Lochman, J. E.,\nMcMahon, R. J., and Pinderhughes, E. (2001), “Marginal mean models for dynamic regimes,” Journal of the\nAmerican Statistical Association, 96(456), 1410–1423. Neumark, D., and Wascher, W. (2000), “Minimum wages and employment: A case study of the fast-food industry\nin New Jersey and Pennsylvania: Comment,” American Economic Review, 90(5), 1362–1396. Neumark, D., and Wascher, W. L. (2008), Minimum Wages, Cambridge, MA: The MIT Press. Newey, W. K. (1994), “The asymptotic variance of semiparametric estimators,” Econometrica, 62(6), 1349–1382. Newey, W. K., and McFadden, D. (1994), “Large sample estimation and hypothesis testing,” in Handbook of\nEconometrics, Vol. 4, Amsterdam: North-Holland: Elsevier, chapter 36, pp. 2111–2245. Oreopoulos, P., von Wachter, T., and Heisz, A. (2012), “The short- and long-term career eﬀects of graduating in a\nrecession,” American Economic Journal: Applied Economics, 4(1), 1–29. Qin, J., and Zhang, B.",
    "content_hash": "05ba1f683ea400b82824372167a62563b1a614758ba60c2cccf0902cce34b329",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "fe51f5c7-accf-435d-9e1a-bd0a5958ace4",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Malani, A., and Reif, J. (2015), “Interpreting pre-trends as anticipation: Impact on estimated treatment eﬀects\nfrom tort reform,” Journal of Public Economics, 124, 1–17. Mammen, E. (1993), “Bootstrap and wild bootstrap for high dimensional linear models,” The Annals of Statistics,\n21(1), 255–285. Marcus, M., and Sant’Anna, P. H. C. (2020), “The role of parallel trends in event study settings : An application to\nenvironmental economics,” Journal of the Association of Environmental and Resource Economists, Forthcoming. McCrary, J. (2007), “The eﬀect of court-ordered hiring quotas on the composition and quality of police,” American\nEconomic Review, 97(1), 318–353. Meer, J., and West, J. (2016), “Eﬀects of the minimum wage on employment dynamics,” Journal of Human\nResources, 51(2), 500–522. Montiel Olea, J. L., and Plagborg-Møller, M. (2018), “Simultaneous conﬁdence bands: Theory, implementation,\nand an application to SVARs,” Journal of Applied Econometrics, pp. 1–64. Murphy, S. A. (2003), “Optimal dynamic treatment regimes,” Journal of the Royal Statistical Society Series B,\n65(2), 331–366. Murphy, S. A., Van der Laan, M. J., Robins, J. M., Bierman, K.",
    "content_hash": "44c5e0a6a898ec17ff623b598876645eec53bdd0580ee41fbc18cba23fc0addc",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "02c90f27-238e-45bb-af7f-c77472e49918",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Yang, S., and Ding, P. (2018), “Asymptotic inference of causal eﬀects with observational studies trimmed by the\nestimated propensity scores,” Biometrika, 105(2), 487–493. 45",
    "content_hash": "b8e8aab5b939be80bfffd5ea878b417c1215492eee2e505ea792dc5c9b403971",
    "location": null,
    "page_start": 45,
    "page_end": 45,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "99216441-cb85-4cdc-8420-460eb6373c51",
    "source_id": "928d7131-b28b-4cfa-ab51-785b4d4b2add",
    "content": "Sianesi, B. (2004), “An evaluation of the Swedish system of active labor market programs in the 1990s,” The Review\nof Economics and Statistics, 86(1), 133–155. S loczy´nski, T. (2018), “A general weighted average representation of the ordinary and two-stage least squares\nestimands,” Working Paper. Sun, L., and Abraham, S. (2020), “Estimating dynamic treatment eﬀects in event studies with heterogeneous\ntreatment eﬀects,” Working Paper. van der Vaart, A. W. (1998), Asymptotic Statistics, Cambridge: Cambridge University Press. van der Vaart, A. W., and Wellner, J. A. (1996), Weak Convergence and Empirical Processes, New York: Springer. Wooldridge, J. M. (2003), “Cluster-sample methods in applied econometrics,” American Economic Review P&P,\n93(2), 133–138. Wooldridge, J. M. (2005a), “Fixed-eﬀects and related estimators for correlated random-coeﬃcient and treatment-\neﬀect panel data models,” Review of Economics and Statistics, 87(2), 385–390. Wooldridge, J. M. (2005b), “Violating ignorability of treatment by controlling for too many factors,” Econometric\nTheory, 21(5), 1026–1028. Wooldridge, J. M. (2007), “Inverse probability weighted estimation for general missing data problems,” Journal of\nEconometrics, 141(2), 1281–1301.",
    "content_hash": "35b218fc282f010c8059fe7d9b98264f6729a42667ab521939c560d7de58be54",
    "location": null,
    "page_start": 45,
    "page_end": 45,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c79f0cf0-1324-4300-8b51-b1d95f77eee6",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Proof of Theorem 4.2. In what follows, we use the notation Q−1 = (ω0\ni j) and Q−1\n·,j := ω0\nj . Deﬁne the following quantities\nsj(λ) := ∥1{|ω0\nj | ≥λ}∥1,\nrj(λ) := ∥(ω0\nj )1{|ω0\nj | ≤λ}∥1. Remark A.1. Assumption 4.6 implies the following bounds\n∥Q−1∥1,∞= max\n1≤j≤d ∥ω0\nj ∥1 ≤AQ\np\n∑\nj=1\nj−aQ ≤AQ\nZ ∞\n1\nj−aQd j ≤AQ/(aQ −1). (D.40)\nFurthermore, if AQ j−aQ ≤λ, then j ≥j∗\nQ := (AQ/λ)1/aQ. This implies\ns j(λ) := ∥1{|ω0\nj | ≥λ}∥1 ≤∥1{AQ j−aQ ≥λ}∥1 ≤\nj∗\nQ\n∑\nj=1\n1 = j∗\nQ = (AQ/λ)1/aQ. r j(λ) ≤\nZ ∞\nj∗\nQ\nAQ j−aQd j = AQ\n(j∗\nQ)1−aQ\naQ −1\n= AQ\n(AQ/λ)(1−aQ)/aQ\naQ −1\n=\nA1/aQ\nQ\n(aQ −1)λ 1−1/aQ.",
    "content_hash": "16e7bb8631b5ccd312ebd208e8da4a96d7faf12a2975f438a526ede9ee63e4af",
    "location": null,
    "page_start": 1,
    "page_end": 70,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a161f74d-0d60-49fc-b73b-ee2df2836493",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "As a\nresult, the rate bound (F.28) reduces to\nvNT =\nq\nd2 log(2d)log(NT)/NT +dRlog(2d)log(NT)/NT. Further improvement of this rate may be possible under additional structure on Vit, see, e.g., The-\norem 1 and Corollary 3 in Banna et al. (2016). Let DNT × LNT be a sequence of realization sets such that the following conditions hold. Let\ndNT,lNT,dNT,4,lNT,4 be the numeric sequences obeying the following bounds\nsup\nd∈DNT\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n(E∥di(Xit)−di0(Xit)∥2)1/2 ≤dNT,\nsup\nd∈DNT\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n(E∥di(Xit)−di0(Xit)∥4)1/4 ≤dNT,4\nsup\nl∈LNT\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n(E(li(Xit)−li0(Xit))4)1/4 ≤lNT,4.",
    "content_hash": "5031e47c9e0cb25e2191484d3cce4169cd2953bf70c4e72697c3120928dd31e1",
    "location": null,
    "page_start": 1,
    "page_end": 96,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a4f6cce7-8d4c-4eaa-bef7-116975902a44",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "70\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\n41(6):2786–2819. Chernozhukov, V., Chetverikov, D., and Kato, K. (2014). Gaussian approximation of suprema of\nempirical processes. Annals of Statistics, 42(4):1564–1597. Chernozhukov, V., Chetverikov, D., and Kato, K. (2015a). Comparison and anti-concentration\nbounds for maxima of gaussian random vectors. Probability Theory and Related Fields,\n(162):47–70. Chernozhukov, V., Chetverikov, D., and Kato, K. (2017). Central limit theorems and bootstrap in\nhigh dimensions. Annals of Probability, 45(4):2309–2352. Chernozhukov, V., Chetverikov, D., and Kato, K. (2019a). Inference on causal and structural\nparameters using many moment inequalities. Review of Economic Studies, 86:1867–1900. Chernozhukov, V., Chetverikov, D., Kato, K., and Koike, Y. (2019b). Improved central limit\ntheorem and bootstrap approximations in high dimensions. Chernozhukov, V., Demirer, M., Duﬂo, E., and Fern´andez-Val, I. (2017). Generic Machine Learn-\ning Inference on Heterogenous Treatment Effects in Randomized Experiments.",
    "content_hash": "a2b25bbbccca1b035396a1324e5f94218e21cc144b42bdd2f6331f04b1236dac",
    "location": null,
    "page_start": 1,
    "page_end": 105,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5d20694f-dd2a-4d79-ab5f-b2f83e789b29",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "arXiv e-prints,\npage arXiv:1712.04802. Chernozhukov, V., Hansen, C., and Spindler, M. (2015b). Valid post-selection and post-\nregularization inference: An elementary, general approach. Annual Review of Economics,\n7(1):649–688. Chernozhukov, V., Hansen, C., and Spindler, M. (2016). High-dimensional metrics in r. Chevalier, J., Kashyap, A., and Rossi, P. (2003). Why don’t prices rise during periods of peak\ndemand? evidence from scanner data. American Economic Review, 93(1):15–37. Chiang, H. D. (2018). Many Average Partial Effects: with An Application to Text Regression. arXiv e-prints, page arXiv:1812.09397. Chiang, H. D., Kato, K., Ma, Y., and Sasaki, Y. (2019). Multiway Cluster Robust Double/Debiased\nMachine Learning. arXiv e-prints, page arXiv:1909.03489. Colangelo, K. and Lee, Y.-Y. (2020). Double Debiased Machine Learning Nonparametric Infer-\nence with Continuous Treatments. arXiv e-prints, page arXiv:2004.03036. Davis, J. M. and Heller, S. B. (2020).",
    "content_hash": "9b3f5a8521423b2ae333d61935d3e289af707e8f210fdde57aff0bebbcbc9303",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0d51c9f5-5d57-4ac4-97da-9463090b211b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Journal of Machine Learning Research, 2(4):2869–2909. Klosin, S. (2021). Automatic double machine learning for continuous treatment effects. Kock, A. B. (2016a). Consistent and conservative model selection with the adaptive lasso in\nstationary and nonstationary autoregressions. Econometric Theory, 32(1):243–259. Kock, A. B. (2016b). Oracle inequalities, variable selection and uniform inference in high-\ndimensional correlated random effects panel data models. Journal of Econometrics, 195:71–85. Kock, A. B. and Tang, H. (2019). Uniform inference in high-dimensional dynamic panel data\nmodels. Econometric Theory, 35:295–359. Ledoux, M. (2001). Concentration of Measure Phenomenon. American Mathematical Society. Leeb, H. and Potcher, B. (2005). Model selection and inference: Facts and ﬁction. Econometric\nTheory, 21:21–59. Lounici, K., Pontil, M., van de Geer, S., and Tsybakov, A. B. (2011). Oracle inequalities and\noptimal inference under group sparsity. The Annals of Statistics, 39(4):2164 – 2204. Lu, X. and Su, L. (2016). Shrinkage estimation of dynamic panel data models with interactive\nﬁxed effects. Journal of Econometrics, 190:148–175. Lu, X. and Su, L. (2020).",
    "content_hash": "1aea304aa609077368e81378d9b5177d4cdc4830a9bf9e3b868de79562a60cd8",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "14adb050-fe11-43f9-98ec-b5ee5b0726af",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Schmidt-Hieber, J. (2017). Nonparametric regression using deep neural networks with ReLU\nactivation function. arXiv e-prints, page arXiv:1708.06633. Schwarz, G. (1980). Finitely determined processes - an indiscrete approach. Journal of Mathe-\nmatical Analysis and Applications, 76:146–158. Semenova, V. and Chernozhukov, V. (2021). Debiased machine learning of conditional average\ntreatment effects and other causal functions. Strassen, V. (1965). The existence of probability measures with given marginals. The Annals of\nMathematical Statistics, 36(2):423–439. Su, L., Shi, Z., and Phillips, P. (2016). Identifying latent structures in panel data. Econometrica,\n84(6):1824–1851. Syrganis, V. and Zampetakis, M. (2020). Estimation and Inference with Trees and Forests in High\nDimensions. arXiv e-prints, page arXiv:2007.03210. Ura, T. (2018). Heterogeneous treatment effects with mismeasured endogenous treatment. Quan-\ntitative Economics, 9(3):1335–1370. van der Geer, S., B¨uhlmann, P., Ritov, Y., and Dezeure, R. (2014). On asymptotically optimal\nconﬁdence regions and tests for high-dimensional models. Annals of Statistics, 42(3):1166–\n1202.",
    "content_hash": "5353d929ed35f10f584e8f2ebe206936d7edf0df2896bade146e2bf243724b15",
    "location": null,
    "page_start": 1,
    "page_end": 107,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a4907349-ea76-4c40-9cb2-d9fd85734065",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "+P\nE\nL\n∏\nl=1\nσ−1\nl\nZl,i,t,j\n≥ε\n! ≤\nL\n∑\nl=1\nP\n\u0010\nσ−1\nl\nZl,i,t,j\n≥ε1/L\u0011\n+1(ε ≤A)\n≤LKe−ε2/L +1(ε2/L ≤A2/L)\n≤LKe−ε2/L +eA2/Le−ε2/L = K′e−ε2/L. (K′ := (LK +eA2/L)). Let\nXi,j := 1\nT\nT\n∑\nt=1\nL\n∏\nl=1\nσ−1\nl\nZl,i,t,j −E\n\"\nL\n∏\nl=1\nσ−1\nl\nZl,i,t,j\n#! . For every ε ≥0,\nP\nXi,j\n≥2ε\n\u0001\n≤P\n\u0012\nmax\n1≤t≤T\nXi,t,j\n≥2ε\n\u0013\n≤TK′e−ε2/L. Consider some positive constant D < 1, then as van der Vaart and Wellner (1996), p.96, using\nFubini and change of order of integration:\nE\n\u0014\neD|Xi,j/2|\n2/L\u0015\n=\nZ\nx∈R\nZ |x/2|2/L\n0\nDeDsdsP(dx)+1 =\nZ ∞\n0 DeDsP\n\u0010\f\fXi,j\n> 2sL/2\u0011\nds+1. This is further bounded by\nZ ∞\n0 TK′De(D−1)sds+1 = TK′D\n1−D +1 ≤BT;\n(B := K′D\n1−D +1).",
    "content_hash": "a2dfec666b8cc04ec8a6645d35f7701b0b8394d9f893c3e339905a9218c81844",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c39da085-fb4a-4f4a-8a01-3b2abfe1afa7",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "van der Vaart, A. and Wellner, J. A. (1996). Weak Convergence and Empirical Processes. Springer. Villani, C. (2007). Topics in optimal transportation, volume 58. American Mathematical Soc. Wager, S. and Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using\nrandom forests. Journal Journal of the American Statistical Association, 113:1228–1242. Wager, S. and Walther, G. (2015). Adaptive concentration of regression trees, with application to\nrandom forests. arXiv e-prints, page arXiv:1503.06388.",
    "content_hash": "0528eb1a1cdf37e7123691af8288c64799fad964fa1f3f0d9c2aeba1636f00f3",
    "location": null,
    "page_start": 1,
    "page_end": 107,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "eb574111-ed55-4a69-98ab-9ef4c287b38e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Suppose Assumption 4.1 (1) holds, and let ϕn j(·) : W →R be a deterministic function. Suppose that ϕnj(Wit) are uniformly ¯σn-sub-Gaussian as in (4.3) for n = 1,2,..., ¯N ( ¯N ≥2 is ﬁxed\nand ﬁnite) and j = 1,2,...,d and any i,t. Then,\nmax\nj,i,t\nE\n\" ¯N\n∏\nn=1\nϕn j(Wit)\n#\f\f\f\f\f ≤CA\n¯N\n∏\nn=1\n¯σn,\n(B.17)\nfor some positive constant CA that depends on ¯N and with probability approaching 1,\n∥S∥∞: = max\n1≤j≤d\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n\" ¯N\n∏\nn=1\nϕn j(Wit)−E\n\" ¯N\n∏\nn=1\nϕn j(Wit)\n##\n(B.18)\n≤¯CV\nq\nlog ¯N+1(d log(NT))log(NT)/NT\n¯N\n∏\nn=1\n¯σn. for some positive constant ¯CV that depends only on ¯N. Proof of Lemma B.6. Deﬁne\nφ(Wit) := {φ j(Wit)}d\nj=1,\nφ j(Wit) :=\n¯N\n∏\nn=1\nϕn j(Wit). Let the block size q, the odd blocks, even blocks, and remainder blocks, and events I1 and I2 be\nas deﬁned in the proof of Lemma B.4.",
    "content_hash": "2e3efe38e1ff84edd9c2ad0469f84b11c60ed8a1c7b3e302b272d58c9f6e1322",
    "location": null,
    "page_start": 1,
    "page_end": 50,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "647faf2d-d077-4e29-923f-2b7d41db0869",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Since K is ﬁnite and logN/Tblock = o(1) gives KNCκ exp(−κTblock) = o(1). ■\nCorollary A.2 (Convenient Rate Implications). Consider the setup above. Suppose there exists a\nsequence VNT such that for some ψ(B∗\nk,Bqc∗\nk ) is OP(VNT) for some measurable function ψ. Then,\nψ(Bk,Bqc\nk ) is OP(VNT). Proof of Corollary A.2. Consider any sequence of constants such that ℓNT →∞. Then\nP(ψ(Bk,Bqc\nk ) > ℓNTVNT) ≤i P(ψ(Bk,Bqc\nk ) > ℓNTVNT ∩Eberbee)+P(E c\nberbee)\n≤ii P(ψ(B∗\nk,Bqc∗\nk ) > ℓNTVNT)+P(E c\nberbee)\n=iii o(1),\nwhere (i) follows from union bound, (ii) holds since ψ(B∗\nk,Bqc∗\nk ) = ψ(Bk,Bqc\nk ) on Eberbee, and (iii)\nis assumed in the statement of Lemma. ■\nConsider the following setup. We assume all spaces to be separable and complete. Consider the\nparameter space T with elements η, typically a space of functions. Consider also a measurable\nfunction (the estimation map) bqc 7→¯η(bqc) that maps W T−q+1 to T . Here W is the metric space\ncontaining realizations of Wit for all i and t.",
    "content_hash": "106756630cae415e3109aafedff4e833964a528c24572196741801d137b777b0",
    "location": null,
    "page_start": 1,
    "page_end": 43,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bea02dab-6c46-43f3-badf-343f0edbc69e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Given the parameter κ in mixing coefﬁcient (4.1), we set block size q to be:\nq = ⌊(2/κ)log(NT)⌋. (B.16)\nInvoking the bound (4.1) in Assumption 4.1 gives\nP(I c\n1 )+P(I c\n2 ) ≤2N(L−1)γ(q) ≤(2NT/q)γ(q)\n≤2(NT/q)(NT)−2 = o((NT)−1) = o(1),\nNT →∞,\nwhich implies (B.7). ■",
    "content_hash": "f2c4d514b452a4e281ec747c1b2f8f841f052ce5894ed6dba6577ce95a1d0af3",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0d8a7a76-af53-466f-af99-11e582175f0a",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(C.8)\nBecause {Xm}M\nm=1 is an m.d.s,\nCov(Xm1,Xm2) = 0 ∈Rd×d for m1 ̸= m2. Therefore, for any r and any I = {i+1,i+2,...,i+r},\na1 ≤Var(r−1/2 ∑\nm∈I\nXm j) = r−1 ∑\nm∈I\nVar(Xm j) ≤A1,\n1 ≤j ≤d,\nwhich implies (C.8). All other conditions of Theorem E.1 in Chernozhukov et al. (2019a) are\nsatisﬁed. Invoking Theorem E.1 in Chernozhukov et al. (2019a) with\nT := max\n1≤j≤2d M−1/2\nM\n∑\nm=1\nXm j = ∥SX∥∞\nand\nGP ∼N(0,ΣGP)\nbeing a centered normal (2d)-vector with\nΣGP =\nΣP\n−ΣP\n−ΣP\nΣP\n! . gives (C.7). ■",
    "content_hash": "048f98b271650cdb0402b31d4f2be4b0de264ec77cb1bdff12ad17269395ffe9",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d3d84f43-14bf-4242-9665-34295211c3a5",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(2.11)\nThen, (2.5), (2.10), and (2.11) imply\ndi0(Xit) = K(Xit)′(p0(Xit)+ξi),\nVit = K(Xit)V P\nit . As we will show later, the interactive structure (2.10) simpliﬁes estimation of treatment residuals.",
    "content_hash": "4581a541039fa61bfd3bcc988a6013f8497a069ec72a86a984be24b0dac826ad",
    "location": null,
    "page_start": 1,
    "page_end": 7,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0ebf336f-f4ce-4b9e-bccc-15b253ae5e0c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE ON HETEROGENEOUS TREATMENT EFFECTS IN\nHIGH-DIMENSIONAL DYNAMIC PANELS UNDER WEAK DEPENDENCE\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nABSTRACT. This paper provides estimation and inference methods for conditional average treat-\nment effects (CATE) characterized by a high-dimensional parameter in both homogeneous cross-\nsectional and unit-heterogeneous dynamic panel data settings. In our leading example, we model\nCATE by interacting the base treatment variable with explanatory variables. The ﬁrst step of our\nprocedure is orthogonalization, where we partial out the controls and unit effects from the outcome\nand the base treatment and take the cross-ﬁtted residuals. This step uses a novel generic cross-ﬁtting\nmethod we design for weakly dependent time series and panel data. This method ”leaves out the\nneighbors” when ﬁtting nuisance components, and we theoretically power it by using Strassen’s\ncoupling. As a result, we can rely on any modern machine learning method in the ﬁrst step, pro-\nvided it learns the residuals well enough. Second, we construct an orthogonal (or residual) learner\nof CATE – the Lasso CATE – that regresses the outcome residual on the vector of interactions of the\nresidualized treatment with explanatory variables. If the complexity of CATE function is simpler\nthan that of the ﬁrst-stage regression, the orthogonal learner converges faster than the single-stage\nregression-based learner. Third, we perform simultaneous inference on parameters of the CATE\nfunction using debiasing. We also can use ordinary least squares in the last two steps when CATE is\nlow-dimensional.",
    "content_hash": "5af4f6d0fdab0a5162d46ae3084c8d6d116e224a3de485041b3bbc3c770cfb9d",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e97386bb-55ee-4851-9e00-c04f702869fd",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(2015a), page 56) implies\n∥Z∥∞≲P log1/2(2d)+log1/2(NT). Since ∥Z∥∞and ∥TΣ,β∥∞converge in distribution to the same limit,\n∥TΣ,β∥∞≲P log1/2(2d)+log1/2(NT). (D.75)\nStep 5.1. We bound supt≥0 |O4(t)|. Take ρj = Σ1/2\nj j /bΣ1/2\nj j\nand let ρ := (ρ1,ρ2,...,ρd)′ be a\nd-vector. Note that all Eucledian j-vectors ej vectors obey ∥ej∥2 = ∥ej∥1 = 1 and therefore belong\nto the set in Theorem 4.2 with Kα = 1. Let αNT = (αΣα)1/2/(αbΣα)1/2 be as in (D.52). Invoking\n(D.51) and the bound (4.15) in Lemma 4.3 gives\nmax\nj\n|ρj −1| ≤\nsup\nα:∥α∥2=∥α∥1=1\n|αNT −1| ≲P γNT. In particular, it implies that the even\nmin\n1≤j≤d ρj > 1/2\noccurs wp 1−o(1). For any ρj > 1/2,\n|ρ−1\nj\n−1| = |1−ρj|/|ρj| ≤2|ρj −1|.",
    "content_hash": "7c13c64b19127cad541b18fff3e2375383718657dbb871adbcbd8ef3b0c02107",
    "location": null,
    "page_start": 1,
    "page_end": 84,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a4274de0-58e5-461a-93c8-49768c95bf32",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "In heterogeneous panel data settings, we model the unobserved unit heterogene-\nity as a weakly sparse deviation from Mundlak (1978)’s model of correlated unit effects as a linear\nfunction of time-invariant covariates and make use of L1-penalization to estimate these models. We\ndemonstrate our methods by estimating price elasticities of groceries based on scanner data. We\nnote that our results are new even for the cross-sectional (i.i.d) case. Keywords: Orthogonal Learning, Residual Learning, CATE, High-dimensional, Cross-section\nData, Dynamic Panel Data, Weakly Dependent Time Series, Neighbours-Left-Out, Cross-ﬁtting. Date: December 13, 2022; Initial ArXiv Submission: December 2017. The authors are grateful to Michael Jansson,\nPat Kline, Sylvia Klosin, Chris Taber, and two anonymous referees whose comments helped improve the paper. University of California, Berkeley, semenovavira@gmail.com. Meta, mattgoldman5850@gmail.com. MIT, vchern@mit.edu. Amazon, mataddy@gmail.com . 1\narXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
    "content_hash": "8b90157ea3576afe4777db97a4114d920df1fc6a8d77ac506b3c27f8b037e9b4",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1def7f0f-3c4d-4faa-9264-3d5ebc0f5f9c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "with\nm = m(i,t) = T(i−1)+t,\nM = NT\nand\nXm := (diagΣ)−1/2VmUm,\nm = 1,2,...,M,\nDM = c−1\nΣ πVU\nM . To verify the condition (C.5), we invoke Assumption 4.8 which gives\nVar(Xmj) = Σ−1/2\nj j\nEV 2\nmjU2\nmΣ−1/2\nj j\n≥¯σ2 min\nit ∥EVitV ′\nit∥∞C−1\nΣ\n=: a1 > 0\nand Remark A.2\nVar(Xmj) = Σ−1/2\nj j\nEV 2\nmjU2\nmΣ−1/2\nj j\n≤¯σ2 max\nit ∥EVitV ′\nit∥∞c−1\nΣ =: A1 < ∞. By Assumption 4.8,\n¯r := (2/κ)·log(NT),\n¯q := (NT)c2 log2 d log2(NT)\nobey (4.18), which implies (C.9). By Lemma C.5, there exist constants c2 ∈(0,1/4) and cX and\nCX depending on ¯σ, ¯σ,c2,Cmin,Cmax such that\nsup\nt≥0\n|O2(t)| = sup\nt≥0\n|P(∥TΣ∥∞≤t)−P(∥Z∥∞≤t)| ≲CX(NT)−cX +(NT)−c2/2. (D.73)",
    "content_hash": "fafb97d16cad6aeec086cbbe53a0ca1c3ff47def874b6cff8c1b8e5ecee7921b",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2cecbbed-1954-4d11-a946-6998e5c73665",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Note that for each t,\nP(∥TbΣ,β∥∞< t)−P(∥bZ∥∞< t | b\nC ) = O4(t)+\n3\n∑\nk=1\nOk(t)+O5(t). Then, (4.20) is equivalent to\nsup\nt≥0\n|P(∥TΣ,β∥∞< t)−P(∥Z∥∞< t)| →0\n(D.70)\nand (4.22) is equivalent to\nsup\nt≥0\n|P(∥TbΣ,β∥∞< t)−P(∥bZ∥∞< t | b\nC )| →P 0. (D.71)\nStep 2. We show that the elements of diagΣ are bounded from above and below. By Assumption\n4.3 (2), there exists a ﬁnite ¯σUV such that maxit E[U2\nit | Vit] ≤¯σ2\nUVa.s.. As a result, Assumption 4.3\ngives\n0 < ¯σ2 ≤min\nit E[U2\nit | Vit] ≤max\nit E[U2\nit | Vit] ≤¯σ2\nUV < ∞a.s. ,",
    "content_hash": "a452f0aa69fefdf72aa8830a7c4e316855e06b52202fc77b1a15e615456fa8ee",
    "location": null,
    "page_start": 1,
    "page_end": 82,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "697bac1c-39cf-4b22-99c3-377fb272457e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Invoking (D.59) and (D.69) gives\n∥bΓ(bβL)∥∞≤∥Γ∥∞+∥bΓ(bβL)−Γ∥∞≲P 1+γNT +κNT log(d2NT) ≲P 1. Invoking (D.43) and (D.40) gives\n∥bΩCLIME∥1,∞≤∥bΩ∥1,∞≤∥Q−1∥1,∞≤(AQ/(aQ −1)). As a result, invoking (4.12) gives\nΣ1 = OP(λQ\n1−1/aQ)·OP(1)·OP(1). Likewise,\nΣ2 := ∥Q−1∥∞,1∥bΓ(bβL)−Γ∥∞∥bΩCLIME∥1,∞= O(1)·OP(γNT +κNT log(d2NT))·OP(1)\n≲P (γNT +κNT log(d2NT))\nΣ3 := ∥bΩCLIME −Q−1∥∞,1∥Γ∥∞∥Q−1∥1,∞= O(1)·OP(γNT)·OP(1) ≲P (λQ\n1−1/aQ). Collecting the terms gives\n∥bΣ(bβL)−Σ∥∞= ∥bΩCLIMEbΓ(bβL)bΩCLIME −Q−1ΓQ−1∥∞\n≤Σ1 +Σ2 +Σ3\n≲P λQ\n1−1/aQ +γNT +κNT log(d2NT)+λQ\n1−1/aQ.",
    "content_hash": "02577afcc728a3e18c6c448a9c3f027eca5e4e9f3f387bc71884d1d7b05bb9dd",
    "location": null,
    "page_start": 1,
    "page_end": 81,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "79b1ab71-8250-4fdf-8f5e-70e83d3d7f87",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(2019), Fan\net al. (2019), Zimmert and Lechner (2019), Colangelo and Lee (2020), Klosin (2021). To conduct inference on high-dimensional parameters of the CATE function, we combine the\napproach of Zhang and Zhang (2014) and van der Geer et al. (2014) with the Cai et al. (2011)’s\napproach to matrix inversion. The inference step can also be carried out by the methods of Javan-\nmard and Montanari (2014) and the double lasso method (Belloni et al. (2014a); Chernozhukov\net al. (2015b)), but we focus on the former. We rely on fast (Gaussian) bootstrap to perform simul-\ntaneous inference based on many debiased Lasso estimators, relying upon (Chernozhukov et al.",
    "content_hash": "eda89243a256eb3a54e0bc3d0d1f69795c71718940472454d0afe40a866f9b31",
    "location": null,
    "page_start": 1,
    "page_end": 4,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8d8b5e53-ae6b-4dd0-a7e2-f0227b27d3f1",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For any ∆∈REG(¯c),\n∥∆∥2\n2,1 ≤(1+ ¯c)2∥∆T ∥2\n2,1 ≤\n(1+ ¯c)2s\nκ2g(M2,T , ¯c)∆′M2∆=: γ ·∆′M2∆. (D.30)\nCombining (D.29) and (D.30) gives\n|∆′(M1 −M2)∆| ≤(gλ M\nNTγ)·∆′M2∆. (D.31)\nNoting that x ≤|x| gives\n∆′(M1 −M2)∆≤|∆′(M1 −M2)∆| ≤(gλ M\nNTγ)·∆′M2∆,\nwhich implies\n∆′M1∆≤∆′M2∆(1+gλ M\nNTγ). (D.32)\nNoting that −x ≤|x| gives\n∆′(M2 −M1)∆≤(gλ M\nNTγ)·(∆′M2∆)\n(D.33)",
    "content_hash": "9420ef80789266768b6f708ae9919fbe2f744a5e7bb5f1d8bc7ab661e44b7c02",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "daf68c80-1eb4-4f45-859a-0eaa999ec620",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n33\nwhich implies\n∆′M1∆≥∆′M2∆·(1−gλ M\nNTγ). (D.34)\nRearranging (D.32) gives an upper bound on κg(M1,T , ¯c):\nκg(M1,T , ¯c) : =\nmin\n∆∈REG(¯c)\n√s(∆′M1∆)1/2\n∥∆T ∥2,1\n≤\nmin\n∆∈REG(¯c)\n√s(∆′M2∆)1/2\n∥∆T ∥2,1\nq\n1+gλ M\nNTγ\n= κg(M2,T , ¯c)\nq\n1+gλ M\nNTγ. A lower bound on κg(M1,T , ¯c) follows analogously, that is,\nκg(M1,T , ¯c) ≥\nmin\n∆∈REG(¯c)\n√s(∆′M2∆)1/2\n∥∆T ∥2,1\nq\n1−gλ M\nNTγ\n= κg(M2,T , ¯c)\nq\n1−gλ M\nNTγ. Taking the squares of both sides of the inequality and rearranging gives (D.28). ■\nLemma D.6 (Oracle Inequality for Group Lasso). On the event G1 := {λ ≥c√g∥S ( ¯β0)∥∞}, the\nerror vector ∆= b¯β −¯β0 belongs to the restricted set:\n∆∈REG(¯c)\nand obeys the bound\n(∆′H ( ¯β0)∆) ≤2λ ¯c∥∆T ∥2,1,\n(D.35)\nwhere ¯c := (c+1)/(c−1). Proof of Lemma D.6.",
    "content_hash": "ad077e44b137c7ebff054aaf9f0d7d61995d0d50c9a495266edc3820396527f7",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "49443d6c-6daa-4952-9494-6e710a9bb704",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Assume the event G1 holds throughout, which implies λ ≥c∥S ( ¯β0)∥2,∞. Negahban et al. (2012) establishes\n∥¯β0∥2,1 −∥b¯β∥2,1 ≤∥∆T ∥2,1 −∥∆T c∥2,1,\n(D.36)\nand shows that ∆∈REG(¯c), which implies\n∥∆∥2,1 ≤(1+ ¯c)∥∆T ∥2,1. (D.37)\nNote that b¯β solves group lasso minimization problem (D.24), so that\nQ(b¯β)+λ∥b¯β∥2,1 ≤Q( ¯β 0)+λ∥¯β0∥2,1. Expanding the least squares criterion gives\nQ(b¯β)−Q( ¯β 0) = S ( ¯β0)′∆+1/2(∆′H ( ¯β0)∆) ≤λ(∥¯β0∥2,1 −∥¯bβ∥2,1)",
    "content_hash": "617ceccdf933b452bd02680d5cbba1ad27be9a21edb50bed8a8cbef739a17e2c",
    "location": null,
    "page_start": 1,
    "page_end": 68,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "45dc2411-9514-4507-82de-a79ee7b8fcbb",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Noting that\n3(d1 +d2)e−t2NT/2(qσ2+qRt/3)\n≤max\n\u0010\n3(d1 +d2)e−(t2NT/4qσ2),3(d1 +d2)e−(tNT/4qR/3)\u0011\n. Plugging t = C′σ\np\nqlog(d1 +d2)/NT and taking C′ large makes the ﬁrst term in the max as small\nas desired. Plugging t = C′ log(d1 +d2)qR/NT and taking C′ large makes the second terms in the\nmax as small as desired. Therefore,\n1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nφ(Wit)\n≲P\n1\n√\nNT\n\u0012\nσ\np\nlog(NT)log(d1 +d2)+\n1\n√\nNT log(NT)Rlog(d1 +d2)\n\u0013\n. ■\nLemma F.3. Let γ(X) : X →Rd1×d2 be a ﬁxed matrix-valued function of a random vector X. Deﬁne the functional\nφ(X) = γ(X)−E[γ(X)]. (F.11)\nLet γ∞\nNT and γNT be numeric sequences obeying\nsup\nit\n∥γ(Xit)∥≤γ∞\nNT a.s.",
    "content_hash": "e1067fc799fcdd95cbffa66f3a789440a0986dd18562b2ae1d72c83d78e86730",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bc24c4d2-80ea-42fe-9f11-5947cbe3a694",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Suppose that the conditional expectation\nfunction is partially linear in P, as in Robinson (1988), namely\nY = e0(X)+β0(X)′P+U,\nE[U | P,X] = 0. Here e0(X) is the conditional average outcome in the “untreated” state (P = 0), and β0(X) de-\nscribes the conditional average treatment effect under the standard conditional ignorability/exogeneity\nconditions. We can orthogonalize the outcome and treatment with respect to controls X, obtain-\ning the residuals eY = Y −E[Y | X] and eP = P −E[P|X], and then observe that the CATE function\nsatisﬁes the equation:\neY = β0(X)′ eP+U. Therefore we can learn the CATE function from this regression equation if we can learn the resid-\nuals eY and eD well using modern machine learning methods. In fact, under certain conditions, we\nprove that our rate of learning the CATE function would be the same as if we knew the true residu-\nals, which is an oracle-type property. Moreover, we show this under both high-level and low-level\nregularity conditions.",
    "content_hash": "ab281062c70c333f767cea96592f60297d70ef7c7e7e13c1619acd7aca83882a",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bc09f76f-5aca-48b0-9d54-03900615bbce",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Our approach consists of approximating β0(X) by a linear combination of terms of a dictionary\nof transformation K = K(X) of X, which includes a constant of 1,\nβ0(X) ≈K′β0,\nIf dimension d = dim(β0) is low, that is, d is much smaller than the sample size n, we can learn β0\nusing least squares at the rate\np\nd/n provided the expectation functions E[Y | X] and E[P | X] are\nlearnable at fast enough rates. If β0 is high-dimensional and sparse, we will rely on Lasso to learn",
    "content_hash": "78f156fdcbf2051ecf521bc8f570ed4f43753b7931266e3149c6932c0ab3800e",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8584c8d9-01e7-4445-9e6e-e5ce1aaef21d",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "2\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\n1. INTRODUCTION\nInference on heterogeneous treatment effects is an important problem (see, e.g, Athey and Im-\nbens (2016), Chernozhukov et al. (2017), Wager and Athey (2018), Davis and Heller (2020),\nBanerjee et al. (2021)). Estimating these effects involves an inherent trade-off between ﬂexibility\nand precision. On the one hand, discovering heterogeneity requires ﬂexible models for the effects\n(e.g., by considering many groups). On the other hand, ﬂexible models produce noisy estimates\nthat are not useful for making decisions (e.g., the noise can result from having too few observations\nper group). To resolve this trade-off, empiricists decide how to create groups after making multiple\nattempts, a subjective, labor-intensive method that is prone to erroneous inference. This paper contributes a method for the estimation and inference of heterogeneous treatment\neffects in a panel data set with many potential controls and unobserved unit heterogeneity, which\naddresses many of the challenges listed above. Our key results are new even to the cross-sectional\nsettings. Thus, we ﬁrst consider the cross-sectional case and the following leading example as our\nmodel to explain the key ideas. Suppose Y is an outcome, and P is a vector of treatment variables\n(for example, polynomials in price), and X are controls.",
    "content_hash": "914941a0c7f8ef4823d9e0ac2ac40aeb203dd8bdc28094f0501d8a38f7e5d242",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b88bcbf2-65aa-40e7-8db2-8796400c5115",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Our construction uses cross-ﬁtting (CF) to estimate nonparametric reduced forms on a subset\nof data and construct the residuals (or scores) on another subset. In the i.i.d. setting, CF removes\nthe overﬁtting biases that can arise from using complex nonparametric methods such as machine\nlearning methods (see, e.g., Belloni et al. (2010); Zheng and van der Laan (2010); Chernozhukov\net al. (2018) for recent examples and Hasminskii and Ibragimov (1979); Schick (1986) for early,\nclassical uses). For regular CF methods to work in a time-series or unit-heterogeneous dynamic\npanel data, the number of periods T must be very large relative to the number of units N. We\nchoose an alternative path and introduce a ”neighbors-left-out” (NLO) cross-ﬁtting method that\napplies to weakly dependent data. The NLO approach ensures that the ﬁrst-stage and the second-\nstage samples are approximately independent. We provide exact bounds on the approximation error\nvia Strassen’s coupling. These results are of independent interest and apply beyond our context. We use our method to estimate heterogeneous price elasticities on grocery data as an empirical\napplication. This dataset consists of textual descriptions of the products, prices, and daily aggre-\ngate sales for each (store, product, distribution channel) combination. We posit a partially linear\nspeciﬁcation where the (log) sales are the dependent variable, and lags of log prices and log sales\nand current product characteristics are the control variables.",
    "content_hash": "d824bf28f804be8b6553429f0eb60a013b7d995b64e91e167937e3e52665416a",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0eb588f1-bab3-4daa-871a-fab4439592cd",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n3\nthe CATE function at the rate\np\nslogd/n where s = ∥β0∥0 is the number of non-zero entries of β0. Finally, we will use debiased Lasso methods to perform Gaussian inference on the components of\nβ, including constructing simultaneous conﬁdence bands using fast (Gaussian) bootstrap methods. We call these new approaches above the Orthogonal Lasso and Debiased Orthogonal Lasso. In\naddition, we also explore the use of grouped Lasso methods to enforce the exclusion or inclusion\nof groups of variables. Our paper considers the dynamic panel data setting arising in many empirical applications. This\nsetup makes the problem a lot more challenging. First, all variables above will be doubly-indexed\nby unit i = 1,...,N and time t = 1,...,T, and controls can include lagged values of outcomes, for\nexample, and we will need to introduce unit-speciﬁc effects in the model above judiciously. We\nadd the unit-speciﬁc effects to the conditional expectations of Yit and Pit, and we model individ-\nual effects as linear functions of time-invariant covariates plus ﬁxed effects that are approximately\nsparse. This constriction allows for the overall individual effect to be “dense” while providing\nenough convenience to make estimation results work. The strategy above is motivated by Mund-\nlack’s and Chamberlain’s approach to viewing and modeling ﬁxed effects as correlated random\neffects.",
    "content_hash": "f00108604cca40426e9f68f19757a8b0306e4c8c90dd7e5c74d798b797b7bb39",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "38930d04-1759-4bef-bb06-969cdd5a20bf",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Assuming that the residual between\nthe price tomorrow and its expectation today is exogenous, we use this variation to identify price\nelasticities. The approximate sparsity assumption helps us to rule out implausible values of price\nelasticities. Our estimates are broadly consistent with ﬁndings in Chevalier et al. (2003).",
    "content_hash": "4b1f8ec773c2e39ada66dd689cb0be54a42cf8ad26fee1329952e9fbac9c95bb",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9d779747-a546-4637-9350-2fdd1aa37add",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "In panel data settings, we rely on Lasso and verify\nthat we can learn the residuals well using lasso-based methods with weakly sparse individual ef-\nfects, relying here upon in Kock and Tang (2019). We expect that other machine learning methods\nare potentially amenable to handling dynamic panel data settings, which is the subject of future\nwork. In this work, we abstract away from clustering (Chiang (2018) and Chiang et al. (2019)),\nbut it would be good to extend the present results in this direction. In a related paper to ours, Nie and Wager (2020) establishes that the oracle rate of learning of\nCATE function is possible in a cross-sectional setting, proposing a similar residual regression ap-\nproach. Our paper is independent, and we circulated the paper around the same time as theirs (both\nin December of 2017 in ArXiv). Moreover, we provide not only the oracle learning rates but also\nstatistical inference results and also cover the dynamic panel data setting. On the other hand, rate\nresults of Nie and Wager (2020) apply to nonlinear learners of the CATE function. A more recent\nwork than ours is Oprescu et al. (2018), which develops orthogonal forest methods. Speciﬁcally,\nthey apply generalized random forest to regress outcome residual on treatment residual interacted\nwith a forest function of controls. They also provide some inferential results. Finally, alternative\napproaches to handling heterogeneous and/or continuous treatment effects are discussed in Ura\n(2018), Wager and Athey (2018), Semenova and Chernozhukov (2021), Jacob et al.",
    "content_hash": "2037c1417e9d5b4714ba1d05a4394d3da48c8fcea38b0ef2ae45029fb5747912",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "20d1f6e9-43e7-4dad-88e8-71372783202e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "4\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nAll of the above constitute the principal contributions of the paper. In what follows, we describe\nthe relations to the literature and mention some additional extensions and contributions. First,\nthe paper contributes to modern literature on estimation and model selection in high-dimensional\nsettings using debiased (orthogonal) machine learning (e.g., Hasminskii and Ibragimov (1979);\nSchick (1986); Belloni et al. (2010); Zheng and van der Laan (2010); Belloni et al. (2011, 2014b);\nZhang and Zhang (2014); van der Geer et al. (2014); Chernozhukov et al. (2018) and references\ntherein) by considering the high-dimensional CATE function as the focus of inference. Prior liter-\nature has mainly focused inference on low-dimensional or many target parameters without lever-\naging the model to help residualization (e.g., Belloni et al. (2014b, 2019)). While our results are\nnew, even for cross-sectional settings, our results cover the dynamic panel data settings. We provide general theoretical guarantees for Orthogonal Lasso methods that apply to any case\nwhere residuals are learned well enough in a preliminary step using general machine learning\nmethods. In cross-sectional settings, this automatically allows a wide range of high-quality ma-\nchine learning tools with rigorous guarantees.",
    "content_hash": "f843fdf7313254948bd081ba4ce631a61e24bce574ea603443e59ba978da5032",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "df262858-e56a-4025-90d8-f6a6cfcd135f",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Our starting point is the structural equation model\nYit = β0(Xit,Pit)+e0(Xit)+ξ E\ni +Uit,\n(2.1)\nwhere i = 1,2,...,N and t = 1,2,...,T. Here,\n• Yit is a scalar outcome of unit i at time t;\n• Pit ∈Rdp is a vector of treatment or policy variables;\n• Xit ∈RdX is a vector of predetermined variables, including possibly the lags of Pit and Yit;\n• ξ E\ni is an unobserved outcome unit ﬁxed effect;\n• Mi = {Mit}T\nt=1 is a a collection ﬁxed variables;\n• Xit can include known functions Mi, e.g. time averages ¯Mi of Mi. The stochastic shock Uit is assumed to satisfy the following sequential conditional exogeneity\ncondition\nE[Uit | Pit,Xit,Φit] = 0,\n∀(i,t)\n(2.2)\nwhere the ﬁltration\nΦit = {(Xit′,Pit′,Yit′)t−1\nt′=1}\n(2.3)\nis the ﬁltration that consists of predetermined variables for unit i prior to period t. Here we view\nMi = {Mit}T\nt=1’s as a ﬁxed realization of strictly exogenous variables that can be time-varying.",
    "content_hash": "42669d81c596709295a740fa5e6641f05200d42ec6aa16a3102e6d80adc4600c",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "21848dc1-f24f-4a83-b50b-cbb69532ac8d",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n5\n(2014, 2017, 2019a)) and suitably extending some results to our settings. Finally, we build on\n(Mundlak (1978), Chamberlain (1982)) and contribute to the panel data literature that develops\nvarious approaches to handling heterogeneity, e.g., Kock (2016b), Manresa (2016), Lu and Su\n(2016), Su et al. (2016), Moon et al. (2018), Kock and Tang (2019), Bonhomme et al. (2019a,b),\nGao and Li (2019), Chen et al. (2020), Lu and Su (2020)) among many others, see Fernandez-Val\nand Weidner (2018) for a recent overview. Structure of the paper. Section 2 introduces the model and outlines the strategy. Section 3 gives\ndeﬁnitions of estimators and outlines some theoretical results. Section 4 states our theoretical re-\nsults under general high-level conditions about the ﬁrst stage. Section 5 veriﬁes the high level\nconditions focusing on the panel data settings. Section 6 gives an empirical application, and Sec-\ntion 7 concludes. 2. THE SET UP\nHere we present the model, explain how we handle unit level heterogeneity, and outline the\noverall inferential strategy. Model.",
    "content_hash": "416a7ed0a99bfc1ea17144870f4e40a07c883fe2bf0ced8ae6324c508b4f478f",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1dcd9c32-8f5b-462d-aecf-ea404e59a45d",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "6\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nThese variables are strictly exogenous, meaning that their entire trajectory has been pre-determined\nrelative to all other variables in the model and relative to stochastic shocks Uit’s. Remark 2.1 (Fixed Effects). Throughout the paper we assume that\n{Mi,ξi}N\ni=1 are ﬁxed. We view this approach as (essentially) equivalent to treating these variables as random initially and\nthen performing the analysis conditional on their realized values.1\nRemark 2.2 (Important Notation Remark). Note that below we will be reassigning notation Xit ←\nt(Xit) to denote variables that have been obtained as transformations of the original variables Xit\nvia some mapping t. Examples of transformations include powers and their interactions. We then\nshall make other modeling assumptions to model the observable unit-level heterogeneity. The structural function p 7→β0(x, p) encodes the conditional average treatment effects (CATE). Therefore, we will simply call this function the CATE function. Indeed consider the intervention\npolicy that ﬁxes Pit = p in the structural equation (2.1), inducing the potential outcome:2\nYit(p) := β0(Xit, p)+e0(Xit)+ξ E\ni +Uit.",
    "content_hash": "e7f6d3ca8dabf5f6d86cea3c28c3ade001229d3845c9620a5186b8a5a39d6744",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fa0afa5e-8dd0-4484-b1e4-f6632c0208a4",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Then we have that\nβ0(Xit, p1)−β0(Xit, p0) = E[Yit(p1) | Xit]−E[Yit(p0) | Xit]\nis the CATE resulting from changing policy value from p0 to p1. In what follows, we will assume that β0(Xit,Pit) is well approximated by a linear combination\nof terms of a dictionary\nDit := D(Xit,Pit)\nof transformations of Xit and Pit so that\nβ0(Xit,Pit) = D′\nitβ0. Putting things together, we arrive at the partially linear model:\nYit = D′\nitβ0 +e0(Xit)+ξ E\ni +Uit,\n(2.4)\nwhere the key parameter β0 is interpretable as a causal or treatment effect parameter. We will refer\nto Pit and Dit as base and technical treatment vectors, respectively. 1As in the standard panel data modes with ﬁxed effects, we do not formalize this conditioning to reduce presentation\ncomplexity. 2Here, as in Haavelmo (1944), we assume that the structural equation remains invariant under the intervention. Judea Pearl refers to the fact that the structural model implies potential outcomes as the ﬁrst law of causal inference.",
    "content_hash": "ba00a9a49f3348fa425d52e82aff7f13a7b7e7328c31901b1510b5b98c876255",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "559e743e-a69f-46be-854b-2da94cfe2677",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Proceeding further, we model the unit-speciﬁc outcome reduced form as\nE[Yit | Xit,Φit] =: li0(Xit) = di0(Xit)′β0 +e0(Xit)+ξ E\ni ,\n(2.7)\nwhere ξ E = (ξ E\n1 ,ξ E\n2 ,...,ξ E\nN ) denotes a ﬁxed vector of unit-speciﬁc outcome effects. Given the outcome and treatment reduced forms, we deﬁne the treatment and outcome residuals\nVit := Dit −di0(Xit),\neYit := Yit −li0(Xit). (2.8)\nEquations (2.4)-(2.2) imply the following orthogonolized regression equation:\neYit = V ′\nitβ0 +Uit,\nE[Uit | Vit,Xit,Φit] = 0. (2.9)\nThis equation identiﬁes β0 as the coefﬁcient of the best linear projection of eYit on Vit. Example 2.1 (Linear in Treatment Base Treatment Structure). Deﬁne\nDit = PitKit,\n(2.10)\nwhere Kit := K(Xit) is a collection of transformations of a subset of variables in Xit, including a\nconstant of 1. Suppose there exists a low-dimensional “base” treatment variable Pit whose reduced\nform is\nPit = p0(Xit)+ξi +V P\nit ,\nE[V P\nit | Xit,Φit] = 0.",
    "content_hash": "dc2d6dd72d325e1d636de5628f35d430384263df85b100e99c5496f9d676de79",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f797bdb8-884e-46c3-9239-1924b81d0f8f",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n7\nIn this paper, we focus on a practical case when the complexity of the control function e0(Xit)\nsubstantially exceeds the complexity of CATE function (see Remark 5.7 for the formal comparison\nof complexities). Reduced Forms and Orthogonalized Equations. To learn the CATE function at its fastest pos-\nsible rate, we need to partial out controls from treatments and outcome. Consider the treatment\nequation:\nDit = di0(Xit)+Vit,\nE[Vit | Xit,Φit] = 0,\n(2.5)\nwhich keeps track of confounding. We assume that the unit-speciﬁc treatment reduced form takes\nthe form:\nE[Dit | Xit,Φit] =: d0i(Xit) = d0(Xit;ξi),\n(2.6)\nwhere ξ = (ξ1,ξ2,...,ξN) denotes a ﬁxed vector of unit-speciﬁc ﬁxed treatment-selection effects. A special case d0i(Xit) := d0(Xit) corresponds to no unobserved unit heterogeneity in treatment. Furthermore, if the function d0(Xit) is constant itself, there is no confounding.",
    "content_hash": "76565d690bce46cecc30ecbc26308a572aef4754bee6ee17d07193814389a0b4",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e3f49613-4981-499b-a07a-5acb8e2de4b9",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "This strategy works\nespecially well in conjunction with linear structures such as Example 2.1, where the same approach\nas above applies, swapping ξ E\ni for ξi, so that\np0(Xit)+ξi = ¯X′\nitδ P\n0 + ¯M′\niδ P\nM0 +ξi\naP\ni\n,\n(2.13)\nwhere aP\ni is the overall unit-level effect, consisting of a dense part ¯M′\niδ P\nM0 plus a weakly sparse\ndeviation ξi from it. 3It would be interesting to consider non-weakly sparse ξ in dynamic panel models, where ξ’s follow some known\ndistribution (which is generally not compatible with the weak sparsity assumption). We leave this important direction\nto future research. Note that Kock (2016a) developed such results for non-dynamic panel data models, which could\nprovide a starting point for such extension.",
    "content_hash": "5126c60960329c515169a7b9b8c47e46613828aa49c70a4722a5c383099f7c52",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8f9d5286-fb63-46bc-ad4b-3e8cc44d811c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "To explain this better, consider the following Mundlack-style model:\ne0(Xit)+ξ E\ni = ¯X′\nitδ X\n0 + ¯M′\niδ E\nM0 +ξ E\ni\naE\ni\n,\n(2.12)\nwhere ¯Xit are time-varying pre-determined covariates and ¯Mi = 1\nT ∑T\nt=1 Mit is time average of ﬁxed\ncovariates. The important difference with Mundlack’s approach is that we consider ξi’s to be\nweakly sparse and to condition on the realizations of ( ¯Mi,ξi)N\ni=1. 3\nWe note that while the residual effects ξi’s are required to be weakly sparse, the overall unit\neffect ai can actually be dense. Finally, the decomposition aE\ni = ¯M′\niδ E\nM0 + ξ E\ni may not be unique. However, our analysis shows that this non-uniqueness does not prevent the overall aE\ni ’s be consis-\ntently estimated, as we illustrate in Figure 1, as long as there exist at least one decomposition with\nδ E\nM0 and ξ E being sufﬁciently sparse. We provide a more technical explanation in Remark 5.2. For the unit-level heterogeneity in treatment, we can proceed similarly.",
    "content_hash": "34869e69dfc6aba250c33297d4275a65a8e9033dc9703e2abd22e9553e558f1f",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "20763e5b-cc00-4ea1-a34e-9304501ba54d",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "8\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nUnit-Level Effects. A standard approach to unit-level additive heterogeneity is demeaning or dif-\nferencing. Because these operations introduce Nickell (1981) bias in dynamic panels, it requires\nan identiﬁcation strategy based on instrumental variables (e.g., Arellano and Bond (1991)). Fur-\nthermore, differencing out time-invariant covariates may lead to an efﬁciency loss. In this paper, we take a ﬁxed effect approach, in which we approximate the vector of unobserved\ncomponents of unit effects ξ E = (ξ E\ni )N\ni=1 by a weakly sparse vector. Informally, the weak sparsity\nassumption requires ξ E to be well approximated by a sparse vector whose number of non-zero\ncomponents is small. The sparsity assumption allows us to use Lasso methods to consistently\nestimate them (Kock and Tang, 2019). The weak sparsity assumption may appear restrictive at the ﬁrst sight. However, it does allow\nfor rich forms of overall unit-level effects driven by time-invariant covariates and the ”residual”\nunit effects ξi.",
    "content_hash": "10e8a398b76a66d64edee2415727e516d3d8a987fc612b70bff032c9d2c44781",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "877bab46-422f-4565-bed5-cc6c1e6fb729",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "The following example is the special case of Example 2.1:\nYit = PitK′\nitβ0 +e0(Xit)+ξ E\ni +Uit\n= PitK′\nitβ0 +\nL\n∑\nl=1\nYi,t−lδ EE\n0l +\nL\n∑\nl=1\nPi,t−lδ EP\n0l + ¯X′\nit ¯δ E\n0 + ¯M′\niδ E\nM0 +ξ E\ni +Uit,\n= PitK′\nitβ0 +X′\nitδ E\n0 +ξ E\ni +Uit,\n(2.14)\nPit = p0(Xit)+ξi +V P\nit\n=\nL\n∑\nl=1\nPi,t−lδ PP\n0l +\nL\n∑\nl=1\nYi,t−lδ PE\n0l + ¯X′\nit ¯δ P\n0 + ¯M′\niδ P\nM0 +ξi +V P\nit\n= X′\nitδ P\n0 +ξi +V P\nit . (2.15)\nIn this example the outcome responds to the current and past values of the treatment as well as\npast values of outcomes; a set of covariates and unit effects provide further shifts. Likewise, the\ntreatment is assigned in response to current and past values of the treatment as well as past values\nof outcomes; and a set of covariates and unit effects provide further shifts. Estimation and Inference Strategy.",
    "content_hash": "481cfe795253fcb27ea9955b9ea0409e7c62b8f8b62bb81d9c2832bae8c06566",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a4c7d7fc-4ea5-4af3-8976-3970c751df94",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n9\nAdditive unit heterogeneity works well for linear models such as in Example 2.1. On the other\nhand, purely additive ﬁxed effects are not well-suited for binary or discrete treatments.4 In the\nlatter case empirical researchers may proceed as follows: supposing Pit is binary, we model\nPit = Λ(ξi + ¯X′\nitβ + ¯M′\niδ)+V P\nit ,\nE[V P\nit | Xit,Φit] = 0,\nwhere z →Λ(z) is the link function such as logit that forces the logical range restriction on the\nconditional expectation function. The ﬁxed effects here are naturally non-additive (though additive\ninside the link function). Then here one can still impose approximate sparsity on ξ = (ξi)N\ni=1 and\napply Lasso-penalized logistic regression to estimate such models in practice. We expect that the\nresults of Kock and Tang (2019) extend to this case, but this requires its own formal analysis that\nwe leave to future work. The above discussion is still somewhat abstract. We thus present the following concrete example\nthat illustrates the ﬂexibility of the proposed framework. In this example the Lasso-based methods\nare particularly helpful for both estimation of reducted forms and residuals. We will use this\nexample to illustrate the plausibility of regularity conditions that we invoke later in the paper. Example 2.2 (Linear Panel Vector Autoregression with High-Dimensional Controls and Unit Ef-\nfects).",
    "content_hash": "67a3b6e10c748adc1311d07ad5cb334a220cb19a53700b7f4cd4621f1e3ee165",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2ed7ddb1-03df-42c7-b9f4-293f52b8592f",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "We are primarily interested in high-dimensional sparse regime,\nwhere the number of technical treatments d is large\nd = dim(Dit) = dim(β0) ≫NT,\n4For example, in the binary case, the conditional expectation function of Pit is naturally bounded by 0 and 1, but\nthe additive ﬁxed effects model does not naturally respect this range.",
    "content_hash": "b13e0913b3d7a22e151914e307ae60a626cf4008cd81ec7e6cb90adf28127752",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "677c39e7-2c71-4f82-ae02-66c8e4285737",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "5We can relax this exact sparsity assumption to approximate/weak sparsity as in Belloni et al. (2014c). We chose a\nsimpler assumption given the complexity of the rest of the analysis.",
    "content_hash": "e93894c95d9e49e40bb3f0474323922e43699ea275da050476998793b502dabf",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "76e5d513-b416-415d-8ab3-9752d9f42f3a",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "10\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\n0.2\n0.4\n0.6\n0.8\n1.0\n0.2\n0.6\n1.0\nM\na\nFIGURE 1. Lasso Approximation of a Correlated Random Effects Model with Ap-\nproximately Sparse Deviations. Notes: Black dotes indicate (Mi,ai), with the horizontal axis showing values of ¯Mi and\nvertical axis the values of ai = M′\niδ M\n0 + ξi. The Lasso estimated unit effects are shown by\nblue rombi (Mi, bai), where bai := MibδM + bξi. The time-invariant controls Mi are generated as\ni.i.d draws from U[0,1]; the sparse deviations are ξi = 1/i2,i = 1,2,...,N = 20,T = 1; and\nδ M\n0 = 1. Here we show the realization just for one experiment. but only a small number s ≪NT of them has non-zero effect5 :\n∥β0∥0 = s,\n(2.16)\nImportantly, the identity and the number of the non-zero coefﬁcients is unknown. Algorithm 1. In this high-dimensional regime, our estimation and inference approach has the\nfollowing steps:\n(1) Estimate the residuals eYit and Vit using machine learning with cross-ﬁtting. (2) Estimate the CATE function by Lasso-penalized regression of estimated eYit on Vit. (3) Perform Gaussian inference parameters of the CATE function using Debiased Lasso.",
    "content_hash": "58ee6b64dc0b4fb54526eba790c17f58b7cb2ab3d7e6339feb6b3e0645fd8e5e",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "036f236d-17c1-46f7-8539-c073a3a520fa",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "We point our, however,\nthat even in this regime, if the sparsity condition s ≪d holds, the Orthogonal Lasso methods can\noutperform OLS in terms of accuracy of estimating the CATE function. Furthermore, we also\nnote that the OLS method is not designed to handle the model selection problem. Indeed, typically\nresearchers combine OLS with prior model selection step, which can lead to well-known inferential\nproblems Leeb and Potcher (2005). The Debiased Orthogonal Lasso explicitly addresses the model\nselection issue and provides rigorous theoretical guarantees for inference. 3. CROSS-FITTING WITH TIME SERIES, ESTIMATION, AND INFERENCE\nIn this section we introduce neighbor-excluding cross-ﬁtting method that is generally applicable\nto weakly dependent time series or panel data. We also provide a key theoretical support for this\nmethod using Strassen’s coupling. In this section we also write down details of some estimators,\nfocusing on the dynamic panel data case. We also informally preview theoretical results. Notation. In the remainder we use notation Wit to refer to the data vector on unit i at time t; W·,t the\ndata vector on all units at time t, and so on. The data and all other random elements are deﬁned on",
    "content_hash": "06b1f64a2e8a13374a55e5e1b48e3a074eac82bf30be1ce431ba6968156316a5",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "911ed3dc-fe14-4d0f-898d-6e4e75f64f2f",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n11\nIn the last step, more speciﬁcally, we are performing inference on classes of the linear functionals\nof parameters β0 of the CATE function D′\nitβ0. In cross-sectional settings, a wide variety of machine\nlearning methods provably apply to carry out step 1. In panel data settings, carrying out step 1\nrequires a judicious mix of modeling structures and machine learning methods that can handle\nﬁxed effects. Structures such as Example 2.1 and Lasso with penalized ﬁxed effects work provably\nwell for this purpose. Other methods potentially apply, but this remains to be proven. Moreover,\nfor step 1 we have to design cross-ﬁtting to respect the panel data structure. The last step uses\ndebiasing most similar to that of van der Geer et al. (2014), but other methods such as double lasso\ncan also be used to carry out debiasing. The next section provides formal deﬁnitions of estimation\nsteps focusing on the dynamic panel data case. If Dit is low-dimensional, namely\nd ≪NT,\nthe sparsity assumption is not required. The steps (2) and (3) are replaced by linear regression\nestimated by ordinary least squares. Algorithm 2. If d ≪NT, we perform the following steps. (1′) Estimate the residuals eYit and Vit using machine learning with cross-ﬁtting. (2′) Estimate the CATE function by linear regression of estimated eYit on Vit. (3′) Perform Gaussian inference on parameters of the CATE function using OLS. This covers many practical cases, and is a very attractive applied option.",
    "content_hash": "2596377e987d256e88df5565b315aa182ce363a00c9a163b104dacb0e2d7bdc8",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2ae37cb0-282a-409c-a911-6c34a334ad95",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "6For example, if we cut a panel into two halves, we end up with two dependent data blocks. We can, in principle,\nmake this approach work under beta-mixing by recognizing that the dependence has a vanishing effect on statistics\nthat are sample averages of a.s. bounded random variables. However, such approach would require the number of time\nperiods (i.e., the length of the panel) to be sufﬁciently large (i.e., logN/T = o((NT)−1/2)). We avoid assuming these\nadditional unpleasant conditions by using the NLO approach.",
    "content_hash": "4ebd79efd7c729db6536ec9a77acb2469117ddc825237853bdc1d46067026285",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7c7a1bc5-04cf-4d78-9f0d-f0b74a58a795",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "In the time series case, there is only one dimension to split on. In the\nunit-heterogeneous panel settings, we can only split by time dimension so that unit-speciﬁc effects\nare estimated for all units on every partition. In both cases, two contiguous time splits may not be\nindependent6. Here, we introduce a ”neighbors-left-out” (NLO) cross-ﬁtting method that applies\nto weakly dependent data. Whenever the data are weakly dependent, the NLO approach ensures\nthat the ﬁrst- and the second-stage samples are approximately independent. We give exact bounds\non the approximation error by independent blocks via Strassen’s coupling below. Deﬁnition 3.1 (Folds and Their Quasi-Complements for Weakly Dependent Data). Consider par-\ntition of {1,...,T} into adjacent blocks {Mk}K\nk=1,\n{1,...,T} = {M1,...,MK},\nwhere each block has length Tk ≥Tblock := ⌊T/(K −1)⌋for each k, such that K ≥3. Let N (k)\ndenote k and its immediate neighbors in {1,..,K}. Deﬁne the quasi-complement of Mk as M qc\nk =\n{M1,...,MK} \\ {Ml : l ∈N (k)}, and the corresponding data blocks Bk = {W·,t : t ∈Mk} and\nBqc\nk = {W·,t : t ∈M qc\nk }.",
    "content_hash": "6f3c7ec680897ccbd80f131a20b1ecf6b512ee50e314a233994e0bfdbaf3a2d7",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "aa2c39e1-ec0f-436e-aa15-a04b242383a6",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "12\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nthe underlying probability space (Ω,F,P) that has been enriched to carry an independent standard\nuniform random variable. We assume that all random variables are random vectors in Euclidean\nspaces (the coupling result below applies to random variables taking values in Polish space). We\ndenote the total variation of a signed measure v the space (Ω,F) as\n∥v∥TV := supv(A)−v(Ac),\nwhere the supremum is taken over all measurable sets A. 3.1. Cross-ﬁtting for Weakly Dependent Data. Cross-ﬁtting (CF) reduces overﬁtting biases\nfrom ﬁtting the model’s nonparametric components via machine learning. In the i.i.d. settings,\nCF uses a one subsample to estimate nonparametric components (e.g., expectation functions) and\nits complement to compute the sample average of i.i.d residuals (depending on these functions). As a result, CF plays an essential role in modern debiased inference in semi-parametric models;\nsee, e.g., Belloni et al. (2010); Zheng and van der Laan (2010); Chernozhukov et al. (2018) for\nrecent examples and Hasminskii and Ibragimov (1979) and Schick (1986) for early, classical uses\nof simpler sample-splitting methods for debiased inference. In cross-sectional cases, we create partitions into folds and their complements by sampling folds\nrandomly from the data.",
    "content_hash": "f97ccca92d4017c0b73e360e9022b987204b5822075f1f9c80b42a273fda6734",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d09377d6-6510-42f6-a8dc-189ab899f9a0",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(3) Obtain the estimated residuals\nbeY it := Yit −blik(Xit), bVit := Dit −bdik(Xit),\ni = 1,2,...,N. (3.17)\nIn the case of the base treatment structure of Example 2.1, the last step reduces to\nbV P\nit = Pit −bpik(Xit), and bVit := K(Xit)bV P\nit ,\ni = 1,2,...,N,\nsince we ﬁrst construct bpik(Xit) and then set bdik(Xit) = K(Xit)bpik(Xit). 3.2. Theoretical Support for the NLO Cross-Fitting Method. To explain the beneﬁts of the\nconstruction, we deﬁne some notation. Suppose X and Y are random elements on the same Polish\nspace. Deﬁne their dependence coefﬁcient (the beta-mixing coefﬁcient) as\nγ(X,Y) = 1\n2 ∥PX,Y −PX ×PY∥TV ,\nwhere PV denotes the distribution of the random element V. The dependence coefﬁcient vanishes\nif and only if X and Y are independent. We also make use of the following coupling result of Strassen (1965) for underlying spaces\nbeing Polish:\nmin{P(X ̸= Y) : X ∼PX,Y ∼PY} = 1\n2∥PX −PY∥TV. (3.18)\n(Note that the problem above is the optimal transportation problem for 0-1 cost; see Villani (2007)\nfor discussion).",
    "content_hash": "8e79891ec46a547459b7a130205e2ed28a5aa75e5ea2b19e885e0cc1eb2c8560",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7cfed9cb-69e3-424c-ae4e-10f68eadac5b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n13\nThe construction creates quasi-complementary sets with the left-out-neighbors. Since we use\nthe quasi-complementary sets to ﬁt the nonparametric nuisance functions, we recommend K ≥10,\nto ensure that at least 70% of data is used for this task. To clarify the construction further, consider\nthe following example: suppose we have K = 10 blocks {Mk}10\nk=1 of adjacent time stamps t ∈\n{1,...,T}, each of size Tblock, so that T = KTblock. Then, the ﬁrst quasi-complementary set M qc\n1\nconsists of {Mk}10\nk=3, the second set M qc\n2\nconsists of {Mk}10\nk=4, the third set M qc\n3\nconsists of\n{M1}∪M 10\nk=5, the fourth set M qc\n4 consists of {Mk}2\nk=1∪M 10\nk=6, ..., and the ﬁnal set M qc\n10 consists\nof {Mk}8\nk=1. The following is the application of the NLO cross-ﬁtting method above in our context. Algorithm 3 (NLO Cross-Fitted Residuals). (1) Construct blocks (Bk,Bqc\nk ) for k = 1,...,K using\nDeﬁnition 3.1; (2) For each k, compute estimators of reduced forms using quasi-complementary\nsets, namely\nbdik(·) = bdik(·,Bqc\nk ), blik(·) = blik(·,Bqc\nk ),\ni = 1,2,...,N.",
    "content_hash": "f3d8bbf5b592c7d089a892c32c711841e5a783e4e797cb7cc00abeca0b9d975e",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c241b1dc-9d82-4076-9326-f29be5f0d8e7",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Since we generally obtain the nuisance parameter estimates using quasi-complements,\nNLO-CF allows us to treat these estimates as if (essentially) independent from the data used to\ncompute semi-parametric scores (residuals in our context). 3.3. First-Stage Estimators for Learning Residuals in Panel Data. In this section, we give\nexamples of the ﬁrst-stage reduced form estimators for dynamic panel data, focusing on the models\nwith base treatment structure (2.10). We rely heavily on the results of Kock and Tang (2019) in\nthis stage. Example 3.1 (First-Stage Treatment Lasso and Reduced Form). Consider the model (2.11) with a\nsingle base treatment. Suppose\npi0(Xit) = X′\nitδ P\n0 +ξi. (3.19)\nHere, to save notation, we reassign Xit to denote the dictionary of transformations of original\ncontrols Xit, that is, Xit ←t(Xit), where the map t(·) generates the dictionary and Xit ∈RdX. For the ﬁrst-stage treatment penalty level λP = CP\nq\nNT log3(dX +N) for some constant CP,\ndeﬁne the k-fold speciﬁc estimator:\n(bδ P\nk , bξk) = argmin\nδ P,ξ\nN\n∑\ni=1 ∑\nt∈M qc\nk\n(Pit −X′\nitδ P −ξi)2 +2λP∥δ P∥1 +2 λP\n√\nN ∥ξ∥1,\nk = 1,...,K.",
    "content_hash": "db4a9017ae3854f57b58ce699aeb0293383f2d452845fb786ba3e6302c49b19d",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "586a74d8-3ab2-41a8-a9d3-95634bf930ee",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "14\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nThe following result follows from the application of Strassen’s coupling (3.18) and Lemma 2.11\nof Dudley and Philipp (1983). 7\nLemma 3.1 (Independent Coupling for NLO Data Blocks via Strassen). By suitably enriching\nprobability space, we can construct eBk and eBqc\nk that are independent of each other and that have\nthe same marginal distributions as Bk and Bqc\nk such that\nP\n\b\n(Bk,Bqc\nk ) ̸= ( ˜Bk, ˜Bqc\nk )\n= 1\n2\nPBk,Bqc\nk −PBk ×PBqc\nk\nTV =: γ(Bk,Bqc\nk ),\nwhere PBk,Bqc\nk is the distribution of (Bk,Bqc\nk ) and PBk ×PBqc\nk is the distribution of ( ˜Bk, ˜Bqc\nk ). If the data sequence (W·,t : t ≥1) is beta-mixing in t, we have that γ(Bk,Bqc\nk ) →0, since the\nblocks are separated by ⌊T/(K −1)⌋→∞periods as T →∞. Thus, under beta-mixing, by us-\ning the NLO-cross-ﬁtting, we can replace each block and its quasi-complement with independent\nblocks, with the probabilistic error determined by the speed of mixing of the weakly dependent\ntime series.",
    "content_hash": "69c830dc9d2db2d9a49e2971e46362da658bae7facab6d9966bfc0991abc3f2e",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "823f9764-59cf-4cce-aa1d-4466aed71b08",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(3.20)\n7Note that Strassen’s couping also underlies the Berbee coupling, Berbee (1987), for real-valued random variables. We extend Berbee coupling to random vectors or, more general, random variables taking values in complete, separable\nmetric spaces in the Appendix, and then use it to obtain concentration results.",
    "content_hash": "2b9cdbbc1940e71532a3e0c99b2e39084619eae251b2889915eb6962a468f640",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ac126f96-5b9e-4f03-bd68-82b29c68ec74",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For the ﬁrst-stage outcome penalty level λE = CE\nq\nNT log3(dX +N) for some constant CE,\ndeﬁne the k-fold speciﬁc estimator:\n( ˇβk, bδ E\nk , bξ E\nk ) = arg min\nβ,δ E,ξ E\nN\n∑\ni=1 ∑\nt∈M qc\nk\n(Yit −D′\nitβ −X′\nitδ E −ξ E\ni )2 +2λE∥(β,δ E)∥1 +2 λE\n√\nN ∥ξ E∥1. (3.22)\nThen, for any t ∈Mk, the outcome reduced form estimate is\nblik(Xit) = bdik(Xit)′ ˇβk +X′\nit bδ E\nk + bξ E\nik,\n(3.23)\nwhere bdik(·) is the treatment reduced form estimate. In what follows, we refer to ˇβk as the prelimi-\nnary, or the one-stage estimator of β0. ■\nLemma 5.2 establishes properties of this estimator under weak sparsity assumptions on δ E and\nξ E, based upon Kock and Tang (2019)’s analysis of dynamic panel data Lasso with weakly sparse\nunit effects. 3.4. The Second Stage: Estimating CATE Functions. Here we describe the second stage esti-\nmators. When Dit is low-dimensional, we can apply ordinary least squares to residuals. Deﬁnition 3.2 (Orthogonal Least Squares).",
    "content_hash": "e5da8d3d6f60c366bf03fea46e3f1aec7f4162267a8ecf4fc6e0b6ea38d48837",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f47aa74f-49ca-439d-b96b-88724e6725dd",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Deﬁne\nbβOLS := arg min\nβ∈Rd\n1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\n(beY it −bV ′\nitβ)2. (3.24)",
    "content_hash": "71cbaa423d772a7586508af2ca78a516aa32752bd102eb7db11b55ce1853f096",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "057f882b-a4b6-47e8-a400-3acee76904bb",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n15\n(Note that here and below the subscript index k in ˆξk serves to indicate the k-speciﬁc estimator\nof the vector of ﬁxed effects ξ, which is not to be confused with the index i that enumerates the\nelements of the vector ξ). Then, for any t ∈Mk and any i = 1,2,...,N, the base treatment reduced form estimate is\nbpik(Xit) = X′\nit bδ P\nk + bξi,k,\nk = 1,..,K. The properties of this estimator under weak sparsity assumptions on δ P and ξ follow from Kock\nand Tang (2019). Example 3.2 (First-Stage Outcome Lasso and Reduced Form). Consider the outcome model:\nYit = D′\nitβ0 +X′\nitδ P\n0 +ξ E\ni +Uit. (3.21)\nHere, to save notation, we reassign Xit to denote the dictionary of transformations of original\ncontrols Xit, that is, Xit ←t(Xit), where the map t(·) generates the dictionary.",
    "content_hash": "0a19bbb1a51bdafaf006952074089bd54acda432f1ba82fa8c10d755aa7fa66d",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3da710bb-cd0d-42db-82b2-7a798e92d3a6",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "The phenomenon our paper points out is more general and is not speciﬁc to using Lasso methods\nused in the ﬁnal stage. More recent works than our paper use orthogonalization procedures like\nours to learn the CATE functions for other choices of the ﬁnal stage estimator; see, e.g., Kennedy\n(2020). Remark 3.4 (Data-Adaptive Penalty Levels). We choose the penalty level for ﬁrst and stage es-\ntimators of the stated simple form above to simplify theoretical arguments. Rigorous and data-\nadaptive choices of penalty levels λ’s, in particular of the constants CP and CP, as well as the\ngeneralization of ℓ1-penalty to its weighted analog, are discussed in e.g., Belloni et al. (2012) and\nBelloni et al. (2017) for cross-sectional case, and implemented in hdm R package by Chernozhukov\net al. (2016). Their choices likely carry over to the dynamic panel data settings under the condi-\ntional sequential exogeneity condition. 3.5. The Third Stage: Debiased Inference on Parameters of CATE Functions. Here we de-\nscribe the third stage that performs debiased inference. Due to the bias induced by ℓ1-shrinkage,\npenalized estimators cannot be used for inference based on the standard Gaussian approximation. We construct a debiased estimator based on a variant of van der Geer et al. (2014) and Zhang and\nZhang (2014) with a new choice of debiasing matrix.",
    "content_hash": "879b3f2d00fba0cac706cdf521a88091c5e574df1ae2f8c2c37fac990fbaa58b",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d3977a8c-9b9d-4f37-bbab-9b2017aed0ae",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "16\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nAppendix G in Online Supplement establishes estimation and inference results for Orthogonal\nLeast Squares. The rate of convergence is\np\nd/NT. Deﬁnition 3.3 (Orthogonal Lasso). Let λβ =Cβ\np\nlogd/NT andCβ be a penalty parameter. Deﬁne\nbβL := arg min\nβ∈Rd\n1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\n(beY it −bV ′\nitβ)2 +λβ\nd\n∑\nj=1\n|βj|. (3.25)\nTheorem 4.1 provides the near-oracle rates of convergence\np\nslogd/NT\nfor the CATE function. We notice that Orthogonal Lasso outperforms Orthogonal Least Squares\neven in low-dimensional settings when slogd ≪d ≪NT, that is, when effective dimension s of\nβ0 is much smaller than its nominal dimension d. Remark 3.3 (Key Point of Orthogonalization). By working with estimated residuals, we attain\nthe quasi-oracle rates of convergence – the rates that result if we knew the true residuals exactly\nand used them instead. As a result, Orthogonal Lasso also outperforms the single-stage Outcome\nRegression estimators when the CATE function is much simpler and, therefore, easier to learn the\noverall regression function. For example, Orthogonal Lasso outperforms the ﬁrst-stage outcome\nLasso when the CATE function is more sparse than the overall regression function, so the near-\noracle rate is much better than the overall rate. Remark 5.7 below provides a formal statement.",
    "content_hash": "1baf72fd393f5ddd94366b1da68a27c22b660de962235311a7d99a818e320798",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4949c736-f8c5-45df-963e-2eac1a7c6b65",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n17\nConsider the covariance matrix of residuals\nQ = 1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nEVitV ′\nit\n(3.26)\nand its inverse Q−1. Deﬁne the sample covariance matrix of the residuals as\nbQ := 1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nbVit bV ′\nit. (3.27)\nEstimate approximate inverse of bQ by\nbΩ= arg min\nΩ∈Rd×d ∥Ω∥1 : ∥bQΩ−Id∥∞≤λQ,\n(3.28)\nwhere\nλQ := CQκNT,\nκNT :=\nq\nlog3(d2 log(NT))logNT/NT,\n(3.29)\nwhere CQ is a tuning constant. Finally, symmetrize the approximate inverse bΩas\nbΩCLIME = ( bωCLIME\nij\n),\nbωCLIME\ni j\n= bωi j1{| bωi j|<| bωji|} + bωji1{| bωi j|>| bωji|}. (3.30)\nIn other words, between bωij and bωji, we take the one with smaller absolute value to obtain a\nsymmetric matrix bΩCLIME, as in Cai et al. (2011). Deﬁnition 3.4 (Debiased Orthogonal Lasso). Deﬁne\nbβDL : = bβL + bΩCLIME 1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nbVit(beY it −bV ′\nit bβL).",
    "content_hash": "5c1cf71d3486236a28d17f464a81787aa03e7a4a29e6e6f557e4a3e60d0cb686",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4abf3ad7-0c63-417d-93d4-2ce42308955d",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(3.31)\nTheorems 4.2 and 4.3 show that\n√\nNT(bβDL −β0) is approximately distributed as N(0,Σ) over\nrectangular regions. The covariance matrix\nΣ := Q−1ΓQ−1 = Q−1 1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nEVitV ′\nitU2\nitQ−1\n(3.32)\nis estimated by its sample analog\nbΣ(bβL) := bQ−1 1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nbVit bV ′\nit(beY it −bV ′\nit bβL)2 bQ−1 =: bQ−1bΓ(bβL) bQ−1. (3.33)\nThis method allows for constructing componentwise and simultaneous conﬁdence intervals for all\ncomponents of β0. This method also allows for performing inference on linear functionals a′β0 of\nβ0 (provided that the ℓ1-norm of a is bounded.)",
    "content_hash": "d66165b98b92231fbcf028389699ff3f8a726cff9c48b0880c653cbc646d00e7",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3a9f9a4d-c286-4ad1-b38d-b765bbb24db5",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Note that the requirement on N comes from the relation\nγ({W·,t}t≤¯t,{W·,t}t≥¯t+q) ≤N max\ni≤N γ({Wi,t}t≤¯t,{Wi,t}t≥¯t+q),\n(4.2)\nwhich can be found using the union bound. The next condition ensures identiﬁcation of the coefﬁcients of the CATE function. Assumption 4.2 (Identiﬁcation). Let Q = (NT)−1 ∑N\ni=1 ∑T\nt=1 EVitV ⊤\nit\ndenote the population co-\nvariance matrix of treatment residuals. Assume that there exist constants Cmin,Cmax such that\n0 < Cmin ≤mineig(Q) ≤maxeig(Q) ≤Cmax < ∞. A collection of centered random variables {Xj} ∈R is said to be uniformly σ2-sub-Gaussian if\nEexp(λXj) ≤exp(λ 2σ2/2),\n∀λ ∈R\n∀j. (4.3)\nAssumption 4.3 (sub-Gaussian Tails). The following conditions hold for some constants 0 <\n¯σ2 < ¯σ2 < ∞. (1) For j = 1,2,...,d, (Vit)j are ¯σ2-sub-Gaussian conditional on Xit,Φit. (2)\nUit is ¯σ2-sub-Gaussian conditional on Vit,Xit,Φit.",
    "content_hash": "5215ee40c429304aa8d393062ffd690a96abdb2c44ec55823f699a83c00defa0",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b0e9cfe1-0a93-40f1-a39c-19b30b48db14",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(3) Uit is conditionally non-degenerate, namely\ninfit E[U2\nit|Vit,Xit,Φit] ≥¯σ2 with probability 1. Assumption 4.4 (Additional Regularity Conditions). We suppose that the true parameter vector\nhas bounded ℓ1-norm: (a)\n∥β0∥1 ≤¯Cβ\nfor some ﬁnite constant ¯Cβ; (b) and that the number of non-zero coefﬁcients does not increase too\nquickly:\n(s∨1)κNT = (s∨1)\nq\nlog3(d2 log(NT))logNT/NT = o(1).",
    "content_hash": "2c030e37491a37ace96802356f8a8467b16661eb84fae2bb205ddc2b8ddea66d",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6956f043-06d1-4165-b5fc-6f89c79e23c7",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "18\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\n4. THEORETICAL RESULTS ON ORTHOGONAL LASSO\n4.1. Consistency of Orthogonal Lasso. The following assumptions impose regularity conditions\non weak dependence, tail behavior, and the reduced form estimators. Assumption 4.1 (Sampling and Asymptotics). (1) The data sequence {{Wit}T\nt=1}N\ni=1 obeys the\nmodel (2.4) of Section 2. (2) The data on units Wi,· are independent across i, and beta-mixing at\ngeometric speed with respect to time t, uniformly in i:\nγ(q) :=\nsup\n¯t≤T,i≤N\nγ\n\u0010\n{Wit}t≤¯t,{Wit}t≥¯t+q\n\u0011\n≤Cκ exp(−κq)\n(4.1)\nfor all q ≥1, and for some constants Cκ ≥0 and κ > 0. (3) The number of time periods T is large\nenough, T −1 log(N) = o(1). Assumption 4.1 limits the data dependence across time periods with exponential mixing step. It\nis a standard weak dependence condition in the literature (Hahn and Kuersteiner (2011), Fernandez-\nVal and Lee (2013)). We incur it to ensure the validity of inference based on the panel cross-ﬁtting\nof Deﬁnition 3.1.",
    "content_hash": "df39b74d411dc3ef734f750a01916b932fc1a32a1cbde79d64652732ffb9d200",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8e27b271-b47a-4ec2-a554-f268b41027c6",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(4.7)",
    "content_hash": "b71c871c5951501a4e18eb3d39e3d0470d6d5cdafc592fca8676872400121f78",
    "location": null,
    "page_start": 18,
    "page_end": 108,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "57a4de36-e7d0-494f-ac48-254e4509245b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(4.5)\nIn particular, we specialize the notion as follows:\n1. For the technical treatment reduced form, we replace the letters g with d:\n• d(·) denotes the parameter and d0(·) the true reduced form for treatment ;\n• DNT denotes the set containing ﬁrst-stage estimates bd(·) of d0(·) w.p. 1−o(1)\n• dNT and dNT,∞are rates of convergence of DNT to d0(·);\n2. For the outcome reduced form, we replace the letters g with l:\n• l(·) denotes the parameter and l0(·) the true value reduced form for outcome;\n• LNT denotes the set containing ﬁrst-stage estimatesbl(·) of l0(·) w.p. 1−o(1)\n• lNT and lNT,∞are rates of convergence of LNT to l0(·). The key assumption on the quality of reduced form estimators is as follows. Assumption 4.5 (Regularity Conditions and Convergence Rates for Residual Learners). We sup-\npose that the reduced form estimators obey: bl(·) ∈LNT and bd(·) ∈DNT such that dNT, dNT,∞,\nlNT,lNT,∞decay sufﬁciently fast:\nd2\nNT +dNTlNT = o((NT)−1/2),\n(4.6)\nlNT,∞= o(log−1/2(dNT)),\ndNT,∞= o(log−1/2(dNT)).",
    "content_hash": "93164a76c5d8500f3bf5a46c1c609b4602fc6cc50acf8cbe4932e7c1c3dd5be9",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "20f9a4e8-1fc1-4971-9068-cb9c7f879bf6",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n19\n(c) The tuning constants Cβ and CQ in the penalty levels λβ and λQ are sufﬁciently large. (d) The\nnumber d →∞. Let 1 ≤i ≤N be a unit index, and let 1 ≤j ≤dP be a component index. We deﬁne a generic\nnuisance function to be\ng(·) = {gi j(·)} : X →RN×dP,\na generic N ×dP-matrix, and we let\ng0(·) = {gi j0(·)} : X →RN×dP\nbe its true value; here X is a subset of RN×dX. Let GNT be a sequence of neighborhoods around\ng0(·) containing realizations of its generic machine learning estimators bg(·) with probability ap-\nproaching 1. As the sample size NT increases, we expect the sets GNT to converge towards g0 in\nsuitable norms. We denote rate of convergence in the mean square norm as:\ngNT := max\n1≤j≤dP\nsup\ng∈GNT\n(NT)−1\nT\n∑\nt=1\nN\n∑\ni=1\nE(gi j(Xit)−gi j0(Xit))2\n!1/2\n(4.4)\nand let gNT,∞be a sequence of nonnegative constants such that with probability 1−o(1):\nmax\n1≤j≤dP\nsup\ng∈GNT\nsup\nit\n|gi j(Xit)−gi j0(Xit)| ≤gNT,∞.",
    "content_hash": "64c53a5fc4dab6585b3798b6cb2be7a7d496a914a7412d2c0431a3b05ad110f7",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7b518aea-4114-45dd-8546-f5336bad7e24",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "20\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nAs we discuss in Section 5, this assumption is plausible for the Lasso-based ﬁrst stage estimators\nthat we consider and may be plausible for others too. The conditon of bounded ℓ1-norm can\nbe relaxed to allow for an increasing norm at the cost of somewhat more complicated regularity\ncondtions, as can be seen from the proofs. Theorem 4.1 establishes the convergence rate of Orthogonal Lasso in ℓ2 and ℓ1 norm. Theorem 4.1 (Oracle Rates for Orthogonal Lasso). Suppose Assumptions 4.1–4.5 hold. Then, the\nOrthogonal Lasso possesses the following oracle rate guarantees:\n\u0010\nENT(V ′\nit(bβL −β0))2\u00111/2\n= OP\nr\nslogd\nNT\n! ,\n∥bβL −β0∥1 = OP\n\n\ns\ns2 logd\nNT\n\n. (4.8)\nTheorem 4.1 is our ﬁrst main result. It establishes the convergence rate of Orthogonal Lasso. This rate coincides with the oracle convergence rate, where oracle knows the ﬁrst-stage function\ne0(·) and the unobserved unit effects {ξ E\ni }N\ni=1 in the model (2.4), and therefore, knows the residuals\nand uses Lasso on these true residuals. 4.2. Estimation of Q−1 and Σ in High-Dimensional Setting. To perform statistical inference\nwe will need to assume approximate sparsity for the inverse Q−1 of the covariance matrix of the\nresiduals Q. We will use the following notation.",
    "content_hash": "357da37ba6bb6f1062c02952e720ea85a9590a21711f9404e033761503113008",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bb4d64d7-6440-4bbc-bdc5-39fe61b90eca",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For a square matrix A = (ai j), denote\n∥A∥1,∞= max\n1≤j≤d\nd\n∑\ni=1\n|ai j|,\n∥A∥∞,1 = max\n1≤i≤d\nd\n∑\nj=1\n|ai j|. Assumption 4.6 (Regularity for Estimating Q−1). (a) Let AQ and aQ > 1 be ﬁnite constants such\nthat for any column j\n(Q−1\nmj)∗≤AQm−aQ,\nm, j = 1,2,...,d\nwhere (Q−1\nj )∗is a non-increasing rearrangement of (|Q−1\nm j|)d\nm=1. Furthermore, for λQ = CQκNT,\nλ 1−1/aQ\nQ\n= o(s−1 log−1/2 d). (4.9)\nAssumption 4.6(a) ensures that the CLIME estimator of Q−1 deﬁned in the equation (3.30)\nconverges sufﬁciently fast. If d ≫NT, it requires Q−1 to be approximately sparse so that it can be\nconsistently estimated with only NT observations. Examples of high-dimensional matrices Q with\nan approximately sparse inverse include block diagonal, Toeplitz, and band matrices. The following lemma establishes the rate bound for the CLIME estimator of Q−1 for bQ in (3.27). The result holds under mixing dependence and approximate sparsity of Q, which may be of inde-\npendent interest.",
    "content_hash": "f59976e6b55396473b9857701b54089043a920261a75163e12611d030e6b39e2",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7501a915-4e67-4ebc-863c-a43d2aea7a27",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n21\nLemma 4.2 (Consistency of the CLIME estimator). Suppose Assumptions 4.1–4.6 hold. The\nCLIME estimator converges in ℓ∞-norm and ℓ∞,1-norm\n∥bΩCLIME −Q−1∥∞= ∥bΩ−Q−1∥∞= OP(λQ)\n(4.10)\n∥bΩCLIME −Q−1∥1,∞= ∥bΩCLIME −Q−1∥∞,1 = OP(λQ\n1−1/aQ)\n(4.11)\n∥Id −bΩCLIME bQ∥∞= ∥Id −bQbΩCLIME∥∞= OP(λQ\n1−1/aQ). (4.12)\n4.3. Pointwise Gaussian Inference with Debiased Orthogonal Lasso. The following theorem\nestablishes validity of Gaussian inference for parameters α′β0, where α is a ﬁxed vector with\nbounded ℓ1 norm. This is our second main result. Theorem 4.2. Let Kα be a ﬁnite constant. Suppose Assumptions 4.1–4.6 hold, and the Lindeberg\ncondition holds for each m > 0:\nlimsup\nNT→∞\nsup\n∥α∥2=1,∥α∥1≤Kα\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE[(α′VitUit)21{|α′VitUit| > m\n√\nNT} = 0.",
    "content_hash": "7e6bd7b4806f289dff251f7d1af68109e110dc19108d0010d8ca422cca08c3bb",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "56486994-7dcb-4df5-960a-b1c2d51b8b99",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Then, the Debiased Lasso estimator is asymptotically Gaussian:\nlim\nNT→∞\nsup\n∥α∥2=1,∥α∥1≤Kα\nsup\nt∈R\nP\n\u0012√\nNTα′(bβDL −β0)\n√\nα′Σα\n< t\n\u0013\n−Φ(t)\n= 0,\nwhere Φ(t) is the CDF of N(0,1). Moreover, the result continues to hold when Σ is replaced by bΣ\nsuch that ∥bΣ−Σ∥∞= oP(1). The following lemma establishes consistency of the high-dimensional covariance matrix for the\napproximate Gaussian distribution of the Debiased Lasso estimator. Notably, the dimension of this\nmatrix can exceed the sample size. Deﬁne\nγNT := (NT)−1/4 +lNT +\np\nslogd/NT +l2\nNT log(d2NT). (4.13)\nAssumption 4.7 (Conditions for Σ Estimation). We suppose that the following condition on the\ngrowth of the dimension d holds\nκNT log2(d2NT) = o(1). (4.14)\nand γNT = o(1). Lemma 4.3 (Consistency of Variance Matrix Estimator). Suppose Assumptions 4.1–4.7 hold. Then, the estimator bΣ(bβL) converges in ℓ∞norm\n∥bΣ(bβL)−Σ∥∞= OP\n\u0010\nγNT +λ 1−1/aQ\nQ\n\u0011\n=: OP(ζNT) = oP(1). (4.15)",
    "content_hash": "db1371a6a19079be651b2c1f71d71ffa4311a33393450c0568a66416e82af066",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c276bacb-3d9b-4f80-9d0c-274ed3c284aa",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "22\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\n4.4. Simultaneous Inference. We next present theoretical results on simultaneous inference on\nmany structural coefﬁcients. Deﬁne the following rates\nρNT :=\np\nlog(dNT)/NT(dNT,∞+lNT,∞)+rNT\n(4.16)\nrNT := d2\nNT +dNTlNT +(d2\nNT,∞+dNT,∞lNT,∞)\nq\n(NT)−1 log(NT)logd\n(4.17)\nAssumption 4.8 (Regularity Conditions for Simultaneous Inference on Many Coefﬁcients). (1)\nThere exists a sequence πUV\nNT ≥1 so that supit ∥VitUit∥∞≤πUV\nNT a.s. and\n0 < min\nit ∥EVitV ′\nit∥∞< ∞. (2) For some constant c2 : 0 < c2 < 1/4, the following rate conditions hold\nπUV\nNT logd log(NT)log7/2(dNT) ≲(NT)1/2−2c2\n(4.18)\nand log4 d log2(NT) = o(\n√\nNT). (3) The following rate conditions holds\n√\nNTρNT +λ 1−1/aQ\nQ\nslog1/2 d = o(log−1/2 d log−1/2 NT). (4.19)\nTheorem 4.3 establishes high-dimensional Gaussian approximation for a treatment effect vector\nβ0 and allows to conduct simultaneous inference on its coefﬁcients. Theorem 4.3 (Simultaneous Inference on Many Coefﬁcients).",
    "content_hash": "89f910df15e0a7c4d86fb6e0c29d02ac018948f27a19ff5e015f207100c503c2",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7837cd20-9d10-48d4-b0d2-1fd3155338b8",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Suppose Assumptions 4.1-4.8 with\nΣ as in (3.32) and bΣ as in (3.33). Then, the following Gaussian approximation result holds for bβDL\nsup\nR∈R\n|P((diag Σ)−1/2√\nNT(bβDL −β0) ∈R)−P(Z ∈R)| →0,\n(4.20)\nwhere Z ∼N(0,C ) is a centered Gaussian random vector with the covariance matrix\nC = (diag Σ)−1/2Σ(diag Σ)−1/2\nand R denotes a collection of cubes in Rd centered at the origin. In addition, if\nγNT +λ 1−1/aQ\nQ\n= o(log−2 d log−1 NT),\n(4.21)\nthen, replacing C with b\nC = (diag bΣ)−1/2bΣ(diag bΣ)−1/2, we also have for bZ | b\nC ∼N(0, b\nC ),\nsup\nR∈R\n|P((diag bΣ)−1/2√\nNT(bβDL −β0) ∈R)−P(bZ ∈R | b\nC )| →P 0.",
    "content_hash": "310acb3db1a57d4a2eeed000de17c9bfe406f81a2c41132645ff052ccdb066ee",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f854d8cf-f4bb-4d50-9fa3-38ec07988e76",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(4.22)\nConsequently, for the c1−ξ = (1−ξ)-quantile of ∥bZ∥∞| b\nC , we have\nP(β0,j ∈[bβDL,j ±c1−ξ bΣ1/2\nj j (NT)−1/2], j = 1,2,...,d) →(1−ξ). Theorem 4.3 is our third main result. It extends the high-dimensional Gaussian approximations\nof Chernozhukov et al. (2013); Zhang and Wu (2017); Chernozhukov et al. (2019a) to a panel\nsetting.",
    "content_hash": "f86e9d76c13f440f2482076aa50274f934c87d31a7982e7c7d5002bec65940c1",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "19393690-64c1-4ee9-9f3f-b75598ac2435",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n23\n4.5. Orthogonal Group Lasso. In this section, we focus on Example 2.1 with a linear control\nfunction e0(·) in (2.4). Applied economists ﬁtting (2.4) often would like to include the variable\n(Kit)j whenever the interaction of the base treatment Pit and the control (Kit)j is selected in the\nsecond stage. However, in case of Orthogonal Lasso, the sets of controls selected in the stage 2\nmay not be a subset of the controls selected in the stage 1. To address this concern, we group the\nmain and interaction effects of controls Kit to attain the desired model selection pattern. Decompose the covariate vector\nXit = (Kit,Zit),\nwhere Kit is a vector of heterogeneity-relevant controls and Zit is its complement. The function can\nbe written as\ne0(Xit) = K′\nitρ0 +Z′\nitδ E\n0Z\nand the linear model (2.4)\nYit = (PitKit,Kit)′\nDit\n(β0,ρ0)\n¯β0\n+Z′\nitδ E\n0Z +ξ E\ni +Uit. (4.23)\nAssuming both the interaction effect β0 and the main effect ρ0 are s-sparse, the vector ¯β0 obeys\ngroup sparsity assumption\n∥β0,ρ0∥2,0 :=\nd\n∑\nj=1\n1{(β0j,ρ0j) ̸= (0,0)} ≤\nd\n∑\nj=1\n1{β0j ̸= 0}+\nd\n∑\nj=1\n1{ρ0j ̸= 0} = 2s ≪d.",
    "content_hash": "f0a3bcfe6ef99de71a5b102fba6ad7e8ca76ca7837f3b4dd84d0b26e74293873",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1637bc2f-4c70-435a-8c4a-9ae368a1f022",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "The unit-speciﬁc treatment reduced form is\nDit = di0(Zit) = d0(Zit;ξi)\nand the unit-speciﬁc outcome reduced form is\nE[Yit | Zit] = di0(Zit)′ ¯β0 +Z′\nitδ E\n0Z +ξ E\ni . The residualized form is\n¯eYit = ¯V ′\nit ¯β0 +Uit,\nwhere\n¯eYit := Yit −E[Yit | Zit], ¯Vit := Dit −E[Dit | Zit] =: Dit −di0(Zit)\n(4.24)\nThe Orthogonal Group Lasso estimator is the ﬁrst component bβGL of the following minimization\nproblem\nbβGL := arg\nmin\n¯β=(β,ρ)∈R2d\n1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\n(¯beY it −b¯V\n′\nit ¯β)2 +λβ\nd\n∑\nj=1\n∥(βj,ρj)∥2. (4.25)",
    "content_hash": "1b1a2f6ee1b14ffcf1422fa4a13135d65d123023399d75f7a9cdc968e5c58ae2",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a15e875a-3afe-4607-af60-e0d07a4439fe",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "24\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nThe Debiased Orthogonal Group Lasso estimator is\nbβDGL : = bβGL + bΩCLIME 1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nbVit(beY it −bV ′\nit bβGL). (4.26)\nLemma 4.4 (Orthogonal Group Lasso). Under Assumptions 4.1–4.5 for ¯Vit and di0(Zit) as in\n(4.24), the Orthogonal Group Lasso attains the following rate:\n\u0010\nENT( ¯V ′\nit(bβGL −β0))2\u00111/2\n= OP\nr\nslogd\nNT\n! ,\n∥bβGL −β0∥1 = OP\n\n\ns\ns2 logd\nNT\n\n. (4.27)\nFurthermore, the statements of Theorems 4.2 and 4.3 hold for the Debiased Orthogonal Group\nLasso. ■\n5. VERIFICATION OF ASSUMPTIONS ON THE FIRST STAGE ESTIMATORS OF RESIDUALS\nThe purpose of this section is to verify Assumption 4.5 in i.i.d. and panel data settings. For\nthe Lasso-based methods of Examples 3.1-3.2, we give examples of nuisance parameter estimates,\nnuisance realization sets, and the low-level assumptions that sufﬁce for Assumption 4.5 to hold. Unless proven immediately, all numbered Remarks are formally proven in the Online Supplement,\nAppendix E. 5.1. No Unobserved Unit Heterogeneity: General ML.",
    "content_hash": "de4ef891346479e39746860962e1ae6a99112e7817162f024ae95c233731d1cb",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f5f3c7dd-e2af-4f6d-b3ca-42863ac14f5a",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Suppose the unit-speciﬁc vector func-\ntion in (2.11) obeys pi0(·) = pj0(·),1 ≤i, j ≤N. Let p0(·) = pi0(·) = p j0(·) be the single coordinate\nof the N-vector p0(·) entering in (4.4). If the covariates Xit are i.i.d. over i and t, the mean square\nrate pNT reduces to\npNT = sup\np∈PNT\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n(E(p(Xit)−p0(Xit))2)1/2 = sup\np∈PNT\n(E(p(X)−p0(X))2)1/2. Furthermore, one can split by unit index to construct independent partitions and use regular cross-\nﬁtting instead of the NLO one. In this case, the condition (2) of Assumption 4.1 redundant. The upper bound on pNT are available for i.i.d. data (across time) for many regularized methods\nunder structured assumptions on the functions p0(x) and e0(x), such as random forest, neural\nnetworks, or boosting. Speciﬁcally, the bound on pNT is achievable by ℓ1 penalized methods in\nsparse models (van der Geer et al. (2014), Belloni et al.",
    "content_hash": "7516fdec4e22fa2bab51ef7624f99e8248476b07ba53f73104466576fdf4a7b7",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cf788ff3-c621-4c1b-a411-e935414e28d1",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(2016), Belloni and Chernozhukov (2013)),\nℓ2 boosting in sparse linear models (Luo and Spindler (2016)), neural networks (Schmidt-Hieber\n(2017), Farrell et al. (2021)), and random forest in small (Wager and Walther (2015)) and high\n(Syrganis and Zampetakis (2020)) dimensions with the sparsity structure. While most of these\nresults are established in an i.i.d. setting, we conjecture that similar rates could be established",
    "content_hash": "cdf525324e131cf63e24eee62092223c6e2e08ab1591ac831979bf280a116b78",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c153af57-c7cf-47f6-97af-800e1ae9efc0",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n25\nunder weak dependence, by relying on Berbee coupling, exponential mixing conditions and/or the\nmartingale-difference property of regression errors. 5.2. Unobserved Unit Heterogeneity: Lasso. In this section, we verify Assumption 4.5 for the\nﬁrst stage Lasso estimator. We will make use of weak sparsity assumptions on ﬁxed effects. Weak sparsity generalizes\nthe exact sparsity restriction to accommodate small deviations from sparsity. Given a constant\nν : 0 < ν < 1, the vector u ∈Rdu is said to be (ν,S)-weakly sparse (Negahban et al. (2012)) if there\nexists a bound S = S(N,T), that may depend on N,T, such that\ndu\n∑\nj=1\n|uj|ν ≤S. (5.28)\nLemma A.1 in Kock and Tang (2019) gives examples of distributions that generate weakly sparse\ndraws with probability 1−o(1). For one example, if ξi are independent Gaussian draws\nξi ∼N(0,σ2\ni ),\nmax\n1≤i≤N σ2\ni = O(log3(dX +N)/(N1/νT)),\ni = 1,2,...,N. (5.29)\nThen, the vector ξ = (ξi)N\ni=1 obeys (5.28) with probability 1−o(1) with SP =\nq\nN log3ν(dX +N)/T ν.",
    "content_hash": "6e870a2455e58ed07cb602cb93e129aa3f0b45f64adc56dedf02b85b3d432055",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ca48e581-0531-4420-82df-23f6cd48826c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "In what follows, we will rely on regressors and treatments being sub-Gaussian. In dynamic\nmodels where regressors include lagged values of outcomes and treatments, this assumption is\nnon-trivial. We verify it from the model primitives in Remark 5.1 below. Remark 5.1 (Plausibility of sub-Gaussian Assumption on Treatments and Outcomes and Their\nLags). Consider Example 2.2. Substituting the treatment equation into the outcome equation gives\nYit\nPit\n! =\nL\n∑\nl=1\nAl,it\nYi,t−l\nPi,t−l\n! +Tit,\n(5.30)\nwhere\nAl,it =\n\"\nδ EE\n0l +K′\nitβ0δ PE\n0l\nδ EP\n0l +K′\nitβ0δ PP\n0l\nδ PE\n0l\nδ PP\n0l\n#\n,\n(5.31)\nTit :=\n\"\n¯X′\nit ¯δ E\n0 + ¯M′\niδ E\nM0 +ξ E\ni +Uit +K′\nitβ0( ¯X′\nit ¯δ P\n0 + ¯M′\niδ P\nM0 +ξi +V p\nit )\n¯X′\nit ¯δ P\n0 + ¯M′\niδ P\nM0 +ξi +V p\nit\n#\n(5.32)\nWe can represent the reduced form as the canonical vector auto-regression of order 1:\nFit = ΠitFi,t−1 +ϕit,\nFit = [(Yi,t−l,Pi,t−l)L−1\nl=0 ]′,",
    "content_hash": "e33194fb6beac2012f52f271bcc5fad12dbafabff49ca3fef41cc996728cfbd3",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "38b1dd92-325b-4f13-bbfa-207e89d2a44a",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "26\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nwhere\nΠit\n(2L×2L)\n:=\n\n\nA1,it\nA1,it\n... AL−1,it\nAL,it\nI2\n02\n... 02\n02\n02\nI2\n02\n... 02\n... ... ... ... ... 02\n... 02\nI2\n02\n\n\n,\nϕit\n(2L×1)\n:=\n\n\nTit\n02\n02\n... 02\n\n\n. We note that there are no restrictions on L here, but our conditions implicitly restrict K′\nitβ0\nto be bounded. Assume that the following conditions holds uniformly in (i,t): (1) The initial\ncondition Fi,0 and Tit’s are ¯σ2-sub-Gaussian vectors. (2) The singular values λ(Πit) of Πit obey\n∥λ(Πit)∥∞≤1−δ for some constant δ > 0.Then, ∥Fit∥is A ¯σ2/(1−δ)- sub-Gaussian, for some\nnumerical constant A. Consider the following condition for learning the treatment reduced form:\n(FS-TL) Consider the model\nPit = X′\nitδ P\n0 +ξi +V P\nit ,\nfor each i, the residuals V P\nit are a martingale difference sequence with respect to\nﬁltration Φit.",
    "content_hash": "7e269111bc6b026fe9f2677731908b6d1eeee616102bdb243339256daca5618a",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3f1a646a-4cc6-4d9a-90ea-204a561ee668",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Suppose (a) the vectors δ P\n0 and ξ are (ν,S)-weakly sparse with S = SP\nand S = N−ν/2SP, respectively, and SP = O(N1/2 log3ν/2(dX +N)T −ν/2); ∥δ P\n0 ∥1 is\nbounded; (b) The Gram matrix\nΨX := (NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE[XitX′\nit]\nhas all of its eigenvalues bounded from above and below by Bmax and Bmin, where\n0 < Bmin ≤Bmax are ﬁnite constants; (c) Each element of Xit and V P\nit is ¯σ2-sub-\nGaussian, where ¯σ2 is a ﬁnite constant; (d) log(dXNT)/N = o(1). All the constants are understood to be independent of (N,T). The condition FS-TL summarizes\nAssumptions A1-A3 in Kock and Tang (2019). Plugging λP = CP\nq\nNT log3(dX +N) into the\nstochastic bounds in Corollary A.1, p. 332, Kock and Tang (2019) results in the lemma below,\nwhich establishes the properties of the ﬁrst stage treatment Lasso. Lemma 5.1 (First-Stage Treatment Lasso).",
    "content_hash": "6fb1532cb01de98d20a5fb246fb13ff7d47e3f04575b7fc3026c9860833f8509",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d51378bf-7750-46d8-a7cb-0754788f1774",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Under Condition (FS-TL), the Lasso estimator in Ex-\nample 3.1 with λP = CP\nq\nNT log3(dX +N) large enough obeys the following bounds wp →1\n∥bδ P −δ P\n0 ∥1 ≤N−1/2ζNT,∞,\n∥bξ −ξ0∥1 ≤ζNT,∞,\n(5.33)\nwhere, for some large enough constant ¯CP,\nζNT,∞= ¯CPSP \u0010\nT −1/2 log3/2(dX +N)\n\u0011(1−ν)\n. (5.34)",
    "content_hash": "450933a7f168d43aeb3a188fec0041785292d52e1763968618a8035f687acdba",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8d1796b4-2525-434c-b926-9dbc78f630b1",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n27\nRemark 5.2 (Time-Invariant Covariates). Consider Example 3.1. Suppose the condition FS-TL\nholds with time-invariant ﬁxed covariates ( ¯Mi)N\ni=1. There are (inﬁnitely) many ways to decompose\nthe total effect vector a = (a1,a2,...,aN) into the observable part M′\niδ and remainder part ξi:\nai = M′\niδ +ξi,\n∀i :\ni = 1,2,...,N. Given the sparsity parameters (ν,SP), the minimizer (bδ, bξ) of the Lasso optimization problem\n(3.20) obeys the bound (5.33) expressed in terms of (ν,SP). Thus, we are free to choose (δ,ξ)\nwhose sparsity parameters ν,S imply the tightest bound on ζNT,∞in (5.34). Multiple sparse decompositions must be equivalent in the following sense. Consider two possi-\nble decompositions\nai = M′\niδ 1 +ξ 1\ni = M′\niδ 2 +ξ 2\ni ,\n∀i,\nwith ζ 1\nNT,∞and ζ 2\nNT,∞rates, respectively, determined by the weak sparsity parameters of (δ 1,ξ 1)\nand (δ 2,ξ 2). Let bξ be any given minimizer to Lasso problem.",
    "content_hash": "11ce1b63be019892c0a2e68b56c538e10ea905976c55d5c1e7e18b23bab3f94f",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "12c23232-4f61-46bd-9b5c-2cc329512f46",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Then, these decompositions must\nbe equivalent in the following sense:\n∥ξ 1 −ξ 2∥1 ≤∥ξ 1 −bξ∥1 +∥bξ −ξ 2∥1 ≤2max(ζ 1\nNT,∞,ζ 2\nNT,∞). Further, consider the following example with exact sparsity. Suppose ξ is s-exactly sparse and\nM is a one-dimensional covariate drawn from Bernoulli distribution. Mi ∼Bern(pM) such that\ns ≪NpM. Consider\nai = Miδ 1 +ξ 1\ni = Miδ 2 +ξ 2\ni ,\n∀i,\nsuch that ξ 1\ni ̸= ξ 2\ni for at least one i (i.e., ξ 1 ̸= ξ 2 as vectors). Then, δ 1 ̸= δ 2, and the vector\nMi(δ 1 −δ 2) ̸= 0 for at least NpM/2 entries, w.p. 1−o(1). Since s ≪NpM, it cannot be the case\nthat ξ 1 and ξ 2 are s-sparse at the same time. In this case, there exists a single decomposition (δ,ξ)\nobeying sparsity assumption. Consider the following condition:\n(FS-OL). Consider the model of Example 3.2:\nYit = D′\nitβ0 +X′\nitδ P\n0 +ξ E\ni +Uit,\nwhere the residuals Uit are an m.d.s with respect to the ﬁltration Φit.",
    "content_hash": "862e09c928afc5b2787e4afd5a1652bf8e36c98ded82c06b6465eaa4ded5ca0c",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9e9e4f75-9f77-455f-8e16-ce5e7c1e9e61",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Suppose (a)\nξ E = (ξ E\ni )N\ni=1 ∈RN and (δ E\n0 ,β0) ∈RdDX are (νE,SE) and (νE,N−νE/2SE) -weakly\nsparse vectors with SE = O(N1/2 log3νE/2(dX +N)T −νE/2); ∥(β0,δ E\n0 )∥1 is bounded\n(b) The Gram matrix\nΨDX := (NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE[(Dit,Xit)(Dit,Xit)′]",
    "content_hash": "8409c362932839cd986dfa46f0e345e94539794e0972a0d43f39ffcd554c1904",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "13a8a6f3-45dd-49c4-a959-a9dcf331dea4",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "28\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nhas all of its eigenvalues bounded from above and below by Bmax and Bmin, where\n0 < Bmin ≤Bmax < ∞, w.p. 1−o(1). (c) Each element of Dit, Xit and Uit is ¯σ-sub-\nGaussian, where ¯σ is a ﬁnite constant. The following lemma follows from Kock and Tang (2019), and establishes the properties of the\nﬁrst stage outcome Lasso. Lemma 5.2 (First-Stage Outcome Lasso). Under the condition (FS-OL), the estimator ( ˇβ, bδ E, bξ E)\ndeﬁned in Example 3.2 obeys the following bounds wp 1−o(1):\n∥( ˇβ, bδ E)−(β0,δ P\nE )∥1 ≤N−1/2ζ E\nNT,∞,\n∥bξ E −ξ E\n0 ∥1 ≤ζ E\nNT,∞,\n(5.35)\nwhere, for some sufﬁciently large constant ¯CE,\nζ E\nNT,∞= ¯CESE \u0010\nT −1/2 log3/2(dX +N)\n\u0011(1−νE)\n. (5.36)\nNext, we proceed to the construction of nuisance realization sets for the treatment and outcome\nmodels. Remark 5.3 (Realization Sets for Reduced Form for Base Treatment).",
    "content_hash": "bff4622d89e401f5120b2d96a920dd099f3b0e2d772821a6e56fe597c9a90072",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7c12630c-993a-4ad8-90c2-4703aacaa949",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Then, the worst-case rates dNT and dNT,∞of the technical treatment reduced form in (2.10) obey\ndNT = O(pNT) and dNT,∞= O(pNT,∞). This follows from the Cauchy-Schwartz inequality. Remark 5.5 (Realization Sets for Reduced Form for Outcome). Suppose the matrix\nΨD := (NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE[(PitK′\nitβ0)2XitX′\nit]",
    "content_hash": "e5a7d22c2378d40d5513ebaa8b7a3bf57f303d7eb241b2321cbf2a1d0ece5e95",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8abfc588-2bb0-4018-8712-9a53ef948a55",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Deﬁne PNT as a collection\nof N-vector functions\nPNT =\n(\np(x) = {pi(xi)}N\ni=1 = {x′\niδ P +ξi}N\ni=1 :\n∥δ P −δ P\n0 ∥1 ≤N−1/2ζNT,∞,∥ξ −ξ0∥1 ≤ζNT,∞\n)\n. (5.37)\nUnder Condition (FS-TL), the mean square rate pNT in (4.4) obeys pNT = O(N−1/2ζNT,∞), and the\nsup-rate upper bound chosen as pNT,∞:= 2ζNT,∞satisﬁes the sup-rate condition (4.5). In the context of Example 2.1, deﬁne DNT as a collection of N-vector functions\nDNT = {d(x) = {di(xi)}N\ni=1 = {pi(xi)K(xi)}n\ni=1 :\np(x) = {pi(xi)}N\ni=1 ∈PNT}. . The following remark helps verifying Assumption 4.5 in models with base treatment structure. Remark 5.4 (Deducing Rates in Base Treatment Cases). Consider the model (2.4)-(2.5) with sin-\ngle base treatment structure (2.10). Suppose the matrix K(·) in (2.10) has a.s. bounded entries. Suppose the base treatment reduced form vector p0(·) in (2.11) converges at rates pNT and pNT,∞.",
    "content_hash": "6898d86d76a4fa16701143b586d31523f90f9ba9d0e5afc8e9a8aaa5b2aff372",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7919e6e8-9f57-41eb-9e49-4fd58ea798e8",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Suppose the scales S and SE are not too big, namely,\n(SP)2N−1/2T ν−1/2 log3(1−ν)(dX +N) = o(1)\nSP ·SEN−1/2T (ν+νE)/2−1 log3(1−(ν+νE)/2)(dX +N) = o(1). Adding the equations above and multiplying by (NT)1/2 gives\n√\nNT(p2\nNT +pNTlNT) = o(1),\nwhich sufﬁces for (4.6). Likewise, assuming\nζ P\nNT,∞= ¯CPSP \u0010\nT −1/2 log3/2(dX +N)\n\u0011(1−ν)\n= o(log−1/2(dNT)),\nζ E\nNT,∞= ¯CESE \u0010\nT −1/2 log3/2(dX +N)\n\u0011(1−νE)\n= o(log−1/2(dNT))\ndirectly veriﬁes (4.7). ■\nOrthogonal Lasso achieves an oracle rate for CATE estimation, which can be strictly better than\nthe non-orthogonal approach. The comparison is provided in terms of the upper bounds on the\nrates. Remark 5.7 (Improvement of Orthogonal Lasso upon One-Stage Lasso).",
    "content_hash": "b120c36a5070c887ef27f9b389f37bdd51f0c7c634551efc09bdcf00d121e371",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "988e7ae7-cd01-4ca1-a826-3d22d0541e54",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Suppose the treatment\neffect vector β0 is ‘’less complex” than the ﬁrst-stage parameter (δ E\n0 ,ξ E\n0 ), that is\nslogd ≪(SE)2T νE log3(1−νE)(dX +N)\n(5.39)\nThen, under Assumption 4.5, the upper bound on the oracle Lasso rate is attained. Diving (5.39)\nby NT and taking square root gives\np\nslogd/NT = o(N−1/2ζ E\nNT,∞),",
    "content_hash": "d45b3536eabe64d6a37535a301e1b1e8fdafdb5b98671caa5117d82cb6cfdc9e",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d75f120b-f8fc-451a-bbdb-464450d5af50",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n29\nhas all of its eigenvalues bounded from above and below by Bmax and Bmin, where 0 < Bmin ≤\nBmax < ∞, w.p. 1 −o(1) and suppose ∥K(Xit)∥∞≤¯K < ∞a.s. for some constant ¯K. Deﬁne the\noutcome nuisance realization set\nLNT =\n(\nl(x) = (li(xi))N\ni=1 = {di(xi)′β +x′\niδ E +ξ E\ni }N\ni=1 : d(x) ∈DNT,\n∥β −β0∥1 +∥δ E −δ E\n0 ∥1 ≤N−1/2ζ E\nNT,∞,∥ξ E −ξ E\n0 ∥1 ≤ζ E\nNT,∞\n)\n. (5.38)\nUnder Condition (FS-OL), the mean square rate lNT in (4.4) obeys\nlNT = O(N−1/2(ζNT,∞+ζ E\nNT,∞)),\nand the sup-rate upper bound chosen as lNT,∞:= 2( ¯K∥β0∥1ζNT,∞+ ζ E\nNT,∞) satisﬁes the sup-rate\ncondition (4.5). Combining the results from Remarks 5.1–5.5, we provide sufﬁcient conditions to verify As-\nsumption 4.5. Remark 5.6 (Veriﬁcation of Assumption 4.5 for First-Stage Lasso Estimators). Consider the setup\nof Remarks 5.1–5.5 with ν,νE < 1.",
    "content_hash": "e3d8c0c7a501d8263df75f804c2490a8cd33126c0e5d3e40db82a22479104f94",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2e29c816-de67-48ff-b094-afeb87964933",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "The outcome variable Yit := log(Q)it is total log demand for unit i, and the base treatment\nPit := log(P)it is the log price. The hierarchy depth H varies between Level 2 (Figure 2a) and\nLevel 3 (Figure 2b), and notation h(i) denotes the hierarchical encoding of the product code pc(i).",
    "content_hash": "7adbef0638f09023d2bfb3ff32f689bd09fea1cb31b03d621d6b719c2ac59d81",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "de63aade-afd7-4123-a75f-f5d17704a02b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "30\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nwhere N−1/2ζ E\nNT,∞is the mean square rate bound of the preliminary non-orthogonal estimator ˇβ in\n(3.22). 6. EMPIRICAL APPLICATION\nTo show the immediate usefulness of the method, we consider the problem of inference on\ndemand elasticities for grocery products. Our transactional data come from a major European food\ndistributor that sells to retailers. The identiﬁer of each observation consists of the cross-sectional\nindex – the product code, the store location, the distribution channel (i.e., Collection or Delivery)\n– and the timestamp. For each value of the index, we compute weekly averages of the price and\nthe quantity sold. Overall, we have 1,163 unique products, sold at 8 site locations via 2 delivery\nchannels, at T = 208 time periods (weeks). In addition to the transactional data, we have access\nto the product catalog, which classiﬁes products into a tree. For example, the product code Vanilla\nSoft Scoop Ice Cream 4 Ltr package is classiﬁed into a hierarchy whose Level1 is Sweets, Level2\nis Ice Cream & Shakes & Syrups, and Level3 is Ice Cream. We ﬁlter out observations whose\neither price or sales is zero, which constitutes less than 5% of the sample. The next step is to convert the categorical covariates representing classiﬁcation into a vector of\nbinary covariates. For each node j and the product i, the binary indicator for the node j is equal\nto one if the product i belongs to the node j and zero otherwise.",
    "content_hash": "ef82615c8f548b1db7a137b7bcdcd0cde6f7cc63907c25b6179e98b7a9e65028",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "95d5f965-d663-4d54-b4af-425ed38c7a0c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Since a binary indicator for a\nparent and all its children creates a linearly dependent covariate set, we exclude one child category\nfor each parent. In the absence of any restrictions, different excluded categories yield numerically\nequivalent results. Under the sparsity assumption (2.16), this is no longer the case. The sparsity\nassumption requires that most siblings have similar treatment effects, albeit for a small number of\nexceptions whose identities are unknown. To obtain the sparse treatment modiﬁcation effect, one\nhas to exclude category that belongs to the majority (i.e., is not an exception). We assume that the\nstore brand belongs to the majority, and can be taken as the baseline (excluded) category. We postulate a partially linear dynamic panel model for weekly log demand\nlog(Q)it = log(P)it ·\n\u0012\n∑\nh∈H\n1{h(i) = h}·β0h\n\u0013\n(6.1)\n+(log(P)it−1,log(Q)it−1)′(αP\n1 ,αS\n1)+γE\nh(i) +aE\npc(i) +ρE\ns(i) +ζ E\nc(i) +γE\nm(t) +Uit,\nwhere i = 1,2,...,N and t = 1,2,...,T with T = 208 time periods (weeks). The cross-sectional\nunit index i indicates the combination of the product pc(i) at the store s = s(i) offered via c = c(i)\nchannel.",
    "content_hash": "06162e038d75b6c6a29020b599d9d416bdbf1ec7343b3260105de7d7762743e6",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "39d45a44-6561-43b6-8914-f9c468e50461",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "To estimate (β0,δ E\n0 ), we invoke the Lasso estimator of Example 3.2 with the outcomeYit = log(Q)it\nand the covariate vector Xit = XE\nit in (6.2), restricting ξ E = (ξ E\ni )N\ni=1 to be equal to zero. The price equation takes the form\nlog(P)it = log(P)it−1 ·\n∑\nh∈H\nh(i)·ζ P\n0h\n! +γP\nh(i) +aP\npc(i) +ρP\ns(i) +ζ P\nc(i) +γE\nm(t) +V P\nit . (6.3)\nTaking\nXP\nit = (log(P)it−1 ·∪h∈H 1{h(i) = h},Ki,Zi,Mt)\n(6.4)\nand ξi = 0 gives\nPit = (XP\nit )′δ P\n0 +0+V P\nit . To estimate δ P\n0 , we invoke the Lasso estimator of Example 3.1 with the outcome Pit and the co-\nvariate vector XPit as in (6.4), restricting ξ = (ξi)N\ni=1 to be equal to zero. In the second stage, we interact the ﬁrst-stage price residuals bV P\nit with hierarchy ﬁxed effects Ki\nto obtain treatment residual\nbVit = bV P\nit Ki. Next, we regress the outcome residual eYit = ^\nlog(Q)it onto Vit\neYit = V ′\nitβ0 +Uit.",
    "content_hash": "5c4daa2ce503a6b5a15eb47a73b8548f228a174a778a76e12a561f3893466518",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fb2567e2-fa1f-4e87-8910-1c53f6650214",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n31\nThe model is a special case of Example 2.1. Here, the interaction covariates are the hierarchy\nﬁxed effects\nKi = ∪h∈H 1{h(i) = h}\nand the parameter β0 is\nβ0 = (∪h∈H β0h),\nwhich results in the CATE function\nεi(β0) = K′\niβ0 = ∑\nh∈H\n1{h(i) = h}·β0h\nbeing equal to the heterogeneous elasticity εi(β0) of unit i. In addition to the hierarchy ﬁxed effects\nKi, the ﬁrst-stage controls include the product, store, channel ﬁxed effects\nZi = (∪pc∈PC1{pc(i) = pc},∪s∈S1{s(i) = s},∪c∈C1{c(i) = c})\nand the time effects Mt = ∪12\nm=1{m(t) = m}. Thus, the ﬁrst-stage controls are\nXE\nit = (log(P)it−1,log(Q)it−1,Ki,Zi,Mt). (6.2)\nTherefore, (6.1) is a special case of (2.4) with Xit = XE\nit in (6.2) and ξ E\ni = 0\n∀i:\nYit = Pit ·(K′\niβ0)+(XE\nit )′δ E\n0 +0+Uit.",
    "content_hash": "3dd786cfa38ed11a74783138ca78da0db544b0e0348b7328c4472fb143e790be",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a29a4beb-17c7-45e6-859e-1c79e2058287",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Let\nC : {1,2,...,N} →{1,2,...G} be a known partition of the products into the set of G independent\nclusters. The notation C (i) stands for all members of the i’th cluster. For any two products i and",
    "content_hash": "95afb15f1de3512c64b5512f53efbc78306bd6701594e58f6fe7291f01f5631a",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "76300998-5fac-4018-8f02-e2ee86faa8e9",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "A small number\nof distinct bars on Figure 2(b, left plot) indicates that the vector of heterogeneous elasticities’\ndifference bβh has many zeroes. As expected, the Lasso elasticity estimate bβL is sparse, which\nmakes the histogram of bεh(bβL) very concentrated. In contrast, the Debiased Lasso bβDOL is not\nsparse, and the histogram of bεh(bβDOL) is more dispersed. We ﬁnd the Snacks category to be relatively homogenous. For example, Lasso estimates suggest\nthat all Sugar products (Figure 2 b, left panel, green bar) have the same elasticity value regardless\nof sugar type or packaging. As a result, all d = 40 heterogeneous groups can be pooled into\ns = 7 distinct ones. Second-stage shrinkage helps to reduce noise, which proves useful to obtain\nelasticities consistent with economic theory. For example, 7 out of d = 40 groups have positive\nOLS estimates, while neither Lasso nor Debiased Lasso have any. We ﬁnd the debiased Lasso\nelasticity estimates to be the ones most consistent with economic theory predictions. 7. EXTENSIONS\nThe following extensions are not formally covered by theoretical framework of Section 4. Nev-\nertheless, we expect the results would extend to these settings with suitable treatment of clustering,\ngiven the recent developments of Chiang et al. (2019). Heterogeneous Own and Cross-Price Elasticities with Many Heterogeneous Products. Con-\nsider a ﬁrm that makes a pricing decision about a large number N of heterogeneous goods.",
    "content_hash": "cd2e054439538d3dd2b00922ca1021856b340edb3928b74273c339aa35cefda2",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7f4f1105-df8a-4c0a-bfe9-ee2185be8cad",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "32\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nThe Lasso estimator bβL is as in Deﬁnition 3.3 with λβ chosen by cross-validation. To simplify\ncomputation, the debiasing matrix is taken to be the Ridge inverse, which has similar properties to\nthe CLIME estimator in the moderate-dimensional case. For each estimate bβ ∈{bβOLS, bβL, bβDL},\nwe report a d-vector of distinct heterogeneous elasticities (∪h∈H bεh(bβ))′, that is,\nbεh(bβ) = ∑\n¯h∈H\n1{¯h = h}· bβh,\nh ∈H . We consider two choices of the partition H : Level 2 partition (Figure 2a) with d = 31 and Level\n3 partition (Figure 2b) with d = 40, respectively. Figure 2 qualitatively summarizes our results. On each panel, the histogram shows estimated\nheterogeneous elasticities. The total number of points is equal to the total number of heterogeneous\ngroups (i.e., the cardinality of H ). It is d = 31 for Figure 2a and d = 40 for Figure 2b. A\nsingle vertical bar represents a collection of heterogeneous groups with the same value of estimated\nelasticity, and its height shows the number of such groups. The distinct parts of the bar are grouped\nby Level 1 (Snacks, Sweets, Sugar and Veggie Meals), as marked by color.",
    "content_hash": "8e021108ce35002fbed408208ac533a2e2faf31afc3795bde442ae0d29cc6a37",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "941e6bbc-1d3a-4fe6-aeee-b8c4170a5a66",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n33\n(A) Level 2 groups whose total is d = 31\n(B) Level 3 groups whose total is d = 40\nFIGURE 2. Histogram of estimated price elasticities for each category of Level 2\n(Figure 2a) and Level 3 (Figure 2b) for Snacks. Estimates: Orthogonal Lasso (left\npanel), Debiased Orthogonal Lasso (middle panel), OLS (right panel). See text for\ndetails. 34\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nj from distinct clusters C (i) ̸= C (j), the cross-price elasticity between i and j is assumed to be\nzero. Deﬁne the average leave-i-out price of products in the i’th cluster as\nP−it := ∑j∈C (i),j̸=i Pjt\n|C (i)−1|\n.",
    "content_hash": "60a02efb6f84a71612a9e5e3d85b054754e56a5badf3eee06c9939cd28f9344d",
    "location": null,
    "page_start": 33,
    "page_end": 34,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c810cfc9-9c36-49b9-a522-353f4943ef88",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "In order to assign a causal interpretation to β0, we assume that, after\nconditioning on all pre-determined variables, the sales shock Uit is mean independent of all the\ninformation (Xjt,Φjt,Pjt)j∈C (i) about members of the i’th cluster (i.e., Uit is dissociated from\n(Xjt,Φjt,Pjt)j∈C (i), Chiang et al. (2019)). The asymptotic results should be clustered at the level\nof independent clusters G rather than individual products N. Equation (2.11) deﬁnes the price effect of interest\nβ0 = (β own\n0\n,β cross\n0\n),\nwhere β own\n0\nand β cross\n0\nare d/2 dimensional vectors of the own- and the cross-price effect, respec-\ntively. A change in the own price ∆Pit affects demand via\n∆D′\nitβ0 = ∆PitK′\nitβ own\n0\n,\nand a change in the average price ∆P−it affects demand via\n∆D′\nitβ0 = ∆P−itK′\nitβ cross\n0\n. Let\nβ own\n0\n:= (αown\n0\n,γown\n0\n) and β cross\n0\n:= (αcross\n0\n,γcross\n0\n). We see that\n• αown\n0\nis the Baseline Own Elasticity, and K′\nitγown\n0\nis the Heterogeneous Own Elasticity;",
    "content_hash": "1ae2600b42b63308948ee0eeec58a21940dd244632405c5b25975bc81329ccc4",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "38d1f81b-85f3-416c-8333-b02d1ae9c756",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(7.5)\nSuppose that in the short term the realizations of prices and sales can be approximated by the\nfollowing partially linear model:\nYit = D′\nitβ0 +e0(Xit)+ξ E\ni +Uit,\nE[Uit|(Xjt,Pjt,Φ jt)j∈C (i)] = 0,\n(7.6)\nDit = [K′\nitPit,K′\nitP−it],\n(7.7)\nPit = p0(Xit)+ξi +V P\nit ,\nE[V P\nit |Xit,Φit] = 0,\n(7.8)\nwhereYit is the log sales of product i at time t, Pit is the log price, Xit is a pX-vector of the observable\nproduct characteristics, the lagged realizations of market quantities Yit,Pit, and the demand-side\nvariables used for strategic price setting by the ﬁrm. The symbol Φit denotes the full information\nset available for unit i prior to period t, spanned by lagged realizations of the demand system. The\ncontrols Xit affect the price variable Pit through p0(·) and the sales through e0(·). The technical treatment Dit is formed by interacting Pit and P−it with the observable prod-\nuct characteristics Kit such that EKit = 0. The parameter β0 stands for the vector of own and\ncross-price elasticities.",
    "content_hash": "29877b5e4d5df34dca541f2bab04cc1cef77abb08a449ef30fe93962e9fa621d",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8ce32ca1-a801-4947-91e5-394e0c533eab",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n35\n• αcross\n0\nis the Baseline Cross-Price Elasticity, and K′\nitγcross\n0\nis the Heterogeneous Cross-Price\nElasticity.\n\nINFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n1\nABSTRACT. Appendix A presents and results on independence couplings. Appendix B develops\nconcentration results for weakly-dependent panel data. Appendix C presents the results for high-\ndimensional CLT for weakly dependent data. Appendix D contains proofs for Section 4, and Appen-\ndix E for Section 5. Appendix F contains tail bounds for empirical rectangular matrices in operator\nnorm. Appendix G contains the analysis of OLS used in stage 3 of our inference procedure.\nCONTENTS\n1.\nIntroduction\n2\nStructure of the paper.\n5\n2.\nThe Set Up\n5\nModel\n5\nReduced Forms and Orthogonalized Equations\n7\nUnit-Level Effects\n8\nEstimation and Inference Strategy\n9\n3.\nCross-Fitting with Time Series, Estimation, and Inference\n11\nNotation\n11\n3.1.\nCross-ﬁtting for Weakly Dependent Data\n12\n3.2.\nTheoretical Support for the NLO Cross-Fitting Method\n13\n3.3.\nFirst-Stage Estimators for Learning Residuals in Panel Data\n14\n3.4.\nThe Second Stage: Estimating CATE Functions\n15\n3.5.\nThe Third Stage: Debiased Inference on Parameters of CATE Functions.\n16\n4.\nTheoretical Results on Orthogonal Lasso\n18\n4.1.\nConsistency of Orthogonal Lasso\n18\n4.2.\nEstimation of Q−1 and Σ in High-Dimensional Setting\n20\n4.3.\nPointwise Gaussian Inference with Debiased Orthogonal Lasso\n21",
    "content_hash": "47433e638899b80f0fb352f5c8fe48fd751702952449ccf9a13953992c87ca43",
    "location": null,
    "page_start": 35,
    "page_end": 36,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3890b899-6131-4f0f-83f2-f42e7952c1b8",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n1\nABSTRACT. Appendix A presents and results on independence couplings. Appendix B develops\nconcentration results for weakly-dependent panel data. Appendix C presents the results for high-\ndimensional CLT for weakly dependent data. Appendix D contains proofs for Section 4, and Appen-\ndix E for Section 5. Appendix F contains tail bounds for empirical rectangular matrices in operator\nnorm. Appendix G contains the analysis of OLS used in stage 3 of our inference procedure.\nCONTENTS\n1.\nIntroduction\n2\nStructure of the paper.\n5\n2.\nThe Set Up\n5\nModel\n5\nReduced Forms and Orthogonalized Equations\n7\nUnit-Level Effects\n8\nEstimation and Inference Strategy\n9\n3.\nCross-Fitting with Time Series, Estimation, and Inference\n11\nNotation\n11\n3.1.\nCross-ﬁtting for Weakly Dependent Data\n12\n3.2.\nTheoretical Support for the NLO Cross-Fitting Method\n13\n3.3.\nFirst-Stage Estimators for Learning Residuals in Panel Data\n14\n3.4.\nThe Second Stage: Estimating CATE Functions\n15\n3.5.\nThe Third Stage: Debiased Inference on Parameters of CATE Functions.\n16\n4.\nTheoretical Results on Orthogonal Lasso\n18\n4.1.\nConsistency of Orthogonal Lasso\n18\n4.2.\nEstimation of Q−1 and Σ in High-Dimensional Setting\n20\n4.3.\nPointwise Gaussian Inference with Debiased Orthogonal Lasso\n21\n\n2\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\n4.4.\nSimultaneous Inference\n22\n4.5.\nOrthogonal Group Lasso\n23\n5.\nVeriﬁcation of Assumptions on the First Stage Estimators of Residuals\n24\n5.1.\nNo Unobserved Unit Heterogeneity: General ML\n24\n5.2.\nUnobserved Unit Heterogeneity: Lasso\n25\n6.\nEmpirical application\n30\n7.\nExtensions\n32\nHeterogeneous Own and Cross-Price Elasticities with Many Heterogeneous Products\n32\nNotation\n4\nAppendix A.\nTools: Strassen and Berbee Couplings. Implications for Cross-Fitting\n5\nA.1.\nStrassen’s Coupling: Weak and Strong Form via Dudley-Philipp\n5\nA.2.\nIndependence Coupling\n6\nA.3.\nBerbee Coupling Extended\n6\nA.4.\nApplications to Cross-Fitting\n7\nAppendix B.\nTools: Tail Bounds for Maxima of Sums for Weakly Dependent Panels\n11\nB.1.\nProperties of Products of Sub-Gaussians\n11\nB.2.\nTails Bounds for Maxima of Sums of Martingale Differences\n12\nB.3.\nTail Bounds for Maxima of Sums of Sub-Gaussian products\n13\nB.4.\nSome Technical Lemmas\n17\nAppendix C.\nTools: High-Dimensional Central Limit Theorems for Weakly Dependent\nData\n21\nAppendix D.\nProofs for Section 4\n26\nD.1.\nBounds on Errors for Estimating Q and Gradient S.\n26\nD.2.\nProof of Orthogonal Lasso Rate: Theorem 4.1\n30",
    "content_hash": "c53fb4f4b14138fe067758c581208b5e57ece5cc82e841ba96f4073bb4e37562",
    "location": null,
    "page_start": 36,
    "page_end": 37,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b5d7ab0a-c4d1-458f-b374-a3bf8a13fe5e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "2\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\n4.4.\nSimultaneous Inference\n22\n4.5.\nOrthogonal Group Lasso\n23\n5.\nVeriﬁcation of Assumptions on the First Stage Estimators of Residuals\n24\n5.1.\nNo Unobserved Unit Heterogeneity: General ML\n24\n5.2.\nUnobserved Unit Heterogeneity: Lasso\n25\n6.\nEmpirical application\n30\n7.\nExtensions\n32\nHeterogeneous Own and Cross-Price Elasticities with Many Heterogeneous Products\n32\nNotation\n4\nAppendix A.\nTools: Strassen and Berbee Couplings. Implications for Cross-Fitting\n5\nA.1.\nStrassen’s Coupling: Weak and Strong Form via Dudley-Philipp\n5\nA.2.\nIndependence Coupling\n6\nA.3.\nBerbee Coupling Extended\n6\nA.4.\nApplications to Cross-Fitting\n7\nAppendix B.\nTools: Tail Bounds for Maxima of Sums for Weakly Dependent Panels\n11\nB.1.\nProperties of Products of Sub-Gaussians\n11\nB.2.\nTails Bounds for Maxima of Sums of Martingale Differences\n12\nB.3.\nTail Bounds for Maxima of Sums of Sub-Gaussian products\n13\nB.4.\nSome Technical Lemmas\n17\nAppendix C.\nTools: High-Dimensional Central Limit Theorems for Weakly Dependent\nData\n21\nAppendix D.\nProofs for Section 4\n26\nD.1.\nBounds on Errors for Estimating Q and Gradient S.\n26\nD.2.\nProof of Orthogonal Lasso Rate: Theorem 4.1\n30\n\nINFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n3\nD.3.\nProof of Theorem 4.2\n35\nD.4.\nEstimation of Σ: Proof of Lemma 4.3\n41\nD.5.\nProof of Theorem 4.3\n47\nAppendix E.\nProofs for Section 5\n51\nAppendix F.\nTools: Tails Bounds for Empirical Rectangular Matrices under Weak\nDependence\n54\nAppendix G.\nAdditional Results on Orthogonal OLS\n61\nReferences\n68",
    "content_hash": "5e0237cb74011c760a96160e5acdb86af246c9d641b8d26b2a7643aad0949c03",
    "location": null,
    "page_start": 37,
    "page_end": 38,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "99e36190-877e-4f9b-b35f-b4d1b6c265bb",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For\na matrix A = (aij) ∈Rd×d, let its operator norm be ∥A∥2 = supα∈S d−1 ∥Aα∥2, the elementwise\nnorm be ∥A∥∞= max1≤i,j≤d |aij|, and the maximal ℓ1-row-norm:\n∥A∥1,∞= max\n1≤j≤d\nd\n∑\ni=1\n|ai j|. Empirical Process Notation. In what follows, we use the standard empirical process notation. For\na generic measurable function f : W →R and a generic sample {{Wit}T\nt=1}N\ni=1, where Wit’s take\nvalues in W , deﬁne the empirical expectation\nENT f(Wit) = 1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nf(Wit)\nand the empirical process:\nGNT f(Wit) =\n√\nNTENT[f(Wit)−EWit f(Wit)].",
    "content_hash": "0a89b6997d0cb05898f9ab381368a3af7838f74e4da26233ac29f04bcdcca10d",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "042ddda9-1b80-4da0-8ec2-fec47297436e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "4\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nNOTATION\nNotation. We use the standard notation for numeric and stochastic dominance. For two numeric\nsequences {an}n≥1 and {bn}n≥1, an ≲bn stands for an = O(bn). For two sequences of ran-\ndom variables {an}n≥1 and {bn}n≥1, an ≲P bn stands for an ≲P (bn). For a random vector V,\nlet V 0 := V −E[V] be the demeaned vector. Let [N] := {1,2,...,N}, [T] := {1,2,...,T} and\n[j] := {1,2,...,d}. Let a∧b = min{a,b} and a∨b = max{a,b}. Matrix and Vector Norms. For a vector v ∈Rd, denote the ℓ2-norm of v as ∥v∥2 :=\nq\n∑d\nj=1 v2\nj, the\nℓ1-norm of v as ∥v∥1 := ∑d\nj=1 |vj|, the ℓ∞-norm of v as ∥v∥∞:= max1≤j≤d |vj|, and the ℓ0-”norm”\nof v as ∥v∥0 := ∑d\nj=1 1{vj̸=0}. Denote a unit sphere in Rd as S d−1 = {α ∈Rd : ∥α∥= 1}.",
    "content_hash": "7265287528f71f5464b12490533fc2f708a2200bb15434690ea0377e855d6492",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "362a7402-5b95-4d36-9f9c-858b6c5438c4",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n5\nAPPENDIX A. TOOLS: STRASSEN AND BERBEE COUPLINGS. IMPLICATIONS FOR\nCROSS-FITTING\nA.1. Strassen’s Coupling: Weak and Strong Form via Dudley-Philipp. Consider the following\nsetup. Let S be a Polish space and PZ,W be a law on S × S, with marginal laws PZ on S and PW on\nS. Let (Ω,B,P) be a probability space and Z be a random variable on Ωwith values in S and\nlaw L (Z) = PZ. Assume that (Ω,B,P) has been extended to carry a random variable U on Ω,\nindependent of Z, with values in [0,1] and law U(0,1). The total variation norm of a signed\nmeasure ν on on the Polish space T is deﬁned as\n∥ν∥TV = sup\nFclosed\nν(F). The total variation distance between laws P and Q deﬁned on the Polish space T is deﬁned by\ntaking ν = P−Q in the deﬁnition above. We also make use of the following Strassen’s weak coupling result (e.g, Villani (2007), p.7):\nmin\nZ∗,W ∗{P(Z∗̸= W ∗) : L (Z∗) = PZ, L (W ∗) = PW} = 1\n2∥PZ −PW∥TV,\n(A.9)\nwhere minimization is done over space of random variables Z∗and W ∗deﬁned on the probability\nspace (Ω,B,P).",
    "content_hash": "e224d6ef7e4992de343a9c54cc8025cd1d7ffe1aee216f20a2dbe1e2a2475521",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "51b75f2b-a5ea-4e7e-bba1-9f5ff80470ef",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Note that the problem above is the optimal transportation problem for 0-1 cost;\nsee Villani (2007) for discussion. The above is a special case of Strassen’s original result; Schwarz\n(1980) (Theorem 1) provides another proof of (A.9). We now recall the following result. Lemma A.3 (Strong Coupling; Lemma 2.11, Dudley and Philipp (1983)). Let S and T be Polish\nspaces and Q a law on S ×T, with marginal PZ on S. Let (Ω,B,P) be a probability space and Z\nbe a random variable on Ωwith values in S and law L (Z) = PZ. Assume that there is a random\nvariable U on Ω, independent of Z, with values in a separable metric space R and law L (U)\non R having no atoms. Then there exists a random variable W on Ωwith values in T and law\nL ((Z,W)) = Q. This result is quoted with minor adaptation of notation. This lemma implies the strong form of\nStrassen’s weak coupling (A.9) as stated in the following lemma. Lemma A.4 (Strong Form of Strassen’s Coupling). Given the setup above with a given random\nvariable Z, there exists a random variable W taking values in S, deﬁned on the same probability\nspace, and having law L (W) = PW such that:\nP(Z ̸= W) = 1\n2∥PZ −PW∥TV. (A.10)",
    "content_hash": "2ac45d1f53db88333c79a50993b9c38bf981f011daecab948210691f48aabd92",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "523a7fd5-3033-485e-aaa1-7556e6ec6bc7",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "6\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nProof. Strassen’s weak coupling implies that there is a pair of random variables (Z∗,W ∗) with\nlaw Q and marginals PZ and PW such that:\nP(Z∗̸= W ∗) = 1\n2∥PZ −PW∥TV. Application of the Dudley-Philipp lemma with S = T and U taken to be uniform random variable\nimplies that for the given Z there is a pairing random variable W, such that law of (Z,W) is Q. Therefore,\nP(Z ̸= W) = P(Z∗̸= W ∗) = 1\n2∥PZ −PW∥TV. ■. A.2. Independence Coupling. Consider a special case of the setup above with S = S1 × S2 and\nT = S, where S1 and S2 are Polish spaces, and where Z = (X,Y) is a pair of random variables such\nthat L (X) = PX on S1 and L (Y) = PY on S2, and L (X,Y) = PX,Y. Lemma A.5 (Strong Coupling with Independence via Strassen-Dudley-Philip). Consider the setup\nabove. We can construct eY and eX that are independent of each other with laws L (X) = PX and\nL (Y) = PY such that\nP\n\b\n(X,Y) ̸= ( ˜X, ˜Y)\n= 1\n2 ∥PX,Y −PX ×PY∥TV . Proof. In the previous lemma take Z = (X,Y) and W = ( ˜X, ˜Y), and note that PW = PX ×PY.",
    "content_hash": "6b49559c3f0459f51da4ed03dcc1cc8e3e71b0d09bd0b2e52875f905595f6853",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a81eabaf-2fa4-456e-bbe1-41d741e23c76",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "■\nA.3. Berbee Coupling Extended. Let (X,Y) be a pair of random variables taking values in the\nPolish space S1 ×S2 as in the setup above. Deﬁne their coefﬁcient of dependence as:\nγ(X,Y) = 1\n2 ∥PX,Y −PX ×PY∥TV . This coefﬁcient vanishes if and only if X and Y are independent. The following lemma is a minor extension of Lemma 2.1 of Berbee from real-valued to Polish-\nspace valued random variables. Lemma A.6 (Berbee Coupling on Polish Spaces). Let X = (Xi)n\ni=1 be a collection of random\nvariables taking values on the Polish space S = (S1 ×...×Sn), and deﬁned on the same probability\nspace (Ω,B,P). Deﬁne for 1 ≤i < n\nγ(i) = γ (Xi,(Xi+1,...,Xn)). The probability space can be extended so that there exist a collection of random variables ˜X =\n( ˜Xi)n\ni=1 that are mutually independent, such that each ˜Xi has the same law as Xi and\nP\nX ̸= ˜X\n\u0001\n≤γ(1) +...+γ(n−1).",
    "content_hash": "0e00c0bc11ca39139fecc94155dd9ed279417d6af70367aa79d7e20a44220d7f",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "735e0153-aa29-41d0-8d5c-8b420b153960",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n7\nProof. Assume that (Ω,B,P) has been extended to carry a random variable U on Ω, independent\nof X, with values in [0,1] and law U(0,1). 1. Application of strong form of Strassen’s coupling in Lemma A.4 implies that one can con-\nstruct ˜X as in the statement of the lemma such that\nP(X ̸= ˜X) = 1\n2 ∥PX −P˜X∥. 2. (Identical to Berbee). To prove the claim of the lemma, we have to estimate the right hand\nside. If X,Y and ˜Y are random variables, with Y and ˜Y having values in the same space, then\nPX,Y −PX,˜Y\n≦∥PX,Y −PX ×PY∥TV +∥PX ×PY −PX ×P˜Y∥TV\n= 2γ(X,Y)+∥PY −P˜Y∥TV\nApplying this rule successively one obtains\n1\n2 ∥PX1,...,Xn −PX1 ×...×PXn∥\n≤γ(1) +∥PX2,...,Xn −PX2 ×...×PXn∥\n≤... ≤γ(1) +...+γ(n−1)\n■\nCorollary A.1 (Berbee’s Coupling for Panel Data). Let {Xi1,Xi2,...,XiL}N\ni=1 be real random ma-\ntrices of possibly distinct dimensions. Suppose the sequences (Xi1,Xi2,...,XiL) are independent\nover i.",
    "content_hash": "80ed41c9f283166ad12d9959f7b0aad633bf42a2fe9a99070b0a9190ccb8ded1",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7c9c8006-f943-4e3a-bf4e-035f31c4e68f",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For each i, (Xi1,Xi2,...,XiL) is β-mixing whose coefﬁcients are bounded as\nsup\n1≤i≤N\nsup\n1≤l≤L−1\nγ((Wi,1,...,Wi,l−1),(Wi,l,...,Wi,L)) ≤ε. (A.11)\nThe probability space can be extended with random variables X∗\nil distributed as Xil such that X∗\nil\nare independent over i,l and\nP(Xil ̸= X∗\nil for some i,l) ≤N(L−1)ε. (A.12)\nThis follows immediately from the union bound and Lemma A.6. ■\nA.4. Applications to Cross-Fitting. Here we recall the setup induced by the NLO construction\ngiven in the main text. Let Mk and M qc\nk\nbe two NLO subsets of time indices {1,2,...,T}, for\nk = 1,...,K. Deﬁne the data blocks\nBk = ∪N\ni=1Bik,\nBik = {Wit}t∈Mk;\n(A.13)\nBqc\nk = ∪N\ni=1Bqc\nik ,\nBqc\nik = {Wit}t∈M qc\nk . By construction, the time periods in Mk and M qc\nk\nare separated by at least Tk ≥Tblock := ⌊T/(K −\n1)⌋time periods.",
    "content_hash": "7b5bfd83e4a91b82b0aa3c38b38c90b931066625f37acab98b082251520a881c",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "312adbbe-36e4-4d39-afb8-5070599c1961",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Let bηk = ¯η(Bqc\nk ) denote an estimator constructed on\nthe data Bqc\nk . Let b 7→φ(b;η) be another measurable mapping, indexed by η, that maps W q to\nRdφ. We assume that the composition map (b,bqc) 7→φ(b; ¯η(bqc)) is measurable.8\n8Otherwise, can use outer probability measures to work with the bounds below.",
    "content_hash": "0e4b0352c7e25695114feaf140caf1df339c17f911cc99d7d6d4d727c24557ec",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "04f8ca37-3f1e-451e-8e2b-17534a8a7406",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "8\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nLemma A.7 (Approximate Independence of Separated Chunks). Suppose Assumption 4.1 holds\nwith\nγ(q) :=\nsup\n¯t≤T,i≤N\nγ\n\u0010\n{Wit}t≤¯t,{Wit}t≥¯t+q\n\u0011\n≤Cκ exp(−κq)\n(A.14)\nand logN/Tblock = o(1). Then, there exist random elements B∗\nk and Bqc∗\nk\nsuch that (1) B∗\nk and Bk\nare equal in law, Bqc∗\nk\nand Bqc\nk are equal in law, (2) B∗\nk and Bqc∗\nk\nare independent, and (3) the event\nEberbee := {(Bk,Bqc\nk ) = B∗\nk,Bqc∗\nk ), for all k = 1,...,K}\n(A.15)\nholds with probability 1−o(1),NT →∞. Proof. Invoking Lemma A.5 shows that the required variables exist and obey\nP\n\u0010\n(Bk,Bqc\nk ) ̸= (B∗\nk,Bqc∗\nk )\n\u0011\n≤γ(Bk,Bqc\nk ) ≤NCκ exp(−κTblock). Invoking union bound over the partitions k = 1,2,...,K gives\nP\n\u0010\n(Bk,Bqc\nk ) ̸= (B∗\nk,Bqc∗\nk ), for some k = 1,...,K\n\u0011\n≤KNCκ exp(−κTblock).",
    "content_hash": "4d582e9d2922393a1fe6f5626e333c25a76a0f46c7422d33f138833e10946ffa",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7a146cc5-fa8c-4f46-9cd8-9809cbd1d56a",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(2) for any sequence {ηNT} ∈¯\nT N,T and any norm |||·|||,\n|||BAk(ηNT)||| = O(ζ B\nNT),\n|||VAk(ηNT)||| ≲P (ζV\nNT). Then,\n(NT)−1\nN\n∑\ni=1\nK\n∑\nk=1 ∑\nt∈Mk\nA(Wit, ˆηk)\n≲P (ζV\nNT +ζ B\nNT).",
    "content_hash": "8289abecfc42f498b2ecc9664e86c6dec491ef44997e00871dc09885938aef58",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "414acac2-351a-4ec8-a4b8-d032f6ca27ff",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n9\nCorollary A.3. Suppose there exists a sequence of sets { ¯\nT N,T} ⊆\n¯\nT obeying the conditions as\nNT →∞: (A) P(bηk ∈\n¯\nT N,T) = 1−o(1) and (B) for any sequence {ηNT} ∈\n¯\nT N,T, φ(Bk,ηNT) =\nOP(VNT). Then, φ(Bk, bηk) = OP(VNT). Proof of Corollary A.3. Invoke Lemma A.2 with\nψ(b,bqc) := φ(b, ¯η(bqc))1{ ¯η(bqc)∈¯\nT N,T }. Union bound implies\nP(φ(Bk, bηk) ≥ℓNTVNT) ≤P(φ(Bk, bηk) ≥ℓNTVNT ∩bηk ∈¯\nT N,T)+P(bηk ̸∈¯\nT N,T)\n= P(ψ(Bk,Bqc\nk ) ≥ℓNTVNT ∩bηk ∈¯\nT N,T)+P(bηk ̸∈¯\nT N,T)\n≤P(ψ(Bk,Bqc\nk ) ≥ℓNTVNT)+o(1),\nwhere the last inequality holds by condition A. We have that\nP\n\u0010\nψ(Bk,Bqc\nk ) ≥ℓNTVNT\n\u0011\n≤P\n\u0010\nψ(B∗\nk,Bqc∗\nk ) ≥ℓNTVNT\n\u0011\n+o(1),\nfrom the previous proof.",
    "content_hash": "f9be9f20c224a3411e1b512b89bbfabfdb0744fdcb10cfad53426ebb70a9f956",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "10905d26-da29-4eea-8ee0-5638ddf10d97",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "By Condition B,\nP(ψ(B∗\nk,Bqc∗\nk ) > ℓNTVNT | Bqc∗\nk ) = P(φ(B∗\nk, ¯η(Bqc∗\nk ))1{ ¯η(Bqc∗\nk ) ∈¯\nTN,T} > ℓNTVNT | Bqc∗\nk ) = oP(1). Therefore, using LIE\nP\n\u0010\nψ(B∗\nk,Bqc∗\nk ) ≥ℓNTVNT\n\u0011\n= E\nh\nP\n\u0010\nψ(B∗\nk,Bqc∗\nk ) ≥ℓNTVNT\nBqc∗\nk\n\u0011i\n= o(1),\nwhere the ﬁnal conclusion holds by the boundedness (and therefore uniform integrability) of the\nintegrand. ■\nLemma A.8 (Bounds on Cross-Fit Sample Averages). Let w 7→A(w,η) be a generic (measurable)\nmatrix-valued function deﬁned on W , indexed by the parameter η ∈¯\nT . Deﬁne\nBAk(η) : = (NTk)−1\nN\n∑\ni=1 ∑\nt∈Mk\nEWitA(Wit,η)\n(A.16)\nVAk(η) : = (NTk)−1\nN\n∑\ni=1 ∑\nt∈Mk\n[A(Wit,η)−EWitA(Wit,η)]\n(A.17)\nSuppose there exist sequences of constants ζ B\nNT and ζV\nNT so that as NT →∞for each k = 1,...,K:\n(1) P(bηk ∈¯\nT N,T) = 1−o(1).",
    "content_hash": "36ae04c4fd6a6ae8f6c96057c984569560141a3838b1b8b7557ad6abcc876cc0",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0f4491ae-035a-4572-9786-e6b9410394d5",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "10\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nIn our case, we will either use |||·||| = ∥·∥∞(sup-norm) or |||·||| = ∥·∥2 (operator norm).\nProof of Lemma A.8. We invoke Corollary A.3 with φ(Bk,η) := BAk(η)+VAk(η). The conditions\n(A) and (B) are directly assumed in Lemma A.8 as conditions (1) and (2), respectively. Therefore,\nfor each k ≤K,\n|||BAk(bηk)+VAk(bηk)||| ≲P (ζV\nNT +ζ B\nNT).\nWe next note that with probability converging to one,\n(NT)−1\nN\n∑\ni=1\nK\n∑\nk=1 ∑\nt∈Mk\nA(Wit, ˆη) = Tk\nT\nK\n∑\nk=1\n[BAk(bηk)+VAk(bηk)].\nSince Tk ≍T, the claim holds by the triangle inequality and the union bound.\n■",
    "content_hash": "6511675c07b96cedbf1007763a8dde250672a40162e3914a90741593c37cf748",
    "location": null,
    "page_start": 45,
    "page_end": 45,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7e28c99a-e32c-4a7b-828d-d3de7c4f1460",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n11\nAPPENDIX B. TOOLS: TAIL BOUNDS FOR MAXIMA OF SUMS FOR WEAKLY DEPENDENT\nPANELS\nHere we collect and develop some useful lemmas, some of which can be of interest. B.1. Properties of Products of Sub-Gaussians. A random variable ξ is (σ2,α)-sub-Exponential\nif\nEeλξ ≤eλ 2σ2/2 a.s. ∀λ : |λ| ≤α−1. (B.1)\nA (σ2,0)-sub-Exponential is σ2-sub-Gaussian. Lemma B.2 states concentration inequality for a\nsub-Exponential martingale difference sequence (m.d.s.). Lemma B.1 (Properties of sub-Gaussian random variables). (1) Let σX,σY > 0. If X is σ2\nX-sub-\nGaussian and Y is σ2\nY-sub-Gaussian, then X +Y is (σX +σY)2-sub-Gaussian. (2) Let {Xm}M\nm=1 be\na sequence of σ2-sub-Gaussian random variables. Then, (2a) ∑M\nm=1 Xm is (M2σ2)-sub-Gaussian\nand (2b) max1≤m≤M Xm ≲P (σ√logd). (3) Furthermore, ∑M\nm=1 Xm is (Mσ2)-sub-Gaussian if\n{Xm}M\nm=1 are independent. (4) If Y ∈[−B,B] a.s.",
    "content_hash": "81701f61f42cc52dd32469a3027336d57191d778131381ccfbcfafd45f62b910",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "27817a16-d9f3-4f93-9346-f15af93884d0",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "The\nstatements (3) and (4) are Theorem 1.6 and Lemma 1.8 in Rigollet and Hutter (2017), respectively. To see (5), observe that E[X | Y] = 0 a.s. by assumption. LIE gives\nEX,Y[X ·Y] = EYE[X | Y]Y = 0. Furthermore,\nEYE[eλXY | Y] ≤EYeλ 2σ2Y 2/2 ≤eλ 2σ2B2/2,\nwhich gives the result. (6) Invoking union bound for any t > 0\nP\nmax\n1≤m≤M\n¯N\n∏\nn=1\n|Xmn| > t\n! ≤\nM\n∑\nm=1\nP\n|\n¯N\n∏\nn=1\nXmn| > t\n! ≤\nM\n∑\nm=1\n¯N\n∑\nn=1\nP\n\u0010\n|Xmn| > t1/ ¯N\u0011\n≤2M ¯Ne−t2/ ¯N/2 ¯σ2.",
    "content_hash": "9023c62b871346a3a83cd32c4c749e2efb9f5910d550d7255293e40291d04785",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8c6b2270-f551-4d4d-86b5-85eb0ee5b635",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": ", Y is B2-sub-Gaussian. (5) If X is σ2\nX-sub-\nGaussian conditional on Y, and Y ∈[−B,B] a.s. , then X ·Y is σ2\nXB2-sub-Gaussian. (6) If Xmn are\n¯σ2-sub-Gaussian for n = 1,2,..., ¯N ( ¯N ﬁnite) and m = 1,2,...,M, then max1≤m≤M ∏\n¯N\nn=1 |Xmn| ≲P\n((2 ¯σ) ¯N log ¯N/2(M ¯N)). Proof of Lemma B.1. We prove (1). By Holder inequality, for any p,q in [1,∞) such that 1/p +\n1/q = 1,\nEeλ(X+Y) ≤(Eeλ pX)1/p(EeλqY)1/q ≤eλ 2/2(pσ2\nX+qσ2\nY ). (B.2)\nPlugging p = (σY +σX)/σX and q = (σY +σX)/σY into (B.2) gives (4.3) with σ2 = (σX +σY)2. We prove (2a) by induction over M. The statement holds for M = 1. The inductive step follows\nfrom (1) with σX = (M−1)σ and σY = σ. (2b) is Theorem 1.14 in Rigollet and Hutter (2017).",
    "content_hash": "41edfe3bb55cbfb2fc184dce416b59a44b507d16c47799bd74df5eb6d40c3b50",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "81ed2da5-1351-4c57-93c1-29510216533c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "12\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nTaking t := ¯C(2 ¯σ) ¯N log ¯N/2(M ¯N) and setting ¯C →∞makes the R.H.S above o(1). Conclude that\nmax\n1≤m≤M\n¯N\n∏\nn=1\n|Xmn| ≲P ((2 ¯σ) ¯N log ¯N/2(M ¯N)). ■\nB.2. Tails Bounds for Maxima of Sums of Martingale Differences. Lemma B.2 (Martingale Difference Sequences; Theorem 2.19 in Wainwright (2019)). (1) Let\n{(ξm,Φm)}M\nm=1 be an m.d.s. obeying\nE[eλξm | Φm−1] ≤eλ 2σ2/2 a.s. for any λ such that |λ| ≤α−1. Then, the following bounds hold: (1) The sum ∑M\nm=1 ξm is (σ2M,α)-\nsub-Exponential and satisﬁes concentration inequality\nP\n\u0012\nM\n∑\nm=1\nξm\n≥t\n\u0013\n≤\n\n\n\n2e−t2/(2Mσ2),\n0 ≤t ≤Mσ2/α\n2e−t/(2α),\nt > Mσ2/α\n(2) For each j : 1 ≤j ≤d, let {(ξmj,Φm)}M\nm=1 be an m.d.s obeying the conditions above with the\nsame parameters (σ2,α).",
    "content_hash": "f9a23f6169e5e3ff643e7d6067f542ccc79c270c8bba7f7f9700e915f426f0d4",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "719c57d0-c1c1-4f5a-b82a-a5a856fdc429",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Then,\nP\n\u0012\n∥M−1\nM\n∑\nm=1\nξm∥∞> t\n\u0013\n≤\n\n\n\n2elogd−t2M/(2σ2),\n0 ≤t ≤σ2/α\n2elogd−tM/(2α),\nt > σ2/α\n(B.3)\nProof of Lemma B.2. Lemma B.2 is essentially Theorem 2.19 in Wainwright (2019). Replacing ξ\nby c·ξ in (B.1) shows that c·ξ is (c2σ2,cα)-sub-Exponential. ■\nLemma B.3. Let 1 ≤i ≤N and 1 ≤t ≤T be the unit and the time indices. Denote the index m as\nm = m(i,t) = T(i−1)+t. (B.4)\nConsider a sequence\nξm = VitUit,\nm = 1,2,...,M = NT. (B.5)\nUnder Assumption 4.3,\nA {ξm}M\nm=1 is a martingale difference sequence with respect to natural ﬁltration:\nE[ξm | Φm−1] := E[ξm | ξ1,...,ξm−1] = 0,\n∀m = 1,2,...M. B Given a large enough constant CVU large enough, there exists NT large enough such that\nthe maximal norm of the empirical moment vector obeys:\nP\n\u0010\n∥ENTVitUit∥∞> CVU\np\nlogd/NT\n\u0011\n≤2/d = o(1). (B.6)",
    "content_hash": "653a445b33f814aa3b08fe6754634a22415101c69a4f86054e398ccce18551fc",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3ec734e7-51f0-4559-85d8-50efd28bcdd9",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Deﬁne the\nodd blocks\nBi(2l−1) := [Wi,(2l−2)q+1,Wi,(2l−2)q+2,...,Wi,(2l−2)q+q],\nl = 1,2,...,L\n(B.8)\nthe even blocks\nBi(2l) := [Wi,(2l−1)q+1,Wi,(2l−1)q+2,...,Wi,(2l−1)q+q],\nl = 1,2,...,L\n(B.9)\nand the remainder block, which can be empty, as\nBir := [Wi,2Lq+1,Wi,2Lq+2,...,Wi,T]. (B.10)\nNote that {Bi(2l−1)}L\nl=1 obeys (A.11) and {Bi(2l)}L\nl=1 obeys (A.11) with ε = γ(q). Let B∗\ni(2l−1) be\nthe Berbee copy of Bi(2l−1). Deﬁne the Berbee event\nI1 := {B∗\ni(2l−1) = Bi(2l−1) for all i,l}.",
    "content_hash": "ae19b3fb28c15dfc169e7ab5b5704c415a590835909e03a2857b7c8d7e5c06cb",
    "location": null,
    "page_start": 48,
    "page_end": 48,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d5363fe2-f66b-429b-bf4a-1f5f3761840f",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For each j = 1,2,...,d, let φ j(Wit) be centered σ2-sub-Gaussian random variable for all\ni,t where σ = σ(N,T) and φj(Wit) can depend on N,T. Then,\n∥S∥∞:= max\n1≤j≤d\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nφj(Wit)\n≲P σ\np\nlog(NT)logd/NT,\n(B.7)\nRemark A.1 (Triangular Arrays). Note that all variables and σ can be indexed by (N,T), but we\nomit the indexing to keep the notation light. Thus, this lemma and all other lemmas stated below\napply to triangular arrays. Proof of Lemma B.4. Let q be the block size such that 1 ≤q ≤T/2 and let L = [T/2q].",
    "content_hash": "7ca502e8d2687d173362ef7375518fd510ac3e0b864018719634d0ec788334b3",
    "location": null,
    "page_start": 48,
    "page_end": 48,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5e855742-fce8-48b5-b9bd-dda443f8a38c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n13\nProof of Lemma B.3. By conditional sequential exogeneity (2.4) and independence over i\nE[Uit | ∪t<t′Vit′Uit′,∪j̸=i{(Vjt,Ujt)T\nt=1}] = 0\n∀i,t. Therefore, the martingale difference property A holds. Union bound and Assumption 4.3 imply\nP(|Vit,jUit| > t) ≤P(|Vit,j| >\n√\nt)+P(|Uit| >\n√\nt) ≤2e−t/2 ¯σ2. By Theorem 2.13 in Wainwright (2019), Vit,jUit is (σ2,α)-sub-Exponential for some σ,α > 0 that\ndo not depend on j, N or T. Since the cutoff point σ2/α in (B.3) does not depend on N,T, for\nCVU large enough and sample size NT\nt := CVU\np\nlogd/NT ≤σ2/α. The bound follows\nP\n\u0010\n∥ENTVitUit∥∞> CVU\np\nlogd/NT\n\u0011\n≤2/d = o(1). ■\nB.3. Tail Bounds for Maxima of Sums of Sub-Gaussian products. Lemma B.4 (Tail Bounds for Weakly Dependent Matrices, ℓ∞-norm). Suppose Assumption 4.1(1)\nholds.",
    "content_hash": "d2a6b3ba61b896e7527182680c7d15472ca30e1318e58ecc24fe7d8b53dffb88",
    "location": null,
    "page_start": 48,
    "page_end": 48,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "288b85c8-d468-4ab2-9c41-3f66082ec08e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(B.15)\nThus,\nP(∥S∥∞≥3t) ≤P(∥\nN\n∑\ni=1\nL\n∑\nl=1\nφ(B∗\ni(2l−1))∥∞≥t)\n+P(∥\nN\n∑\ni=1\nL\n∑\nl=1\nφ(B∗\ni(2l))∥∞≥t)+P(∥\nN\n∑\ni=1\nφ(Bir)∥∞≥t)+2NLγ(q). For each j, S∗\nodd j(q) is (NT)−2(NL)q2σ2 ≤(q/NT)σ2-sub-Gaussian by Lemma B.1; simi-\nlarly,for each j, S∗\neven j(q) is (q/NT)σ2-sub-Gaussian. Note that here the dependency on L is\nlinear and not square, because the Berbee blocks are independent. For the remainder block, for\neach j, Srem j(q) is (NT)−2(N)q2σ2 ≤(q/NT)σ2-sub-Gaussian since q ≤T by Lemma B.1,\nwhere we use only independence across i.Since S∗\nodd j(q) is (q/NT)σ2-sub-Gaussian for each j,\n∥S∗\nodd(q)∥∞≲P σ\np\nqlogd/NT by Lemma B.1 (2b). Likewise, ∥Srem(q)∥∞≲P σ\np\nqlogd/NT by\nLemma B.1 (2b).",
    "content_hash": "8969d55e5724e471c1460b9e733cef1d6e9ce42af65c57c7000c392122ce1531",
    "location": null,
    "page_start": 49,
    "page_end": 49,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a77aca91-ba47-4dba-90af-6e7f020afdfe",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "14\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nLikewise, let B∗\ni(2l) and I2 be the analogs of B∗\ni(2l−1) and I1 for even indices. Deﬁne the blockwise\nsum\nφ(B∗\ni(2l−1)) :=\nt=(2l−2)q+q\n∑\nt=(2l−2)q+1\nφ(W ∗\nit )\n(B.11)\nS∗\nodd(q) := (NT)−1\nN\n∑\ni=1\nL\n∑\nl=1\nφ(B∗\ni(2l−1)). (B.12)\nLet S∗\neven(q) be the analog of S∗\nodd(q) for even indices. If T ̸= 2Lq, the remainder block is non-\nempty, in which case deﬁne\nφ(Bir) :=\nT\n∑\nt=2Lq+1\nφ(Wit)\n(B.13)\nSrem(q) := (NT)−1\nN\n∑\ni=1\nφ(Bir). (B.14)\nOn the event I1 ∩I2, the union bound gives\n∥S∥∞≤∥S∗\nodd(q)∥∞+∥S∗\neven(q)∥∞+∥Srem(q)∥∞.",
    "content_hash": "e066db313abde11abe91506d4e207f0b92977dddd5e8dff0f932fc6e72eff3c4",
    "location": null,
    "page_start": 49,
    "page_end": 49,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "273ae1d3-2005-4e30-a117-e957be91fa5f",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Likewise, let S∗\nodd(q) be as in (B.11), that is,\nφ(B∗\ni(2l−1)) :=\nt=(2l−2)q+q\n∑\nt=(2l−2)q+1\nφ(W ∗\nit ),\nS∗\nodd(q) := (NT)−1\nN\n∑\ni=1\nL\n∑\nl=1\nφ(B∗\ni(2l−1)),",
    "content_hash": "9efd9746967baad48214a8fdf256c0a2bf3d7c9b3885b33e39276f033c7ebee7",
    "location": null,
    "page_start": 50,
    "page_end": 50,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "06e6ab29-bdf5-4854-8475-bd90d3b4f049",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n15\nThe following is an extension/clariﬁcation of a useful lemma due to Kock and Tang (2019). Lemma B.5 (Concentration of Products of sub-Gaussian Random Variables with Independent\nBlocks). Suppose the random variables Zn,m,v,j are uniformly ¯σ2\nn-sub-Gaussian as in (4.3) for\nn = 1,2,..., ¯N, ( ¯N ≥2 is ﬁxed and ﬁnite), m = 1,2,...,M, v = 1,2,...,V, and j = 1,2,...,d. Suppose Zn1,m1,v1,j1 and Zn2,m2,v2,j2 are independent as long as m1 ̸= m2 regardless of the values of\nother subscripts. Then,\nmax\nj,v,m E\n¯N\n∏\nn=1\nZn,m,v,j\n≤CA\n¯N\n∏\nn=1\n¯σn,\nfor some positive constant CA that depends on ¯N and with probability approaching 1,\nmax\n1≤j≤d\n(MV)−1\nM\n∑\nm=1\nV\n∑\nv=1\n\u0012\n¯N\n∏\nn=1\nZn,m,v,j −E\n¯N\n∏\nn=1\nZn,m,v,j\n\u0013\f\f\f\f ≤CV\nq\nlog ¯N+1(dV)/M\n¯N\n∏\nn=1\n¯σn,\nfor some positive constant CV that depends only on ¯N. Lemma B.6 (Concentration of Products of sub-Gaussian Random Variables under Weak Depen-\ndence).",
    "content_hash": "1c0a5604812dac82e99b6d08754f736f66cfd8ac0432c3ffff31dded445888c5",
    "location": null,
    "page_start": 50,
    "page_end": 50,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "158e8d89-6f7f-48eb-9a07-599a1141b65e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "16\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nS∗\neven(q) be its analog for the even-numbered blocks, and S∗\nrem(q) be as in (B.13). The ﬁrst claim\n(B.17) is immediate from the previous lemma. Lemma B.5 with L ≥2 and M = NL and V = q\nimplies that wp 1−o(1)\n∥S∗\nodd(q)∥∞: = (Lq/T)\n(NLq)−1\nN\n∑\ni=1\nL\n∑\nl=1\n{ϕ(B∗\ni(2l−1))−Eϕ(B∗\ni(2l−1))}\n∞\n≤(Lq/T)CV\n\u0012q\nlog ¯N+1(dq)/NL\n\u0013\n≤i CV\n\u0012q\nlog ¯N+1(dq)q/NT\n\u0013\n,\nwhere (i) follows from L = ⌊T/2q⌋≤T/2q and L ≥⌊T/2q⌋≥T/2q −1 ≥T/4q. A similar\nbounds holds for S∗\neven(q). If Trem ̸= 0, Lemma B.5 with M = N and V = Trem ≤q implies that wp\n1−o(1):\n∥Srem(q)∥∞:= Trem/T\n(NTrem)−1\nN\n∑\ni=1\n(ϕ(Bir)−Eϕ(Bir))\n∞\n≤Trem/TCV\n\u0012q\nlog ¯N+1(dTrem)/N\n\u0013\n≤q/TCV\n\u0012q\nlog ¯N+1(dq)/N\n\u0013\n.",
    "content_hash": "c9679f7f5e35fba762a827c094d265a753c120b58cdafff918e1c7bddbebf4c6",
    "location": null,
    "page_start": 51,
    "page_end": 51,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7fd0d795-ff73-42f0-8ce0-49904bda8cac",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Then,\nmax\n1≤k,j≤d max\ni,t\nE\n\" ¯N\n∏\nn=1\nZ1,nitkZ2,nit jU2g\nit\n#\f\f\f\f\f ≤CA( ¯σ1 ¯σ2 ¯σ2g) ¯N\n(B.19)\nmax\n1≤k,j≤d\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n\" ¯N\n∏\nn=1\nZ1,nitkZ2,nit jU2g\nit −E\n\" ¯N\n∏\nn=1\nZ1,nitkZ2,nit jU2g\nit\n##\n≤¯CV( ¯σ1 ¯σ2 ¯σ2g) ¯N\n\u0012q\nlog2 ¯N+2g+1(d2 log(NT))log(NT)/NT\n\u0013\n(B.20)\nRemark A.2. Suppose Assumptions 4.1 and 4.3 hold. Invoking (B.19) with ¯N = 1 and Z1,it =\nZ2,it = Vit implies for some ﬁnite σV < ∞,\nmax\nit ∥EVitV ′\nit∥∞≤max\nit j EV 2\nit j ≤σ2\nV.",
    "content_hash": "575c7d43f8e7eb5e1ba7c881ff60a5c1db1cd0c5b82e200e832a91a0aa81f382",
    "location": null,
    "page_start": 51,
    "page_end": 51,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "059c7e74-ffe6-4eb2-a082-9a615d6681fb",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Plugging q2/NT 2 ≤q/NT into the R.H.S above gives the bound CV\n\u0012q\nlog ¯N+1(dq)q/NT\n\u0013\n. Let\nNT be large enough so that L = ⌊T/2q⌋≥2 and (2/κ) ≤log(NT) so that q ≤log2(NT) and\ndq ≤(d log(NT))2. Collecting the bounds gives (B.18). Adding up the bounds and plugging\nchoice of q = (2/κ)log(NT) as in (B.16) and noting that L = ⌊T/2q⌋≥2 for T large enough\ngives (B.18). ■\nCorollary B.4. Suppose Assumption 4.1 (1) holds. Suppose Z1,nit and Z2,nit are d-vectors obtained\nas (measurable) transformations of Wit, whose entries are uniformly ¯σ2\n1 and ¯σ2\n2-sub-Gaussian for\nn = 1,2,..., ¯N. Let Uit be uniformly ¯σ2-sub-Gaussian and g ≥0 be a ﬁnite power.",
    "content_hash": "e96ca2eb5b49bef867243653fbd59937c897df9ae353df8ffaa8c72df5cad56f",
    "location": null,
    "page_start": 51,
    "page_end": 51,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "533293f1-b7cf-483f-ab1d-1449a97c5c1f",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Assume that (Xi,Fi)n\ni=1 is\na sequence of martingale differences satisfying supi E\n\u0014\neD|Xi|\n2α\n1−α\n\u0015\n≤C1 for some positive constant\nD, where C1 ≥1 can change with the sample size n. Then, for all ε ≥1/√n,\nP\nn\n∑\ni=1\nXi\n≥nε\n! ≤C1A(α)e−K(ε2n)\nα\n,\nK = (D\n1−α\n2α /4)2α. where\nA = A(α) = 2+35\n\"\n1\n161−α +\n\u00123(1−α)\n2α\n\u0013 1−α\nα\n#\n. ■\nLemma B.8 restates Proposition F.2 in Kock and Tang (2019) with explicit constants in Tail\nBounds. Proof. Note that for some positive constant D,\nP\nn\n∑\ni=1\nXi ≥nε\n! = P\nn\n∑\ni=1\nD\n1−α\n2α Xi ≥nD\n1−α\n2α ε\n! = P\nn\n∑\ni=1\nYi ≥nδ\n! ,",
    "content_hash": "4458235c7a5c6cb8b63344563aab23a18179ad78b3d298dd3635ada9ce77aef8",
    "location": null,
    "page_start": 52,
    "page_end": 52,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5302fb02-6d6f-4e76-9841-d8d5acba7f0a",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n17\nLikewise, Assumption 4.3 implies for some ﬁnite σVU < ∞,\nsup\nit\nE[U2\nit | Vit] ≤σ2\nVU a.s. . B.4. Some Technical Lemmas. Here we provide technical extensions of the results in Kock and\nTang (2019), keeping the notation as in the original Kock and Tang (2019) and references therein. Lemma B.7 (Theorem 2.1 in Fan et al. (2012), Proposition F.1 in Kock and Tang (2019) ). . Let α ∈(0,1). Assume that (Xi,Fi)n\ni=1 is a sequence of supermartingale differences satisfying\nsupi E\n\u0014\ne|Xi|\n2α\n1−α\n\u0015\n≤C1 for some constant C1 ∈(0,∞). Deﬁne Sk := ∑k\ni=1 Xi. Then, for all ε > 0,\nP\n\u0012\nmax\n1≤k≤nSk ≥nε\n\u0013\n≤C(α,n,ε)e−(ε/4)2αnα\nwhere\nC(α,n,ε) := 2+35C1\n\"\n1\n161−α (nε2)α + 1\nnε2\n\u00123(1−α)\n2α\n\u0013 1−α\nα\n#\n. Lemma B.8 (Proposition F.2 in Kock and Tang (2019)). Let α ∈(0,1).",
    "content_hash": "590aaff42ad7f358c70694d6cd1793ce55d7fa9dc56373ed3a5648952630f308",
    "location": null,
    "page_start": 52,
    "page_end": 52,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "518620fd-42ff-4754-b8b4-195c3b9fc97c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "This inequality is not true\n(for example, with x = 10, y = 1, L = 4, the inequality implies 3 < 2.163.), so we changed the middle part of the proof;\nthe end result is preserved; none of conclusions in KT are affected.",
    "content_hash": "b43332fa4598575a2aaa69362b7a78fb54973b93bd19f6aa74290a0b915884f5",
    "location": null,
    "page_start": 53,
    "page_end": 53,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c24c4f6b-fa36-494d-a644-f25f7d134020",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "18\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nwhereYi := D\n1−α\n2α Xi and δ := D\n1−α\n2α ε. Now (Yi)n\ni=1 is a sequence of martingale differences satisfying\nsupi E\n\u0014\ne|Yi|\n2α\n1−α\n\u0015\n≤C1. Invoking the preceding theorem, we have\nP\nn\n∑\ni=1\nYi ≥nδ\n! ≤C(α,n,δ)e−(δ/4)2αnα. (−Yi)n\ni=1 is also a sequence of martingale differences satisfying the same exponential moment\ncondition. Thus,\nP\nn\n∑\ni=1\nXi\n≥nε\n! = P\nn\n∑\ni=1\nYi\n≥nδ\n! ≤2C(α,n,δ)e−(δ/4)2αnα\n= 2C\n\u0010\nα,n,D\n1−α\n2α ε\n\u0011\ne\n−\n\u0012\nD\n1−α\n2α ε/4\n\u00132α\nnα\n≤C1A(α)e−Kε2αnα,\nwhere we can select\nA = A(α) = 2+35\n\"\n1\n161−α +\n\u00123(1−α)\n2α\n\u0013 1−α\nα\n#\nand K as deﬁned above. ■\nThe following Lemma is inspired by Prop F.3 of Kock and Tang (2019). The difference is that\nthe constants are made explicit to make the results applicable to arrays; and part of the proof was\nreplaced by another argument (as we were not able to follow one step in their proof). 9\nLemma B.9.",
    "content_hash": "99dcdfead2bb65005bb8db3c4f4efb180ce66fb969a5b5fbd33004f9281a65ad",
    "location": null,
    "page_start": 53,
    "page_end": 53,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c678a387-e378-47c6-b080-06c776405943",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Suppose we have random variables Zl,i,t,j uniformly (K,σ2\nl ) > 0 subgaussian for\nl = 1,...,L(L ≥2 ﬁxed ),i = 1,...,N,t = 1,...,T and j = 1,..., p that is,\nP(|σ−1\nl\nZl,t,i,j| ≥ε) ≤K exp(−ε2),\nand Zl2,i2,t2,j2 are independent as long as i1 ̸= i2 regardless of the values of other subscripts. Then,\nwe have that (1)\nmax\nj,t,i E\nL\n∏\nl=1\nZl,i,t,j\n≤\n\u0010\nL!(log2)−1/2(1+K)1/2\u0011 L\n∏\nl=1\nσl,\nand (2) with probability 1−A′(pT)−1/2,\nmax\n1≤j≤d\n1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nL\n∏\nl=1\nZl,i,t,j −E\n\"\nL\n∏\nl=1\nZl,i,t,j\n#! ≤M\nr\n(log(pT))L+1\nN\n! L\n∏\nl=1\nσl,\nfor M > M′, and some positive constants A′ and M′ that only depend on L and K. 9KT’s proof uses the inequality (x−(y∧x))2/L ≤x2/L −(y∧x)2/L, for x > 0 and y > 0.",
    "content_hash": "df1cb04bbd8e6fa5c2468056163f393df453167e72e693955eb49407b0351c85",
    "location": null,
    "page_start": 53,
    "page_end": 53,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "54725065-9808-4cd9-b5e9-bbd299e5a3d1",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n19\nProof. H¨older’s inequality gives\nmax\nj,t,i E\nL\n∏\nl=1\nσ−1\nl\nZl,i,t,j\n≤max\nj,t,i\nL\n∏\nl=1\n\u0010\nE\nσ−1\nl\nZl,i,t,j\nL\u0011 1\nL ,\nwhere\n\u0010\nE\nσ−1\nl\nZl,i,t,j\nL\u0011 1\nL ≤L! σ−1\nl\nZl,i,t,j\nψ1\n≤L!(log2)−1/2 \r\rσ−1\nl\nZl,i,t,j\nψ2 ≤L!(log2)−1/2 (1+K)1/2 =: A,\nwhere the ﬁrst two inequalities are from van der Vaart and Wellner (1996), p.95 and the third\ninequality from Lemma 2.2.1 in van der Vaart and Wellner (1996). Thus,\nmax\nj,t,i E\nL\n∏\nl=1\nσ−1\nl\nZl,i,t,j\n≤\n\u0010\nL!(log2)−1/2(1+K)1/2\u0011\n=: A. This implies the ﬁrst claim, after multiplying both sides by ∏L\nl=1 σl. Let\nXi,t,j =\nL\n∏\nl=1\nσ−1\nl\nZl,i,t,j −E\n\"\nL\n∏\nl=1\nσ−1\nl\nZl,i,t,j\n#\n. For every ε ≥0,\nP\nXi,t,j\n≥2ε\n\u0001\n≤P\nL\n∏\nl=1\nσ−1\nl\nZl,i,t,j\n≥ε\n!",
    "content_hash": "959879e39065a0bbc49bb24e393abca56c05f6a20a9aed87a1aecd07bbbe3fc5",
    "location": null,
    "page_start": 54,
    "page_end": 54,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a8412091-94ae-4644-a95b-e52e591bc391",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "20\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nThen we can use independence across i to invoke the previous Lemma B.8 with α =\n1\nL+1 and\nC1 = BT, for ε ≥\n1\n√\nN\nP\nN\n∑\ni=1\n1\nT\nT\n∑\nt=1\nXi,t,j\n≥2Nε\n!\n≤A′Te−K′′(ε2N)\n1\nL+1\nfor positive constants A′ and K′′ that depend only on K, L, and D.\nSetting\nε =\nr\nM(log(pT))L+1\nN\nfor some M ≥1, we have\nP\nmax\n1≤j≤p\nN\n∑\ni=1\n1\nT\nT\n∑\nt=1\nXi,t,j\n≥2Nε\n!\n≤pA′Te−K′′(ε2N)\n1\nL+1 = A′(pT)1−K′′M\n1\nL+1.\nTherefore, with probability 1−A′(pT)1−K′′M\n1\nL+1,\nmax\nj\n1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nXi,t,j\n\u0001\n≤2M\nr\n(log(pT))L+1\nN\n!\n,\nfor any M ≥1. Setting M large enough such that\n1−K′′M\n1\nL+1 < −1\n2,\nguarantees that the bound holds with probability at most\nA′(pT)−1/2,\nwhich decreases to zero if T →0. The bounds can be then be restated as in the statement of the\ntheorem.\n■\n■",
    "content_hash": "df60a41ee221d0b8113c747c8cc09a81ebc3375571ec155266a27bb1a039637f",
    "location": null,
    "page_start": 55,
    "page_end": 55,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "afb316dd-6bb5-452a-817d-19ac3b667673",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n21\nAPPENDIX C. TOOLS: HIGH-DIMENSIONAL CENTRAL LIMIT THEOREMS FOR WEAKLY\nDEPENDENT DATA\nLet {Xm}M\nm=1 be a weakly dependent martingale difference sequence (m.d.s.) with respect to\nnatural ﬁltration. Deﬁne its β-mixing coefﬁcient\nγX(q) = sup\nm≤M\nγ((X1,...,Xm−1,Xm),(Xm+q,Xm+q+1,...)). The scaled sum\nSX = M−1/2\nM\n∑\nm=1\nXm\nhas the variance\nΣG := M−1\nM\n∑\nm=1\nEXmX′\nm. (C.1)\nThe distribution of the scaled sum over the cubes can be approximated by the Gaussian distribution\nN(0,ΣG) over the cubes, as shown in the lemma below. We will introduce the following notation.",
    "content_hash": "3e23d600302f2c4288d0b2c948b864e184108514ab637f58c01d8151ba07a5de",
    "location": null,
    "page_start": 56,
    "page_end": 56,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "736cc568-de96-4d29-be2f-3512fa80c4dc",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Then,\nsup\nt≥0\n|P(∥X∥∞≤t)−P(∥Y∥∞≤t)| ≤C′(∆XY log2(2d))1/2,\n(C.3)",
    "content_hash": "be3f4d257da6e159176c0813c7f6119dce9659a673ccbe482bba6a7e97a12031",
    "location": null,
    "page_start": 56,
    "page_end": 56,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "dcca376f-634d-4ee2-995c-ff63cacfc1ec",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For some numbers ¯r = ¯rNT, ¯q = ¯qNT and L = ⌊M/( ¯q+\n¯r)⌋, deﬁne Bernstein’s ”large” and ”small” blocks of size ¯q and ¯r:\nPl = {(l −1)( ¯q+ ¯r)+1,...,(l −1)( ¯q+ ¯r)+ ¯q},\nl = 1,2,...,L\nQl = {(l −1)( ¯q+ ¯r)+1+ ¯q,...,l( ¯q+ ¯r)}\nand let\nSl := ∑\nm∈Pl\nXm,\nUl := ∑\nm∈Ql\nXm,\nUL+1 :=\nM\n∑\nm=L( ¯q+¯r)+1\nXm. Denote\nΣP := (L ¯q)−1\nL\n∑\nl=1\nESlS′\nl = (L ¯q)−1\nL\n∑\nl=1 ∑\nm∈Pl\nEXmX′\nm\n(C.2)\nand observe that\nΣG = (L ¯q/M)ΣP +M−1\nL+1\n∑\nl=1\nEUlU′\nl . The following result is useful both in the proof below and also for performing Gaussian infer-\nence, where we replaced unknown variance-covariance matrix by an estimated one. Lemma C.1 (Comparison of distributions). Let X ∼N(0,ΣX) and Y ∼N(0,ΣY) be centered nor-\nmal d-vectors, and let ∆XY := ∥ΣX −ΣY∥∞. Suppose min1≤j≤d(ΣY)j j > 0.",
    "content_hash": "ae87e38b3a8262aa58f2e595d8e0a3af10bd385269fac8c2d6269ed28a4cbb57",
    "location": null,
    "page_start": 56,
    "page_end": 56,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "897e4d1e-959b-452d-aea6-c1267fc42292",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "The following result is a consequence of Theorem E.1 in Chernozhukov et al. (2019a) for mar-\ntingale difference sequence. Lemma C.3 (High-dimensional CLT for martingale difference sequence under weak dependence). Let {Xm}M\nm=1 be a weakly dependent m.d.s. of d-vectors obeying for DM ≥1:\nsup\nm≤M\n∥Xm∥∞≤DM a.s. Suppose there exist constants 0 < a1 ≤A1 and 0 < c2 < 1/4 such that\na1 ≤min\n1≤j≤d min\n1≤m≤MVarXm j ≤max\n1≤j≤d\nsup\n1≤m≤M\nVarXm j ≤A1,\n(C.5)\nand let ¯r and ¯q be such that ¯r/ ¯q ≤A1M−c2 log−2 d and\nmax{¯rDM log3/2 d, ¯qDM log1/2 d,√¯qDM log7/2(dM)} ≤A1M1/2−c2. (C.6)\nThen, there exist constants cX,CX > 0 depending only on a1,A1,c2 such that\nsup\nt≥0\nP(∥SX∥∞≤t)−P(∥GP∥∞< t)\n≤2 M\n¯q+ ¯rγX(¯r)+CXM−cX,\n(C.7)\nwhere GP ∼N(0,ΣP) is a centered normal d-vector.",
    "content_hash": "968931df2ed4544b6fd82a752f16637c62726db4597ca0eee151297d505ab905",
    "location": null,
    "page_start": 57,
    "page_end": 57,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "34020712-8de5-4157-aa77-9b29391c3c62",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "22\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nwhere C′ > 0 depends only on min1≤j≤d(ΣY)j j and max1≤j≤d(ΣY)j j. Lemma C.1 follows from Proposition 2.1 in Chernozhukov et al. (2019b) for vectors X =\n(X,−X) and Y = (Y,−Y) and\nΣX =\nΣX\n−ΣX\n−ΣX\nΣX\n! ,\nΣY =\nΣY\n−ΣY\n−ΣY\nΣY\n! ,\n∥ΣX −ΣY∥∞= ∆XY. Another result is the following anti-concentration property. This result is useful for showing that\nlinearization errors do not impact the behavior of the key statistics. The statistics are approximate\nmeans, namely averages of some centered inﬂuence functions plus linearization errors. Lemma C.2 (Anti-concentration). Let X = (X1,X2,...,Xd)′ ∼N(0,ΣX) be a centered Gaussian\nrandom vector in Rd. Assume min1≤j≤d(ΣX)j j > 0. Then,\nsup\nt∈R\nP(|∥X∥∞−t| ≤ε) ≤Cε\np\n1∨log(2d/ε),\n(C.4)\nwhere C > 0 depends on min1≤j≤d(ΣX)j j and max1≤j≤d(ΣX)j j. Lemma C.2 follows from Corollary 1 in Chernozhukov et al. (2015a) with X = (X,−X).",
    "content_hash": "18b284395d339b59f0de15bcebe6edfa987f3cf508a4124b017afac22fac5d60",
    "location": null,
    "page_start": 57,
    "page_end": 57,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7f4d5158-add6-4843-9917-c9e3af6f3f1b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n23\nNote that this result uses ΣP as the variance in the Gaussian approximation. In our application,\nwe will be using ΣG in place of ΣP (i.e., Lemma C.5) so as not to worry about omitting small\nblocks. Therefore below, we will provide a sequence of the results that allow this replacement. Proof of Lemma C.3. Let\nXm := (Xm,−Xm),\nm = 1,2,...,M\nbe a sequence of 2d-vectors. Observe that {Xm}M\nm=1 is an m.d.s. It obeys\nsup\nm≤M\n∥Xm∥∞≤DM, a.s.,\nγ ¯X(q) = γX(q)\n∀q. By construction, for any integer r\n¯σ2(r) = max\n1≤j≤d max\nI\nVar(r−1/2 ∑\nm∈I\nXm j) = max\n1≤j≤2d max\nI\nVar(r−1/2 ∑\nm∈I\nXm j),\nσ2(r) = min\n1≤j≤d min\nI Var(r−1/2 ∑\nm∈I\nXm j) = min\n1≤j≤2d min\nI Var(r−1/2 ∑\nm∈I\nXm j),\nwhere maxI and minI are taken over the sets I = {i+1,i+2,...,i+r} of size r. Theorem E.1 in\nChernozhukov et al. (2019a) requires\na1 ≤σ2( ¯q) ≤¯σ2( ¯q)∨¯σ2(¯r) ≤A1.",
    "content_hash": "238823428bee63d3378d18015de6d6b2d6c047c98142b2de5035c26e2a362e83",
    "location": null,
    "page_start": 58,
    "page_end": 58,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "584f02fe-56e4-4621-9b5d-5e0f61c34b99",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "24\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nLemma C.4 (Comparison of distributions, cont.). Consider the setup above with ΣX = ΣG and\nΣY = ΣP, where ΣG and ΣP are as in (C.2) and (C.1) where\nsup\n1≤m≤M\n∥EXmX′\nm∥∞≤\nsup\n1≤m≤M\nsup\n1≤j≤d\nVar(Xm j) ≤A1. For some c2 ∈(0,1/4), assume that the growth condition holds:\nDM logd logM log7/2(dM) ≲M1/2−2c2. and log4 d log2 M = o(\n√\nM). Then, the max distance ∆GP := ∥ΣG −ΣP∥∞obeys\n(∆GP log2 d)1/2 ≲M−c2/2. Proof of Lemma C.4. Observe that\nΣG −ΣP = (L ¯q/M −1)ΣP +M−1\nL+1\n∑\nl=1\nEUlU′\nl . Since L = ⌊M/( ¯q+ ¯r)⌋,L ≥M/( ¯q+ ¯r)−1. Therefore,\n1−L ¯q/M ≤1−¯q/( ¯q+ ¯r)+ ¯q/M = ¯r/( ¯q+ ¯r)+ ¯q/M ≤¯r/ ¯q+ ¯q/M. Furthermore, (L+1)/M ≤2L/M ≤2/ ¯q.",
    "content_hash": "934352ff5b64390f7d5ca827e236de45eb4500c8856ac9abafc3a343ce4aef40",
    "location": null,
    "page_start": 59,
    "page_end": 59,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3d34e23a-5b34-4e25-a6fb-d3e3cd891822",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "The following bound holds\n∆GP ≤((1−L ¯q/M)+(L+1)/M) sup\n1≤m≤M\n∥EXmX′\nm∥∞= O(¯r/ ¯q∨¯q/M ∨1/ ¯q),\nTaking ¯q = Mc2 log2 d log2 M and ¯r = (2/κ)logM gives:\n¯r/ ¯q = (2/κ)M−c2 log−2 d log−1 M = o(M−c2 log−2 d)\n¯q/M = Mc2−1 log2 d log2 M =i o(M−c2 log−2 d)\n1/ ¯q = M−c2 log−2 d log−2 M = o(M−c2 log−2 d),\nwhere (i) follows from c2 < 1/4 and\nlog4 d log2 M = o(M1−2·1/4) = o(M1−2c2)\nPlugging ∆GP = o(M−c2 log−2 d) into (∆GP log2(2d))1/2 gives\n(∆GP log2(2d))1/2 = o(M−c2/2). ■\nRemark A.1 (Sufﬁcient Growth Condition). If the growth condition holds\nDM logd logM log7/2(dM) ≲M1/2−2c2\n(C.9)\nholds, then\n¯r = logM,\n¯q = Mc2 log2 d log2 M\n(C.10)\nobeys (C.6) and ¯r/ ¯q ≤A1M−c2 log−2 d for M large enough.",
    "content_hash": "fa191f2db0d48e21e68e91e775e26b13634dfc71d68c4fbe07b51c9f2e615d4f",
    "location": null,
    "page_start": 59,
    "page_end": 59,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "837dd196-dacf-4bd1-83d4-a12a35b232a6",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n25\nProof of Remark A.1. Let M be large enough such that M−c2/2 ≤A1 and (2/κ)log−1 M ≤A1. Then, the growth condition\nDM logd logM log7/2(dM) ≲M1/2−2c2 ≤A1M1/2−3/2c2\nimplies the third inequality in (C.6)\n√¯qDM log7/2(dM) ≤A1M1/2−c2. Next, for d ≥e such that logd ≥1, and\nM−c2DM ¯qlog1/2 d = DM log5/2 d log2 M\n≤DM log5/2(dM)logM log(dM)logd ≤A1M1/2−3/2c2. Multiplying both sides by Mc2 gives\nDM ¯qlog1/2 d ≤A1M1/2−c2,\nwhich coincides with the second inequality in (C.6). Finally,\nDM ¯rlog3/2 d = (2/κ)DM logM log3/2 d ≤DM ¯qlog1/2 d,\nas long as (2/κ) ≤logM, which veriﬁes (C.6). For M large enough ¯r/ ¯q = 2/κM−c2 log−2 d log−1 M ≤\nA1. ■\nLemma C.5 (Summary). Let {Xm}M\nm=1 be a weakly dependent m.d.s. of d-vectors obeying for\nDM ≥1:\nsup\nm≤M\n∥Xm∥∞≤DM a.s.",
    "content_hash": "564d78b237eb987ee0eaeefa5f66b14694eb9e5b0f52e32a5a88339077c0be28",
    "location": null,
    "page_start": 60,
    "page_end": 60,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bb97268f-53d9-44fc-92c1-a6d8910dbb25",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Suppose there exist constants 0 < a1 ≤A1 such that\na1 ≤min\n1≤j≤d min\n1≤m≤MVarXm j ≤max\n1≤j≤d\nsup\n1≤m≤M\nVarXm j ≤A1. For some constant c2 ∈(0,1/4), the growth condition (C.5) holds, namely\nDM logd logM log7/2(dM) ≲M1/2−2c2. and log4 d log2 M = o(M1/2). Then, there exist constants cX,CX > 0 depending only on a1,A1,c2\nsuch that for ¯r = (2/κ logM) and ¯q = Mc2 log2 d log2 M\nsup\nt≥0\n|P(∥SX∥∞≤t)−P(∥GΣ∥∞< t)| ≲CXM−cX +M−c2/2,\n(C.11)\nwhere GΣ ∼N(0,ΣG) is a centered normal d-vector.",
    "content_hash": "1758b8b11010d5247d8b8f0964ddf64036a852c7cf4bbc06da94d5583bc08217",
    "location": null,
    "page_start": 60,
    "page_end": 60,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e260ae18-02aa-45bc-94a2-dd49e476e605",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(D.1)\nDeﬁne the ﬁrst-order error terms\n¯a := ENTVit(di0(Xit)−bdi(Xit)) = ENTVit(bVit −Vit)\n(D.2)\n¯m = ENTVit(li0(Xit)−bli(Xit)) = ENTVit(beY it −eYit)\n(D.3)\n¯f = ENTUit(di0(Xit)−bdi(Xit)) = ENTUit(bVit −Vit),\n(D.4)\n¯e = ENTVitRit(bd,bl) = ¯m−¯a′β0. (D.5)\nthe second-order error terms\n¯b = ENT(di0(Xit)−bdi(Xit))(di0(Xit)−bdi(Xit))′\n(D.6)\n¯z = ENT(di0(Xit)−bdi(Xit))(li0(Xit)−bli(Xit))\n(D.7)\n¯g = ENT(di0(Xit)−bdi(Xit))Rit(bd,bl) = ¯z−¯b′β0. (D.8)\nLemma D.1 (First-Order Terms).",
    "content_hash": "7d02070410b2dba944cb5475aa98e7ab9fde3475284a5dfcac6968d74b6b452d",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "83b3cdbd-1e5e-45b3-a176-18e1d2970eff",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "26\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nTriangular inequality gives\nsup\nt≥0\n|P(∥SX∥∞≤t)−P(∥GΣ∥∞< t)|\n(C.12)\n≤sup\nt≥0\n|P(∥SX∥∞≤t)−P(∥GP∥∞< t)|+sup\nt≥0\n|P(∥GP∥∞< t)−P(∥GΣ∥∞< t)|\n≲2 M\n¯q+ ¯rγ(¯r)+CXM−cX +M−c2/2 = o(M−c2/2 +M−cX). (C.13)\nAPPENDIX D. PROOFS FOR SECTION 4\nD.1. Bounds on Errors for Estimating Q and Gradient S. Below, we deﬁne the following terms\nthat appear in the analysis of bQ and the least squares gradient S. In what follows we use the\nnotations deﬁned in the main text heavily, without further warning. Deﬁne the ﬁrst-stage approximation error as a function of d(·) and l(·):\nRit(d,l) := li0(Xit)−li(Xit)−(di0(Xit)−di(Xit))′β0.",
    "content_hash": "d85af0177e7ed68aebc186cddfd38c2e6418e12a35b90e19fad573723dd2abac",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "82f8cc78-5719-446c-bf21-cf2aba76801e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Under Assumptions 4.1–4.5, we have that\n∥¯a∥∞≲P\n\u0010\ndNT,∞\np\nlog(dNT)/NT\n\u0011\n(D.9)\n∥¯m∥∞≲P\n\u0010\nlNT,∞\np\nlog(dNT)/NT\n\u0011\n(D.10)\n∥¯f∥∞≲P (dNT,∞\np\nlog(dNT)/NT)\n(D.11)\n∥¯e∥∞≲P (\np\nlog(dNT)/NT(dNT,∞∥β0∥1 +lNT,∞)). (D.12)\nProof of Lemma D.1. Deﬁne\nζV\nNT := dNT,∞\np\nlog(dNT)/NT,\nζ B\nNT = 0,",
    "content_hash": "313bef02f7a6d4b829edb36ed5b3fdccae707147051c0e9939b4e8d475031434",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ebc38d01-4ab7-4fc3-9e49-dabd1d72131e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n27\nand the A-function as\nA(Wit,η) = Vit(di0(Xit)−di(Xit)). Deﬁne BAk(η) and VAk(η) with η = d as in (A.16)–(A.17). Consider any η = ηNT ∈DNT in what follows. SinceVit obeys the martingale difference property\nby assumption, we have that\nE[Vit | ∪t′≤t,t′∈Mk(Vit′,Xit′)] = 0,\n(D.13)\nand it follows that ∥BAk(ηNT)∥∞= 0. By Assumption 4.3 and Lemma B.1, each entry ofVit(di0(Xit)−\ndi(Xit)) is ¯σ2d2\nNT,∞-sub-Gaussian. Invoking Lemma B.2 gives\n∥VAk(ηNT)∥∞≲P ( ¯σdNT,∞\np\nlogd/NTk) = oP(ζV\nNT)\nsince Tk ≍T (as we keep number of blocks K ﬁxed). By Assumption 4.5, we have that P(bdk ∈\nDNT, ∀k = 1,...,K) →1. We conclude by Lemma A.8 that (D.9) holds.",
    "content_hash": "989fb2b5ba8287c00c3a33959e33a16ebf1cce03a99915e7fa0f0782e102e296",
    "location": null,
    "page_start": 62,
    "page_end": 62,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f202535e-bbd0-406a-b754-a8c07a0a4a1f",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Repeating the same\nargument for\nA(Wit,η) = Vit(li0(Xit)−li(Xit)) and A(Wit,η) = Uit(di0(Xit)−di(Xit))\nestablishes claims (D.10) and (D.11). Finally, (D.12) holds by deﬁnition of ¯e = ¯m −¯a′β0 and\nHolder inequalities. ■\nLemma D.2 (Second-Order Term). Under Assumptions 4.1–4.5, we have that\n∥¯z∥∞≲P\n\u0012\ndNTlNT +dNT,∞lNT,∞\nq\n(NT)−1 log(NT)logd\n\u0013\n(D.14)\n∥¯b∥∞≲P\n\u0012\nd2\nNT +d2\nNT,∞\nq\n(NT)−1 log(NT)logd\n\u0013\n(D.15)\n∥¯g∥∞≲P\n\u0012\n∥β0∥1d2\nNT +dNTlNT +(∥β0∥1d2\nNT,∞+dNT,∞lNT,∞)\nq\n(NT)−1 log(NT)logd\n\u0013\n(D.16)\nProof of Lemma D.2. Deﬁne the A-function as\nA(Wit,η) = (di0(Xit)−di(Xit))(li0(Xit)−li(Xit)),\nη = (d,l). Let BAk(η) and VAk(η) be deﬁned according to (A.16)–(A.17). Let\nζ B\nNT = lNTdNT,\nζV\nNT =\nq\nl2\nNT,∞d2\nNT,∞logd logNT/NT.",
    "content_hash": "69a707b54360f10ece59c97db247f465e066c84063fd194fc745e98dfc2dd434",
    "location": null,
    "page_start": 62,
    "page_end": 62,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cf145677-af95-4d68-a217-0f55fe61340c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For any i and t, m and j, the Cauchy-Schwarz inequality gives\nE[|(di0(Xit)−di(Xit))m(li0(Xit)−li(Xit))|]\n≤\nq\nE(di0(Xit)−di(Xit))2mE(li0(Xit)−li(Xit))2 =:\nq\na2\nitb2\nit = |ait||bit|",
    "content_hash": "cfc101a97e55b6bb68fcb72473474b4ffd0f66ee3e5341829519faa805be43fc",
    "location": null,
    "page_start": 62,
    "page_end": 62,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d3ab4229-790c-44c2-8ed9-ee05e3b08c23",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "We obtain\n∥¯g∥∞≲P\n\u0012\n∥β0∥1\n\u0012\nd2\nNT +d2\nNT,∞\nq\n(NT)−1 log(NT)logd\n\u0013\n+dNTlNT +dNT,∞lNT,∞\nq\n(NT)−1 log(NT)logd\n\u0013\nThen we rewrite the bound as in (4.17). ■\nDeﬁne\nbQ = ENT bVit bV ′\nit,\neQ = ENTVitV ′\nit,\nbS := ENT bVit(beY it −bV ′\nitβ0),\nS := ENTVitUit\nand the following rates\nκNT :=\nq\nlog3 (d2 log(NT))log(NT)/NT\n(D.17)\nqNT := dNT,∞\np\nlog(dNT)/NT +d2\nNT +d2\nNT,∞\np\nlog(NT)log(d)/NT\n(D.18)\nWe will also use the following rates deﬁned in the Section 4 of main text\nρNT := dNT,∞\np\nlog(dNT)/NT +\np\nlog(dNT)/NT(dNT,∞∥β0∥1 +lNT,∞)+rNT\nrNT := ∥β0∥1d2\nNT +dNTlNT +(∥β0∥1d2\nNT,∞+lNT,∞)\nq\n(NT)−1 log(NT)logd",
    "content_hash": "0c62f886c5fae962ee240d5563de53eea192d66a8b0ded97f1f8b0ed30443cb5",
    "location": null,
    "page_start": 63,
    "page_end": 63,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5e49c838-9a87-43ae-a67f-b9904af5d571",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "28\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nAnother application of the Cauchy-Schwarz gives\n(TkN)−1∑\ni ∑\nt∈Mk\n|ait||bit| ≤\nv\nu\nu\nt(TkN)−1\nN\n∑\ni=1 ∑\nt∈Mk\na2\nit\nv\nu\nu\nt(TkN)−1\nN\n∑\ni=1 ∑\nt∈Mk\nb2\nit\n≤\ns\n(TkN)−1\nN\n∑\ni=1\nT\n∑\nt=1\na2\nit\ns\n(TkN)−1\nN\n∑\ni=1\nT\n∑\nt=1\nb2\nit ≤dNTlNTT/Tk. Therefore ∥BAk(ηNT)∥∞= O(ζ B\nNT). Furthermore, each entry of A(Wit,η) is bounded by dNT,∞lNT,∞,\nand, therefore, is d2\nNT,∞l2\nNT,∞-sub-Gaussian. By Lemma B.4,\n∥VAk(ηNT)∥∞≲P (ζV\nNT),\nsince Tk ≍T. Furthermore, by Assumption 4.5 P((bdk,blk) ∈DNT × LNT,∀k = 1,...,K) →1. We\nconclude by Lemma A.8 that (D.14) holds. The bound (D.15) follows from the same argument. Finally, the bound (4.17) follows from the deﬁnition ¯g = ¯z−¯b′β0 and Holder inequality and union\nbounds.",
    "content_hash": "f529f54d83707a9a44561129688c4437567c1cf14ff5fb3a7b4e8951e67db5cd",
    "location": null,
    "page_start": 63,
    "page_end": 63,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2de78672-d0d2-4d37-ba9b-d1a4475c855c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n29\nLemma D.3 (Summary of Gram Matrix and Gradient Error Bounds). Suppose Assumptions 4.1–\n4.5 hold. Then, the following bounds hold wp 1−o(1)\n∥eQ−Q∥∞≲P o(κNT log(d2NT)). (D.19)\n∥eQ−bQ∥∞≲P (qNT) = oP((NT)−1/2)\n(D.20)\n∥bQ−Q∥∞≲P o(κNT log(d2NT))\n(D.21)\n∥bS−S∥∞≲P (ρNT) = oP((NT)−1/2),\n(D.22)\nProof of Lemma D.3. Decompose matrix ﬁrst-stage estimation error gives\nbQ = ENT bVit bV ′\nit\n= ENT(Vit +(di0(Xit)−bdi(Xit)))(Vit +(di0(Xit)−bdi(Xit)))′\n= ENTVitV ′\nit\n+ENTVit(di0(Xit)−bdi(Xit))′ +(ENTVit(di0(Xit)−bdi(Xit))′)′\n+ENT(di0(Xit)−bdi(Xit))(di0(Xit)−bdi(Xit))′ = eQ+ ¯a+ ¯a′ + ¯b. Then, an application of Lemma B.6 with ¯N = 2 gives wp 1 −o(1) ∥eQ −Q∥∞≤¯CκκNT for large\nenough ¯Cκ. The bounds on ∥¯a∥∞and ∥¯b∥∞are given in (D.9) and (D.15), respectively.",
    "content_hash": "0be3e6db91921b3f28eb5e71822a9fca6e9641bfe7b4117c896b4051e9e0bb64",
    "location": null,
    "page_start": 64,
    "page_end": 64,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e79be8ea-9ab0-4084-873b-bd3370bebe33",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(D.23)\nDecompose the gradient:\nbS = ENT bVit(beY it −bV ′\nitβ0) = ENTVit(beY it −bV ′\nitβ0)+ENT(bVit −Vit)(beY it −bV ′\nitβ0) = bS1 + bS2,\nwhere\nbS1 = ENTVitUit +ENTVitRit(bd,bl) = S+ ¯e,\nbS2 = ENT(di0(Xit −bdi(Xit))Uit +ENT(di0(Xit)−bdi(Xit))Rit(bd,bl) = ¯f + ¯g. Invoking bounds on ¯e, ¯f, and ¯g in (D.12)–(D.16) gives the result. ■",
    "content_hash": "ed505c74e77f8170ba75bf734eafacfb941758f58d2ee5ad2a68266d46449d38",
    "location": null,
    "page_start": 64,
    "page_end": 64,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e7aae5c9-e0a3-442c-95eb-89e55c90c542",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Collecting\nterms gives the bound (D.20). The (D.21) follows from the triangle inequality and qNT = oP(κNT). We can decompose the gradient error bS−S as follows. Note that\nbeY it −eYit = Yit −bli(Xit)−(Yit −li0(Xit)) = li0(Xit)−bli(Xit)\nbVit −Vit = Dit −bdi(Xit)−(Dit −di0(Xit)) = di0(Xit)−bdi(Xit)\nThe difference of the two equations is\nbeY it −eYit −(bVit −Vit)′β0 = Rit(bd,bl). Therefore,\nbeY it −bV ′\nitβ0 = (eYit −V ′\nitβ0)+((beY it −eYit)−(bVit −Vit)′β0) = Uit +Rit(bd,bl).",
    "content_hash": "7e3c9cc193abbf44d80e7d5d4113070557247b876adaa6b9f3b9594e0ecd243c",
    "location": null,
    "page_start": 64,
    "page_end": 64,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4bdb3f3f-7a2c-490f-acaf-195d92177558",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "30\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nD.2. Proof of Orthogonal Lasso Rate: Theorem 4.1. Group Sparsity Notation. We use the same notation as Lounici et al. (2011). Consider a generic\ncovariate vector of size g·d, where d is the number of groups and g is the group size. Partition the\nset of indices {1,2,...,gd} into d groups of size g:\nJj := { j,d + j,...,(g−1)d + j},\nj = 1,2,...,d,\n|Jj| = g. For a group index j and a subset of group indices T , and vector ∆∈Rgd, denote\n∆j = (∆m)m∈Jj ∈Rg,\n∆T = (∆m){m∈Jj,j∈T } ∈R|T |·g. For any ∆∈Rgd, deﬁne the group-vector norms\n∥∆∥2,∞= max\n1≤j≤d ∥∆j∥2,\n∥∆∥2,1 =\nd\n∑\nj=1\n∥∆j∥2. For a symmetric matrix M, deﬁne\n∥M∥2,∞= ∥M′∥2,∞= max\n1≤i≤dg max\n1≤j≤d\n∑\nk∈Jj\nM2\ni,k\n!1/2\n. Deﬁne the group restricted cone as\nREG(¯c) :=\n(\n∆∈Rgd : ∑\nj∈T c\n∥∆j∥2 ≤¯c ∑\nj∈T\n∥∆j∥2,\n∆̸= 0\n)\n.",
    "content_hash": "8a0725862d54cf55eee40bb042bf12ded6335324c17c7ec13447bf4bf029507b",
    "location": null,
    "page_start": 65,
    "page_end": 65,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "51aba6c1-700d-4593-814e-4580fa4ec59e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Given a matrix M ∈Rgd ×Rgd, deﬁne the restricted group-sparse eigenvalue\nκg(M,T , ¯c) =\nmin\n∆∈REG(¯c)\n√s(∆′M∆)1/2\n∥∆T ∥2,1\n. When the group size g is equal to 1, the objects above reduce to the following quantities:\n∆j = ∆j,∆T = ∆T = (∆m){m∈T },∥M∥2,∞= ∥M∥∞,\nthe group restricted cone is regular restricted cone\nREG(¯c) = RE(¯c) = {∆∈Rd : ∥∆T c∥1 ≤¯c∥∆T ∥1,\n∆̸= 0},\nand the the restricted group-sparse eigenvalue reduces to restricted eigenvalue\nκ1(M,T , ¯c) = κ(M,T , ¯c) =\nmin\n∆∈RE(¯c)\n√s(∆′M∆)1/2\n∥∆T ∥1\n. Let ¯Xit ∈Rgd be a generic covariate (dg)-vector and ¯Yit be a generic outcome. Given a parameter\n¯β0, decompose\n¯Yit = ¯X′\nit ¯β0 +Uit.",
    "content_hash": "41ba614506bb1fb4c8a9fc67b12fe875a01abd365f88afd3b5e5ea5b380bd281",
    "location": null,
    "page_start": 65,
    "page_end": 65,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "660218d9-65c5-43ac-88e8-b62893ee713a",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n31\nThe least squares loss function is\nQ( ¯β) := 1/2(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n(¯Yit −¯X′\nit ¯β)2. The group lasso estimator is\nb¯β := argmin\n¯β\nQ( ¯β)+λ∥¯β∥2,1. (D.24)\nThe least squares gradient is\nS ( ¯β0) := ∇¯β0Q( ¯β0) = (NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n(¯Yit −¯X′\nit ¯β0) ¯Xit. and the Hessian is\nH ( ¯β0) := (NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n¯Xit ¯X′\nit. Lemma D.4 (Grouped Norm Inequalities). For any two vectors a,b ∈Rgd and matrix M ∈Rgd×Rgd,\nthe following inequalities hold\n|a′b| ≤∥a∥2,1∥b∥2,∞\n(D.25)\n|v′Mv| ≤√g∥v∥2\n2,1 ·∥M∥2,∞\n(D.26)\n∥M∥2,∞≤∥M∥∞\n√g\n(D.27)\nProof.",
    "content_hash": "4fac04af88fd0ff247e8979de2cb81726c1cf61827ee4786d2819ad3e2a32e3a",
    "location": null,
    "page_start": 66,
    "page_end": 66,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5bc4b554-7fe9-499a-95ef-3aa6e6c45a86",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For each group j = 1,2,...,d, Cauchy inequality gives\n∑\nk∈Jj\nakbk\n≤\n∑\nk∈Jj\na2\nk\n!1/2\n∑\nk∈Jj\nb2\nk\n!1/2\n≤max\n1≤j≤d\n∑\nk∈Jj\na2\nk\n!1/2\n∥b j∥2 =\n\u0012\nmax\n1≤j≤d ∥aj∥2\n\u0013\n∥bj∥2,\nwhich implies\n|a′b| ≤\nd\n∑\nj=1\n∑\nk∈Jj\nakbk\n≤\n\u0012\nmax\n1≤j≤d ∥aj∥2\n\u0013 d\n∑\nj=1\n∥bj∥2 = ∥a∥2,∞∥b∥2,1\nFor each index i,1 ≤i ≤kg, the following bound holds:\ngd\n∑\nk=1\nMi,kvk\n≤\nd\n∑\nj=1\n∑\nk∈Jj\nMi,kvk\n≤\nd\n∑\nj=1\n∑\nk∈Jj\nM2\ni,k\n!1/2\n∑\nk∈Jj\nv2\nk\n!1/2\n≤max\n1≤j≤d\n∑\nk∈Jj\nM2\ni,k\n!1/2 d\n∑\nj=1\n∑\nk∈Jj\nv2\nk\n!1/2\n≤∥M∥2,∞∥v∥2,1.",
    "content_hash": "a6dfb9f9295274c7355276a8f283746d00da29d25555df11605eb90563e3dd0b",
    "location": null,
    "page_start": 66,
    "page_end": 66,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1988db82-95bd-4235-b4e6-855b200c0555",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "32\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nThen,\n∥Mv∥2,∞= max\n1≤j≤d ∥(Mv)j∥2 = max\n1≤j≤d\n∑\ni∈Jj\n|Mv|2\ni\n!1/2\n≤max\n1≤j≤d\n∑\ni∈Jj\n∥v∥2\n2,1∥M∥2\n2,∞\n!1/2\n≤√g∥v∥2,1∥M∥2,∞. Therefore, we obtain (D.26) by combining inequalities above:\n|v′Mv| ≤∥v∥2,1∥Mv∥2,∞≤√g∥M∥2,∞·∥v∥2\n2,1. Finally, the bound (D.27) follows from\nM2,∞= max\n1≤j≤d ∥M j∥2 ≤max\n1≤j≤d\n√g∥M j∥∞\nusing the fact that ∥v∥2 ≤\np\ndim(v)∥v∥∞. ■\nLemma D.5 (First-Stage Effect on the Curvature). Let M1,M2 ∈Rgd×gd be two matrices. Let\nλ M\nNT := ∥M1 −M2∥∞. On the event κ2\ng(M2,T , ¯c) > 0, for any ∆∈REG(¯c),\n|κ2\ng(M2,T , ¯c)−κ2\ng(M1,T , ¯c)| ≤λ M\nNT(1+ ¯c)2sg. (D.28)\nProof of Lemma D.5. For any ∆∈Rgd, the difference can be bounded as\n|∆′(M1 −M2)∆| ≤i √g∥M1 −M2∥2,∞∥∆∥2\n2,1 ≤ii gλ M\nNT∥∆∥2\n2,1,\n(D.29)\nwhere i follows from (D.26) and ii from (D.27).",
    "content_hash": "e898c83325a9a4eef8f32a8fc6a431c60dbf313e2b19a3bd82df133aa693f381",
    "location": null,
    "page_start": 67,
    "page_end": 67,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0505cf62-a7e1-454c-8ac7-2e9edf6f7c90",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(D.28) gives\n|κ2( bQ,T , ¯c)−κ2( eQ,T , ¯c)| ≤s∥bQ−bQ∥∞(1+ ¯c)2 ≲P (sqNT),",
    "content_hash": "571c13302bffac936061674d830f907d6b332a8ac6cbc1ac2c52d1144c45435c",
    "location": null,
    "page_start": 69,
    "page_end": 69,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "72c70c50-1c02-4953-9257-dd2dbd9eeb3e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Union bound implies\nP(λβ ≤c√g∥bS∥∞) ≤P(λβ/2 ≤c√g∥S∥∞)+P(λβ/2 ≤c√g∥bS−S∥∞)\n= PS +PbS−S ≤o(1)+o(1),\nwhere PS ≤2/d = o(1) is given in (B.6) and PbS−S = o(1) since\n∥bS−S∥∞≲P (ρNT) = oP(\np\nlogd/NT). Step 2. Let M2 := Q = (NT)−1 ∑N\ni=1 ∑T\nt=1 EVitV ′\nit and M1 := eQ = ENTVitV ′\nit. Observe that\nκ2(Q,T , ¯c) =\nmin\nδ∈RE(¯c)\nsδ ′Qδ\n∥δT ∥2\n1\n≥\nmin\nδ∈RE(¯c)\nsmineig(Q)∥δ∥2\n2\n∥δT ∥2\n1\n≥i mineig(Q),\n(D.39)\nwhere (i) follows from\ns∥δ∥2\n2 ≥s∥δT ∥2\n2 ≥∥δT ∥2\n1\n∀δ ∈Rd. The bounds (D.28) and (D.19) imply\n|κ2( eQ,T , ¯c)−κ2(Q,T , ¯c)| ≤s∥eQ−Q∥∞(1+ ¯c)2 ≲P (sκNT). Therefore, the event G2 := {κ2( eQ,T , ¯c) > Cmin/2} holds w.p. 1−o(1). Step 3. Invoke Lemma D.5 on the event G2 with M2 := eQ and M1 := bQ.",
    "content_hash": "2af7f214bf116a445c0d7b3d0ab09485da31f67c85152eac2862ec4a60571a79",
    "location": null,
    "page_start": 69,
    "page_end": 69,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8a3c6bea-6b83-4860-8b84-5b02c907d1fb",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "34\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nInvoking inequality (D.25) for S ( ¯β0)′∆gives\n1/2(∆′H ( ¯β0)∆) ≤λ(∥¯β0∥2,1 −∥¯bβ∥2,1)+∥S ( ¯β0)∥2,∞∥∆∥2,1. Then\n1/2(∆′H ( ¯β0)∆) ≤i λ(∥∆T ∥2,1 −∥∆T c∥2,1)+λ/c∥∆∥2,1\n≤λ∥∆T ∥2,1 +0+λ/c∥∆∥2,1\n≤ii λ∥∆T ∥2,1 +(λ/c)(1+ ¯c)∥∆T ∥2,1\n=iii λ ¯c∥∆T ∥2,1,\n(D.38)\nwhere (i) follows from (D.36), (ii) from (D.37), and (iii) from\n1+c−1(¯c+1) = (c+(c+1)/(c−1))/c = (c+1)/(c−1) = ¯c. Since ∆∈REG(¯c), (D.35) follows. ■\nProof of Theorem 4.1. We invoke Lemma D.6 with the group size g = 1, ¯β0 = β0 and ¯Uit = Uit +\nRit(bd,bl). The gradient S (β0) = bS, the Hessian is H (β0) = bQ and the penalty λ = λβ. Note that\nδ ∈RE(¯c) has been established in the proof of Lemma D.6. Step 1.",
    "content_hash": "58ba06dfa2670eb76ea5ab5b335f71b57439b0c4d48816ce18d1d1981bfa1bb5",
    "location": null,
    "page_start": 69,
    "page_end": 69,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d7f8f20b-e776-4d5d-9c67-7cf1af88e661",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n35\nwhich implies\n|κ2( bQ,T , ¯c)−κ2(Q,T , ¯c)| ≲P (s(qNT +κNT)). Therefore, the event {κ2( bQ,T , ¯c) > Cmin/2} holds w.p. 1−o(1). Thus, the event\nG3 := s∥bQ−eQ∥∞(1+ ¯c)2/κ2( bQ,T , ¯c) < 1/2\nis well-deﬁned and holds w.p. 1−o(1). Step 4. On the event G1 ∩G2 ∩G3, invoking (D.34) with M2 = eQ and M1 = bQ gives\nδ ′ bQδ ≥(1/2)·δ ′ eQδ\nCombining inequality above with (D.35) gives\nδ ′ eQδ ≤2δ ′ bQδ ≤4λβ ¯c∥δT ∥1 ≤√sλβ\n4¯c(δ ′ eQδ)1/2\nκ( eQ,T , ¯c)\n. Dividing LHS and RHS by (δ ′ eQδ)1/2 gives\n(δ ′ eQδ)1/2 ≤√sλβ\n4¯c\nκ( eQ,T , ¯c)\n≲P (√sλβ)\nand\n∥δ∥1 ≤(1+ ¯c)∥δT ∥1 ≤(1+ ¯c)\n√s(δ ′ eQδ)1/2\nκ( eQ,T , ¯c)\n≤4(1+ ¯c)\nsλβ ¯c\nκ2( eQ,T , ¯c)\n. ■\nD.3.",
    "content_hash": "3c935c8948617237a1cccb4cde1c1b4f30a413c9bfe722c8f5829793a00460fd",
    "location": null,
    "page_start": 70,
    "page_end": 70,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8b4ce4cf-b40e-4d54-99ac-e4c2c647aace",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "On the event GQ,\nthe bound holds\n∥QbΩ−Id∥∞= ∥Q(bΩ−Q−1)∥∞≤∥(Q−bQ)(bΩ−Q−1)∥∞+∥bQ(bΩ−Q−1)∥∞\n≤∥Q−bQ∥∞∥bΩ−Q−1∥1,∞+∥bQbΩ−Id∥∞+∥Id −bQQ−1∥∞\n≤∥Q−bQ∥∞(∥Q−1∥1,∞+∥bΩ∥1,∞)+λQ +∥bQQ−1 −Id∥∞. Invoking (D.42) and (D.44) gives\n∥QbΩ−Id∥∞≤2∥Q−bQ∥∞∥Q−1∥1,∞+λQ +λQ ≤2λQ +2λQ = 4λQ. (D.45)\nPre-multiplying QbΩ−Id by Q−1 and invoking (D.40) gives\n∥bΩ−Q−1∥∞= ∥Q−1(QbΩ−Id)∥∞≤∥Q−1∥∞,1∥QbΩ−Id∥∞≤4\nAQ\naQ −1λQ. Since Q is a symmetric matrix, so is Q−1, and\n|bΩCLIME\nmj\n−Q−1\nmj| ≤max(|bΩm j −Q−1\nm j|,|bΩjm −Q−1\njm|) ≤∥bΩ−Q−1∥∞,\nwhich implies (4.10).",
    "content_hash": "7454fc9e2d7dd9226e62b92081c404280e4055e1bad1185d63c6251a38983a58",
    "location": null,
    "page_start": 71,
    "page_end": 71,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "24edbed3-c0be-45c1-b919-e43862544528",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "36\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nProof of Lemma 4.2. Step 0. Suppose Assumptions 4.1–4.6 hold. We claim that the event\nGQ :=\nn\n∥bQ−Q∥∞∥Q−1∥1,∞≤λQ\no\n,\n(D.41)\nholds w.p. 1−o(1). On this event GQ, by deﬁnition of bΩ, we have\n∥bΩ∥1,∞≤∥Q−1∥1,∞,\n(D.42)\nand, therefore,\n∥bΩCLIME∥1,∞≤∥bΩ∥1,∞≤∥Q−1∥1,∞. (D.43)\nTo show that P(GQ) = 1−o(1), decompose\nbQQ−1 −Id = bQQ−1 −QQ−1 = ( bQ−Q)Q−1. By Lemma D.3 for some ¯Cκ > 0, w.p. 1−o(1),\n∥bQ−Q∥∞≤¯CκκNT. Therefore, w.p. 1−o(1),\n∥bQQ−1 −Id∥∞≤∥bQ−Q∥∞∥Q−1∥1,∞≤¯Cκ2κNT∥Q−1∥1,∞≤λQ\n(D.44)\nas long as CQ ≥2 ¯Cκ∥Q−1∥1,∞. Since ∥Q−1∥1,∞≤AQ/(aQ −1), CQ ≥2 ¯Cκ∥Q−1∥1,∞holds by\nAssumption 4.6. Step 1. We establish (4.10). Speciﬁcally, we show that, on the event GQ, we have\nbΩCLIME −Q−1\n∞≤\nbΩ−Q−1\n∞≤\n4AQ\n(aQ −1)λQ. The argument repeats the proof of equation (13) in Cai et al. (2011), Theorem 6.",
    "content_hash": "51a68a86a49dd5185e3f07c18e0e9bd4d5525326ae841828d82e0ace6ec44702",
    "location": null,
    "page_start": 71,
    "page_end": 71,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "898f5575-35e8-4161-a0f5-1a40fbd510ff",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Since tn ≤∥bΩ−Q−1∥∞from Step 1, we have\n∥(bΩCLIME −Q−1)′∥1,∞= ∥bΩCLIME −Q−1∥1,∞:= max\n1≤j≤d ∥h j∥1 ≤C′\nQ(∥bΩ−Q−1∥∞)1−1/aQ\n≤¯CQλQ\n1−1/aQ\nwhere ¯CQ = C′\nQ(4AQ/(aQ −1))1−1/aQ is a constant that depends on Q. Thus, (4.11) follows.",
    "content_hash": "6981ac2296704d5e43d2a769ea1c1fb6ffe4a77ca790d576c248db565638168e",
    "location": null,
    "page_start": 72,
    "page_end": 72,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e76720c1-77ce-454d-9ffc-e00e92a639ed",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n37\nStep 2. We show that (4.11) holds. Speciﬁcally, we show that on the event GQ we have that\n∥bΩCLIME −Q−1∥1,∞≤¯CQλQ\n1−1/aQ,\nfor some constant ¯CQ that depends on Q. We closely follow the proof of (14), page 605 in Cai et al. (2011). Using their notation, let\ntn := ∥bΩCLIME −Q−1∥∞,\nω0\nj := Q−1\n·,j\nh j := bΩCLIME\n·,j\n−ω0\nj ,\nh1\nj := ( bωi j1{| bωi j| ≥2tn})p\ni=1 −ω0\nj ,\nh2\nj := h j −h1\nj. By deﬁnition of CLIME, on the event GG,\n∥ω0\nj ∥1 −∥h1\nj∥1 +∥h2\nj∥1 ≤∥h1\nj +ω0\nj ∥1 +∥h2\nj∥1\n=i ∥h2\nj +h1\nj +ω0\nj ∥1 = ∥bΩCLIME\n·,j\n∥1 ≤∥bΩ·,j∥1 ≤∥ω0\nj ∥1,\nwhere (i) follows from h1\nj +ω0\nj and h2\nj having non-overlapping support. This implies\n∥h j −h1\nj∥1 := ∥h2\nj∥1 ≤∥h1\nj∥1,\n∥hj∥1 ≤2∥h1\nj∥1.",
    "content_hash": "db9f7becb94d20bfc2d04f0924f9131f5326c7abbbd28a42893ff71678258eec",
    "location": null,
    "page_start": 72,
    "page_end": 72,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c9951a37-54b9-4796-82c9-65588d197cb4",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Then, the following bound holds:\n∥h1\nj∥1 =\nd\n∑\ni=1\n| bωij1{| bωij| ≥2tn}−ω0\ni j|\n≤\nd\n∑\ni=1\n|ω0\nij|1{ω0\nij ≤2tn}+\nd\n∑\ni=1\n| bωi j1{| bωi j| ≥2tn}−ω0\ni j1{|ω0\ni j| ≥2tn}|\n≤rj(2tn)+tn\nd\n∑\ni=1\n1{| bωij| ≥2tn}|+\nd\n∑\ni=1\n|ω0\ni j||(1{| bωi j| ≥2tn}−1{|ω0\ni j| ≥2tn})|\n≤rj(2tn)+tn\nd\n∑\ni=1\n1\n\b\f\fω0\nij\n≥tn\n+\nd\n∑\ni=1\nω0\ni j\nI\n\b\n||ω0\ni j |−2tn| ≤\nˆωi j −ω0\ni j\n≤rj(2tn)+tns j(tn)+\nd\n∑\ni=1\n|ω0\ni j|1{ω0\ni j ≤3tn}\n≤rj(2tn)+tns j(tn)+r j(3tn)\n≤C′\nQt1−1/aQ\nn\n. (C′\nQ :=\nA1/aQ\nQ\n(aQ −1)(21−1/aQ +(aQ −1)+31−1/aQ)).",
    "content_hash": "1c62e944a3f80fb2f0cdfd53b493197b886f7d6820ec3566ff3ad8bb65da7ae8",
    "location": null,
    "page_start": 72,
    "page_end": 72,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a945c9a0-e74e-4ba0-8772-faefd3cb575b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "38\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nStep 3. We show (4.12). Speciﬁcally, we show that on the event GQ and ∥bQ−Q∥≤1 and once\nλQ ≤1, we have that\n∥Id −bΩCLIME bQ∥∞= ∥Id −bQbΩCLIME∥∞≤C′\nQλQ\n1−1/aQ,\nfor some constant C′\nQ that depends only on Q. Indeed,\n∥Id −bQbΩCLIME∥∞≤∥Id −bQQ−1∥∞+∥bQ(Q−1 −bΩCLIME)∥∞\n≤∥Id −bQQ−1∥∞+(∥Q∥∞+1)∥bΩCLIME −Q−1∥1,∞\n(D.46)\n≤λQ +(∥Q∥∞+1) ¯CQλ 1−1/aQ\nQ\n≤C′\nQλ 1−1/aQ\nQ\nfor example, taking C′\nQ to bound:\n(λ 1−1/aQ\nQ\n+(∥Q∥∞+1)CQ) ≤(1+(∥Q∥∞+1)CQ) =: C′\nQ\n■\nLemma D.7 (Linearization in Sup-Norm). Suppose Assumptions 4.1–4.6 hold. Then, the debiased\nestimator bβDL is asymptotically linear\n√\nNT(bβDL −β0) = Q−1GNTVitUit +RNT,\n(D.47)\n∥RNT∥∞≲P λQ\n1−1/aQp\ns2 logd +\n√\nNTρNT = oP(1). (D.48)\nProof of Lemma D.7. Step 1. Recall that\nRit(d,l) := li0(Xit)−li(Xit)−(di0(Xit)−di(Xit))′β0.",
    "content_hash": "6baf64fd899796783c1f9d4ffcae36e1c70433a92626df0632319fd3d59b4df3",
    "location": null,
    "page_start": 73,
    "page_end": 73,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "be637df2-fefa-4153-86df-777ffa0c8513",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "and invoking (D.23), which states that\nbeY it −bV ′\nitβ0 = (eYit −V ′\nitβ0)+((beY it −eYit)−(bVit −Vit)′β0) = Uit +Rit(bd,bl). we can see that\nbeY it −bV ′\nit bβL = beY it −bV ′\nitβ0 + bV ′\nit(β0 −bβL)\nENT bVit(beY it −bV ′\nit bβL) = ENT bVit(Uit +Rit(bd,bl))+ bQ(β0 −bβL). Since\nbβDL −β0 = bβL −β0 + bΩCLIME(ENT bVit(beY it −bV ′\nit bβL))",
    "content_hash": "e1b32afd9c15a7b2fe295c2c945fe203af831128af66db1548c55c5a1f1b4b48",
    "location": null,
    "page_start": 73,
    "page_end": 73,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3f2f5517-2c1c-4c75-afe7-4f6a4c555e7e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Lemma\nD.7 implies\nα′(α′Σα)−1/2(\n√\nNT(bβDL −β0)) = α′(α′Σα)−1/2Q−1√\nNTENTVitUit +oP(1),",
    "content_hash": "236a7ed5efc88beabdc1c19842ca19984ae520772fa796dc21794a5eb2e76111",
    "location": null,
    "page_start": 74,
    "page_end": 74,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a3b5622a-d589-4b21-a25e-f6b633ef2469",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n39\nwe have that\nbβDL −β0 = bΩCLIME(ENT bVit(Uit +Rit(bd,bl))+ bΩCLIME bQ(β0 −bβL)+ bβL −β0\n= bΩCLIME(ENT bVit(Uit +Rit(bd,bl))+(Id −bΩCLIME bQ)(bβL −β0)\nL3\n= Q−1ENTVitUit +(bΩCLIME −Q−1)ENTVitUit\n+ bΩCLIME(ENT[bVit(Uit +Rit(bd,bl)−VitUit])+L3\n= Q−1ENTVitUit +L1 +L2 +L3,\nwhere\nL1 = (bΩCLIME −Q−1)ENTVitUit\nL2 = bΩCLIMEENT[VitRit(bd,bl)+(bVit −Vit)(Uit +Rit(bd,bl))]\nL3 = (Id −bΩCLIME bQ)(bβL −β0). Term L1.",
    "content_hash": "79981740f06a09a39094b0fe5c67425a903fd7c96a4e0b919cba72730635d5b0",
    "location": null,
    "page_start": 74,
    "page_end": 74,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "34cad64f-3e92-494b-8161-5eb0fcd9ba15",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "The bounds (4.11) and (B.6) imply\n∥L1∥∞≤∥bΩCLIME −Q−1∥1,∞\n√\nNT∥ENTVitUit∥∞\n(D.49)\n≲P λQ\n1−1/aQ√\nNT\np\nlogd/NT = oP(1),\nbecause λQ\n1−1/aQ = o(s−1 log−1/2 d) = o(log−1/2 d) as assumed in (4.11). Term L2. The bounds (D.43) and the gradient error bound (D.22) imply\n∥L2∥∞≤∥bΩCLIME∥∞,1∥\n√\nNTENT[VitRit(bd,bl)+(bVit −Vit)(Uit +Rit(bd,bl))]∥∞\n≲P 1\n√\nNTρNT = o(1). because\n√\nNTρNT = o(1) is implied by our assumption Assumptions 4.1–4.5. Term L3. The conditions (4.12) and (4.8) imply\n√\nNT∥L3∥∞=\n√\nNT∥(Id −bΩCLIME bQ)(bβL −β0)∥∞\n≤\n√\nNT∥Id −bΩCLIME bQ∥∞∥bβL −β0∥1\n≲P (λQ\n1−1/aQ√\nNT\nq\ns2 logd/NT) = o(1),\nwhere ∥Id −bΩCLIME bQ∥∞≲P λQ\n1−1/aQ = o(s−1 log−1/2 d) as assumed in (4.11). ■\nProof of Theorem 4.2. Step 1. Let α ∈Rd be such that ∥α∥1 = Kα = O(1) and ∥α∥2 = 1.",
    "content_hash": "aae06a94f15be9c6a93c819802589e3794d8e975483c46670cc1f210d535a4c9",
    "location": null,
    "page_start": 74,
    "page_end": 74,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d3ba326b-9e95-4af2-a76c-9dd4269d6d66",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "By Polya’s theorem, the convergence is uniform in t ∈R. Since the result holds for any sequence\n{α} (indexed by N,T obeying conditions above, the convergence is uniform over such sequences.)\nStep 2. Let Kα be a ﬁnite constant in the statement of the theorem. Thus,\nsup\nα:∥α∥2≤1,∥α∥1≤Kα\n|α′(bΣ−Σ)α| ≤K2\nα∥bΣ−Σ∥∞= oP(1)\nby assumption. Since min∥α∥2=1 α′Σα ≥¯σ2C−1\nmax, by assumption, we conclude that, for N and T\nlarge enough, the event\nGK :=\nn\ninf\n∥α∥2=1,∥α∥1≤Kα\nα′bΣα > ¯σ2C−1\nmax/2\no\noccurs wp 1−o(1). Hence wp 1−o(1). αNT −1 := (α′Σα)1/2\n(α′bΣα)1/2 −1\n(D.52)\nobeys\n|αNT −1| ≤(¯σ2C−1\nmax/2)−1K2\nα∥bΣ−Σ∥∞= oP(1)\nwhich follows from the inequality\n1−\n√x\n√y\n= |√x−√y|\n√y\n=\n|x−y|\n√y(√x+√y);\nx > 0,y > 0",
    "content_hash": "9cbb4c093a00fb6d6d99e2cfe66e302c7aa50c6db068a53c3aafa2cc1f07a730",
    "location": null,
    "page_start": 75,
    "page_end": 75,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cef90eaf-17bf-45a6-90dd-64b47ab3cb20",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "40\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nwhere (α′Σα)−1/2 = O(1) because\nα′Σα ≥¯σ2α′Q−1α ≥¯σ2C−1\nmax > 0\n(D.50)\nby the assumptions of the Theorem, so that\n|α′(α′Σα)−1/2RNT| ≤O(1)Kα∥RNT∥∞= oP(1). (D.51)\nConsider a sequence\nξm(α) := α′Q−1(α′Σα)−1/2VmUm,\nm = 1,2,...,M\nwith\nm = m(i,t) = T(i−1)+t,\n1 ≤t ≤T,1 ≤i ≤N. As shown in Corollary B.3, {ξm(α)}M\nm=1 is a martingale difference sequence w.r.t. natural ﬁltration\nwith M = NT. By Law of Large Numbers in Hansen (2019) and the assumed Lindeberg condition\n1\nNT\nNT\n∑\nm=1\nξ 2\nm(α) →p\nα′Q−1ΓQ−1α\nα′Σα\n= 1. As discussed in McLeish (1974), the Lindeberg condition assumed in the Theorem 4.2 implies\nconditions (i) and (ii) in Theorem 2.3 of McLeish (1974), which implies the ﬁrst part of the Theo-\nrem:\nP(α′(α′Σα)−1/2√\nNT(bβDL −β0) ≤t) →Φ(t).",
    "content_hash": "70e576489442437304167c78982908a83bf5acf91e03e52a074e9c2a44aac447",
    "location": null,
    "page_start": 75,
    "page_end": 75,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8b0a988e-d73e-4740-937c-cc476ea0518b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Deﬁne the following terms\n¯b1 = ENTVitV ′\nitU2\nit −Γ\n(D.53)\n¯b2 = ENT(di0(Xit)−bdi(Xit))VitU2\nit\n(D.54)\n¯b3 = ENT(di0(Xit)−bdi(Xit))(di0(Xit)−bdi(Xit))′U2\nit\n(D.55)\n¯b4 = ENTVitV ′\nit( bU2\nit −U2\nit)\n(D.56)\n¯b5 = ENT(di0(Xit)−bdi(Xit))Vit( bU2\nit −U2\nit)\n(D.57)\n¯b6 = ENT(di0(Xit)−bdi(Xit))(di0(Xit)−bdi(Xit))′( bU2\nit −U2\nit)\n(D.58)\nThe following Lemma establishes tail bound on ¯b1. Recall that κNT from Lemma D.3 is\nκNT :=\nq\nlog3(d2 log(NT))logNT/NT. Lemma D.8 (Higher-Order Term ¯b1). Under Assumptions 4.1–4.5 and 4.7,\n∥Γ∥∞=\nmax\n1≤m,j≤d |EVit jVitmU2\nit| = O(1)\n(D.59)\n∥¯b1∥∞≲P\nq\nlog5(d2 log(NT))logNT/NT ≤κNT log(d2NT) = o(1). (D.60)\nProof.",
    "content_hash": "d5ff88577aae192488572cdca18a07768014bda807ce2bf04a25b928822269cc",
    "location": null,
    "page_start": 76,
    "page_end": 76,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7e0a2a1f-b8b4-4212-b014-91504369d3d9",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n41\nThen\n|(αNT −1)α′(α′Σα)−1/2√\nNT(bβL−β0)| ≤|αNT −1||α′(α′Σα)−1/2√\nNT(bβL−β0)| = op(1)OP(1). Therefore,\nα′(α′bΣα)−1/2√\nNT(bβDL −β0) = α′(α′Σα)−1/2√\nNT(bβL −β0)+oP(1). Then convergence in distribution for the left side follows by Slutsky’s lemma and Step 1. ■\nD.4. Estimation of Σ: Proof of Lemma 4.3.",
    "content_hash": "7150b61fcb1c11e784044020b0e05294368624f3429a9f683f40c6c4558cbbf6",
    "location": null,
    "page_start": 76,
    "page_end": 76,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "60303e94-cdaf-4504-a140-638ff718d183",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "The bound (D.59) and (D.60) follow from (B.19) and (B.20) with Z1,nit = Z2,nit = Vit,\n¯N = 1 and g = 1. ■\nLemma D.9 (Higher-Order Term ¯b2). Under Assumptions 4.1–4.5 and 4.7,\n∥¯b2∥∞≲P (NT)−1/4. (D.61)\nProof of Lemma D.9. Step 1. For ¯b as in (D.6) and qNT as in (D.18),\nP2\n1 := max\n1≤j≤d ENT(di0(Xit)−bdi(Xit))2\nj ≤∥¯b∥∞≲P qNT.",
    "content_hash": "74808ac0efbdcc02435baf80568084572f339ce05fe2cc19df55a5d6c9f83567",
    "location": null,
    "page_start": 76,
    "page_end": 76,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4cc48938-5b63-41c5-be5f-f5993d0044ed",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "42\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nInvoking the convergence requirement (4.14) gives\n\u0012\n1+\nq\nlog7(d2 log(NT))log(NT)/NT\n\u0013\n≲1+κNT log2(d2NT) ≲1. Invoking the bounds (B.19)–(B.20) with Z1,nit = 1 and Z2,nit = Vit and ¯N = 2 and g = 2 gives\nP2\n2 := max\n1≤m≤d ENTV 2\nitmU4\nit ≲P\n\u0012\n1+\nq\nlog7(d2 log(NT))log(NT)/NT\n\u0013\n≲P 1. Cauchy inequality implies\nmax\n1≤m,j≤d |ENT|(di0(Xit)−bdi(Xit))j||Vitm|U2\nit|\n≤max\n1≤j≤d(ENT(di0(Xit)−bdi(Xit))2\nj)1/2 max\n1≤m≤d(ENTV 2\nitmU4\nit)1/2\n≲P\np\nqNT ·1 = oP((NT)−1/4),\nwhere the last bound is established in (D.20). ■\nLemma D.10 (Higher-Order Term ¯b3). Under Assumptions 4.1–4.5 and 4.7,\n∥¯b3∥∞≲P o((NT)−1/4). (D.62)\nProof of Lemma D.10.",
    "content_hash": "e8d75d08f3b576f9cfb658da6cb62d58c9dce7749a3ef3a15f3fdb38807f08d7",
    "location": null,
    "page_start": 77,
    "page_end": 77,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c1ec7417-f08e-47d2-84ce-89a8569a801b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "On the event supit |di0(Xit) −bdi(Xit)| ≤dNT,∞≤1, which happens with\nprobability 1−o(1),\nmax\n1≤m,j≤d ENT|(di0(Xit)−bdi(Xit))j||(di0(Xit)−bdi(Xit))m|U2\nit\n≤max\n1≤j≤d ENT|(di0(Xit)−bdi(Xit))j|U2\nit\n≤max\n1≤j≤d(ENT(di0(Xit)−bdi(Xit))2\nj)1/2(ENTU4\nit)1/2\n≤\nq\nP2\n1 (ENTU4\nit)1/2 ≲P o((NT)−1/4). ■\nRecall that the ﬁrst-order estimation error is\nRit(d,l) := li0(Xit)−li(Xit)−(di0(Xit)−di(Xit))′β0. Lemma D.11 (Squared Error). Under Assumptions 4.1–4.5, we have that\nENTR2\nit(bd,bl) ≲P l2\nNT +o((NT)−1/2). (D.63)",
    "content_hash": "1ec8095fb6540cf532032120adf0aecc036d5ddebbd88cd1a9908ff3cbd07eeb",
    "location": null,
    "page_start": 77,
    "page_end": 77,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "55b838d5-0d64-4c06-8b9e-5ab35278d392",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "The following bound holds\nENTR2\nit(bd,bl) ≤2ENT((di0(Xit)−bdi(Xit))′β0)2 +2ENT(li0(Xit)−bli(Xit))2\n= 2β ′\n0¯bβ0 +2ENT(li0(Xit)−bli(Xit))2\n≤2∥¯b∥∞∥β0∥2\n1 +2ENT(li0(Xit)−bli(Xit))2\n≲P ∥β0∥2\n1\n\u0012\nd2\nNT +d2\nNT,∞\nq\n(NT)−1 log(NT)logd\n\u0013\n+l2\nNT +o((NT)−1/2)\n≲i\nP o((NT)−1/2)+l2\nNT +o((NT)−1/2),\nwhere (i) follows combining ∥β0∥1 ≤¯Cβ assumed in Assumption 4.5 (a) and qNT = o((NT)−1/2),\nestablished in (D.20). ■\nLemma D.12 (Higher-Order Terms ¯b4, ¯b5, ¯b6 with bU2\nit −U2\nit). Under Assumptions 4.1–4.7,\n6\n∑\nk=4\n∥¯bk∥∞≲P ((NT)−1/4 +lNT +\np\nslogd/NT +l2\nNT log(d2NT)) =: γNT. (D.64)\nProof of Lemma D.12. Step 1. Decompose\nbU2\nit −U2\nit = ( bUit −Uit +Uit)2 −U2\nit = 2Uit( bUit −Uit)+( bUit −Uit)2.",
    "content_hash": "147f6779fd88b987392ea1ae81f9ce3b0614bf29fe9be78f4ead20083d4e066d",
    "location": null,
    "page_start": 78,
    "page_end": 78,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "44fd5583-b63c-4605-b2cf-d5fa8d637e77",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Invoking (D.23) gives\nbUit = beY it −bV ′\nit bβL = (beY it −bV ′\nitβ0)+(bV ′\nitβ0 −bV ′\nit bβL)\n= Uit +Rit(bd,bl)+ bV ′\nit(β0 −bβL). Cauchy inequality implies\nENT( bUit −Uit)2 ≤2ENTR2\nit(bd,bl)+2ENT(bV ′\nit(bβL −β0))2\n= 2ENTR2\nit(bd,bl)+2(bβL −β0)′ bQ(bβL −β0) =: U1 +U2,",
    "content_hash": "7fc071e6cf3fac19021b698a526a2c2a2301cefdddb639209d0f251680ff9e5a",
    "location": null,
    "page_start": 78,
    "page_end": 78,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "46c218d4-2012-498d-8853-3a3e77f65cb2",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n43\nProof of Lemma D.11. Step 1. Consider a term ¯z in (D.7) in a special case when\ndi0(Xit) := li0(Xit)·(1,1),\nd = 2. Then, ¯z reduces to a 2-vector\n¯z := ENT(li0(Xit)−bli(Xit))2 ·(1,1),\nand\n∥¯z∥∞= ENT(li0(Xit)−bli(Xit))2. Invoking (G.16) with dNT and dNT,∞replaced by lNT and lNT,∞gives the bound. Step 2.",
    "content_hash": "9654a66d49a81385fc2407ba5fb65d3ca02e96b3a83fa9de7899ef6e235a3970",
    "location": null,
    "page_start": 78,
    "page_end": 78,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "360d88f3-1c86-47a9-913e-dd6278e9a23a",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "44\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nwhere U1 ≲P o(NT)−1/2 + l2\nNT in established in Lemma D.11 and U2 ≲P slogd/NT is Theorem\n4.1. Step 2. LetC(Wit,η) = (Cmj(Wit,η)) be a d×d matrix. For any coordinates m and j, decompose\nENTCm j(Wit, bη)( bU2\nit −U2\nit) = 2ENTCm j(Wit, bη)Uit( bUit −Uit)+ENTCm j(Wit, bη)( bUit −Uit)2\n=: 2D1m j(bη)+D2m j(bη). Cauchy inequality gives\n|D1mj(bη)| ≤(ENTC2\nmj(Wit, bη)U2\nit)1/2(ENT( bUit −Uit)2)1/2\n≤\nmax\n1≤m,j≤d(ENTC2\nm j(Wit, bη)U2\nit)1/2(ENT( bUit −Uit)2)1/2. Maximal inequality gives\n|D2mj(bη)| ≤max\nit max\nm j |Cm j(Wit, bη)|ENT( bUit −Uit)2. (D.65)\nStep 3.",
    "content_hash": "f3da7017d3bbeb544265c281e93ea1f3d64692f97dea104d7ee91495ebf585b8",
    "location": null,
    "page_start": 79,
    "page_end": 79,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "122de403-727c-4595-ad84-47ebf8ddfe44",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "If one can verify\nmax\n1≤m,j≤d ENTC2\nm j(Wit, bη)U2\nit ≲P 1,\n(D.66)\nwe have that\n∥D1(bη)∥∞=\nmax\n1≤m,j≤d |D1m j(bη)|\n≤OP(1)·OP((NT)−1/4 +lNT +\np\nslogd/NT)\n≲P ((NT)−1/4 +lNT +\np\nslogd/NT). If one can verify another condition\nmax\nit max\nmj |Cm j(Wit, bη| ≲P (log(d2NT)),\n(D.67)\nwe have that\n∥D2(bη)∥∞= OP(log(d2NT))·OP(slogd/NT +l2\nNT +(NT)−1/2). Step 4.1 Take C(Wit, bη) = VitV ′\nit, which corresponds to ¯b4 in (D.56). Invoking (B.19) and (B.20)\nwith Z1,nit = Z2,nit = Vit and ¯N = 2 and g = 1 as well the assumed bound (4.14) gives\nmax\njk |ENTV 2\nitkV 2\nit jU2\nit| ≲P (1+κNT log2(d2NT)) ≲P 1,\nwhich veriﬁes (D.66). By Lemma B.1 (6),\nmax\nit max\nm j |VitkVit j| ≲P (log(d2NT)),\nwhich veriﬁes (D.67).",
    "content_hash": "cf61e962cd6610e629cf97a83aaf0d00f9c1eae42f13f099f33ccce4817650a8",
    "location": null,
    "page_start": 79,
    "page_end": 79,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e81bb297-c611-4808-a2c8-ef92a129c18e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Under Assumptions 4.1–4.5 and 4.7, we have that:\n∥bΓ(bβL)−Γ∥∞≲P (γNT +κNT log(d2NT)) = op(1). (D.69)",
    "content_hash": "6e88652c368e20b282187330131b029d00024f4a25dd154fcb2e0f4791607ad4",
    "location": null,
    "page_start": 80,
    "page_end": 80,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fc8ccc54-c20a-467c-8147-82cce13f38f4",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "On the event supit |di0(Xit)−bdi(Xit)| ≤dNT,∞≤1, the condition (D.66) becomes\nmax\n1≤m,j≤d ENT(di0(Xit)−bdi(Xit))2\nm(di0(Xit)−bdi(Xit))2\njU2\nit ≤ENTU2\nit ≲P 1. Noting that maxit maxmj |C(Wit, bη)| ≲P d2\nNT,∞≲P 1 veriﬁes (D.67). Step 5. (Conclusion). Collecting the bounds and invoking (s∨1)κNT = o(1) gives\no(NT)−1/4 +lNT +\np\nslogd/NT +(log(d2NT)(o(NT)−1/2 +l2\nNT +slogd/NT)) ≲γNT. (D.68)\nFor N and T large enough,\nlog(d2NT)/(NT)−1/4 ≤1,\nwhich implies log(d2NT)(NT)−1/4 = o((NT)−1/4). Likewise,\nlog(d2NT)\np\nslogd/NT ≤s\nq\nlog2(d2NT)logd/NT ≤(s∨1)κNT = o(1),\nwhich gives (D.68). ■\nLemma D.13 (Bound on ∥bΓ(bβL)−Γ∥∞).",
    "content_hash": "e4181954578faeb619eb0c80fd7a84167e93034fa764bc30f3a70fe0b8302cf7",
    "location": null,
    "page_start": 80,
    "page_end": 80,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4223b0cf-17b3-44db-bfc2-428dc35ccf44",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n45\nStep 4.2 Take C(Wit, bη) = Vit(di0(Xit) −bdi(Xit)), which corresponds to ¯b5 in (D.57). In what\nfollows, we focus on the event supit |di0(Xit) −bdi(Xit)| ≤dNT,∞≤1. Invoking (B.18) with ¯N = 2\nand g = 1 gives\nmax\n1≤m,j≤d |ENTV 2\nitm(di0(Xit)−bdi(Xit))2\njU2\nit|\n≲P d2\nNT,∞max\n1≤m≤d |ENTV 2\nitmU2\nit|\n≲P dNT,∞\n\u0012\n1+\nq\nlog5(d logNT)log(NT)/NT\n\u0013\n= dNT,∞(1+κNT log(d logNT))) ≲dNT,∞. Likewise,\nmax\nit max\nmj |C(Wit, bη)| ≤dNT,∞max\nit\nmax\n1≤j≤d |Vit j| ≲P (log(d2NT)dNT,∞)\nveriﬁes (D.67). Step 4.3 Take C(Wit, bη) = (di0(Xit) −bdi(Xit))(di0(Xit) −bdi(Xit))′, which corresponds to ¯b6 in\n(D.58).",
    "content_hash": "eb55c3cf594521068a3aa4704329d2031fd2bf01a1472591e2c59cb5857acbc2",
    "location": null,
    "page_start": 80,
    "page_end": 80,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2cc37289-e6de-4012-bab9-fbb241102f01",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "46\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nProof. Decompose the matrix ﬁrst-stage error\nbΓ(bβL)−Γ = ENT bVit bV ′\nit bU2\nit −Γ\n= ENT bVit bV ′\nit( bU2\nit −U2\nit)+ENT(bVit bV ′\nit −VitV ′\nit)U2\nit +ENTVitV ′\nitU2\nit −Γ\n= ¯b6 + ¯b5 + ¯b′\n5 + ¯b4 + ¯b3 + ¯b2 + ¯b′\n2 + ¯b1. The bound on ¯b1 is given in (D.60), Lemma D.8 . The bound on ¯b2 is given in (D.61), Lemma\nD.9. The bound on ¯b3 is given in (D.62), Lemma D.10. The bounds on ¯b4 −¯b6 are given in (D.64),\nLemma D.12. Summing the bounds gives (D.69). ■\nProof of Lemma 4.3. Step 1. Deﬁne the following bounds\nΣ1 : = ∥bΩCLIME −Q−1∥∞,1∥bΓ(bβL)∥∞∥bΩCLIME∥1,∞\nΣ2 : = ∥Q−1∥∞,1∥bΓ(bβL)−Γ∥∞∥bΩCLIME∥1,∞\nΣ3 : = ∥bΩCLIME −Q−1∥∞,1∥Γ∥∞∥Q−1∥1,∞\nand note that\n∥bΣ(bβL)−Σ∥∞= ∥bΩCLIMEbΓ(bβL)bΩCLIME −Q−1ΓQ−1∥∞≤Σ1 +Σ2 +Σ3. Step 2.",
    "content_hash": "3fb5c855538e8462670baea6a10a87c4d9db886c95263f59f5715eed8a553906",
    "location": null,
    "page_start": 81,
    "page_end": 81,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c2e2982e-ef77-43e3-8282-60b533523054",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n47\n■\nD.5. Proof of Theorem 4.3. The proof is divided in several steps. Step 1 outlines the proof. Step\n2-5 establish (4.20). Steps 6-8 establish (4.22). Proof. Step 1. (Outline) Let Z ∼N(0,C ) and bZ | b\nC ∼N(0, b\nC ) be as deﬁned in the Theorem. Deﬁne\nTΣ,β :=\n√\nNTΣ−1/2\nj j\n(bβDL,j −β0),\nTbΣ,β :=\n√\nNTbΣ−1/2\nj j\n(bβDL,j −β0)\nand\nTΣ := Σ−1/2\nj j\nGNTVit jUit. Deﬁne\nO1(t) : = P(∥TΣ,β∥∞< t)−P(∥TΣ∥∞< t +δ1)\nO2(t) : = P(∥TΣ∥∞≤t +δ1)−P(∥Z∥∞< t +δ1)\nO3(t) : = P(∥Z∥∞< t +δ1)−P(∥Z∥∞< t)\nand note that for each t\nP(∥TΣ,β∥∞< t)−P(∥Z∥∞< t) =\n3\n∑\nk=1\nOk(t). Likewise, deﬁne\nO4(t) := P(∥TbΣ,β∥∞< t)−P(∥TΣ,β∥∞< t)\nand\nO5(t) := P(∥Z∥∞< t)−P(∥bZ∥∞< t | b\nC ).",
    "content_hash": "095f6de7efc61e0d208dcd68eaf95e765b1254a21a64bb026c4b0f15c40787c8",
    "location": null,
    "page_start": 82,
    "page_end": 82,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "bad28f4a-6704-48c4-9c91-0a92f9c24dd2",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "48\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nwhich implies ¯σ2Q ⪯Γ ⪯¯σ2\nUVQ, and ¯σ2Q−1 ⪯Σ ⪯¯σ2\nUVQ−1. As a result,\n0 < ¯σ2C−1\nmax ≤cΣ = min\n1≤j≤d Σj j ≤CΣ = max\n1≤j≤d Σj j ≤¯σ2\nUVC−1\nmin < ∞. Likewise, the elements of (diagΣ)−1/2 are bounded from above by c−1/2\nΣ\nand from below by C−1/2\nΣ\n. Step 3. We bound supt≥0 |O1(t)| with δ1 = log−1/2 d log−1/2 NT. Decomposition (D.47) implies\n∥TΣ∥∞−∥RNT∥∞≤∥TΣ,β∥∞≤∥TΣ∥∞+∥RNT∥∞,\nand union bound gives\nP(∥TΣ,β∥∞< t) ≤P(∥TΣ∥∞≤t +δ1)+P(∥RNT∥∞≥δ1)\nP(∥TΣ∥∞< t) ≤P(∥TΣ,β∥∞≤t +δ1)+P(∥RNT∥∞≥δ1)\nwhich gives\nsup\nt≥0\n|O1(t)| ≤P(∥RNT∥∞≥δ1) =i o(1),\n(D.72)\nwhere (i) follows from\n∥RNT∥∞≲P λ 1−1/aQ\nQ\nslog1/2 d +\n√\nNTρNT = oP(log−1/2 d log−1/2 NT)\ngiven in (D.48) and (4.19) . Step 4. We verify the conditions of Lemma C.5 for the m.d.s.",
    "content_hash": "f85ecca99923a3de8e9b37b32887c0a7de0867a6342bad3337ba1514454518a5",
    "location": null,
    "page_start": 83,
    "page_end": 83,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2aaf6bc5-c89f-43ea-8cff-a171c0b866f9",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n49\nStep 4. Bound on supt≥0 |O3(t)|. Invoking Lemma C.2 gives\nsup\nt≥0\n|O3(t)|\nsup\nt≥0\n|P(∥Z∥∞< t +δ1)−P(∥Z∥∞< t)|\n≤sup\nt≥0\n|P(∥Z∥∞< t +δ1)−P(∥Z∥∞< t −δ1)|\n= sup\nt≥0\nP(|∥Z∥∞−t| ≤δ1) ≤Cδ1\np\n1∨log(2d/δ1). Notie that the R.H.S is a non-decreasing function of δ1 in some neighborhood of 0 and that\np\n1∨(x+y) ≤1+√x+√y for x,y > 0. Plugging in δ1 = log−1/2 d log−1/2 NT gives\nsup\nt≥0\n|O3(t)| ≤Clog−1/2 d log−1/2 NT\nq\n1∨(log(2d)+log(log1/2 d log1/2 NT))\n≲log−1/2 NT +log−1/2 d log−1/2 NT log1/2 log(NT) = o(1). (D.74)\nCombining (D.72) and (D.73) and (D.74) gives (D.70). By a standard calculation we have E∥Z∥∞≲\n√log2d. Invoking Gaussian concentration inequality (see, e.g., Ledoux (2001), Theorem 7.1 or\nComment 4 in Chernozhukov et al.",
    "content_hash": "7e35f60811bfb04c921dc1d11dbcc34216fe790db483ea14a3ecdcbc0c9f4815",
    "location": null,
    "page_start": 84,
    "page_end": 84,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ae1bc883-6295-4dc9-852f-59d005d5242b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Combining the bounds above on the event min1≤j≤d ρj > 1/2 gives\nmax\n1≤j≤d |bΣ−1/2\nj j\n/Σ−1/2\nj j\n−1| = max\n1≤j≤d |ρ−1\nj\n−1| ≤2 max\n1≤j≤d |ρj −1| ≲P γNT. (D.76)",
    "content_hash": "e529e187576eb78d7259af46f01a2aaeb6be785de8e1dc3a6622d883cd6abf60",
    "location": null,
    "page_start": 84,
    "page_end": 84,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fc566d7a-4cff-48e8-af9d-576d33db46cd",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "In Step 7, we show\nthat for ζNT in (4.15),\nb∆:= ∥C −b\nC ∥∞≲i\nP ζNT =ii oP(log−2 d log−1 NT),\n(D.77)\nwhere (i) is veriﬁed in Steps 7-8 and (ii) is directly assumed in (4.21). Step 7. Note that\n∥Σ∥∞= ∥Q−1ΓQ−1∥∞≤∥Q−1∥∞,1∥Γ∥∞∥Q−1∥1,∞≤(AQ/(aQ −1))2∥Γ∥∞= O(1). As a result,\n∥bΣ∥∞≤∥bΣ−Σ∥∞+∥Σ∥∞≲P 1+γNT ≲P 1. Likewise,\n∥(diagbΣ)−1/2∥∞,1 = ∥(diagbΣ)−1/2∥1,∞= max\n1≤j≤d\nbΣ−1/2\nj j\n≲P ζNT +c−1/2\nΣ\n≲P 1. Step 8. Deﬁne\nC1 : = max\n1≤j≤d |bΣ−1/2\nj j\n−Σ−1/2\nj j\n|∥bΣ∥∞∥(diagbΣ)−1/2∥1,∞\nC2 : = ∥(diagΣ)−1/2∥∞,1∥bΣ−Σ∥∞∥(diagbΣ)−1/2∥1,∞\nC3 : = ∥(diagΣ)−1/2∥∞,1∥Σ∥∞max\n1≤j≤d |bΣ−1/2\nj j\n−Σ−1/2\nj j\n|\nand note that\n∥b\nC −C ∥∞= ∥(diagbΣ)−1/2bΣ(diagbΣ)−1/2 −(diagΣ)−1/2Σ(diagΣ)−1/2∥∞≤C1 +C2 +C3.",
    "content_hash": "e2a9d8b0eedebe77947ab3812e04c5bb714c0e21aaa856105747acdf412243d8",
    "location": null,
    "page_start": 85,
    "page_end": 85,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7d9ccc2a-ac00-4a7d-b3c3-e9ff02bfb90b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Invoking (D.76) and (4.15)\nmax\n1≤j≤d |bΣ−1/2\nj j\n−Σ−1/2\nj j\n| ≲P ζNT,\n∥bΣ−Σ∥∞≲P ζNT",
    "content_hash": "8c140aa8f835a609f52958ffde1707411e11a35696eec1fb76bf83b8e817f067",
    "location": null,
    "page_start": 85,
    "page_end": 85,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "88bed31a-79ca-47c3-a8b5-cbbbd242763c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "50\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nStep 5.2. Let v1 ·v2 denote (v1 ·v2)j = v1j ·v2j for j = 1,2,...,d. Note that\nTbΣ,β = TΣ,β ·ρ−1,\nor, equivalently,\nTbΣ,β −TΣ,β = (ρ−1 −1)TΣ,β. Invoking (D.76) and (D.75) gives\n∥TbΣ,β −TΣ,β∥∞≤max\n1≤j≤d |ρ−1\nj\n−1|∥TΣ,β∥∞\n= OP(ζNT)·OP(log1/2 d +log1/2 NT) =i oP(1),\nwhere (i) follows from (4.21). Thus, ∥TbΣ,β∥∞and ∥TΣ,β∥∞converge to the same limit in distribu-\ntion. Step 6. We bound supt≥0 |O5(t)|. Invoking Lemma C.1 with X ∼N(0,C )| b\nC and Y ∼N(0,C )\nand b∆= ∥C −b\nC ∥∞\nsup\nt≥0\n|O5(t)| ≤C′(b∆log2(2d))1/2,\nwhere C depends only on the constants deﬁned in Assumptions 4.2 and 4.3.",
    "content_hash": "326e731ba092743ad1d020866d7c9e0184161704b4d711c2b1b4cdf0f72b8716",
    "location": null,
    "page_start": 85,
    "page_end": 85,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2e672f53-e2a8-464f-88e0-bd138533b5ce",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "■\n■\nProof of Remark 5.3. Step 1 shows that pNT ≤N−1/2(2(Bmax +1))1/2ζNT,∞. Step 2 shows that wp\n1−o(1),\nsup\nit\n|pi(Xit)−pi0(Xit)| ≤2ζNT,∞. Step 1. For any δ P and ξ ∈¯PNT,\n∥δ P −δ P\n0 ∥2 ≤∥δ P −δ P\n0 ∥1 ≤N−1/2ζNT,∞\n(E.1)\n∥ξ −ξ0∥2 ≤∥ξ −ξ0∥1 ≤ζNT,∞. (E.2)\nCauchy inequality gives\n(pi(Xit)−pi0(Xit))2 = (X′\nit(δ P −δ P\n0 )+ξi −ξi0)2 ≤2(X′\nit(δ P −δ P\n0 ))2 +2(ξi −ξi0)2. Summing over i = 1,2,...,N and t = 1,2,...,T gives\np2\nNT ≤2(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE(X′\nit(δ P −δ P\n0 ))2 +2N−1∥ξ −ξ0∥2\n≤2BmaxN−1ζ 2\nNT,∞+2N−1ζ 2\nNT,∞.",
    "content_hash": "0be6fa17ca402a8c0582fbf2d0ff10f656230b1111a9995adea67adc23acad31",
    "location": null,
    "page_start": 86,
    "page_end": 86,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "18e07c90-1a7c-43de-a021-8c7e88f0d0bd",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n51\nimplies that each term Cj is a product of two OP(1) terms and a single OP(ζNT) term. Thus,\nC1 +C2 +C3 ≲P ζNT veriﬁes (i) in (D.77). ■\nProof of Lemma 4.4. We invoke Lemma D.6 with ¯Vit = Dit −di0(Zit) and ¯eYit = Yit −li0(Zit) and\n¯β0 = (β0,ρ0) and g = 2. Steps 1, 2 and 3 are established similarly to the proof of Theorem\n4.1. Thus, the bounds (4.27) hold for the Orthogonal Group Lasso. As a result, ∥bβL −β0∥1 ≤\np\ns2 logd/NT wp 1−o(1). As a result, the debiased Orthogonal Group Lasso obeys the uniform\nlinearization result (D.47), and Theorems 4.2 and 4.3 hold. ■\nAPPENDIX E. PROOFS FOR SECTION 5\nProof of Remark 5.1. To prove this, let ∥·∥ψ2 denote the Orlizs sub-Gaussian norm under the prob-\nability measure P (see van der Vaart and Wellner (1996)). Then\n∥∥Fit∥∥ψ2 ≤∥∥ΠitFi,t−1∥∥ψ2 +∥∥QTit∥∥ψ2 ≤(1−δ)∥∥Fi,t−1∥∥ψ2 +A′ ¯σ2,\nwhere A′ is a numerical constant. Iterating on this inequality exactly t times we obtain\n∥∥Fit∥∥ψ2 ≤(1−δ)t∥∥Fi,0∥∥ψ2 +A′\nt−1\n∑\n¯t=1\n(1−δ)¯t ¯σ2 ≤A′\n¯σ2\n1−δ .",
    "content_hash": "8bf786062f54aa55a3707e92218ec22c4355ad546e874134fab8da8d1c9b1153",
    "location": null,
    "page_start": 86,
    "page_end": 86,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "97450a0f-aede-44ad-b15c-e57c6d331ced",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "52\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nWp 1−o(1), max1≤i≤N,1≤t≤T ∥Xit∥∞≤CX\n√logdXNT for some ﬁnite CX by Lemma B.1. Step 2. The following bound holds wp 1−o(1),\nsup\nit\n|pi(Xit)−pi0(Xit)| ≤sup\nit\n|X′\nit(δ P −δ P\n0 )|+|ξi −ξi0|\n≤sup\nit\n∥Xit∥∞∥δ P −δ P\n0 ∥1 +∥ξ −ξ0∥1\n≤CX\np\nlog(dXNT)N−1/2ζNT,∞+ζNT,∞\n≤2ζNT,∞,\nwhere the last step holds assuming N is large enough and CX\np\nlog(dXNT)/N ≤1. ■\nProof of Remark 5.5. Step 1 shows that lNT = O(N−1/2(ζNT,∞+ ζ E\nNT,∞)). Step 2 shows that wp\n1−o(1),\nsup\nit\n|li(Xit)−li0(Xit)| ≤2 ¯K∥β0∥1ζNT,∞+2ζ E\nNT,∞. Step 1.",
    "content_hash": "28134e31060b98ebf4c86237293e0f1874e19c98c5f2ed233a172eee1fcf336b",
    "location": null,
    "page_start": 87,
    "page_end": 87,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "555d0c46-0e44-411d-9990-e8c799d76f97",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Decompose\nli(Xit)−li0(Xit) = (di(Xit)−di0(Xit))′β0 +X′\nit(δ E −δ E\n0 )+ξ E\ni −ξ E\ni0 +di(Xit)′(β −β0)\nCauchy inequality gives\n(li(Xit)−li0(Xit))2 ≤4\n\u0012\n((di(Xit)−di0(Xit))′β0)2\n+(X′\nit(δ E −δ E\n0 ))2 +(ξ E\ni −ξ E\ni0)2 +(di(Xit)′(β −β0))2\n\u0013\n. Note that di(Xit) = K(Xit)pi(Xit) = K(Xit)(X′\nitδ P + ξi).",
    "content_hash": "5caed6e49f4901bf8dc9d7d044316ac06e2f7fc1ddf936dd0ed4ec3f052655a5",
    "location": null,
    "page_start": 87,
    "page_end": 87,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "39791b4b-b4ac-4a4b-8655-a8bfcd658551",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Summing over i = 1,2,...,N and t =\n1,2,...,T gives\nl2\nNT ≤4(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n(δ P −δ P\n0 )′E[(K′\nitβ0)2XitX′\nit](δ P −δ P\n0 )\n(δ P−δ P\n0 )′ΨD(δ P−δ P\n0 )\n+4(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE(X′\nit(δ E −δ E\n0 ))2 +4N−1∥ξ E −ξ E\n0 ∥2\n2 +4(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE∥di(Xit)∥2\n∞∥β −β0∥2\n1\n≤4(BmaxN−1ζ 2\nNT,∞+BmaxN−1(ζ E\nNT,∞)2 +N−1(ζ E\nNT,∞)2 +B4N−1(ζ E\nNT,∞)2)\n≤4(Bmax +1+B4)N−1(ζ E\nNT,∞)2 +4N−1Bmaxζ 2\nNT,∞.",
    "content_hash": "d8d6455184e0f61efce5759692fd3a3a44f04984d1d0913a95d2478e3dfae8b0",
    "location": null,
    "page_start": 87,
    "page_end": 87,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b698f6f3-4871-4b11-9002-0dd3a8079bbb",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n53\nNote that for N,T large enough such that ∥δ P∥2\n2 ≤2∥δ P\n0 ∥2\n2 ≤2∥δ P\n0 ∥2\n1, which is bounded,\nB4 = (NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE∥di(Xit)∥2\n∞\n≤¯K2(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE(X′\nitδ P +ξi)2 ≤2 ¯K2(δ P′ΨXδ P +N−1∥ξ∥2)\n≤2 ¯K2(Bmax∥δ P∥2\n2 +N−1∥ξ∥2\n2)\n≤4 ¯K2Bmax∥δ P\n0 ∥2\n2 +1. Step 2.",
    "content_hash": "e0d2ee6430697c640e69b293157461c248b1b8b9b1a71bb0683a995e46903724",
    "location": null,
    "page_start": 88,
    "page_end": 88,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1e9c7ac1-4268-4e6c-abda-6b18ed266075",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For N,T large enough, wp 1−o(1),\nsup\nit\n|li(Xit)−li0(Xit)| ≤sup\nit\n|K′\nitβ0|sup\nit\n|pi(Xit)−pi0(Xit)|\n+sup\nit\n∥Xit∥∞∥δ E −δ E\n0 ∥1\n+sup\ni\n|ξi −ξ E\ni |\n+sup\nit\n∥K(Xit)∥∞|X′\nitδ E|∥β −β0∥1\n≤2 ¯K∥β0∥1ζNT,∞+∥Xit∥∞(N−1/2ζ E\nNT,∞+ ¯K∥δ E∥1N−1/2ζ E\nNT,∞)+∥ξ E −ξ E\n0 ∥1\n≤2 ¯K∥β0∥1ζNT,∞+CX\np\nlog(dXNT)N−1/2(1+ ¯K∥δ E∥1)ζ E\nNT,∞+ζ E\nNT,∞\n≤2 ¯K∥β0∥1ζNT,∞+2ζ E\nNT,∞,\nwhere the last step holds assuming N is large enough and ∥δ E∥1 ≤∥δ E\n0 ∥1 and\nCX\np\nlog(dXNT)/N(1+2 ¯K∥δ E\n0 ∥1) ≤1. ■\nProof of Remark 5.6. Invoking Remark 5.3 and the bound (5.34) on ζ P\nNT,∞in Lemma 5.1 gives\n√\nNTp2\nNT ≲\n√\nNTN−1(ζ P\nNT,∞)2 ≲(SP)2N−1/2T 1/2T ν−1 log3(1−ν)(dX +N)\n≲(SP)2N−1/2T ν−1/2 log3(1−ν)(dX +N) = o(1).",
    "content_hash": "53038b41b7bdde2094ab40bb7b98ffca6c18b58c971b7f72f3b8519e71d78dab",
    "location": null,
    "page_start": 88,
    "page_end": 88,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "623a501d-6e1d-425a-b31d-5882fddf8bc5",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "In addition, the bound (5.36) on ζ E\nNT,∞in Lemma 5.2 gives\n√\nNTpNTlNT ≲\n√\nNTN−1ζ P\nNT,∞ζ E\nNT,∞\n≲SP ·SEN−1/2T (ν+νE)/2−1 log3(1−(ν+νE)/2)(dX +N) = o(1). ■\n■",
    "content_hash": "27ea8743f6d63ee4af98af25fddd0ff68b3973584842823488e2d5495749d175",
    "location": null,
    "page_start": 88,
    "page_end": 88,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5d3ddbc1-805a-4ad8-92dd-526a387f67ba",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(F.4)\nand\nmax\n∥\nN\n∑\ni=1\nL\n∑\nl=1\nEφi(B∗\ni(2l))′φi(B∗\ni(2l))∥,∥\nN\n∑\ni=1\nL\n∑\nl=1\nEφi(B∗\ni(2l))φi(B∗\ni(2l))′∥\n! ≤qNTσ2\n(F.5)\nmax\n∥\nN\n∑\ni=1\nL\n∑\nl=1\nEφi(B∗\ni(2l−1))′φi(B∗\ni(2l−1))∥,∥\nN\n∑\ni=1\nL\n∑\nl=1\nEφi(B∗\ni(2l−1))φi(B∗\ni(2l−1))′∥\n! ≤qNTσ2\n(F.6)\nmax\n∥\nN\n∑\ni=1\nEφi(Bir)φi(Bir)′∥,∥\nN\n∑\ni=1\nEφi(Bir)′φi(Bir)∥\n! ≤qNTσ2\n(F.7)",
    "content_hash": "ebefaf00bf0138f3327e331d425f66725b4b92fbe0d9a7df333847ed280bd608",
    "location": null,
    "page_start": 89,
    "page_end": 89,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "25216706-0f05-4594-93cc-2e1a69a668a5",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "54\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nAPPENDIX F. TOOLS: TAILS BOUNDS FOR EMPIRICAL RECTANGULAR MATRICES UNDER\nWEAK DEPENDENCE\nLemma F.1 (Rectangular Matrix Bernstein, Theorem 1.6 in Tropp (2012)). Consider a ﬁnite se-\nquence {Ξm}M\nm=1 of independent, random matrices with dimensions d1 × d2. Assume that there\nexist constants RΞ and σΞ such that\nEΞm = 0,\n∥Ξm∥≤RΞ a.s. . (F.1)\nDeﬁne\nσ2\nΞ = max\nE\nM\n∑\nm=1\nΞ′\nmΞm\n,\nE\nM\n∑\nm=1\nΞmΞ′\nm\n! . (F.2)\nThen, for all t ≥0,\nP\nM\n∑\nm=1\nΞm\n≥t\n! ≤(d1 +d2)e−(t2/2(σ2\nΞ+RΞt/3)). (F.3)\nLemma F.2 (Tail Bounds for Weakly Dependent Sums, Operator Norm). Consider the setup of\nLemma B.4 with weakly dependent data {Wit} and matrix-valued functions {φi(·)}N\ni=1 : W →\nRd1×d2. Let q = ⌊(2/κ)log(NT)⌋be as in (B.16) and L = ⌊T/2q⌋.",
    "content_hash": "a449ac1b2541351c1f14f3da99f7b0d3bcedd11913268af9b81af36b90fcd8a8",
    "location": null,
    "page_start": 89,
    "page_end": 89,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8706aa9e-0c37-4098-ba29-c41c2586b8d3",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For i = 1,2,...,N and\nl = 1,2,...,L, let the data blocks Bi(2l−1), Bi2l and Bir be as in (B.8)–(B.10). Let the full-sized\nodd-block sums φi(Bi(2l−1)) be as in (B.11), that is,\nφi(Bi(2l−1)) =\nt=(2l−2)q+q\n∑\nt=(2l−2)q+1\nφi(Wit),\nφi(Bi(2l)) =\nt=(2lq)\n∑\nt=(2l−1)q+1\nφi(Wit)\nand let φi(B∗\ni(2l−1)) and φi(B∗\ni(2l)) be their Berbee copies. In case T ̸= 2Lq, the remainder block\nφi(Bir) as in (B.13), that is\nφi(Bir) :=\nT\n∑\nt=2Lq+1\nφi(Wit)\nSuppose that there exist constants R and σ such that the following conditions hold\nEφi(Wit) = 0,\nsup\nit\n∥φi(Wit)∥≤R a.s.",
    "content_hash": "c3460f8ddbd8e58916f398c8aff2adb47e366b3d2c051a3bc35dffa608bd215d",
    "location": null,
    "page_start": 89,
    "page_end": 89,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b272263a-4721-4c62-a5a5-e7f7d5af3f3c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n55\nThen, for any t ≥0,\nP(∥(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nφi(Wit)∥≥3t) ≤3(d1 +d2)e−t2NT/2(qσ2+qRt/3) +2NLγ(q)\n(F.8)\nand under geometric beta-mixing condition (4.1),\n1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nφi(Wit)\n≲P\n1\n√\nNT\n\u0012\nσ\np\nlog(NT)log(d1 +d2)+\n1\n√\nNT log(NT)Rlog(d1 +d2)\n\u0013\n. (F.9)\nRemark F.1. In what follows, we write φ(Wit) in place of φi(Wit), but subsume the dependence on\ni. Proof of Lemma F.2. Union bound gives\nP(∥\nN\n∑\ni=1\nT\n∑\nt=1\nφ(Wit)∥≥3t)\n(F.10)\n≤P(∥\nN\n∑\ni=1\nL\n∑\nl=1\nφ(B∗\ni(2l−1))∥≥t)\n+P(∥\nN\n∑\ni=1\nL\n∑\nl=1\nφ(B∗\ni(2l))∥≥t)+P(∥\nN\n∑\ni=1\nφ(Bir)∥≥t)+2NLγ(q). We ﬁrst establish the bound for the odd-block sums. Deﬁne\nm = m(i,l) = L·(i−1)+l,\nM = NL,\nΞm := φ(B∗\ni(2l−1)).",
    "content_hash": "df1b806e7dcabae7a40d1b0848b7a49c4d1d3e4218d200be2201df5e8e5848a5",
    "location": null,
    "page_start": 90,
    "page_end": 90,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "637512cf-99be-4232-a72d-87d077243b15",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Since φ(B∗\ni(2l−1)) consists of q summands and W ∗\nit and Wit have the same marginal distributions,\nthe bound (F.4) gives\n∥φ(B∗\ni(2l−1))∥≤qR a.s. ,\nwhich veriﬁes (F.1) with RΞ = qR. Likewise, (F.5) directly veriﬁes (F.2) with the bound σ2\nΞ =\nqNTσ2. Invoking Lemma F.1 gives\nP\n∥\nN\n∑\ni=1\nL\n∑\nl=1\nφ(B∗\ni(2l−1))∥≥t\n! ≤(d1 +d2)e−t2/2(qNTσ2+qRt/3). A similar bound holds for the even-numbered sums. For the remainder blocks, we take\nm = i,\nM = N,\nΞm = φ(Bir). Since the remainder block has at most q elements,\n∥φ(Bir)∥≤qR a.s.",
    "content_hash": "bf253e42c8c3c6a434491d9f3af7b2be1196c58ef71857db6a1f40a7e00dd829",
    "location": null,
    "page_start": 90,
    "page_end": 90,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "46d6d2fe-d279-458c-a377-322cd1521a5b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": ",\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE∥γ(Xit)∥2 ≤γ2\n2NT\n(F.12)\nThen, the conditions (F.4) and (F.5)–(F.7) hold with\nR = 2γ∞\nNT,\nσ2 = 2γ2\n2NT. (F.13)",
    "content_hash": "54f63e241564b10f119693321291f596bce84d81acb4e33c88c5d7aa719d2d41",
    "location": null,
    "page_start": 91,
    "page_end": 91,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6dd2e343-8753-481a-b3a2-baf235f5dd23",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "56\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nwhich implies (F.1) with RΞ = qR. Likewise, (F.7) directly veriﬁes (F.2) with the bound σ2\nΞ =\nqNTσ2. Therefore,\nP\n∥\nN\n∑\ni=1\nφ(Bir)∥> t\n! ≤(d1 +d2)e−t2/2(qNTσ2+qRt/3). Invoking union bound (F.10) gives\nP\n∥\nN\n∑\ni=1\nT\n∑\nt=1\nφ(Wit)∥≥3t\n! ≤3(d1 +d2)e−t2/2(qNTσ2+qRt/3) +2NLγ(q). Plugging t(NT) in place of t gives and dividing each side by NT gives\nP\n∥1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nφ(Wit)∥≥3t\n! ≤3(d1 +d2)e−t2(NT)2/2(qNTσ2+qRNTt/3) +2NLγ(q)\n= 3(d1 +d2)e−t2NT/2(qσ2+qRt/3) +2NLγ(q),\nwhich coincides with (F.8). For geometric mixing, taking q as in (B.16) gives NLγ(q) = o(1).",
    "content_hash": "e91a16fe8344f80eac5316a437f551ec22f8e4dbbef1eba09f8aa025da786b03",
    "location": null,
    "page_start": 91,
    "page_end": 91,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8d7ddb68-7a0c-4429-b844-a4874b315f40",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n57\nAs a result, the bound (F.9) in Lemma F.2 reduces to\n1\nNT\nN\n∑\ni=1\nT\n∑\nt=1\nφ(Xit)\n≲P\n1\n√\nNT\n\u0012\nγ2NT\np\nlog(NT)log(d1 +d2)+\n1\n√\nNT log(NT)γ∞\nNT log(d1 +d2)\n\u0013\n. (F.14)\nProof of Lemma F.3 . Step 1. Let X and ¯X be two random vectors, and γ(X) and γ( ¯X) be d1 ×d2\nmatrices. The following inequalities hold\n∥Eγ(X)γ( ¯X)′∥≤i E∥γ(X)γ( ¯X)′∥≤ii E∥γ(X)∥∥γ( ¯X)′∥\n≤iii\nq\nE∥γ(X)∥2E∥γ( ¯X)′∥2\n≤iv 1/2(E∥γ(X)∥2 +E∥γ( ¯X)′∥2) =v 1/2(E∥γ(X)∥2 +E∥γ( ¯X)∥2),\n(F.15)\nwhere (i) follows from the convexity of the norm and Jensen’s inequality, (ii) from sub-multiplicativity\nof operator norm ∥AB∥≤∥A∥∥B∥, (iii)-(iv) from Cauchy inequalities and (v) from ∥A′∥= ∥A∥.",
    "content_hash": "d13d9d3a78a00791305c77e1f541180907b8a4e61589b954db3d62e7521e9e6f",
    "location": null,
    "page_start": 92,
    "page_end": 92,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e2007554-e616-4d39-8680-d1a6ef88647b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Likewise,\n∥Eγ(X)Eγ( ¯X)′∥≤i ∥Eγ(X)∥∥Eγ( ¯X)′∥\n≤ii 1/2((∥Eγ(X)∥)2 +(∥Eγ( ¯X)′∥)2)\n≤iii 1/2(E∥γ(X)∥2 +E∥γ( ¯X)′∥2) =iv 1/2(E∥γ(X)∥2 +E∥γ( ¯X)∥2).,\n(F.16)\nwhere (i) follows from ∥AB∥≤∥A∥∥B∥, (ii) from Cauchy inequality, (iii) from the convexity\nof composition t →t2 and · →∥· ∥and Jensen’s inequality and (iv) from ∥A′∥= ∥A∥. Finally,\nsince the RHS of (F.15) and (F.16) is invariant under transposition, the same bound holds for the\ntransposed quantities:\nmax(∥Eγ(X)′γ( ¯X)∥,∥Eγ(X)γ( ¯X)′∥) ≤1/2(E∥γ(X)∥2 +E∥γ( ¯X)∥2)\nmax(∥Eγ(X)′Eγ( ¯X)∥,∥Eγ(X)Eγ( ¯X)′∥) ≤1/2(E∥γ(X)∥2 +E∥γ( ¯X)∥2). Step 2.",
    "content_hash": "8ec13cbde766d94bdba86fb098a2576d6c65c4f97bffbc20f02241bf438558cc",
    "location": null,
    "page_start": 92,
    "page_end": 92,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9ba37190-392c-46aa-9dfb-59a68ae7b0d5",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "For φ(X) = γ(X)−Eγ(X),\nEφ(X)φ( ¯X)′ = Eγ(X)γ( ¯X)′ −Eγ(X)Eγ( ¯X)′ −Eγ(X)Eγ( ¯X)′ +Eγ(X)Eγ( ¯X)′\n(F.17)\n= Eγ(X)γ( ¯X)′ −Eγ(X)Eγ( ¯X)′. Let {Xmz}M,Z\nm,z=1 be a double-indexed sequence. For every value of m,\n(\nZ\n∑\nz=1\nγ(Xmz))(\nZ\n∑\nz′=1\nγ(Xmz′))′ =\n∑\n1≤z,z′≤Z\nγ(Xmz)γ(Xmz′)′.",
    "content_hash": "245c5d18161e10a65a9bca4bcdaf6e989a596bcd99c721eeecd5296e6588a2fe",
    "location": null,
    "page_start": 92,
    "page_end": 92,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "734e8eac-510b-4322-b2d2-5d68e887ff31",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(F.18)\nLikewise,\n∥M2∥≤\nM\n∑\nm=1 ∑\n1≤z,z′≤Z\n∥Eγ(Xmz)Eγ(Xmz′)′∥\n≤1/2\nM\n∑\nm=1\nZ\n∑\nz=1\nZ\n∑\nz′=1\n(E∥γ(Xmz)∥2 +E∥γ(Xmz′)∥2)\n= Z\nM\n∑\nm=1\nZ\n∑\nz=1\nE∥γ(Xmz)∥2. (F.19)\nAs a result,\n∥M1 −M2∥≤∥M1∥+∥M2∥≤2Z\nM\n∑\nm=1\nZ\n∑\nz=1\nE∥γ(Xmz)∥2. Because the bounds (F.15) and (F.16) are invariant to transpositions of γ(X) and/or γ( ¯X),\nM\n∑\nm=1\nE[\nZ\n∑\nz=1\nφ(Xmz)′][\nZ\n∑\nz=1\nφ(Xmz′)]\n≤2Z\nM\n∑\nm=1\nZ\n∑\nz=1\nE∥γ(Xmz)∥2. (F.20)",
    "content_hash": "a1eea18374021247e010c767ac854edcf12dac5e7fee1b4dd98190988552bf91",
    "location": null,
    "page_start": 93,
    "page_end": 93,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4e239f15-c43d-48a9-b2eb-985f5da391a4",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "58\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nDeﬁne\nM1 := E\nM\n∑\nm=1\n(\nZ\n∑\nz=1\nγ(Xmz))(\nZ\n∑\nz=1\nγ(Xmz′))′ =\nM\n∑\nm=1 ∑\n1≤z,z′≤Z\nEγ(Xmz)γ(Xmz′)′\nM2 :=\nM\n∑\nm=1\n(E\nZ\n∑\nz=1\nγ(Xmz))(E\nZ\n∑\nz=1\nγ(Xmz′))′ =\nM\n∑\nm=1 ∑\n1≤z,z′≤Z\nEγ(Xmz)Eγ(Xmz′)′\nInvoking (F.17) gives\nM\n∑\nm=1\nE[\nZ\n∑\nz=1\nφ(Xmz)][\nZ\n∑\nz=1\nφ(Xmz′)]′ = M1 −M2. Step 3. The bound on ∥M1∥is\n∥M1∥≤\nM\n∑\nm=1 ∑\n1≤z,z′≤Z\n∥Eγ(Xmz)γ(Xmz′)′∥≤1/2\nM\n∑\nm=1\nZ\n∑\nz=1\nZ\n∑\nz′=1\n(E∥γ(Xmz)∥2 +E∥γ(Xmz′)∥2)\n= Z/2(\nM\n∑\nm=1\nZ\n∑\nz=1\nE∥γ(Xmz)∥2 +\nM\n∑\nm=1\nZ\n∑\nz′=1\nE∥γ(Xmz′)∥2) = Z\nM\n∑\nm=1\nZ\n∑\nz=1\nE∥γ(Xmz)∥2.",
    "content_hash": "27d00bd2875bb4fb15c2fcfa6074f10d68ca7469a4542afb4dcbc003147e8f0f",
    "location": null,
    "page_start": 93,
    "page_end": 93,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c30a4fec-e266-4b90-b634-6216752e48c3",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(F.23)\nA similar argument for even-numbered full-sized blocks and φ(B∗\ni(2l)) = ∑\nt=(2l)q\nt=(2l−1)q+1 φ(X∗\nit) veri-\nﬁes condition (F.6) of Lemma F.2. Finally, if the remainder block is non-empty, i.e., T −2Lq ̸= 0,\ninvoking (F.20) with\nm = i,\nM = N,\nZ := T −2Lq\nand noting that\nN\n∑\ni=1\nEφ(Bir)φ(Bir)′\n≤2q\nN\n∑\ni=1\nT−2Lq\n∑\nz=1\nE∥γ(Xiz)∥2 ≤2q\nN\n∑\ni=1\nT\n∑\nt=1\nE∥γ(Xit)∥2 = 2qNTγ2\n2NT,\n(F.24)\nwhich veriﬁes condition (F.7) of Lemma F.2. Finally, the condition (F.4) follows from\n∥φ(B∗\ni(2l−1))∥≤qR a.s. ,∥φ(B∗\ni(2l))∥≤qR a.s. ,∥φ(Bir)∥≤qR,\nsince each block has at most q summands. Plugging R = 2γ∞\nNT and q = 2γ2\n2NT into (F.9) gives\n(F.14). ■\nCorollaries F.5 and F.6 are special cases of Lemma F.3 with various cases of γ-function. Corollary F.5 (Covariance Matrix Moments).",
    "content_hash": "8d2b212516a69e0a35333ce4bb318aec82db4b7bac1030ccd9bf14c34ad613c7",
    "location": null,
    "page_start": 94,
    "page_end": 94,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ae3acddc-e875-44f4-b50a-fe35821c3f50",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n59\nStep 4. We ﬁrst verify the condition (F.5) for the odd-numbered full-sized blocks. We note that\nthe L.H.S of (F.5) is a special case of the L.H.S of (F.20) with\nm = m(i,l) = L·(i−1)+l,\nM = NL,\nZ = q\nXmz : = Xi,(2l−2)q+z\nφ(Bi(2l−1)) =\nt=(2l−2)q+q\n∑\nt=(2l−2)q+1\nφ(Xit) =\nq\n∑\nz=1\nφ(Xmz). As a result,\nN\n∑\ni=1\nL\n∑\nl=1\nEφ(B∗\ni(2l−1))φ(B∗\ni(2l−1))′\n≤2q\nN\n∑\ni=1\nL\n∑\nl=1\nq\n∑\nz=1\nE∥γ(X∗\ni(2l−1),z)∥2\n(F.21)\n= 2q\nN\n∑\ni=1\nL\n∑\nl=1\nq\n∑\nz=1\nE∥γ(Xi(2l−1),z)∥2\n(F.22)\n≤2q\nN\n∑\ni=1\nT\n∑\nt=1\nE∥γ(Xit)∥2 = 2qNTγ2\n2NT.",
    "content_hash": "0a603aa84199abb89e79145381f435f9351490244edfeeb84a8b8693d935fc66",
    "location": null,
    "page_start": 94,
    "page_end": 94,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8c2de4c2-713c-46dc-b91c-df5393cd54e0",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Let ψ(X) : X →Rd×1 be a ﬁxed vector function\nof a random vector X. Deﬁne\nγ(X) = ψ(X)ψ(X)′\n(F.25)\nand the φ-function\nφ(X) := γ(X)−E[γ(X)] = ψ(X)ψ(X)′ −E[ψ(X)ψ(X)′].",
    "content_hash": "7d760962c2190b06a179dbd0d115362f72731eff806e8b0947472d89584dd231",
    "location": null,
    "page_start": 94,
    "page_end": 94,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d9d14991-0859-4ccf-98f5-10eec0e1f0f9",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Let ψ(X) : X →Rd×1 be a ﬁxed vector function of a random\nvector X and ξ(X) be a random variable. Deﬁne\nγ(X) := ψ(X)·ξ(X)\nLet the numeric sequences ψ∞\nNT,ξ ∞\nNT and ψ4NT,ξ4NT obey\nsup\nit\n∥ψ(Xit)∥≤ψ∞\nNT a.s. ,\nsup\nit\n|ξ(Xit)| ≤ξ ∞\nNT a.s. (F.29)\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE∥ψ(Xit)∥4 ≤ψ4\n4NT,\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nEξ 4(Xit) ≤ξ 4\n4NT. (F.30)\nThen, the bound (F.12) holds with\nγ∞\nNT := (ψ∞\nNT)·ξ ∞\nNT,\nγ2\n2NT := 1/2(ψ4\n4NT +ξ 4\n4NT). As a result, the rate (F.14) reduces to\n∥(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nφ(Xit)∥≲P\nq\n(ψ4\n4NT +ξ 4\n4NT)log(NT)log(d +1)/NT\n(F.31)\n+log(NT)ψ∞\nNTξ ∞\nNT log(d +1)/NT.",
    "content_hash": "a9c01850116d65413af15d4d04c37dd6a458dc4a7ce51d9923df1add8bd97b25",
    "location": null,
    "page_start": 95,
    "page_end": 95,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c9e60315-937b-4b6f-867a-877bc3fccb4e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "60\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nLet the numeric sequences ψ∞\nNT and ψ4NT obey\nsup\nit\n∥ψ(Xit)∥≤ψ∞\nNT a.s. (F.26)\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE∥ψ(Xit)∥4 ≤ψ4\n4NT\n(F.27)\nThen, the bound (F.12) hold with γ∞\nNT := (ψ∞\nNT)2 and γ2\n2NT := ψ4\n4NT. As a result, the rate (F.14)\nreduces to\n∥(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nφ(Xit)∥≲P\nq\nψ4\n4NT log(NT)log(2d)/NT +log(NT)(ψ∞\nNT)2 log(2d)/NT. (F.28)\nProof of Corollary F.5. Noting that\n∥γ(Xit)∥∞≤∥ψ(Xit)∥2\n∞≤(ψ∞\nNT)2\nand\n∥γ(Xit)∥2 = ∥ψ(Xit)ψ(Xit)′∥2 ≤∥ψ(Xit)∥2∥ψ(Xit)′∥2 = ∥ψ(Xit)∥4\nTherefore,\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE∥γ(Xit)∥2 ≤(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE∥ψ(Xit)∥4 ≤ψ4\n4NT. Application of Lemma F.3 yields the result. ■\nCorollary F.6 (Product Moments).",
    "content_hash": "89a24229ae50fffcf4d8c13d95d7accfe26d70d1126489313302b8a1b5056ea4",
    "location": null,
    "page_start": 95,
    "page_end": 95,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "95d1677a-e2f7-42fe-84fd-9763954735d7",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n61\nProof of Corollary F.6 . Noting that\n∥γ(Xit)∥∞≤∥ψ(Xit)∥∞|ξ(Xit)| ≤ψ∞\nNTξ ∞\nNT\nand\n∥γ(Xit)∥2 = ∥ψ(Xit)∥2ξ 2(Xit). Cauchy inequality gives\nE∥γ(Xit)∥2 = E∥ψ(Xit)∥2ξ 2(Xit) ≤1/2(E∥ψ(Xit)∥4 +Eξ 4(Xit)). Therefore,\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE∥γ(Xit)∥2 ≤(NT)−1(\nN\n∑\ni=1\nT\n∑\nt=1\nE∥ψ(Xit)∥4 +\nN\n∑\ni=1\nT\n∑\nt=1\nEξ 4(Xit)) ≤(ψ4\n4NT +ξ 4\n4NT)/2. Application of Lemma F.3 yields the result. ■\nAPPENDIX G. ADDITIONAL RESULTS ON ORTHOGONAL OLS\nAssumption G.1 (Tail Bound on Empirical Covariance Matrix in ℓ2 norm). For some sequence\nvNT = o(1), in the regime where d →∞, we have that\n∥eQ−Q∥≲P vNT. (G.1)\nRemark G.1. Suppose Assumptions 4.1–4.3 hold and supit ∥Vit∥∞≤R a.s. and\nmax\nit j EV 4\nit j ≤σ4\n4V\nWe invoke Corollary F.5 with ψ(Wit) = Vit and ψ∞\nNT :=\n√\ndR and (F.27) with ψ4\n4NT = d2σ4\n4V.",
    "content_hash": "ede277340a424d56b1c201d29b23421da24ab154065df5f8862ac6814b06550b",
    "location": null,
    "page_start": 96,
    "page_end": 96,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cf2740da-e96b-4025-a5cd-be8cde39dd2c",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(G.5)\n(2) For any deterministic sequence {α} = {αN,T} with ∥αN,T∥= 1, the estimator α′ bβOLS of\nα′β0 is asymptotically linear:\n√\nNTα′(bβOLS −β0) = α′Q−1GNTVitUit +oP(1),\n(G.6)\n(3) If the Lindeberg condition holds for each M > 0:\nlimsup\nNT→∞\nsup\n∥α∥2=1\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\nE[(α′VitUit)21{|α′VitUit| > M\n√\nNT} = 0,\nthen the Orthogonal Least Squares estimator is asymptotically Gaussian:\nlim\nNT→∞sup\n∥α∥2=1\nsup\nt∈R\nP\n√\nNTα′(bβOLS −β0)\n∥α′Σ∥1/2\n< t\n! −Φ(t)\n= 0. (G.7)",
    "content_hash": "97bccd8834254fa5b26e6d334d4285862b808f868a6540ed2aa693b69a0e1d55",
    "location": null,
    "page_start": 97,
    "page_end": 97,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "118cf227-01ab-4eb3-9a44-aae3f33c6ea8",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "62\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nDeﬁne the following rates\nr2NT : = dNTlNT +\ns\n(d4\nNT,4 +l4\nNT,4)log(NT)log(d +1)\nNT\n+\n√\nd log(d +1)log(NT)/NT\n(G.2)\nχNT : = d2\nNT +\nq\nd4\nNT,4 log(2d)log(NT)/NT +d log(2d)log(NT)/NT\n(G.3)\nAssumption G.2. We suppose that the true parameter vector has bounded ℓ2-norm:\n∥β0∥2 ≤¯Cβ\nfor some ﬁnite constant ¯Cβ; We suppose that the reduced form estimators obey: bl(·) ∈LNT and\nbd(·) ∈DNT if such that dNT, dNT,4, lNT,lNT,4 decay sufﬁciently fast:\nr2NT + χNT = o((NT)−1/2)\n(G.4)\nFurthermore, the reduced form estimates are bounded as\nsup\nd∈DNT\n∥di(Xit)∥≤\n√\ndD,\nsup\nl∈LNT\n|li(Xit)| ≤L\n∀i. Theorem G.1 (Orthogonal Least Squares). Suppose Assumptions 4.1–4.3, G.1 and G.2 hold. Then,\nthe following statements hold. (1) The Orthogonal Least Squares estimator converges at the rate\np\nd/NT:\n∥bβOLS −β0∥2 ≲P\np\nd/NT.",
    "content_hash": "0d9caf845a1b4523df1dbe833f01b97f9b176b187d835797575b92950dae5818",
    "location": null,
    "page_start": 97,
    "page_end": 97,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7dd47d1f-fea8-4332-85be-1a23d19569ba",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n63\nLemma G.1 (First-Order Terms, ℓ2-norm). Let ¯a, ¯m, ¯f, ¯e be as in (D.2)–(D.5). Under Assumptions\n4.1–4.3, the following bounds hold\n∥¯a∥≲P\n\u0010p\nd/NTdNT\n\u0011\n(G.8)\n∥¯m∥≲P\n\u0010p\nd/NTlNT\n\u0011\n(G.9)\n∥¯f∥≲P (dNT(NT)−1/2)\n(G.10)\n∥¯e∥≲P (\np\nd/NT(dNT +lNT))\n(G.11)\nProof of Lemma G.1. Deﬁne\nξV\nNT :=\np\nd/NTdNT,\nξ B\nNT = 0,\nand the A-function as\nA(Wit,η) = Vit(di0(Xit)−di(Xit)). (G.12)\nDeﬁne BAk(η) and VAk(η) with η = d as in (A.16)–(A.17). Consider any η = ηNT ∈DNT in what\nfollows. Since Vit obeys the martingale difference property (D.13), it follows that ∥BAk(ηNT)∥= 0. Furthermore, for any 1 ≤j, j′ ≤d,\nE[(α′Vit)(α′V(it′))(di0(Xit)−di(Xit))j(di0(Xit′)−di(Xit′))j′] = 0.",
    "content_hash": "b9553a9ddae0401e3d1deffee890bce79465f3400c3233f7c401b348916e2029",
    "location": null,
    "page_start": 98,
    "page_end": 98,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fab530bc-b034-4f4d-943c-dd1f97fae678",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "(G.13)\nCombining (G.13) and Assumption 4.3\nE[∥α′VAk(ηNT)∥2] =i (NTk)−2\nN\n∑\ni=1 ∑\nt∈Mk\nd\n∑\nj=1\nE[(α′Vit)2(di0(Xit)−di(Xit))2\nj]\n≤(NTk)−2 sup\nd∈DNT\nN\n∑\ni=1 ∑\nt∈Mk\nd\n∑\nj=1\nE\n\u0014\nE[∥Vit∥2|Φit,Xit](di0(Xit)−di(Xit))2\nj\n\u0015\n≤(NTk)−2 sup\nd∈DNT\nN\n∑\ni=1\nT\n∑\nt=1\nE∥di0(Xit)−di(Xit)∥2dσ2\nV\n≤ii (d/NTk)σ2\nV(T/Tk)d2\nNT,\nwhere (i) follows from (G.13) and (ii) follows from deﬁnition of dNT. By Assumption 4.5, we\nhave that P(bdk ∈DNT, ∀k = 1,...,K) →1. Moreover, since the number of cross-ﬁt folds is ﬁnite,\nthe size Tk of each fold obeys\n1 ≲Tk/T ≤1. We conclude by Lemma A.8 that (G.8) holds.",
    "content_hash": "096121a8439adbc00535880a8fa91eb019f9ddefe05f9fab9e03222e46423b63",
    "location": null,
    "page_start": 98,
    "page_end": 98,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1c502601-0dff-4005-b6bf-8a355c8324f0",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Repeating the same argument for\nA(Wit,η) = Vit(li0(Xit)−li(Xit)) and A(Wit,η) = Uit(di0(Xit)−di(Xit))\nestablishes claims (G.9) and (G.10). Finally, (D.12) holds by deﬁnition of ¯e = ¯m−¯a′β0 and Holder\ninequalities.",
    "content_hash": "e48304bbd73d5eac0a029382f9d2244d2aed91654443030b1f23080a290c2654",
    "location": null,
    "page_start": 98,
    "page_end": 98,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "26fedb85-7c63-488b-850f-17ad9fc3eeff",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "64\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\n■\nIn the Lemma below, abusing the notation, we treat li as some generic vector-valued function. Lemma G.2 (Second-Order Bias). Let di0(Xit) be a d1-vector and li0(Xit) be a d2-vector. Suppose\nthat\nsup\nd∈DNT\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n(E∥di(Xit)−di0(Xit)∥2)1/2 ≤dNT,\nsup\nl∈DNT\n(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\n(E∥li(Xit)−li0(Xit)∥2)1/2 ≤lNT\nConsider the A-function as\nA(Wit,η) = (di0(Xit)−di(Xit))(li0(Xit)−li(Xit)),\nη = (d,l)\n(G.14)\nand its bias BAk(η) as in (A.16). Then, we have the bias bound:\nsup\nη∈(DNT ,LNT )\n∥BAk(η)∥2 ≤dNTlNT(T/Tk). Proof of Lemma G.2. Take α ∈S d−1.",
    "content_hash": "f9dbdfec006c5f8f66e2d9134394edf8f2b8bcc34338532bb92e57397987c1ea",
    "location": null,
    "page_start": 99,
    "page_end": 99,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0d30b6e6-9bc5-49d5-93ea-b7a1ca65fce6",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Let Xit(α) := α′(di0(Xit) −di(Xit)) and Yit j := (li0(Xit) −\nli(Xit))j and\na2\nit = ∥di0(Xit)−di(Xit)∥2,\nb2\nit = ∥li0(Xit)−li(Xit)∥2 =\nd\n∑\nj=1\nb2\nit j\nRecognize that\nα′BAk j(η) = (NTk)−1\nN\n∑\ni=1 ∑\nt∈Mk\nEα′(di0(Xit)−di(Xit))(li0(Xit)−li(Xit))j\n= (NTk)−1\nN\n∑\ni=1 ∑\nt∈Mk\nEXit(α)Yit j. Cauchy inequality gives\n|EXit(α)Yit j| ≤\nq\nEX2\nit(α)EY 2\nit j ≤\nq\nE∥di0(Xit)−di(Xit)∥2EY 2\nit j =:\nq\na2\nitb2\nit j. Summing over i and t and invoking Cauchy inequality gives\nα′BAk j(η) ≤(NTk)−1\nN\n∑\ni=1 ∑\nt∈Mk\nq\na2\nitb2\nit j ≤(NTk)−1\nv\nu\nu\nt\nN\n∑\ni=1 ∑\nt∈Mk\na2\nit\nv\nu\nu\nt\nN\n∑\ni=1 ∑\nt∈Mk\nb2\nit j.",
    "content_hash": "859d71cd00677f96bf212603671f698aeb4fbb7167b127bb21f501fa5ec0ac51",
    "location": null,
    "page_start": 99,
    "page_end": 99,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "eb8f7cf1-86f8-4bf8-9ba2-4ce1db8ed76b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n65\n∥α′BAk(η)∥2 =\nd\n∑\nj=1\n|α′BAk j(η)|2 ≤(NTk)−1(\nN\n∑\ni=1 ∑\nt∈Mk\na2\nit)(NTk)−1(\nd\n∑\nj=1\nN\n∑\ni=1 ∑\nt∈Mk\nb2\nit j)\n≤(NTk)−1(\nN\n∑\ni=1\nT\n∑\nt=1\na2\nit)(NTk)−1(\nN\n∑\ni=1\nT\n∑\nt=1\nb2\nit)\n≤l2\nNTd2\nNT(T/Tk)2. ■\nNext, we invoke Lemmas F.2 and F.3 and Corollary F.5-F.6 from Appendix F. Lemma G.3 (Second-Order Covariance Term). Deﬁne\nζ B\nNT = d2\nNT,\nζV\nNT =\nq\nd4\nNT,4 log(2d)logNT/NT +dDlog(2d)log(NT)/NT. Under Assumptions 4.1–4.3, the following bounds hold for the term ¯b deﬁned in (D.6)\n∥¯b∥≲P ζ B\nNT +ζV\nNT = χNT. (G.15)\nProof of Lemma G.3. Deﬁne the A-function as\nA(Wit,η) = (di0(Xit)−di(Xit))(di0(Xit)−di(Xit))′,\nη = d = l.",
    "content_hash": "cf472665c54e2942b34b26cdbca4276204c0dfe7e7a1049fe64ab09c9487e3fc",
    "location": null,
    "page_start": 100,
    "page_end": 100,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9bb92b29-8350-4721-ad23-30a355930e35",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Let BAk(η) and VAk(η) be deﬁned according to (A.16)–(A.17). Invoking Lemma G.2 with l = d\ngives ∥BAk(ηNT)∥∞= O(ζ B\nNT) for any partition k. Note that\nVAk(ηNT) = (NTk)−1\nN\n∑\ni=1 ∑\nt∈Mk\n\u0012\n(di0(Xit)−di(Xit))(di0(Xit)−di(Xit))′\n−E[(di0(Xit)−di(Xit))(di0(Xit)−di(Xit))′]\n\u0013\n=: (NTk)−1\nN\n∑\ni=1 ∑\nt∈Mk\nφi(Xit). Deﬁne\nψi(Xit) = (di0(Xit)−di(Xit))\nγi(Xit) = ψi(Xit)ψi(Xit)′ = (di0(Xit)−di(Xit))(di0(Xit)−di(Xit))′\nφi(Xit) = γi(Xit)−E[γi(Xit)]. Note that ψi(Xit) = (di0(Xit)−di(Xit)) obeys the conditions (F.26) and (F.27) with\nψ∞\nNT :=\n√\ndD,\nψNT,4 := dNT,4.",
    "content_hash": "3db15f75ae95279eacf3f3df4482c1e856ca1b4296155cc4a58e55b1e7ad2b7a",
    "location": null,
    "page_start": 100,
    "page_end": 100,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6cbc81e9-51eb-49e4-ab0d-b75c41d898c6",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "As a result, the bound (F.28) reduces to ζV\nNT for each partition k and T = Tk. Since Tk/T ≍1, the\nbound follows.",
    "content_hash": "ebe88e20fcf1ecd9be63ab4b84d1d657710d7166852ea9ea83b2db2af91e4d92",
    "location": null,
    "page_start": 100,
    "page_end": 100,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0b018257-064c-4628-ad37-26059af7f757",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Note that\nVAk(ηNT) = (NTk)−1\nN\n∑\ni=1 ∑\nt∈Mk\n\u0012\n(di0(Xit)−di(Xit))(li0(Xit)−li(Xit))\n−E[(di0(Xit)−di(Xit))(li0(Xit)−li(Xit))′]\n\u0013\n=: (NTk)−1\nN\n∑\ni=1 ∑\nt∈Mk\nφi(Xit). Deﬁne\nψi(Xit) = (di0(Xit)−di(Xit)),\nξi(Xit) = li0(Xit)−li(Xit)\nγi(Xit) = ψi(Xit)ξi(Xit) = (di0(Xit)−di(Xit))(li0(Xit)−li(Xit))\nφi(Xit) = γi(Xit)−E[γi(Xit)]. Note that ψi(Xit) = (di0(Xit)−di(Xit)) and ξi(Xit) = li0(Xit)−li(Xit) obey the conditions (F.29) and\n(F.30) with\nψ∞\nNT : =\n√\ndD,\nψNT,4 := dNT,4,\nξ ∞\nNT := L,\nξNT,4 := lNT,4. As a result, the bound (F.31) reduces to ζV\nNT for each partition k and T = Tk.",
    "content_hash": "8d13d226bc05fa7af1dac6f09e98a391ea17c086b39da7281e1cd66daf4f4590",
    "location": null,
    "page_start": 101,
    "page_end": 101,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8bb564d0-7f1e-4605-9d19-16a75f4ad4af",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Since Tk/T ≍1, the\nbound (G.16) follows. Recognizing that ¯g = ¯z −¯b′β0 and invoking ∥β0∥≤Cβ as in Assumption\nG.2 gives\n∥¯g∥≤∥¯z∥+∥¯bβ0∥≤∥¯z∥+∥¯b∥∥β0∥,\n(G.17) follows. ■",
    "content_hash": "14447e329261e3cf9a457d907e174c1abce83c58f6f3b9d5177313462be654df",
    "location": null,
    "page_start": 101,
    "page_end": 101,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ab3e17ba-bf68-40ed-a44b-08a1d6655f76",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "66\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\n■\nLemma G.4 (Second-Order Covariance Term, cont.). Suppose Assumptions 4.1–4.3 and G.2 hold. Let ¯z and ¯g be as deﬁned in (D.7) and (D.8). Then,\n∥¯z∥≲P r2NT\n(G.16)\n∥¯g∥≲P r2NT + χNT. (G.17)\nProof of Lemma G.4. Deﬁne the A-function as\nA(Wit,η) = (di0(Xit)−di(Xit))(li0(Xit)−li(Xit)),\nη = (d,l). Let BAk(η) and VAk(η) be deﬁned according to (A.16)–(A.17). Let\nζ B\nNT = dNTlNT,\nζV\nNT =\ns\n(d4\nNT,4 +l4\nNT,4)log(NT)log(d +1)\nNT\n+\n√\ndDlog(2d)log(NT)/NT. Invoking Lemma G.2 with d1 = d and d2 = 1 gives ∥BAk(ηNT)∥∞= O(ζ B\nNT) for any partition k.",
    "content_hash": "7b744914194c2a7858964275e83de5643956a6aafa07b3912fa9ceae8b87c9ba",
    "location": null,
    "page_start": 101,
    "page_end": 101,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "75fc8372-7a42-4c57-90b2-49894a2ae8a7",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "There-\nfore, we can decompose bβOLS −β0 as\nbβOLS −β0 = bQ−1ENT[bVitbeY it]−bQ−1 bQ′β0 = bQ−1ENT[bVitbeY it]−bQ−1(ENT bVit bV ′\nit)β0\n= bQ−1ENT[bVit(beY it −bV ′\nitβ0)]\n= bQ−1ENT[bVit(Uit +Rit(bd,bl))]\n= bQ−1ENTVitUit + bQ−1ENT[bVit(Uit +Rit(bd,bl))−VitUit]. Therefore the following bound holds by triangle and Holder inequalities:\n∥bβOLS −β0∥≤∥bQ−1∥∥ENTVitUit∥+∥bQ−1∥∥bS−S∥=: ∥bQ−1∥(L1 +L2). The ﬁrst term L1 is bounded as\nE∥ENTVitUit∥2 =\nd\n∑\nj=1\nE(ENT(Vit)jUit)2\n=i (NT)−2\nd\n∑\nj=1\nN\n∑\ni=1\nT\n∑\nt=1\nE((Vit)jUit)2\n≤(NT)−2\nN\n∑\ni=1\nT\n∑\nt=1\nE∥Vit∥2 sup\nit\nE[U2\nit | Vit]\n≤¯σ2(NT)−1\nN\n∑\ni=1\nT\n∑\nt=1\ntrace(EVitV ′\nit)\n= ¯σ2trace(Q) ≤ii (d/NT)Cmax,",
    "content_hash": "c33ad6b2e007da341d8a95f0ebba11cf0505d66f1cbf8ff534805ef806c3aa07",
    "location": null,
    "page_start": 102,
    "page_end": 102,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "494b6f4d-c948-47b5-85fd-2b33d7147811",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n67\nProof of Theorem G.1 . Step 0. Let Rit( bd,bl) be as deﬁned in (D.1). Let ¯a, ¯b, ¯e, ¯f, ¯g be as deﬁned in\n(D.2), (D.6), ..., (D.8). As shown in the proof of Lemma D.3, the Gram matrix estimation error\nbQ−eQ = ENT bVit bV ′\nit −ENTVitV ′\nit = ¯a+ ¯a′ + ¯b\nand gradient estimation error\nbS−S = ENT bVit(Uit +Rit(bd,bl))−VitUit] = ¯e+ ¯f + ¯g. We have that\n∥bQ−Q∥≤∥bQ−eQ∥+∥eQ−Q∥≲i\nP (χNT +\np\nd/NTdNT +vNT) = o(1),\nwhere (i) follows from Lemmas G.1-G.4 and Assumption G.1.Furthermore, by Lemmas\n∥bS−S∥= ∥¯e+ ¯f + ¯g∥≲P (r2NT + χNT) = o(1/\n√\nNT),\nwhere we used Assumptions 4.1–4.3 and G.2 to conclude that r2NT + χNT = o(1/\n√\nNT). Step 1. Since Q is invertible by assumption, bQ is also invertible wp 1−o(1) by Step 0.",
    "content_hash": "f33140031640c76c57436e282f267e503a325910065f84ad70d22d0fad1071d9",
    "location": null,
    "page_start": 102,
    "page_end": 102,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3b80b6fd-1860-40cd-bdad-1d6e728d44c0",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "The bound on S2(α) follows from:\n|S2(α)| ≤∥α∥C−1\nmin∥bS−S∥≲P (r2NT + χNT) = oP((NT)−1/2),\nwhere we are using the results of Step 0. As a result,\n√\nNTα′(bβOLS −β0) = α′Q−1GNTVitUit +oP(1),\nwhich gives (G.6). Step 3. The proof of pointwise normality follows similarly to Step 1 of the proof of Theorem\n4.2, where the step (D.51) is replaced by\n|α′(α′Σα)−1/2RNT| ≤∥α∥2O(1)∥RNT∥2 ≲P\n√\nNT(r2NT + χNT) = oP(1). ■\nREFERENCES\nArellano, M. and Bond, S. (1991). Some tests of speciﬁcation for panel data. monte carlo evidence\nand an application to employment equations. Review of Economic Studies, 58(2):277–297. Athey, S. and Imbens, G. (2016). Recursive partitioning for heterogeneous causal effects. Pro-\nceedings of the National Academy of Sciences, 113(27):7353–7460. Banerjee, A., Chandrasekhar, A. G., Dalpath, S., Duﬂo, E., Floretta, J., Jackson, M. O., Kannan,\nH., Loza, F., Sankar, A., Schrimpf, A., and Shrestha, M. (2021).",
    "content_hash": "090334b6fc0c83d9037e9bdbd1838d88f66faf26bebe35975f3bac900b263b3d",
    "location": null,
    "page_start": 103,
    "page_end": 103,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "53f7a155-ada9-49e9-ab5c-b1c22dc9f92e",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "68\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nwhere (i) follows from the m.d.s property in Lemma B.3, and (ii) from maxeig(Q) ≤Cmax. Markov\ninequality gives L1 ≲P (\np\nd/NT). The second term L2 := ∥bS−S∥is oP(1/\n√\nNT) by Step 0. Step\n0 implies maxeig( bQ−1) < 2C−1\nmin w.p. 1−o(1). Therefore, the rate bound (G.5) follows. Step 2. From Step 1,\nα′(bβOLS −β0) = α′ bQ−1ENT[bVit(Uit +Rit( bd,bl)]\n= α′Q−1ENTVitUit\n+α′( bQ−1 −Q−1)ENTVitUit +α′ bQ−1[ENT[bVit(Uit +Rit( bd,bl)−VitUit]]\n=: α′Q−1ENTVitUit +S1(α)+S2(α). The bound on S1(α) follows\n|S1(α)| ≤∥bQ−1 −Q−1∥∥ENTVitUit∥\n≤∥bQ−1∥∥bQ−Q∥∥Q−1∥∥ENTVitUit∥\n= OP(1)·oP(1)·OP(1)·OP((NT)−1/2) = oP((NT)−1/2),\nwhere OP(·) and oP(·) bounds are established in Steps 0-2.",
    "content_hash": "f6942f69dea9f039569fd13c9383312848a6606d89a9ebffec0b235d29756a1e",
    "location": null,
    "page_start": 103,
    "page_end": 103,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "534e28a2-e9e5-4386-b8e0-3037d065c09a",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Selecting the most effective\nnudge: Evidence from a large-scale experiment on immunization.",
    "content_hash": "29eca94997604bcb258db40f34775a0416a7300411213f24275a9f6f155513ac",
    "location": null,
    "page_start": 103,
    "page_end": 103,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ba5520c7-cb1d-4ddc-a3a6-fa6a1330e42a",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Belloni, A., Chernozhukov, V., and Hansen, C. (2011). Inference for high-dimensional sparse\neconometric models. arXiv preprint arXiv:1201.0220. Belloni, A., Chernozhukov, V., and Hansen, C. (2014a). Inference on treatment effects after selec-\ntion among high-dimensional controls. Review of Economic Studies, 81(2):608–650. Belloni, A., Chernozhukov, V., and Kato, K. (2014b). Uniform post-selection inference for least\nabsolute deviation regression and other Z-estimation problems. Biometrika, 102(1):77–94. Belloni, A., Chernozhukov, V., and Kato, K. (2014c). Uniform post-selection inference for least\nabsolute deviation regression and other z-estimation problems. Biometrika, 102(1):77–94. Belloni, A., Chernozhukov, V., and Wei, Y. (2016). Post-selection inference for generalized linear\nmodels with many controls. Journal of Business & Economic Statistics, 34(4):606–619. Berbee, H. (1987). Convergence laws in the strong law for bounded mixing sequences. Probability\nTheory and Related Fields, 74:255–270. Bonhomme, S., Lamadon, T., and Manresa, E. (2019a). Discretizing unobserved heterogeneity.",
    "content_hash": "dab4ab97cf31283fbf1574643322d454b72da99a29dc8be964adad57942854cc",
    "location": null,
    "page_start": 104,
    "page_end": 104,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5feba8bf-943c-4b0f-84bc-d7934ffdf13f",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Bonhomme, S., Lamadon, T., and Manresa, E. (2019b). A distributional framework for matched\nemployer employee data. Econometrica, 87(3):699–739. Cai, T., Liu, W., and Luo, X. (2011). A constrained l1 minimization approach to sparse precision\nmatrix estimation. Journal of the American Statistical Association, 106:594–607. Chamberlain, G. (1982). Multivariate regression models for panel data. Journal of Econometrics,\n18:5–46. Chen, M., Fernandez-Val, I., and Weidner, M. (2020). Nonlinear factor models for network and\npanel data. Journal of Econometrics. Chernozhukov, V., Chetverikov, D., Demirer, M., Duﬂo, E., Hansen, C., Newey, W., and Robins,\nJ. (2018). Double/debiased machine learning for treatment and structural parameters. Econo-\nmetrics Journal, 21:C1–C68. Chernozhukov, V., Chetverikov, D., and Kato, K. (2013). Gaussian approximations and multi-\nplier bootstrap for maxima of sums of high-dimensional random vectors. Annals of Statistics,",
    "content_hash": "f361c6d059b9ebff63f4854bbc72ee026a4ec7cef609d6111289126c7354a681",
    "location": null,
    "page_start": 104,
    "page_end": 104,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e1470314-98d2-447c-9cb5-980a0b9e9c72",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n69\nBanna, M., Merlevede, F., and Youssef, P. (2016). Bernstein-type inequality for a class of depen-\ndent random matrices. Random Matrices: Theory and Applications, 05(02):1650006. Belloni, A., Chen, D., Chernozhukov, V., and Hansen, C. (2012). Sparse models and methods for\noptimal instruments with an application to eminent domain. Econometrica, 80:2369–2429. Belloni, A. and Chernozhukov, V. (2013). Least squares after model selection in high-dimensional\nsparse models. Bernoulli, (19):521–547. Belloni, A., Chernozhukov, V., Chetverikov, D., and Wei, Y. (2019). Uniformly valid post-\nregularization conﬁdence regions for many functional parameters in z-estimation framework. Annals of Statistics, 46(6):3643–3675. Belloni, A., Chernozhukov, V., Fernandez-Val, I., and Hansen, C. (2017). Program evaluation and\ncausal inference with high-dimensional data. Econometrica, 85:233–298. Belloni, A., Chernozhukov, V., and Hansen, C. (2010). Lasso methods for gaussian instrumental\nvariables models. arXiv preprint arXiv:1012.1297.",
    "content_hash": "d2f67c43bc953899574534bb933247ec1880d241d0e740e0c753ec0d87db2848",
    "location": null,
    "page_start": 104,
    "page_end": 104,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a9e05544-49ff-4c80-937e-1d3c846f0c40",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Rethinking the Beneﬁts of Youth Employment Programs: The\nHeterogeneous Effects of Summer Jobs. The Review of Economics and Statistics, 102(4):664–\n677. Dudley, R. and Philipp, W. (1983). Invariance principles for sums of banach space valued random\nelements and empirical processes. Zeitschrift f¨ur Wahrscheinlichkeitstheorie und verwandte\nGebiete, 62(4):509–552. Fan, Q., Hsu, Y.-C., Lieli, R. P., and Zhang, Y. (2019). Estimation of conditional average treatment\neffects with high-dimensional data. arXiv e-prints, page arXiv:1908.02399. Fan, X., Grama, I., and Liu, Q. (2012). Large deviation exponential inequalities for supermartin-\ngales. Electronic Communications in Probability, 17(none):1 – 8. Farrell, M., Liang, T., and Misra, S. (2021). Deep neural networks for estimation and inference. Econometrica, 89(1):181–213.",
    "content_hash": "88db3fe83b4f5a04cf1c3eb8eee66ad639a1d8c04e45a5c82fe59fb457c8dfba",
    "location": null,
    "page_start": 105,
    "page_end": 105,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f34857db-d695-4e9f-aefa-7b03620dd88b",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n71\nFernandez-Val, I. and Lee, J. (2013). Panel data models with non-additive unobserved heterogene-\nity: Estimation and inference. Quantitative Economics, 4:453–481. Fernandez-Val, I. and Weidner, M. (2018). Fixed effect estimation of large t panel data model. The\nAnnual Review of Economics, 10(1):109–138. Gao, W. Y. and Li, M. (2019). Robust semiparametric estimation in panel multinomial choice\nmodels. Haavelmo, T. (1944). The probability approach in econometrics. Econometrica, 12:1–115. Hahn, J. and Kuersteiner, G. (2011). Bias reduction for dynamic nonlinear panel models with ﬁxed\neffects. Econometric Theory, 27(6):1152–1191. Hansen, B. (2019). A weak law of large numbers under weak mixing. Hasminskii, R. Z. and Ibragimov, I. A. (1979). On the nonparametric estimation of functionals. In\nProceedings of the Second Prague Symposium on Asymptotic Statistics. Jacob, D., H¨ardle, W. K., and Lessmann, S. (2019). Group average treatment effects for observa-\ntional studies. Javanmard, A. and Montanari, A. (2014). Conﬁdence intervals and hypothesis testing for high-\ndimensional regression.",
    "content_hash": "3fbe4936c0983239edd8f43ad6ced9ff5977f2a614c08d22101efcbbd4f381be",
    "location": null,
    "page_start": 106,
    "page_end": 106,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1f42effb-ac9b-4efa-b07d-47e83761b041",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "Determining individual or time effects in panel data models. Journal of\nEconometrics, 215(1):60–83. Luo, Y. and Spindler, M. (2016). High-dimensional L2-boosting: rate of convergence. arXiv\ne-prints, page arXiv:1602.08927. Manresa, E. (2016). Estimating the structure of social interactions using panel data. McLeish, D. L. (1974). Dependent central limit theorems and invariance principles. The Annals of\nProbability, 2(4):620–628. Moon, H. R., Shum, M., and Weidner, M. (2018). Estimation of random coefﬁcients logit demand\nmodels with interactive ﬁxed effects. Journal of Econometrics, 206(2):613–644.",
    "content_hash": "f14cc7c08c75ff931dae4892143efb250b67b60de5a77ef951c7ca9a0b52537b",
    "location": null,
    "page_start": 106,
    "page_end": 106,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "93c072c2-e936-41fb-9205-6be15eb3bde8",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "72\nVIRA SEMENOVA, MATT GOLDMAN, VICTOR CHERNOZHUKOV, AND MATT TADDY\nMundlak, Y. (1978). On the pooling of time series and cross section data. Econometrica, 46(1):69–\n85. Negahban, S. N., Ravikumar, P., Wainwright, M. J., and Yu, B. (2012). A uniﬁed framework for\nhigh-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science,\n27(4):538–557. Nickell, S. (1981). Biases in dynamic models with ﬁxed effects. Econometrica, 49(6):1824–1851. Nie, X. and Wager, S. (2020). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika. Oprescu, M., Syrgkanis, V., and Wu, Z. S. (2018). Orthogonal random forest for causal inference. arXiv e-prints, page arXiv:1806.03467. Rigollet, P. and Hutter, J.-C. (2017). High-dimensional statistics. Robinson, P. M. (1988). Root-n-consistent semiparametric regression. Econometrica, 56(4):931–\n954. Schick, A. (1986). On asymptotically efﬁcient estimation in semiparametric models. The Annals\nof Statistics, 14(3):1139–1151.",
    "content_hash": "2ef1317d82f328ce4de97e9dabca95ee4a7cd82c6976886be12f789283b555c7",
    "location": null,
    "page_start": 107,
    "page_end": 107,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7ede8ed8-aee9-401d-be6a-2eb6f46b414f",
    "source_id": "a24af953-6046-4759-b7bd-be16e818481c",
    "content": "INFERENCE IN HIGH-DIMENSIONAL DYNAMIC PANELS\n73\nWainwright, M. J. (2019). High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge\nSeries in Statistical and Probabilistic Mathematics.\nZhang, C.-H. and Zhang, S. (2014). Conﬁdence intervals for low-dimensional parameters in high-\ndimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Method-\nology), 76(1):217–242.\nZhang, D. and Wu, W. B. (2017). Gaussian approximation for high dimensional time series. Annals\nof Statistics, 45(5):1895–1919.\nZheng, W. and van der Laan, M. J. (2010). Asymptotic theory for cross-validated targeted maxi-\nmum likelihood estimation. Technical report, UC Berkeley Division of Biostatistics.\nZimmert, M. and Lechner, M. (2019). Nonparametric estimation of causal heterogeneity under\nhigh-dimensional confounding. arXiv e-prints, page arXiv:1908.08779.",
    "content_hash": "22c9cb749a82dee64b0ff6abc47c9325766d49a5f0de8fe355ca432fe09d0343",
    "location": null,
    "page_start": 108,
    "page_end": 108,
    "metadata": {
      "section": "arXiv:1712.09988v6  [stat.ML]  11 Dec 2022",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "87aea063-0c7d-4f4e-91b0-19293b65b103",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Archives of General Psy-\nchiatry 59 877–883. MacKinnon, D. P. (2008). Introduction to Statistical Medi-\nation Analysis. Taylor & Francis, New York. Nelson, T. E., Clawson, R. A. and Oxley, Z. M. (1997). Media framing of a civil liberties conﬂict and its eﬀect on\ntolerance. American Political Science Review 91 567–583. Pearl, J. (2001). Direct and indirect eﬀects. In Proceedings of\nthe Seventeenth Conference on Uncertainty in Artiﬁcial In-\ntelligence (J. S. Breese and D. Koller, eds.) 411–420. Mor-\ngan Kaufman, San Francisco, CA. Pearl, J. (2010). An introduction to causal inference. Int. J. Biostat. 6 Article 7. Petersen, M. L., Sinisi, S. E. and van der Laan, M. J. (2006). Estimation of direct causal eﬀects. Epidemiology\n17 276–284. Robins, J. (1999). Marginal structural models versus struc-\ntural nested models as tools for causal inference. In Statis-\ntical Models in Epidemiology, the Environment and Clini-\ncal Trials (M. E. Halloran and D. A. Berry, eds.) 95–134. Springer, New York. MR1731682\nRobins, J. M. (2003). Semantics of causal DAG models and\nthe identiﬁcation of direct and indirect eﬀects. In Highly\nStructured Stochastic Systems (P.",
    "content_hash": "0d4b843f11b526964726a3e9f59f25978648c89eb84cf795e06f769ea4a0efd9",
    "location": null,
    "page_start": 1,
    "page_end": 21,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7b1648ff-5629-4c35-86f3-9e6eeb143458",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Journal of\nEducational and Behavioral Statistics 33 230–251. Ten Have, T. R., Joffe, M. M., Lynch, K. G., Brown,\nG. K., Maisto, S. A. and Beck, A. T. (2007). Causal me-\ndiation analyses with rank preserving models. Biometrics\n63 926–934. MR2395813\nVanderWeele, T. J. (2008). Simple relations between prin-\ncipal stratiﬁcation and direct and indirect eﬀects. Statist. Probab. Lett. 78 2957–2962. VanderWeele, T. J. (2009). Marginal structural models for\nthe estimation of direct and indirect eﬀects. Epidemiology\n20 18–26. VanderWeele, T. J. (2010). Bias formulas for sensitivity\nanalysis for direct and indirect eﬀects. Epidemiology. To\nappear. Zellner, A. (1962). An eﬃcient method of estimating seem-\ningly unrelated regressions and tests for aggregation bias. J. Amer. Statist. Assoc. 57 348–368. MR0139235",
    "content_hash": "8e505983b22c6a97db8f2782bb2c302b78e0c9dcf89fd478a88f378d51f15d21",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3fbfecd9-21fd-4c7d-b86e-a6ef9a14c338",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Thus, sensitivity analysis is essential in order to\nexamine the robustness of empirical ﬁndings to the possible existence\nof an unmeasured confounder. Finally, we apply the proposed methods\nto a randomized experiment from political psychology. We also make\neasy-to-use software available to implement the proposed methods. Key words and phrases:\nCausal inference, causal mediation analysis,\ndirect and indirect eﬀects, linear structural equation models, sequential\nignorability, unmeasured confounders. 1. INTRODUCTION\nCausal mediation analysis is routinely conducted\nby applied researchers in a variety of scientiﬁc disci-\nKosuke Imai is Assistant Professor, Department of\nPolitics, Princeton University, Princeton, New Jersey\n08544, USA e-mail: kimai@princeton.edu; URL:\nhttp://imai.princeton.edu. Luke Keele is Assistant\nProfessor, Department Political Science, Ohio State\nUniversity, 2140 Derby Hall, Columbus, Ohio 43210,\nUSA e-mail: keele.4@polisci.osu.edu. Teppei Yamamoto\nis Ph.D. Student, Department of Politics, Princeton\nUniversity, 031 Corwin Hall, Princeton, New Jersey\n08544, USA e-mail: tyamamot@princeton.edu. plines including epidemiology, political science, psy-\nchology and sociology (see MacKinnon, 2008). The\ngoal of such an analysis is to investigate causal mech-\nanisms by examining the role of intermediate vari-\nables thought to lie in the causal path between the\ntreatment and outcome variables.",
    "content_hash": "3603c524f5cb5cce1105224a6022f08693fc1ca9f6753329f53ae5566b796bd2",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a530b807-93b7-4678-8213-a9fe96c8fe8a",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Over ﬁfty years\nago, Cochran (1957) pointed to both the possibility\nand diﬃculty of using covariance analysis to explore\nThis is an electronic reprint of the original article\npublished by the Institute of Mathematical Statistics in\nStatistical Science, 2010, Vol. 25, No. 1, 51–71. This\nreprint diﬀers from the original in pagination and\ntypographic detail. 1",
    "content_hash": "5adc549c0f6940d04fad4d657733b3e6c5ae173ad23a2d8dea27ccddcd0055ad",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3c70f181-3b9e-4bb0-a445-6e3081d0c09a",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Journal of Personality and Social Psychology 51\n1173–1182. Cochran, W. G. (1957). Analysis of covariance: Its nature\nand uses. Biometrics 13 261–281. MR0090952\nDeaton, A. (2009). Instruments of development: Random-\nization in the tropics, and the search for the elusive keys\nto economic development. Proc. Br. Acad. 162 123–160. Egleston, B., Scharfstein, D. O., Munoz, B. and West,\nS. (2006). Investigating mediation when counterfactuals\nare not metaphysical: Does sunlight UVB exposure me-\ndiate the eﬀect of eyeglasses on cataracts? Working Paper\n113, Dept. Biostatistics, Johns Hopkins Univ., Baltimore,\nMD. Elliott, M. R., Raghunathan, T. E. and Li, Y. (2010). Bayesian inference for causal mediation eﬀects using prin-\ncipal stratiﬁcation with dichotomous mediators and out-\ncomes. Biostatistics. 11 353–372. Gallop, R., Small, D. S., Lin, J. Y., Elliot, M. R.,\nJoffe, M. and Ten Have, T. R. (2009). Mediation anal-\nysis with principal stratiﬁcation. Stat. Med. 28 1108–1130. Geneletti, S. (2007). Identifying direct and indirect eﬀects\nin a non-counterfactual framework. J. Roy. Statist. Soc. Ser.",
    "content_hash": "19f349ce6981e90623901db7c8bd4022e3f1d2a67a48818db49f0d8560e53362",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "688e0e2e-00c6-4a05-9a93-1cb6ff4b7598",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "J. Green, N. L. Hjort and\nS. Richardson, eds.) 70–81. Oxford Univ. Press, Oxford. MR2082403\nRobins, J. M. and Greenland, S. (1992). Identiﬁability and\nexchangeability for direct and indirect eﬀects. Epidemiol-\nogy 3 143–155. Roy, J., Hogan, J. W. and Marcus, B. H. (2008). Principal\nstratiﬁcation with predictors of compliance for randomized\ntrials with 2 active treatments. Biostatistics 9 277–289. Rubin, D. B. (2004). Direct and indirect causal eﬀects via\npotential outcomes (with discussion). Scand. J. Statist. 31\n161–170. MR2066246\nRubin, D. B. (2005). Causal inference using potential out-\ncomes: Design, modeling, decisions. J. Amer. Statist. As-\nsoc. 100 322–331. MR2166071\nSj¨olander, A. (2009). Bounds on natural direct eﬀects in the\npresence of confounded intermediate variables. Stat. Med. 28 558–571. Skrabanek, P. (1994). The emptiness of the black box. Epi-\ndemiology 5 5553–5555. Sobel, M. E. (1982). Asymptotic conﬁdence intervals for\nindirect eﬀects in structural equation models. Sociological\nMethodology 13 290–321. Sobel, M. E. (2008). Identiﬁcation of causal parameters in\nrandomized studies with mediating variables.",
    "content_hash": "a49b4f31dee8df82b379e47df126f4b6bcbfc39c747fcd7ce166ca7ee9b05db7",
    "location": null,
    "page_start": 1,
    "page_end": 21,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f80fc1eb-ad19-40f5-b6b2-6035415bd216",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "CAUSAL MEDIATION ANALYSIS\n15\nFig. 2. An alternative interpretation of the sensitivity analysis. The plot presents the results of the sensitivity analysis described in Section 5. Each plot contains\nvarious mediation eﬀects under an unobserved pre-treatment confounder of various magnitudes. The left plot contains the contours for R2∗\nM and R2∗\nY which represent\nthe proportion of unexplained variance that is explained by the unobserved confounder for the mediator and outcome, respectively. The right plot contains the contours\nfor ˜R2\nM and ˜R2\nY which represent the proportion of the variance explained by the unobserved pre-treatment confounder. Each line represents the estimated ACME\nunder proposed values of either (R∗2\nM,R2∗\nY ) or ( ˜R2\nM, ˜R2\nY ). The term sgn(λ2λ3) represents the sign on the product of the coeﬃcients of the unobserved confounder. 16\nK. IMAI, L. KEELE AND T. YAMAMOTO\nNext, we present the same sensitivity analysis us-\ning the alternative interpretation of ρ which is based\non two coeﬃcients of determination as deﬁned in\nSection 5; (1) the proportion of unexplained variance\nthat is explained by an unobserved pre-treatment\nconfounder (R2∗\nM and R2∗\nY ) and (2) the proportion\nof the original variance explained by the same un-\nobserved confounder ( ˜R2\nM and ˜R2\nY ). Figure 2 shows\ntwo plots based on the types of coeﬃcients of de-\ntermination.",
    "content_hash": "2a95cf27186aa7fef458aaa3b013560b7ef77d8785506d1e48a527043ed0add7",
    "location": null,
    "page_start": 1,
    "page_end": 16,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "398dee2a-e280-484e-b988-fb5a0dacf112",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "arXiv:1011.1079v1  [stat.ME]  4 Nov 2010\nStatistical Science\n2010, Vol. 25, No. 1, 51–71\nDOI: 10.1214/10-STS321\nc\n⃝Institute of Mathematical Statistics, 2010\nIdentiﬁcation, Inference and Sensitivity\nAnalysis for Causal Mediation Eﬀects\nKosuke Imai, Luke Keele and Teppei Yamamoto\nAbstract. Causal mediation analysis is routinely conducted by applied\nresearchers in a variety of disciplines. The goal of such an analysis is\nto investigate alternative causal mechanisms by examining the roles of\nintermediate variables that lie in the causal paths between the treat-\nment and outcome variables. In this paper we ﬁrst prove that under\na particular version of sequential ignorability assumption, the aver-\nage causal mediation eﬀect (ACME) is nonparametrically identiﬁed. We compare our identiﬁcation assumption with those proposed in the\nliterature. Some practical implications of our identiﬁcation result are\nalso discussed. In particular, the popular estimator based on the linear\nstructural equation model (LSEM) can be interpreted as an ACME\nestimator once additional parametric assumptions are made. We show\nthat these assumptions can easily be relaxed within and outside of the\nLSEM framework and propose simple nonparametric estimation strate-\ngies. Second, and perhaps most importantly, we propose a new sensi-\ntivity analysis that can be easily implemented by applied researchers\nwithin the LSEM framework. Like the existing identifying assumptions,\nthe proposed sequential ignorability assumption may be too strong in\nmany applied settings.",
    "content_hash": "7bcfe9ea5b36c76e961eb0a126fe900a01a7f1fea2a3744d77caaab4eb0225f0",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1011.1079v1  [stat.ME]  4 Nov 2010",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3ba1f318-e006-4331-ae7b-7b0e3637d785",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "In Section 5 we\npropose a new sensitivity analysis that can be imple-\nmented by applied researchers within the standard\nLSEM framework. This method directly evaluates\nthe robustness of empirical ﬁndings to the possi-\nble existence of unmeasured pre-treatment variables\nthat confound the relationship between the media-\ntor and the outcome. Given the fact that the se-\nquential ignorability assumption cannot be directly\ntested even in randomized experiments, sensitivity\nanalysis must play an essential role in causal media-\ntion analysis. Finally, in Section 6 we apply the pro-\nposed methods to the empirical example, to which\nwe now turn. 2. AN EXAMPLE FROM THE SOCIAL\nSCIENCES\nSince the inﬂuential article by Baron and Kenny\n(1986), mediation analysis has been frequently used\nin the social sciences and psychology in particu-\nlar. A central goal of these disciplines is to iden-\ntify causal mechanisms underlying human behavior\nand opinion formation. In a typical psychological ex-\nperiment, researchers randomly administer certain\nstimuli to subjects and compare treatment group be-\nhavior or opinions with those in the control group. However, to directly test psychological theories, es-\ntimating the causal eﬀects of the stimuli is typically\nnot suﬃcient. Instead, researchers choose to inves-\ntigate psychological factors such as cognition and\nemotion that mediate causal eﬀects in order to ex-\nplain why individuals respond to a certain stimulus\nin a particular way. Another diﬃculty faced by many\nresearchers is their inability to directly manipulate\npsychological constructs. It is in this context that\ncausal mediation analysis plays an essential role in\nsocial science research.",
    "content_hash": "84c1964043bbcdf8df29da625b0407ea89a55439ead8bf6408c639e472bf67d4",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ee4ed544-d0ce-4250-a898-ae8ac4ccc327",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "In this paper we contribute to this fast-growing lit-\nerature in several ways. After brieﬂy describing our\nmotivating example in the next section, we prove in\nSection 3 that under a particular version of the se-\nquential ignorability assumption, the average causal\nmediation eﬀect (ACME) is nonparametrically iden-\ntiﬁed. We compare our identifying assumption with\nthose proposed in the literature, and discuss practi-\ncal implications of our identiﬁcation result. In par-\nticular, Baron and Kenny’s (1986) popular estima-\ntor (Google Scholar records over 17 thousand cita-\ntions for this paper), which is based on a linear struc-\ntural equation model (LSEM), can be interpreted\nas an ACME estimator under the proposed assump-\ntion if additional parametric assumptions are satis-\nﬁed. We show that these additional assumptions can\nbe easily relaxed within and outside of the LSEM\nframework. In particular, we propose a simple non-\nparametric estimation strategy in Section 4. We con-\nduct a Monte Carlo experiment to investigate the\nﬁnite-sample performance of the proposed nonpara-\nmetric estimator and its asymptotic conﬁdence in-\nterval. Like many identiﬁcation assumptions, the proposed\nassumption may be too strong for the typical sit-\nuations in which causal mediation analysis is em-\nployed. For example, in experiments where the treat-\nment is randomized but the mediator is not, the ig-\nnorability of the treatment assignment holds but the\nignorability of the mediator may not.",
    "content_hash": "d27d9bc8286a6b59a77a0c49f6b33f2cd8e80d521301479bc0f3375fd5220c3f",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "611c7a10-3d54-4ffa-bca0-2e1cada8bbf4",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "2\nK. IMAI, L. KEELE AND T. YAMAMOTO\ncausal mechanisms by stating: “Sometimes these av-\nerages have no physical or biological meaning of in-\nterest to the investigator, and sometimes they do not\nhave the meaning that is ascribed to them at ﬁrst\nglance” (page 267). Recently, a number of statisti-\ncians have taken up Cochran’s challenge. Robins and\nGreenland (1992) initiated a formal study of causal\nmediation analysis, and a number of articles have\nappeared in more recent years (e.g., Pearl, 2001;\nRobins, 2003; Rubin, 2004; Petersen, Sinisi and van\nder Laan, 2006; Geneletti, 2007; Joﬀe, Small and\nHsu, 2007; Ten Have et al., 2007; Albert, 2008; Jo,\n2008; Joﬀe et al., 2008; Sobel, 2008; VanderWeele,\n2008, 2009; Glynn, 2010). What do we mean by a causal mechanism? The\naforementioned paper by Cochran gives the follow-\ning example. In a randomized experiment, researchers\nstudy the causal eﬀects of various soil fumigants on\neelworms that attack farm crops. They observe that\nthese soil fumigants increase oats yields but wish to\nknow whether the reduction of eelworms represents\nan intermediate phenomenon that mediates this ef-\nfect. In fact, many scientists across various disci-\nplines are not only interested in causal eﬀects but\nalso in causal mechanisms because competing scien-\ntiﬁc theories often imply that diﬀerent causal paths\nunderlie the same cause-eﬀect relationship.",
    "content_hash": "fed336b0d4d98bfcc0bc9667894963194b8b38ef9c3d2ca44e801b224fcfd341",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "248e1216-8dc7-493e-a881-4134f9db09e1",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "In Section 6 we apply our methods to an inﬂuen-\ntial randomized experiment from political psychol-\nogy. Nelson, Clawson and Oxley (1997) examine how\nthe framing of political issues by the news media af-\nfects citizens’ political opinions. While the authors\nare not the ﬁrst to use causal mediation analysis in\npolitical science, their study is one of the most well-\nknown examples in political psychology and also",
    "content_hash": "31a45f2dfe4daf2223c3bac9ccf2e73d99529c0b3718d93612d5404058d8afdd",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "98173290-c0bf-4d29-9cad-876203ef6d50",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "The\nresearchers used additional survey questions and a\nscaling method to measure these hypothesized me-\ndiating factors after the experiment was conducted. Table 1 reports descriptive statistics for these me-\ndiator variables as well as the treatment and out-\ncome variables. The sample size is 136, with 67 sub-\njects exposed to the free speech frame and 69 sub-\njects assigned to the public order frame. As is clear\nfrom the last column, the media frame treatment\nappears to inﬂuence both types of response vari-\nables in the expected directions. For example, be-\ning exposed to the public order frame as opposed to\nthe free speech frame signiﬁcantly increased the sub-\njects’ perceived importance of public order, while de-\ncreasing the importance of free speech (although the\nlatter eﬀect is not statistically signiﬁcant). More-\nover, the public order treatment decreased the sub-\njects’ tolerance toward the Ku Klux Klan speech in\nthe news clips compared to the free speech frame. It is important to note that the researchers in\nthis example are primarily interested in the causal\nmechanism between media framing and political tol-\nerance rather than various causal eﬀects given in\nthe last column of Table 1. Indeed, in many so-\ncial science experiments, researchers’ interest lies in\nthe identiﬁcation of causal mediation eﬀects rather\nthan the total causal eﬀect or controlled direct ef-\nfects (these terms are formally deﬁned in the next\nsection). Causal mediation analysis is particularly\nappealing in such situations.",
    "content_hash": "0700c33f1640c506ebeb1e4ed5e0e4bad1edab6766a4067bc96a7012a7c39b3c",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "883f8881-2f69-43c1-9950-d9de03098eb6",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "One crucial limitation of this study, however, is\nthat like many other psychological experiments the\noriginal researchers were only able to randomize news\nstories but not subjects’ attitudes. This implies that\nthere is likely to be unobserved covariates that con-\nfound the relationship between the mediator and the\noutcome. As we formally show in the next section,\nthe existence of such confounders represents a vio-\nlation of a key assumption for identifying the causal\nTable 1\nDescriptive statistics and estimated average treatment eﬀects from the media framing experiment. The middle four columns\nshow the means and standard deviations of the mediator and outcome variables for each treatment group. The last column\nreports the estimated average causal eﬀects of the public order frame as opposed to the free speech frame on the three\nresponse variables along with their standard errors. The estimates suggest that the treatment aﬀected each of these variables\nin the expected directions\nTreatment media frames\nPublic order\nFree speech\nResponse variables\nMean\nS.D. Mean\nS.D. ATE (s.e.)\nImportance of free speech\n5.25\n1.43\n5.49\n1.35\n−0.231 (0.239)\nImportance of public order\n5.43\n1.73\n4.75\n1.80\n0.674 (0.303)\nTolerance for the KKK\n2.59\n1.89\n3.13\n2.07\n−0.540 (0.340)\nSample size\n69\n67",
    "content_hash": "7eb5e38e4bfbae1bdf00e36177460a849d50596e310be669efc95c3c75cef5db",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "83ac67f5-f6be-46c9-a2d7-704f87549857",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "CAUSAL MEDIATION ANALYSIS\n3\nrepresents a typical application of causal mediation\nanalyses in the social sciences. Media framing is the\nprocess by which news organizations deﬁne a po-\nlitical issue or emphasize particular aspects of that\nissue. The authors hypothesize that diﬀering frames\nfor the same news story alter citizens’ political tol-\nerance by aﬀecting more general political attitudes. They conducted a randomized experiment to test\nthis mediation hypothesis. Speciﬁcally, Nelson, Clawson and Oxley (1997)\nused two diﬀerent local newscasts about a Ku Klux\nKlan rally held in central Ohio. In the experiment,\nstudent subjects were randomly assigned to watch\nthe nightly news from two diﬀerent local news chan-\nnels. The two news clips were identical except for\nthe ﬁnal story on the Klan rally. In one newscast,\nthe Klan rally was presented as a free speech issue. In the second newscast, the journalists presented\nthe Klan rally as a disruption of public order that\nthreatened to turn violent. The outcome was mea-\nsured using two diﬀerent scales of political toler-\nance. Immediately after viewing the news broadcast,\nsubjects were asked two seven-point scale questions\nmeasuring their tolerance for the Klan speeches and\nrallies. The hypothesis was that the causal eﬀects of\nthe media frame on tolerance are mediated by sub-\njects’ attitudes about the importance of free speech\nand the maintenance of public order. In other words,\nthe media frame inﬂuences subjects’ attitudes to-\nward the Ku Klux Klan by encouraging them to\nconsider the Klan rally as an event relevant for the\ngeneral issue of free speech or public order.",
    "content_hash": "906e16f1d1a55057de130dedac6c2bce306320a261811fd624167496450e7813",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "55088c3b-37bd-4d76-b77e-926da8ed8b3f",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Be-\nyond this minimal requirement, what constitutes a\nmediator is determined solely by the scientiﬁc the-\nory under investigation. Consider the following ex-\nample, which is motivated by a referee’s comment. Suppose that the treatment is parents’ decision to\nhave their child receive the live vaccine for H1N1 ﬂu\nvirus and the outcome is whether the child develops\nﬂu or not. For a virologist, a mediator of interest\nmay be the development of antibodies to H1N1 live\nvaccine. But, if parents sign a form acknowledging\nthe risks of the vaccine, can this act of form signing\nalso be a mediator? Indeed, social scientists (if not\nvirologists!) may hypothesize that being informed of\nthe risks will make parents less likely to have their\nchild receive the second dose of the vaccine, thereby\nincreasing the risk of developing ﬂu. This example\nhighlights the important role of scientiﬁc theories in\ncausal mediation analysis. To deﬁne the causal mediation eﬀects, we use the\npotential outcomes framework. Let Mi(t) denote the\npotential value of the mediator for unit i under the\ntreatment status Ti = t. Similarly, we use Yi(t,m)\nto represent the potential outcome for unit i when\nTi = t and Mi = m. Then, the observed variables can\nbe written as Mi = Mi(Ti) and Yi = Yi(Ti,Mi(Ti)). Similarly, if the mediator takes J diﬀerent values,\nthere exist 2J potential values of the outcome vari-\nable, only one of which can be observed.",
    "content_hash": "6eb975c9fc9c9968f8b91a71816b4e68bacc78c574260e3969dbf8b4133911ec",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "570f0382-a205-4152-9877-0799c6b0f02c",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "4\nK. IMAI, L. KEELE AND T. YAMAMOTO\nmechanism. For example, it is possible that subjects’\nunderlying political ideology aﬀects both their pub-\nlic order attitude and their tolerance for the Klan\nrally within each treatment condition. This scenario\nis of particular concern since it is well established\nthat politically conservative citizens tend to be more\nconcerned about public order issues and also, in\nsome instances, be more sympathetic to groups like\nthe Klan. In Section 5 we propose a new sensitivity\nanalysis that partially addresses such concerns. 3. IDENTIFICATION\nIn this section we propose a new nonparametric\nidentiﬁcation assumption for the ACME and discuss\nits practical implications. We also compare the pro-\nposed assumption with those available in the litera-\nture. 3.1 The Framework\nConsider a simple random sample of size n from a\npopulation where for each unit i we observe (Ti,Mi,\nXi,Yi). We use Ti to denote the binary treatment\nvariable where Ti = 1 (Ti = 0) implies unit i re-\nceives (does not receive) the treatment. The mediat-\ning variable of interest, that is, the mediator, is rep-\nresented by Mi, whereas Yi represents the outcome\nvariable. Finally, Xi denotes the vector of observed\npre-treatment covariates, and we use M, X and Y\nto denote the support of the distributions of Mi, Xi\nand Yi, respectively. What qualiﬁes as a mediator? Since the media-\ntor lies in the causal path between the treatment\nand the outcome, it must be a post-treatment vari-\nable that occurs before the outcome is realized.",
    "content_hash": "6e74d0862fdad76028f38629b4bd2a976265262bae15ae544703c4f1f5b0487b",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "47a898df-880e-45bb-90fb-37f76f8392b8",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Using the potential outcomes notation, we can\ndeﬁne the causal mediation eﬀect for unit i under\ntreatment status t as (see Robins and Greenland,\n1992; Pearl, 2001)\nδi(t) ≡Yi(t,Mi(1)) −Yi(t,Mi(0))\n(1)\nfor t = 0,1. Pearl (2001) called δi(t) the natural in-\ndirect eﬀect, while Robins (2003) used the term the\npure indirect eﬀect for δi(0) and the total indirect\neﬀect for δi(1). In words, δi(t) represents the dif-\nference between the potential outcome that would\nresult under treatment status t, and the potential\noutcome that would occur if the treatment status is\nthe same and yet the mediator takes a value that\nwould result under the other treatment status. Note\nthat the former is observable (if the treatment vari-\nable is actually equal to t), whereas the latter is by\ndeﬁnition unobservable [under the treatment status\nt we never observe Mi(1 −t)]. Some feel uncom-\nfortable with the idea of making inferences about\nquantities that can never be observed (e.g., Rubin,\n2005, page 325), while others emphasize their impor-\ntance in policy making and scientiﬁc research (Pearl,\n2001, Section 2.4, 2010, Section 6.1.4; Hafeman and\nSchwartz 2009). Furthermore, the above notation implicitly assumes\nthat the potential outcome depends only on the val-\nues of the treatment and mediating variables and, in\nparticular, not on how they are realized.",
    "content_hash": "45c02b85a5ca8b843edccdda361c127f93cf1cbaa262d9ef72f3d2bc4b987d80",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "71365e5b-b6c3-4a66-8b14-8e876267d133",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "For exam-\nple, this assumption would be violated if the out-\ncome variable responded to the value of the me-\ndiator diﬀerently depending on whether it was di-\nrectly assigned or occurred as a natural response to\nthe treatment, that is, for t = 0,1 and all m ∈M,\nYi(t,Mi(t)) = Yi(t,Mi(1 −t)) = Yi(t,m) if Mi(1) =\nMi(0) = m. Thus, equation (1) formalizes the idea that the\nmediation eﬀects represent the indirect eﬀects of the",
    "content_hash": "a6c0e09e89acf68d1ff31bcc3dab08475441de97d7e89dbce0ef0636fc57ac8b",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e5e09973-23a6-4eb9-a1a1-22ad008bf40e",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Un-\nder Assumption 1, the ACME and the average natu-\nral direct eﬀects are nonparametrically identiﬁed as\nfollows for t = 0,1:\n¯δ(t) =\nZ Z\nE(Yi|Mi = m,Ti = t,Xi = x)\n{dFMi|Ti=1,Xi=x(m)\n−dFMi|Ti=0,Xi=x(m)}dFXi(x),\n¯ζ(t) =\nZ Z\n{E(Yi|Mi = m,Ti = 1,Xi = x)\n−E(Yi|Mi = m,Ti = 0,Xi = x)}\ndFMi|Ti=t,Xi=x(m)dFXi(x),\nwhere FZ(·) and FZ|W(·) represent the distribution\nfunction of a random variable Z and the conditional\ndistribution function of Z given W.",
    "content_hash": "feb5864e05e0fee87b7d042629d75bea3f768ac51f555c33a6d14b8911de6b62",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3dce305b-4327-4136-b4ae-0ce956866d1f",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "CAUSAL MEDIATION ANALYSIS\n5\ntreatment through the mediator. In this paper we fo-\ncus on the identiﬁcation and inference of the average\ncausal mediation eﬀect (ACME), which is deﬁned as\n¯δ(t) ≡E(δi(t))\n(2)\n= E{Yi(t,Mi(1)) −Yi(t,Mi(0))}\nfor t = 0,1. In the potential outcomes framework,\nthe causal eﬀect of the treatment on the outcome for\nunit i is deﬁned as τi ≡Yi(1,Mi(1)) −Yi(0,Mi(0)),\nwhich is typically called the total causal eﬀect. There-\nfore, the causal mediation eﬀect and the total causal\neﬀect have the following relationship:\nτi = δi(t) + ζi(1 −t),\n(3)\nwhere ζi(t) = Yi(1,Mi(t)) −Yi(0,Mi(t)) for t = 0,1. This quantity ζi(t) is called the natural direct ef-\nfect by Pearl (2001) and the pure/total direct eﬀect\nby Robins (2003). This represents the causal eﬀect\nof the treatment on the outcome when the media-\ntor is set to the potential value that would occur\nunder treatment status t. In other words, ζi(t) is\nthe direct eﬀect of the treatment when the mediator\nis held constant. Equation (3) shows an important\nrelationship where the total causal eﬀect is equal\nto the sum of the mediation eﬀect under one treat-\nment condition and the natural direct eﬀect under\nthe other treatment condition.",
    "content_hash": "77dedfacad0aa5a513dc272258ba275e6d0b86449bc16192d7af6bd7141eeacb",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "58f84648-9cb3-41f3-b942-8d63e069be8a",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "We show that under a particular version of sequen-\ntial ignorability assumption, the ACME is nonpara-\nmetrically identiﬁed. We ﬁrst deﬁne our identifying\nassumption:\nAssumption 1 (Sequential ignorability). {Yi(t′,m),Mi(t)} ⊥⊥Ti|Xi = x,\n(4)\nYi(t′,m) ⊥⊥Mi(t)|Ti = t,Xi = x\n(5)\nfor t,t′ = 0,1, and all x ∈X where it is also as-\nsumed that 0 < Pr(Ti = t|Xi = x) and 0 < p(Mi(t) =\nm|Ti = t,Xi = x) for t = 0,1, and all x ∈X and\nm ∈M. Thus, the treatment is ﬁrst assumed to be ignor-\nable given the pre-treatment covariates, and then\nthe mediator variable is assumed to be ignorable\ngiven the observed value of the treatment as well\nas the pre-treatment covariates. We emphasize that,\nunlike the standard sequential ignorability assump-\ntion in the literature (e.g., Robins, 1999), the con-\nditional independence given in equation (5) of As-\nsumption 1 must hold without conditioning on the\nobserved values of post-treatment confounders. This\nissue is discussed further below. The following theorem presents our main identi-\nﬁcation result, showing that under this assumption\nthe ACME is nonparametrically identiﬁed. Theorem 1 (Nonparametric identiﬁcation).",
    "content_hash": "3baa1c57a355d9652c92ece55732286d7dc8e5fe98fe0683d7ca93f21a511683",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6ca25c7a-a324-4e20-86f3-fe3e8b4b8b8e",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Clearly, this equality\nalso holds for the average total causal eﬀect so that\n¯τ ≡E{Yi(1,Mi(1)) −Yi(0,Mi(0))} = ¯δ(t) + ¯ζ(1 −t)\nfor t = 0,1 where ¯ζ(t) = E(ζi(t)). The causal mediation eﬀects and natural direct\neﬀects diﬀer from the controlled direct eﬀect of the\nmediator, that is, Yi(t,m)−Yi(t,m′) for t = 0,1 and\nm ̸= m′, and that of the treatment, that is, Yi(1,m)−\nYi(0,m) for all m ∈M (Pearl, 2001; Robins, 2003). Unlike the mediation eﬀects, the controlled direct\neﬀects of the mediator are deﬁned in terms of spe-\nciﬁc values of the mediator, m and m′, rather than\nits potential values, Mi(1) and Mi(0). While causal\nmediation analysis is used to identify possible causal\npaths from Ti to Yi, the controlled direct eﬀects may\nbe of interest, for example, if one wishes to under-\nstand how the causal eﬀect of Mi on Yi changes as\na function of Ti. In other words, the former exam-\nines whether Mi mediates the causal relationship\nbetween Ti and Yi, whereas the latter investigates\nwhether Ti moderates the causal eﬀect of Mi on Yi\n(Baron and Kenny, 1986). 3.2 The Main Identiﬁcation Result\nWe now present our main identiﬁcation result us-\ning the potential outcomes framework described above.",
    "content_hash": "e9500297b42b511ed4597d48f64dc04e9a6d6178af92fcbe9757f87295e6673e",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4e1fb74b-a81a-4a07-9eb6-a72b2870ddde",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "This assumption, called the no-interaction assump-\ntion, states that the controlled direct eﬀect of the\ntreatment does not depend on the value of the medi-\nator. In practice, this assumption can be violated in\nmany applications and has sometimes been regarded\nas “very restrictive and unrealistic” (Petersen, Sinisi\nand van der Laan, 2006, page 280). In contrast, The-\norem 1 shows that under the sequential ignorabil-\nity assumption that does not condition on the post-\ntreatment covariates, the no-interaction assumption\nis not required for the nonparametric identiﬁcation. Therefore, there exists an important trade-oﬀ; al-\nlowing for conditioning on observed post-treatment\nconfounders requires an additional assumption for\nthe identiﬁcation of the ACME. Third, Petersen, Sinisi and van der Laan (2006)\npresent yet another set of identifying assumptions. In particular, they maintain equation (5) but replace",
    "content_hash": "25cf46f64fc41184a3d97b9c1abbd09de72fb330c05df34242fd8bcb536de05e",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0276b6c5-a371-409e-a889-49a73559fb7e",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "6\nK. IMAI, L. KEELE AND T. YAMAMOTO\nA proof is given in Appendix A. Theorem 1 is\nquite general and can be easily extended to any\ntypes of treatment regimes, for example, a contin-\nuous treatment variable. In fact, the proof requires\nno change except letting t and t′ take values other\nthan 0 and 1. Assumption 1 can also be somewhat\nrelaxed by replacing equation (5) with its corre-\nsponding mean independence assumption. However,\nas mentioned above, this identiﬁcation result does\nnot hold under the standard sequential ignorabil-\nity assumption. As shown by Avin, Shpitser and\nPearl (2005) and also pointed out by Robins (2003),\nthe nonparametric identiﬁcation of natural direct\nand indirect eﬀects is not possible without an ad-\nditional assumption if equation (5) holds only af-\nter conditioning on the post-treatment confounders\nZi as well as the pre-treatment covariates Xi, that\nis, Yi(t′,m) ⊥Mi(t)|Ti = t,Zi = z,Xi = x, for t,t′ =\n0,1, and all x ∈X and z ∈Z where Z is the support\nof Zi. This is an important limitation since assuming\nthe absence of post-treatment confounders may not\nbe credible in many applied settings. In some cases,\nhowever, it is possible to address the main source of\nconfounding by conditioning on pre-treatment vari-\nables alone (see Section 6 for an example).",
    "content_hash": "06eba41203292aa3e31f27f26a9ded1b244212ca251b6efc4aefb03429dcafac",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "289b6e87-c045-442c-b711-61eb33659724",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "3.3 Comparison with the Existing Results\nin the Literature\nNext, we compare Theorem 1 with the related\nidentiﬁcation results in the literature. First, Pearl\n(2001, Theorem 2) makes the following set of as-\nsumptions in order to identify ¯δ(t∗):\np(Y (t,m)|Xi = x)\nand\n(6)\np(Mi(t∗)|Xi = x)\nare identiﬁable,\nYi(t,m) ⊥⊥Mi(t∗)|Xi = x\n(7)\nfor all t = 0,1, m ∈M, and x ∈X . Under these as-\nsumptions, Pearl arrives at the same expressions for\nthe ACME as the ones given in Theorem 1. Indeed,\nit can be shown that Assumption 1 implies equa-\ntions (6) and (7). While the converse is not necessar-\nily true, in practice, the diﬀerence is only technical\n(see, e.g., Robins, 2003, page 76). For example, con-\nsider a typical situation where the treatment is ran-\ndomized given the observed pre-treatment covari-\nates Xi and researchers are interested in identifying\nboth ¯δ(1) and ¯δ(0). In this case, it can be shown that\nAssumption 1 is equivalent to Pearl’s assumptions. Moreover, one practical advantage of equation (5)\nof Assumption 1 is that it is easier to interpret than\nequation (7), which represents the independence be-\ntween the potential values of the outcome and the\npotential values of the mediator.",
    "content_hash": "7c4a3705d2e0186912782ca1950a1aadcf770215aaeaf4c3fcc6792ddcba38e4",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b420b183-962e-4885-9d7d-46d061347588",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Pearl himself rec-\nognizes this diﬃculty, and states “assumptions of\ncounterfactual independencies can be meaningfully\nsubstantiated only when cast in structural form”\n(page 416). In contrast, equation (5) simply means\nthat Mi is eﬀectively randomly assigned given Ti\nand Xi. Second, Robins (2003) considers the identiﬁcation\nunder what he calls a FRCISTG model, which sat-\nisﬁes equation (4) as well as\nYi(t,m) ⊥Mi(t)|Ti = t,Zi = z,Xi = x\n(8)\nfor t = 0,1 where Zi is a vector of the observed values\nof post-treatment variables that confound the rela-\ntionship between the mediator and outcome. The\nkey diﬀerence between Assumption 1 and a FR-\nCISTG model is that the latter allows conditioning\non Zi while the former does not. Robins (2003) ar-\ngued that this is an important practical advantage\nover Pearl’s conditions, in that it makes the ignora-\nbility of the mediator more credible. In fact, not al-\nlowing for conditioning on observed post-treatment\nconfounders is an important limitation of Assump-\ntion 1. Under this model, Robins (2003, Theorem 2.1)\nshows that the following additional assumption is\nsuﬃcient to identify the ACME:\nYi(1,m) −Yi(0,m) = Bi,\n(9)\nwhere Bi is a random variable independent of m.",
    "content_hash": "2536e06fb0454bd25450af92e751adce819107b26e107258db9ed6ca4764a290",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "741a7cba-a237-4b5e-b2be-fc952b634c37",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Although some estimate E(Yi(1,Mi(1)) −\nYi(0,Mi(0))|Mi(1) ̸= Mi(0)) and compare it with the\nabove average direct eﬀect, as VanderWeele (2008)\npoints out, the problem of such comparison is that\ntwo quantities are deﬁned for diﬀerent subsets of\nthe population. Another diﬃculty of this approach\nis that when the mediator is continuous the popula-\ntion proportion of those with Mi(1) = Mi(0) can be\nessentially zero. This explains why the application\nof this approach has been limited to the studies with\na discrete (often binary) mediator. 3.4 Implications for Linear Structural\nEquation Model\nNext, we discuss the implications of Theorem 1\nfor LSEM, which is a popular tool among applied\nresearchers who conduct causal mediation analysis. In an inﬂuential article, Baron and Kenny (1986)\nproposed a framework for mediation analysis, which\nhas been used by many social science methodolo-\ngists; see MacKinnon (2008) for a review and Imai,\nKeele and Tingley (2009) for a critique of this lit-\nerature. This framework is based on the following\nsystem of linear equations:\nYi = α1 + β1Ti + εi1,\n(11)\nMi = α2 + β2Ti + εi2,\n(12)\nYi = α3 + β3Ti + γMi + εi3. (13)\nAlthough we adhere to their original model, one may\nfurther condition on any observed pre-treatment co-\nvariates by including them as additional regressors\nin each equation.",
    "content_hash": "c41c726bae3e4ecdf5acf94c02e90b8b21a8f7580fdcd2ae3464d35b5e78d265",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ca4525ba-47c3-43a5-984e-57f82d473582",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Fourth, in the appendix of a recent paper, Hafe-\nman and VanderWeele (2010) show that if the me-\ndiator is binary, the ACME can be identiﬁed with a\nweaker set of assumptions than Assumption 1. How-\never, it is unclear whether this result can be gener-\nalized to cases where the mediator is nonbinary. In\ncontrast, the identiﬁcation result given in Theorem 1\nholds for any type of mediator, whether discrete or\ncontinuous. Both identiﬁcation results hold for gen-\neral treatment regimes, unlike some of the previous\nresults. Finally, Rubin (2004) suggests an alternative ap-\nproach to causal mediation analysis, which has been\nadopted recently by other scholars (e.g., Egleston et\nal., 2006; Gallop et al., 2009; Elliott, Raghunathan\nand Li, 2010). In this framework, the average direct\neﬀect of the treatment is given by E(Yi(1,Mi(1)) −\nYi(0,Mi(0))|Mi(1) = Mi(0)), representing the aver-\nage treatment eﬀect among those whose mediator\nis not aﬀected by the treatment. Unlike the aver-\nage direct eﬀect ¯ζ(t) introduced above, this quan-\ntity is deﬁned for a principal stratum, which is a\nlatent subpopulation. Within this framework, there\nexists no obvious deﬁnition for the mediation ef-\nfect unless the direct eﬀect is zero (in this case, the\ntreatment aﬀects the outcome only through the me-\ndiator).",
    "content_hash": "c715153f2b4402967942f553ab3464a7f2bc0c3162871144e0c0e64647ed2dc9",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "eba51536-b922-4d43-ac1b-331d6bf8b97c",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "CAUSAL MEDIATION ANALYSIS\n7\nequation (4) with the following slightly weaker con-\ndition:\nYi(t,m) ⊥⊥Ti|Xi = x\nand\n(10)\nMi(t) ⊥⊥Ti|Xi = x\nfor t = 0,1 and all m ∈M. In practice, this diﬀer-\nence is only a technical matter because, for exam-\nple, in randomized experiments where the treatment\nis randomized, equations (4) and (10) are equiva-\nlent. However, this slight weakening of equation (4)\ncomes at a cost, requiring an additional assump-\ntion for the identiﬁcation of the ACME. Speciﬁcally,\nPetersen, Sinisi and van der Laan (2006) assume\nthat the magnitude of the average direct eﬀect does\nnot depend on the potential values of the media-\ntor, that is, E{Yi(1,m) −Yi(0,m)|Mi(t∗) = m,Xi =\nx} = E{Yi(1,m) −Yi(0,m)|Xi = x} for all m ∈M. Theorem 1 shows that if equation (10) is replaced\nwith equation (4), which is possible when the treat-\nment is randomized, then this additional assumption\nis unnecessary for the nonparametric identiﬁcation. In addition, this additional assumption is somewhat\ndiﬃcult to interpret in practice because it entails the\nmean independence relationship between the poten-\ntial values of the outcome and the potential values\nof the mediator.",
    "content_hash": "3844ad72cf30bb4d7205a1fbb3be554f965d369b29d58347dffb577191976819",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e0aa77c4-6d1a-466e-894d-a24bac9d9026",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "This will change none of the re-\nsults given below so long as the model includes no\npost-treatment confounders. Under this model, Baron and Kenny (1986) sug-\ngested that the existence of mediation eﬀects can\nbe tested by separately ﬁtting the three linear re-\ngressions and testing the null hypotheses (1) β1 = 0,\n(2) β2 = 0, and (3) γ = 0. If all of these null hy-\npotheses are rejected, they argued, then β2γ could\nbe interpreted as the mediation eﬀect. We note that\nequation (11) is redundant given equations (12) and\n(13). To see this, substitute equation (12) into equa-\ntion (13) to obtain\nYi = (α3 + α2γ) + (β3 + β2γ)Ti\n(14)\n+ (γεi2 + εi3). Thus, testing β1 = 0 is unnecessary since the ACME\ncan be nonzero even when the average total causal",
    "content_hash": "89784bc9b9534e4fc93e98d4cf1d08eb40ce78f5a8d495531c9e58b5e82fde15",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6e56f9ee-3dbd-4eb6-af17-a6a7c6f0fc39",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "8\nK. IMAI, L. KEELE AND T. YAMAMOTO\neﬀect is zero. This happens when the mediation ef-\nfect oﬀsets the direct eﬀect of the treatment. The next theorem proves that within the LSEM\nframework, Baron and Kenny’s interpretation is valid\nif Assumption 1 holds. Theorem 2 (Identiﬁcation under the LSEM). Consider the LSEM deﬁned in equations (11), (12)\nand (13). Under Assumption 1, the ACME is identi-\nﬁed and given by ¯δ(0) = ¯δ(1) = β2γ, where the equal-\nity between ¯δ(0) and ¯δ(1) is also assumed. A proof is in Appendix B. The theorem implies\nthat under the same set of assumptions, the aver-\nage natural direct eﬀects are identiﬁed as ¯ζ(0) =\n¯ζ(1) = β3, where the average total causal eﬀect is\n¯τ = β3 + β2γ. Thus, Assumption 1 enables the iden-\ntiﬁcation of the ACME under the LSEM. Egleston\net al. (2006) obtain a similar result under the as-\nsumptions of Pearl (2001) and Robins (2003), which\nwere reviewed in Section 3.3. It is important to note that under Assumption 1,\nthe standard LSEM deﬁned in equations (12) and\n(13) makes the following no-interaction assumption\nabout the ACME:\nAssumption 2 (No-interaction between the Treat-\nment and the ACME). ¯δ(1) = ¯δ(0).",
    "content_hash": "6af9ec508661a9f74c47cc65c0c6beb39f8df458b60908cbf804aeb672fc9a6e",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7967c8a2-4a89-4a2a-b378-e5d2678ccb0d",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "(2008)\nthat the existence of mediation eﬀects can be es-\ntablished by testing either γ = 0 or κ = 0, which is\nclearly neither a necessary nor suﬃcient condition\nfor ¯δ(t) to be zero. The connection between the parametric and non-\nparametric identiﬁcation becomes clearer when both\nTi and Mi are binary. To see this, note that ¯δ(t) can\nbe equivalently expressed as [dropping the integra-\ntion over P(Xi) for notational simplicity]\n¯δ(t) =\nJ−1\nX\nm=0\nE(Yi|Mi = m,Ti = t,Xi)\n· {Pr(Mi = m|Ti = 1,Xi)\n(16)\n−Pr(Mi = m|Ti = 0,Xi)},\nwhen Mi is discrete. Furthermore, when J = 2, this\nreduces to\n¯δ(t) = {Pr(Mi = 1|Ti = 1,Xi)\n−Pr(Mi = 1|Ti = 0,Xi)}\n(17)\n· {E(Yi|Mi = 1,Ti = t,Xi)\n−E(Yi|Mi = 0,Ti = t,Xi)}. Thus, the ACME equals the product of two terms\nrepresenting the average eﬀect of Ti on Mi and that\nof Mi on Yi (holding Ti at t), respectively. Finally, in the existing methodological literature\nSobel (2008) explores the identiﬁcation problem of\nmediation eﬀects under the framework of LSEM with-\nout assuming the ignorability of the mediator (see\nalso Albert, 2008; Jo, 2008).",
    "content_hash": "696af2925fbac712e4be2ffe958f19389d87dca7688bb99b44fef6bbb491be63",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "428dc14c-8093-4527-b1a8-436dc19c8a0b",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "However, Sobel (2008)\nmaintains, among others, the assumption that the\ncausal eﬀect of the treatment is entirely through\nthe mediator and applies the instrumental variables\ntechnique of Angrist, Imbens and Rubin (1996). That\nis, the natural direct eﬀect is assumed to be zero for\nall units a priori, that is, ζi(t) = 0 for all t = 0,1\nand i. This assumption may be undesirable from\nthe perspective of applied researchers, because the\nexistence of the natural direct eﬀect itself is often of\ninterest in causal mediation analysis. See Joﬀe et al. (2008) for an interesting application. 4. ESTIMATION AND INFERENCE\nIn this section we use our nonparametric identiﬁ-\ncation result above and propose simple parametric\nand nonparametric estimation strategies.",
    "content_hash": "816204dfa60cc7c6bdbd926daefc9cabfa826600426c059e11631e9f8b8cd3d1",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "de02724e-2f24-4ded-9e47-3db60e0e5fce",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "This assumption is equivalent to the no-interaction\nassumption for the average natural direct eﬀects,\n¯ζ(1) = ¯ζ(0). Although Assumption 2 is related to\nand implied by Robins’ no-interaction assumption\ngiven in equation (9), the key diﬀerence is that As-\nsumption 2 is written in terms of the ACME rather\nthan controlled direct eﬀects. As Theorem 1 suggests, Assumption 2 is not re-\nquired for the identiﬁcation of the ACME under the\nLSEM. We extend the outcome model given in equa-\ntion (13) to\nYi = α3 + β3Ti + γMi + κTiMi + εi3,\n(15)\nwhere the interaction term between the treatment\nand mediating variables is added to the outcome\nregression while maintaining the linearity in param-\neters. This formulation was ﬁrst suggested by Judd\nand Kenny (1981) and more recently advocated by\nKraemer et al. (2008, 2002) as an alternative to Bar-\nron and Kenny’s approach. Under Assumption 1 and\nthe model deﬁned by equations (12) and (15), we can\nidentify the ACME as ¯δ(t) = β2(γ + tκ) for t = 0,1. The average natural direct eﬀects are identiﬁed as\n¯ζ(t) = β3 +κ(α2 +β2t), and the average total causal\neﬀect is equal to ¯τ = β2γ + β3 + κ(α2 + β2). This\nconﬂicts with the proposal by Kraemer et al.",
    "content_hash": "0a5519b2ddbc91e7db1b925d2c76236ab0d4e2af526755f44187da414cb594bd",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "655767ce-059b-4713-969b-68a83a4f3b2e",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "For example, using the delta method,\nwe have Var(ˆδ(t)) ≈(γ +tκ)2 Var(ˆβ2)+β2\n2{Var(ˆγ)+\ntVar(ˆκ) + 2tCov(ˆγ, ˆκ)}\nfor\nt = 0,1. Similarly,\nVar(ˆζ(t)) ≈Var(ˆβ3) + (α2 + tβ2)2 Var(ˆκ) + 2(α2 +\ntβ2)Cov(ˆβ3, ˆκ)+κ2{Var(ˆα2)+tVar(ˆβ2)+2tCov(ˆα2,\nˆβ2)}. For the average total causal eﬀect, the variance\ncan be obtained from the regression of Yi on Ti. 4.2 Nonparametric Estimation and Inference\nNext, we consider a simple nonparametric esti-\nmator. Suppose that the mediator is discrete and\ntakes J distinct values, that is, M = {0,1,...,J −\n1}. The case of continuous mediators is considered\nfurther below. First, we consider the cases where\nwe estimate the ACME separately within each stra-\ntum deﬁned by the pre-treatment covariates Xi. One\nmay then aggregate the resulting stratum-speciﬁc\nestimates to obtain the estimated ACME.",
    "content_hash": "5ee69ee35fad5ada51e50c6afa6971fca81d10a7f11334d9dac1242e57dd2360",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2e63a34b-6bc2-4baa-b91b-11cbc033add5",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "The second and perhaps more general strategy is\nto use nonparametric regressions to model µtm(x) ≡\nE(Yi|Ti = t,Mi = m,Xi = x) and νtm(x) ≡Pr(Mi =\nm|Ti = t,Xi = x), and then employ the following es-\ntimator:\nˆδ(t) = 1\nn\n( n\nX\ni=1\nJ−1\nX\nm=0\nˆµtm(Xi)\n(19)\n· (ˆν1m(Xi) −ˆν0m(Xi))\n)\nfor t = 0,1. This estimator is also asymptotically\nconsistent for the ACME under Assumption 1 if\nˆµtm(x) and ˆνtm(x) are consistent for µtm(x) and\nνtm(x), respectively. Unfortunately, in general, there",
    "content_hash": "5082625aabb80e6e56ce2dd23f4f4a11f961eb83244279dc19b1d06bd358cf04",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "19b212ff-43cd-405a-8ddb-89a8e74d85a3",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Then, the variance of the nonparametric estimator\ndeﬁned in equation (18) is asymptotically approxi-\nmated by\nVar(ˆδ(t)) ≈1\nnt\nJ−1\nX\nm=0\nν1−t,m\n\u001a\u0012ν1−t,m\nνtm\n−2\n\u0013\n· Var(Yi|Mi = m,Ti = t)\n+ nt(1 −ν1−t,m)µ2\ntm\nn1−t\n\u001b\n−\n2\nn1−t\nJ−1\nX\nm′=m+1\nJ−2\nX\nm=0\nν1−t,mν1−t,m′µtmµtm′\n+ 1\nnt\nVar(Yi|Ti = t)\nfor t = 0,1 where νtm ≡Pr(Mi = m|Ti = t) and µtm ≡\nE(Yi|Mi = m,Ti = t). A proof is based on a tedious but simple appli-\ncation of the Delta method and thus is omitted. This asymptotic variance can be consistently esti-\nmated by replacing unknown population quantities\nwith their corresponding sample counterparts. The\nestimated overall variance can be obtained by ag-\ngregating the estimated within-strata variances ac-\ncording to the sample size in each stratum.",
    "content_hash": "8f05dec16c690b65a83fe1e13a772abf2f33d9f7df4f71baf39c8ae7977a9c52",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7e0e2f56-373f-4f0d-b240-ac30d5c8070f",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "CAUSAL MEDIATION ANALYSIS\n9\n4.1 Parametric Estimation and Inference\nUnder the LSEM given by equations (12) and (13)\nand Assumption 1, the estimation of the ACME is\nstraightforward since the error terms are indepen-\ndent of each other. Thus, one can follow the pro-\nposal of Baron and Kenny (1986) and estimate equa-\ntions (12) and (13) by ﬁtting two separate linear\nregressions. The standard error for the estimated\nACME, that is, ˆδ(t) = ˆβ2ˆγ, can be calculated ei-\nther approximately using the Delta method (Sobel,\n1982), that is, Var(ˆδ(t)) ≈β2\n2 Var(ˆγ) + γ2 Var(ˆβ2),\nor exactly via the variance formula of Goodman\n(1960), that is, Var(ˆδ(t)) = β2\n2 Var(ˆγ) + γ2 Var(ˆβ2) +\nVar(ˆγ)Var(ˆβ2). For the natural direct and total ef-\nfects, standard errors can be obtained via the re-\ngressions of Yi on Ti and Mi [equation (13)] and Yi\non Ti [equation (11)], respectively. When the model contains the interaction term as\nin equation (15) (so that Assumption 2 is relaxed),\nthe asymptotic variance can be computed in a sim-\nilar manner.",
    "content_hash": "ce305036eaa627de91b59053ed3991ff6d738604e2ccf52259bebff14f89543e",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8f0cda74-e196-4fe4-ae47-77beab62f30e",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "In such\nsituations, a nonparametric estimator can be ob-\ntained by plugging in sample analogues for the pop-\nulation quantities in the expression given in Theo-\nrem 1,\nˆδ(t) =\nJ−1\nX\nm=0\n(Pn\ni=1 Yi1{Ti = t,Mi = m}\nPn\ni=1 1{Ti = t,Mi = m}\n·\n1\nn1\nn\nX\ni=1\n1{Ti = 1,Mi = m}\n(18)\n−1\nn0\nn\nX\ni=1\n1{Ti = 0,Mi = m}\n!)\n,\nwhere nt = Pn\ni=1 1{Ti = t} and t = 0,1. By the law\nof large numbers, this estimator asymptotically con-\nverges to the true ACME under Assumption 1. The\nnext theorem derives the asymptotic variance of the\nnonparametric estimator deﬁned in equation (18)\ngiven the realized values of the treatment variable. Theorem 3 (Asymptotic variance of the nonpara-\nmetric estimator). Suppose that Assumption 1 holds.",
    "content_hash": "daad5ca1e884f4389daae680c19624d510a1dc4d7a8737243d375a005d978d81",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9bcf7e6a-4f6d-457f-8aea-90cc0fe0d956",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Also, note that this data generating process implies\nthe following parametric regression models for the\nobserved data:\nPr(Mi = 1|Ti) = Φ(α2 + β2Ti),\n(21)\nYi|Ti,Mi ∼lognormal(α3 + β3Ti + γMi\n(22)\n+ κTiMi,σ2\n3),\nwhere\n(α2,β2,α3,β3,γ,κ,σ2\n3) = (−0.5,1,0.5,−0.5,\n0.5,1.5,1) and Φ(·) is the standard normal distri-\nbution function. We can then obtain the parametric\nTable 2\nFinite-sample performance of the proposed estimators and their variance estimators. The table presents the results of a\nMonte Carlo experiment with varying sample sizes and ﬁfty thousand iterations. The upper half of the table represents the\nresults for ˆδ(0) and the bottom half ˆδ(1). The columns represent (from left to right) the following: sample sizes, estimated\nbiases, root mean squared errors (RMSE) and the coverage probabilities of the 95% conﬁdence intervals of the nonparametric\nestimators, and the same set of quantities for the parametric estimators. The true values of ¯δ(0) and ¯δ(1) are 0.675 and\n4.03, respectively. The results indicate that nonparametric estimators have smaller bias than the parametric estimator though\nits variance is much larger. The conﬁdence intervals converge to the nominal coverage as the sample size increases.",
    "content_hash": "e4ebb0001dd563a0f7ccd52ea28f3b95742feace56f02e3f3963d3833a252dcb",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7b4301bd-09ff-416b-9df4-8ea97ebb0c61",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "10\nK. IMAI, L. KEELE AND T. YAMAMOTO\nis no simple expression for the asymptotic variance\nof this estimator. Thus, one may use a nonparamet-\nric bootstrap [or a parametric bootstrap based on\nthe asymptotic distribution of ˆµtm(x) and ˆνtm(x)]\nto compute uncertainty estimates. Finally, when the mediator is not discrete, we may\nnonparametrically model µtm(x) ≡E(Yi|Ti = t,Mi =\nm,Xi = x) and ψt(x) = p(Mi|Ti = t,Xi = x). Then,\none can use the following estimator:\nˆδ(t) =\n1\nnK\nn\nX\ni=1\nK\nX\nk=1\n{ˆµt ˜m(k)\n1i (Xi) −ˆµt ˜m(k)\n0i (Xi)},\n(20)\nwhere ˜m(k)\nti\nis the kth Monte Carlo draw of the me-\ndiator Mi from its predicted distribution based on\nthe ﬁtted model ˆψt(Xi). These estimation strategies are quite general in\nthat they can be applied to a wide range of statisti-\ncal models. Imai, Keele and Tingley (2009) demon-\nstrate the generality of these strategies by apply-\ning them to common parametric and nonparamet-\nric regression techniques often used by applied re-\nsearchers. By doing so, they resolve some confusions\nheld by social science methodologists, for example,\nhow to estimate mediation eﬀects when the out-\ncome and/or the mediator is binary. Furthermore,\nthe proposed general estimation strategies enable\nImai et al.",
    "content_hash": "91558f00f514cc45407d18b50041f87e39b7e61c1d2dc2037125124f484fbedc",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b8b7e226-181d-4fb8-9bbe-14a4ab19685a",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "(2010) to develop an easy-to-use R pack-\nage, mediation, that implements these methods and\ndemonstrate its use with an empirical example. 4.3 A Simulation Study\nNext, we conduct a small-scale Monte Carlo ex-\nperiment in order to investigate the ﬁnite-sample\nperformance of the estimators deﬁned in equations (18)\nand (19) as well as the proposed variance estima-\ntor given in Theorem 3. We use a population model\nwhere the potential outcomes and mediators are given\nby Yi(t,m) = exp(Y ∗\ni (t,m)), Mi(t) = 1{M∗\ni (t) ≥0.5}\nand Y ∗\ni (t,m), M∗\ni (t) are jointly normally distributed.",
    "content_hash": "8add5639c5af5c85a3223fc4e8d2ee9940e79383661409f138694ad12f7fdd61",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "cbf853ad-4afe-477f-a113-1f82e5dd8568",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "The population parameters are set to the following\nvalues: E(Y ∗\ni (1,1)) = 2; E(Y ∗\ni (1,0)) = 0; E(Y ∗\ni (0,1)) =\n1; E(Y ∗\ni (0,0)) = 0.5; E(M∗\ni (1)) = 1; E(M∗\ni (0)) = 0;\nVar(Y ∗\ni (t,m)) = Var(M∗\ni (t)) = 1 for t ∈{0,1} and\nm ∈{0,1}; Corr(Y ∗\ni (t,m),Y ∗\ni (t′,m′)) = 0.5 for t,t′ ∈\n{0,1} and m,m′ ∈{0,1}; Corr(Y ∗\ni (t,m),M∗\ni (t′)) =\n0 for t ∈{0,1} and m ∈{0,1}; and Corr(M∗\ni (1),\nM∗\ni (0)) = 0.3. Under this setup, Assumption 1 is satisﬁed. Thus,\nwe can consistently estimate the ACME by applying\nthe nonparametric estimator given in equation (18).",
    "content_hash": "3f02f68c4c2f196d846abcb1eb3626bc1f0c48b78af590df62cfc6bde84eb10b",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c892ec04-8649-4981-8d2c-288d8c33ff94",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "The\nconvergence occurs much more quickly for the parametric estimator\nNonparametric estimator\nParametric estimator\nSample size\nBias\nRMSE\n95% CI coverage\nBias\nRMSE\n95% CI coverage\nˆδ(0)\n50\n0.002\n1.034\n0.824\n0.096\n0.965\n0.919\n100\n0.006\n0.683\n0.871\n0.044\n0.566\n0.933\n500\n−0.002\n0.292\n0.922\n0.006\n0.229\n0.947\nˆδ(1)\n50\n0.010\n2.082\n0.886\n−0.010\n1.840\n0.934\n100\n0.005\n1.462\n0.912\n0.003\n1.290\n0.944\n500\n0.001\n0.643\n0.939\n0.001\n0.570\n0.955",
    "content_hash": "d59c4de6821cba084f2b4afdaa2de7bbf6f68880cbe566c2f7fe8c5a8e15c5a7",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "82485915-0301-4f6b-aa99-86e43e40679e",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "A nonzero correla-\ntion parameter can be interpreted as the existence\nof omitted variables that are related to both the ob-\nserved value of the mediator Mi and the potential\noutcomes Yi even after conditioning on the treat-\nment variable Ti (and the observed covariates Xi). Note that these omitted variables must causally pre-\ncede Ti. Then, we vary the value of ρ and compute\nthe corresponding estimate of the ACME. In a quite\ndiﬀerent context, Roy, Hogan and Marcus (2008)\ntake this general strategy of computing a quantity\nof interest at various values of an unidentiﬁable sen-\nsitivity parameter.",
    "content_hash": "94baf5946a235cd8a398287c8b98067f3ec94e5c09061e609b98cc7040a73b4d",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d4b9c868-3857-44d5-ba20-f5cf41c4122f",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "The 95% conﬁdence intervals\nconverge to the nominal coverage as the sample size\nincreases. The convergence occurs much more quickly\nfor the parametric estimator. (Although not reported\nin the table, we conﬁrmed that for both estimators\nthe coverage probabilities fully converged to their\nnominal values by the time the sample size reached\n5000.)\n5. SENSITIVITY ANALYSIS\nAlthough the ACME is nonparametrically identi-\nﬁed under Assumption 1, this assumption, like other\nexisting identifying assumptions, may be too strong\nin many applied settings. Consider randomized ex-\nperiments where the treatment is randomized but\nthe mediator is not. Causal mediation analysis is\nmost frequently applied to such experiments. In this\ncase, equation (4) of Assumption 1 is satisﬁed but\nequation (5) may not hold for two reasons. First,\nthere may exist unmeasured pre-treatment covari-\nates that confound the relationship between the me-\ndiator and the outcome. Second, there may exist ob-\nserved or unobserved post-treatment confounders. These possibilities, along with other obstacles en-\ncountered in applied research, have led some schol-\nars to warn against the abuse of mediation analyses\n(e.g., Green, Ha and Bullock, 2010). Indeed, as we\nformally show below, the data generating process\ncontains no information about the credibility of the\nsequential ignorability assumption.",
    "content_hash": "885a8d7d7b5f464ace3e94fcf876b67fe448906b7f64db4f94d8c74accdf898b",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "3db66d13-18dc-49d1-ad69-fbb52517b477",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "CAUSAL MEDIATION ANALYSIS\n11\nmaximum likelihood estimate of the ACME by ﬁt-\nting these two models via standard procedures and\nestimating the following expression based on Theo-\nrem 1 [see equation (17)]:\n¯δ(t) = {exp(α3 + β3t + γ + κt + σ2\n3/2)\n−exp(α3 + β3t + σ2\n3/2)}\n(23)\n· {Φ(α2 + β2) −Φ(α2)}\nfor t = 0,1. We compare the performances of these two es-\ntimators via Monte Carlo simulations. Speciﬁcally,\nwe set the sample size n to 50, 100 and 500 where\nhalf of the sample receives the treatment and the\nother half is assigned to the control group, that is,\nn1 = n0 = n/2. Using equation (23), the true val-\nues of the ACME are given by ¯δ(0) = 0.675 and\n¯δ(1) = 4.03. Table 2 reports the results of the experiments based\non ﬁfty thousand iterations. The performance of the\nestimators turns out to be quite good in this partic-\nular setting. Even with sample size as small as 50,\nestimated biases are essentially zero for the nonpara-\nmetric estimates. The parametric estimators are\nslightly more biased for the small sample sizes, but\nthey converge to the true values by the time the\nsample size reaches 500. As expected, the variance\nis larger for the nonparametric estimator than the\nparametric estimator.",
    "content_hash": "5dac4c603ca32096db907069c446fe2581255bda97c21e41d486433117ae86fb",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "052b91ef-15f7-4ca3-8a09-eff02625ebab",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "To address this problem, we develop a method to\nassess the sensitivity of an estimated ACME to un-\nmeasured pre-treatment confounding (The proposed\nsensitivity analysis, however, does not address the\npossible existence of post-treatment confounders). The method is based on the standard LSEM frame-\nwork described in Section 3.4 and can be easily used\nby applied researchers to examine the robustness of\ntheir empirical ﬁndings. We derive the maximum\ndeparture from equation (5) that is allowed while\nmaintaining their original conclusion about the di-\nrection of the ACME (see Imai and Yamamoto, 2010). For notational simplicity, we do not explicitly con-\ndition on the pre-treatment covariates Xi. However,\nthe same analysis can be conducted by including\nthem as additional covariates in each regression. 5.1 Parametric Sensitivity Analysis Based on the\nResidual Correlation\nThe proof of Theorem 2 implies that if equation (4)\nholds, εi2 ⊥Ti and εi3 ⊥⊥Ti hold but εi2 ⊥⊥εi3 does\nnot unless equation (5) also holds. Thus, one way\nto assess the sensitivity of one’s conclusions to the\nviolation of equation (5) is to use the following sen-\nsitivity parameter:\nρ ≡Corr(εi2,εi3),\n(24)\nwhere −1 < ρ < 1. In Appendix C we show that As-\nsumption 1 implies ρ = 0. (Of course, the contra-\npositive of this statement is also true; ρ ̸= 0 implies\nthe violation of Assumption 1).",
    "content_hash": "b5e46104e7414682442b8c2b45d13e2287f2c56f060bc2d90901b457260c89d8",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e91116e2-616d-430a-8182-31fd159a87e4",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Second, the partial derivative of the ACME with\nrespect to ρ implies that the ACME is either mono-\ntonically increasing or decreasing in ρ, depending on\nthe sign of β2. The ACME is also symmetric about\n(ρ, ¯δ(t)) = (0,β2 ˜ρσ1/σ2). Third, the ACME is zero if and only if ρ equals\n˜ρ. This implies that researchers can easily check the\nrobustness of their conclusion obtained under the se-\nquential ignorability assumption via correlation be-\ntween εi1 and εi2. For example, if ˆδ(t) = ˆβ2ˆγ is neg-\native, the true ACME is also guaranteed to be neg-\native if ρ < ˜ρ holds. Fourth, the expression of the ACME given in The-\norem 4 is cumbersome to use when computing the\nstandard errors. A more straightforward and general\napproach is to apply the iterative feasible general-\nized least square algorithm of the seemingly unre-\nlated regression (Zellner, 1962), and use the asso-\nciated asymptotic variance formula. This strategy\nwill also work when there is an interaction term be-\ntween the treatment and mediating variables as in\nequation (15) and/or when there are observed pre-\ntreatment covariates Xi. Finally, Theorem 4 implies the following corollary,\nwhich shows that under the LSEM the data generat-\ning process is not informative at all about either the\nsensitivity parameter ρ or the ACME without equa-\ntion (5).",
    "content_hash": "6941b76c68c2168353bb4e11148eda8c5f75e6a47a155bea974ad6d647674d21",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9fc8f3da-633d-4532-8dd9-b8fae85fad16",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "12\nK. IMAI, L. KEELE AND T. YAMAMOTO\nThe next theorem shows that if the treatment is\nrandomized, the ACME is identiﬁed given a partic-\nular value of ρ. Theorem 4 (Identiﬁcation with a given error cor-\nrelation). Consider the LSEM deﬁned in equations\n(11), (12) and (13). Suppose that equation (4) holds\nand the correlation between εi2 and εi3, that is, ρ,\nis given. If we further assume −1 < ρ < 1, then the\nACME is identiﬁed and given by\n¯δ(0) = ¯δ(1) = β2σ1\nσ2\n{˜ρ −ρ\np\n(1 −˜ρ2)/(1 −ρ2)},\nwhere σ2\nj ≡Var(εij) for j = 1,2 and ˜ρ ≡Corr(εi1,\nεi2). A proof is in Appendix D. We oﬀer several re-\nmarks about Theorem 4. First, the unbiased esti-\nmates of (α1,α2,β1,β2) can be obtained by ﬁtting\nthe equation-by-equation least squares of equations\n(11) and (12). Given these estimates, the covari-\nance matrix of (εi1,εi2), whose elements are (σ2\n1,σ2\n2,\n˜ρσ1σ2), can be consistently estimated by computing\nthe sample covariance matrix of the residuals, that\nis, ˆεi1 = Yi −ˆα1 −ˆβ1Ti and ˆεi2 = Mi −ˆα2 −ˆβ2Ti.",
    "content_hash": "7165aa5020c7691f7a10f06659587f2930ed05d9af7c69a24847dde0792d3f12",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "096fccf1-1f7f-494c-bd32-afd1ae44ddcb",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "We can then express the in-\nﬂuence of the unobserved pre-treatment confounder\nusing the following coeﬃcients of determination:\nR2∗\nM ≡1 −Var(ε′\ni2)\nVar(εi2)\nand\nR2∗\nY ≡1 −Var(ε′\ni3)\nVar(εi3),\nwhich represent the proportion of previously unex-\nplained variance (either in the mediator or in the\noutcome) that is explained by the unobserved con-\nfounder (see Imbens, 2003). Another interpretation is based on the proportion\nof original variance that is explained by the unob-\nserved confounder. In this case, we use the following\nsensitivity parameters:\neR2\nM ≡Var(εi2) −Var(ε′\ni2)\nVar(Mi)\n= (1 −R2\nM)R2∗\nM",
    "content_hash": "1c3388cd69eb349fdba81d3ddd97086232b616e82c3372a0ade85eb962f66710",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a91af621-f54f-4403-a72d-2fed89068fb1",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "This result highlights the diﬃculty of causal\nmediation analysis and the importance of sensitivity\nanalysis even in the parametric modeling setting. Corollary 1 (Bounds on the sensitivity parame-\nter). Consider the LSEM deﬁned in equations (11),\n(12) and (13). Suppose that equation (4) holds but\nequation (5) may not. Then, the sharp, that is, best\npossible, bounds on the sensitivity parameter ρ and\nACME are given by (−1,1) and (−∞,∞), respec-\ntively. The ﬁrst statement of the corollary follows di-\nrectly from the proof of Theorem 4, while the second\nstatement can be proved by taking a limit of δ(t) as\nρ tends to −1 or 1. 5.2 Parametric Sensitivity Analysis Based on the\nCoeﬃcients of Determination\nThe sensitivity parameter ρ can be given an alter-\nnative deﬁnition which allows it to be interpreted as\nthe magnitude of an unobserved confounder. This\nalternative version of ρ is based on the following de-\ncomposition of the error terms in equations (12) and\n(13):\nεij = λjUi + ε′\nij\nfor j = 2,3, where Ui is an unobserved confounder\nand the sequential ignorability is assumed given Ui\nand Ti. Again, note that Ui has to be a pre-treatment\nvariable so that the resulting estimates can be given\na causal interpretation. In addition, we assume that\nε′\nij ⊥Ui for j = 2,3.",
    "content_hash": "140dac16da2ad1194058f3eb33318b5ed437d420b16eb60f0eadf68ce8a50cf6",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f35d2554-25f6-4732-a83a-129a064bcee7",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "CAUSAL MEDIATION ANALYSIS\n13\nand\neR2\nY ≡Var(εi3) −Var(ε′\ni3)\nVar(Yi)\n= (1 −R2\nY )R2∗\nY ,\nwhere R2\nM and R2\nY represent the coeﬃcients of de-\ntermination from the two regressions given in equa-\ntions (12) and (13). Note that unlike R2∗\nM and R2∗\nY\n(as well as ρ given in Corollary 1), eR2\nM and eR2\nY\nare bounded from above by Var(εi2)/Var(Mi) and\nVar(εi3)/Var(Yi), respectively. In either case, it is straightforward to show that\nthe following relationship between ρ and these pa-\nrameters holds, that is, ρ2 = R2∗\nMR2∗\nY = eR2\nM eR2\nY /{(1−\nR2\nM)(1 −R2\nY )} or, equivalently,\nρ = sgn(λ2λ3)R∗\nMR∗\nY =\nsgn(λ2λ3) eRM eRY\nq\n(1 −R2\nM)(1 −R2\nY )\n,\nwhere R∗\nM,R∗\nY , eRM and eRY are in [0,1]. Thus, in\nthis framework, researchers can specify the values of\n(R2∗\nM,R2∗\nY ) or ( eR2\nM, eR2\nY ) as well as the sign of λ2λ3\nin order to determine values of ρ and estimate the\nACME based on these values of ρ.",
    "content_hash": "9670dd5255bba729308b010c7a939956a323f288c69c9dc93301045689021442",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "4ed083da-6348-4a32-a005-a3a923346956",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "The second column of the table shows the results of the parametric LSEM approach, while\nthe third column of the table presents those of the nonparametric estimator. The lower part of the\ntable shows the results of parametric mediation analysis under the no-interaction assumption\n[ˆδ(1) = ˆδ(0)], while the upper part presents the ﬁndings without this assumption, thereby showing the\nestimated average mediation eﬀects under the treatment and the control, that is, ˆδ(1) and ˆδ(0)\nParametric\nNonparametric\nAverage mediation eﬀects\nFree speech frame ˆδ(0)\n−0.566\n−0.596\n[−1.081, −0.050]\n[−1.168, −0.024]\nPublic order frame ˆδ(1)\n−0.451\n−0.374\n[−0.871, −0.031]\n[−0.823, 0.074]\nAverage total eﬀect ˆτ\n−0.540\n−0.540\n[−1.207, 0.127]\n[−1.206, 0.126]\nWith the no-interaction assumption\nAverage mediation eﬀect\n−0.510\nˆδ(0) = ˆδ(1)\n[−0.969, −0.051]\nAverage total eﬀect ˆτ\n−0.540\n[−1.206, 0.126]",
    "content_hash": "ae569f09810e72948a3e14e1f425a9930f23d86b573ddf629726b916f5cb6b1f",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "96dc6dae-5355-4244-ad0b-465c59996efc",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Then, the analyst\ncan examine variation in the estimated ACME with\nrespect to change in these parameters. 5.3 Extensions to Nonlinear and\nNonparametric Models\nThe proposed sensitivity analysis above is devel-\noped within the framework of the LSEM, but some\nextensions are possible. For example, Imai, Keele\nand Tingley (2009) show how to conduct sensitiv-\nity analysis with probit models when the mediator\nand/or the outcome are discrete. In Appendix E,\nwhile it is substantially more diﬃcult to conduct\nsuch an analysis in the nonparametric setting, we\nconsider sensitivity analysis for the nonparametric\nplug-in estimator introduced in Section 4.2 (see also\nVanderWeele, 2010 for an alternative approach). 6. EMPIRICAL APPLICATION\nIn this section we apply our proposed methods to\nthe inﬂuential randomized experiment from political\npsychology we described in Section 2. 6.1 Analysis under Sequential Ignorability\nIn the original analysis, Nelson, Clawson and Ox-\nley (1997) used a LSEM similar to the one discussed\nin Section 3.4 and found that subjects who viewed\nthe Klan story with the free speech frame were sig-\nniﬁcantly more tolerant of the Klan than those who\nTable 3\nParametric and nonparametric estimates of the ACME under sequential ignorability in the media\nframing experiment. Each cell of the table represents an estimated average causal eﬀect and its 95%\nconﬁdence interval. The outcome is the subjects’ tolerance level for the free speech rights of the Ku\nKlux Klan, and the treatments are the public order frame (Ti = 1) and the free speech frame\n(Ti = 0).",
    "content_hash": "49721aa0e224e36ebbd026bc49b006427f92c10c67cc886f1354cdc568bbd8da",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f09715f2-8249-4687-a567-2bce09ff5371",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "14\nK. IMAI, L. KEELE AND T. YAMAMOTO\nsaw the story with the public order frame. The re-\nsearchers also found evidence supporting their main\nhypothesis that subjects’ general attitudes mediated\nthe causal eﬀect of the news story frame on toler-\nance for the Klan. In the analysis that follows, we\nonly analyze the public order mediator, for which\nthe researchers found a signiﬁcant mediation eﬀect. As we showed in Section 3.4, the original results\ncan be given a causal interpretation under sequen-\ntial ignorability, that is, Assumption 1. Here, we ﬁrst\nmake this assumption and estimate causal eﬀects\nbased on our theoretical results. Table 3 presents the\nﬁndings. The second and third columns of the table\nshow the estimated ACME and average total eﬀect\nbased on the LSEM and the nonparametric estima-\ntor, respectively. The 95% asymptotic conﬁdence in-\ntervals are constructed using the Delta method. For\nmost of the estimates, the 95% conﬁdence intervals\ndo not contain zero, mirroring the ﬁnding from the\noriginal study that general attitudes about public\norder mediated the eﬀect of the media frame. As shown in Section 3.4, we can relax the no-\ninteraction assumption (Assumption 2) that is im-\nplicit in the LSEM of Baron and Kenny (1986). The ﬁrst and second rows of the table present esti-\nmates from the parametric and nonparametric anal-\nysis without this assumption.",
    "content_hash": "1fa9f491dac7751b1dc1d9fb95403f6d68464e4ca9fe0f7ea92859833f61a6be",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "958b5f2a-0de9-429c-8490-db46855cecd8",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "These results show\nthat the estimated ACME under the free speech\ncondition [ˆδ(0)] is larger than the eﬀect under the\npublic order condition [ˆδ(1)] for both the paramet-\nric and nonparametric estimators. In fact, the 95%\nconﬁdence interval for the nonparametric estimate\nof ¯δ(1) includes zero. However, we fail to reject the\nnull hypothesis of ¯δ(0) = ¯δ(1) under the parametric\nanalysis, with a p-value of 0.238. Based on this ﬁnding, the no-interaction assump-\ntion could be regarded as appropriate. The last two\nrows in Table 3 contain the analysis based on the\nparametric estimator under this assumption. As ex-\npected, the estimated ACME is between the previ-\nous two estimates, and the 95% conﬁdence interval\ndoes not contain zero. Finally, the estimated aver-\nage total eﬀect is identical to that without Assump-\ntion 2. This makes sense since the no-interaction as-\nsumption only restricts the way the treatment eﬀect\nis transmitted to the outcome and thus does not af-\nfect the estimate of the overall treatment eﬀect. 6.2 Sensitivity Analysis\nThe estimates in Section 6.1 are identiﬁed if the\nsequential ignorability assumption holds. However,\nsince the original researchers randomized news sto-\nries but subjects’ attitudes were merely observed, it\nis unlikely this assumption holds.",
    "content_hash": "26d12777e7c4460c8bc5aedb473f5c1ec6c9545da3d3497bd6f3f4fa86ec66ee",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "781f6de3-349d-45be-832d-f5cd929bf305",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "The solid line represents the esti-\nmated ACME for the attitude mediator for diﬀering values of\nthe sensitivity parameter ρ, which is deﬁned in equation (24). The gray region represents the 95% conﬁdence interval based\non the Delta method. The horizontal dashed line is drawn at\nthe point estimate of ¯δ under Assumption 1.",
    "content_hash": "603c8457940f439973c448379f6bcdba90cc3214d3741c913a18887a32b8ecc5",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1f6aad00-6124-40c7-ba14-d2cf2f7d7d23",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "As we discussed\nin Section 2, one particular concern is that sub-\njects’ pre-existing ideology aﬀects both their atti-\ntudes toward public order issues and their tolerance\nfor the Klan within each treatment condition. Thus,\nwe next ask how sensitive these estimates are to vi-\nolations of this assumption using the methods pro-\nposed in Section 5. We consider political ideology to\nbe a possible unobserved pre-treatment confounder. We also maintain Assumption 2. Figure 1 presents the results for the sensitivity\nanalysis based on the residual correlation. We plot\nthe estimated ACME of the attitude mediator against\ndiﬀering values of the sensitivity parameter ρ, which\nis equal to the correlation between the two error\nterms of equations (27) and (28) for each. The anal-\nysis indicates that the original conclusion about the\ndirection of the ACME under Assumption 1 (repre-\nsented by the dashed horizontal line) would be main-\ntained unless ρ is less than −0.68. This implies that\nthe conclusion is plausible given even fairly large\ndepartures from the ignorability of the mediator. This result holds even after we take into account the\nsampling variability, as the conﬁdence interval cov-\ners the value of zero only when −0.79 < ρ < −0.49. Thus, the original ﬁnding about the negative ACME\nis relatively robust to the violation of equation (5)\nof Assumption 1 under the LSEM. Fig. 1. Sensitivity analysis for the media framing experi-\nment. The ﬁgure presents the results of the sensitivity analy-\nsis described in Section 5.",
    "content_hash": "11d6f640508e58060d2896ebf62d86eedd66acfc4c7777bc07113a8834ae4259",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5d1d58be-53f5-4da7-8d6d-da67aa62527e",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Indeed,\nthe formal deﬁnition of the concept of causal me-\ndiation had to await the later works by epidemiolo-\ngists and statisticians (Robins and Greenland, 1992;\nPearl, 2001; Robins, 2003). The progress made on\nthe identiﬁcation of causal mediation eﬀects by these\nauthors has led to the recent development of alter-\nnative and more general estimation strategies (e.g.,\nImai, Keele and Tingley, 2009; VanderWeele, 2009). In this paper we show that under a set of assump-\ntions this popular product of coeﬃcients estima-\ntor can be given a causal interpretation. Thus, over\ntwenty years later, the work of Baron and Kenny\nhas come full circle. Despite its natural appeal to applied scientists,\nstatisticians often ﬁnd the concept of causal medi-\nation mysterious (e.g., Rubin, 2004). Part of this\nskepticism seems to stem from the concept’s inher-\nent dependence on background scientiﬁc theory;\nwhether a variable qualiﬁes as a mediator in a given\nempirical study relies crucially on the investigator’s\nbelief in the theory being considered. For example,\nin the social science application introduced in Sec-\ntion 2, the original authors test whether the eﬀect of\na media framing on citizens’ opinion about the Klan",
    "content_hash": "57541268da60dc22423e55c2fb11f57dec010a3ce76098fe609b6cb2a4be7746",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "80351518-6f77-4343-8082-8c3d71e92340",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "The lower left quadrant of each plot\nin the ﬁgure represents the case where the product\nof the coeﬃcients for the unobserved confounder is\nnegative, while the upper right quadrant represents\nthe case where the product is positive. For example, this product will be positive if the\nunobserved pre-treatment confounder represents sub-\njects’ political ideology, since conservatism is likely\nto be positively correlated with both public order\nimportance and tolerance for the Klan. Under this\nscenario, the original conclusion about the direc-\ntion of the ACME is perfectly robust to the viola-\ntion of sequential ignorability, because the estimated\nACME is always negative in the upper right quad-\nrant of each plot. On the other hand, the result is\nless robust to the existence of an unobserved con-\nfounder that has opposite eﬀects on the mediator\nand outcome. However, even for this alternative sit-\nuation, the ACME is still guaranteed to be nega-\ntive as long as the unobserved confounder explains\nless than 27.7% of the variance in the mediator or\noutcome that is left unexplained by the treatment\nalone, no matter how large the corresponding por-\ntion of the variance in the other variable may be. Similarly, the direction of the original estimate is\nmaintained if the unobserved confounder explains\nless than 26.7% (14.7%) of the original variance in\nthe mediator (outcome), regardless of the degree of\nconfounding for the outcome (mediator). 7. CONCLUDING REMARKS\nIn this paper we study identiﬁcation, inference\nand sensitivity analysis for causal mediation eﬀects.",
    "content_hash": "ea22ae9bbcd47d5cd2b04bc0e328282ef1498d70c2acd2008b29eb44c189c182",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "56779401-5063-493b-951c-badb6baa0031",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Causal mediation analysis is routinely conducted\nin various disciplines, and our paper contributes to\nthis fast-growing methodological literature in sev-\neral ways. First, we provide a new identiﬁcation con-\ndition for the ACME, which is relatively easy to in-\nterpret in substantive terms and also weaker than\nexisting results in some situations. Second, we prove\nthat the estimates based on the standard LSEM\ncan be given valid causal interpretations under our\nproposed framework. This provides a basis for for-\nmally analyzing the validity of empirical studies us-\ning the LSEM framework. Third, we propose simple\nnonparametric estimation strategies for the ACME. This allows researchers to avoid the stronger func-\ntional form assumptions required in the standard\nLSEM. Finally, we oﬀer a parametric sensitivity anal-\nysis that can be easily used by applied researchers in\norder to assess the sensitivity of estimates to the vi-\nolation of this assumption. We view sensitivity anal-\nysis as an essential part of causal mediation analy-\nsis because the assumptions required for identifying\ncausal mediation eﬀects are unveriﬁable and often\nare not justiﬁed in applied settings. At this point, it is worth brieﬂy considering the\nprogression of mediation research from its roots in\nthe empirical psychology literature to the present. In\ntheir seminal paper, Baron and Kenny (1986) sup-\nplied applied researchers with a simple method for\nmediation analysis. This method has quickly gained\nwidespread acceptance in a number of applied ﬁelds. While psychologists extended this LSEM framework\nin a number of ways, little attention was paid to\nthe conditions under which their popular estima-\ntor can be given a causal interpretation.",
    "content_hash": "41076ae48b5cfd48ec0b9b08156bed770814b25fea86a2dd6922c0fc49236114",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "505ce790-9be9-4976-bfe4-5d4dbe3067f1",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "(25)\nNow, for any t,t′, we have\nE(Yi(t,Mi(t′))|Xi = x)\n=\nZ\nE(Yi(t,m)|Mi(t′) = m,Xi = x)\ndFMi(t′)|Xi=x(m)\n=\nZ\nE(Yi(t,m)|Mi(t′) = m,Ti = t′,Xi = x)\ndFMi(t′)|Xi=x(m)\n=\nZ\nE(Yi(t,m)|Ti = t′,Xi = x)\ndFMi(t′)|Xi=x(m)\n=\nZ\nE(Yi(t,m)|Ti = t,Xi = x)\ndFMi(t′)|Ti=t′,Xi=x(m)\n=\nZ\nE(Yi(t,m)|Mi(t) = m,Ti = t,Xi = x)\ndFMi(t′)|Ti=t′,Xi=x(m)\n=\nZ\nE(Yi|Mi = m,Ti = t,Xi = x)\ndFMi(t′)|Ti=t′,Xi=x(m)\n=\nZ\nE(Yi|Mi = m,Ti = t,Xi = x)\n(26)\ndFMi|Ti=t′,Xi=x(m),\nwhere the second equality follows from equation (25),\nequation (5) is used to establish the third and ﬁfth\nequalities, equation (4) is used to establish the fourth\nand last equalities, and the sixth equality follows\nfrom the fact that Mi = Mi(Ti) and Yi = Yi(Ti,Mi(Ti)).",
    "content_hash": "020b737873bc3739b0aa4587cc3585b53cb295c465101e0b2c181bc7ae5dbf9b",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "34c096be-b5eb-4078-8223-6fb09f0846a1",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "For example, the media framing study we\nanalyzed in this paper included another measure-\nment (on a separate group randomly split from the\nstudy sample) which was purported to test an alter-\nnative causal pathway. The formal treatment of this\nissue will be a major topic of future research. Third,\nimplications of measurement error in the mediator\nvariable have yet to be analyzed. This represents an-\nother important research topic, as mismeasured me-\ndiators are quite common, particularly in psycholog-\nical studies. Fourth, an important limitation of our\nframework is that it does not allow the presence of a\npost-treatment variable that confounds the relation-\nship between mediator and outcome. As discussed\nin Section 3.3, some of the previous results avoid\nthis problem by making additional identiﬁcation as-\nsumptions (e.g., Robins, 2003). The exploration of\nalternative solutions is also left for future research. Finally, it is important to develop new experimen-\ntal designs that help identify causal mediation ef-\nfects with weaker assumptions. Imai, Tingley and\nYamamoto (2009) present some new ideas on the\nexperimental identiﬁcation of causal mechanisms. APPENDIX A: PROOF OF THEOREM 1\nFirst, note that equation (4) in Assumption 1 im-\nplies\nYi(t′,m) ⊥Ti|Mi(t) = m′,\nXi = x.",
    "content_hash": "1a093022d7a18d61959fbb66a86534c7d0ddd066a8d4dd639c7d70579185622c",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "82c56c52-3e3d-4140-9c34-0f715cebc1fc",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "CAUSAL MEDIATION ANALYSIS\n17\nrally is mediated by a change in attitudes about gen-\neral issues. Such a setup might make no sense to an-\nother political psychologist who hypothesizes that\nthe change in citizens’ opinion about the Klan rally\nprompts shifts in their attitudes about more gen-\neral underlying issues. The H1N1 ﬂu virus example\nmentioned in Section 3.1 also highlights the same\nfundamental point. Thus, causal mediation analysis\ncan be uncomfortably far from a completely data-\noriented approach to scientiﬁc investigations. It is,\nhowever, precisely this aspect of causal mediation\nanalysis that makes it appealing to those who resist\nstandard statistical analyses that focus on estimat-\ning treatment eﬀects, an approach which has been\nsomewhat pejoratively labeled as a “black-box” view\nof causality (e.g., Skrabanek, 1994; Deaton, 2009). It may be the case that causal mediation analysis\nhas the potential to signiﬁcantly broaden the scope\nof statistical analysis of causation and build a bridge\nbetween scientists and statisticians. There are a number of possible future generaliza-\ntions of the proposed methods. First, the sensitiv-\nity analysis can potentially be extended to various\nnonlinear regression models. Some of this has been\ndone by Imai, Keele and Tingley (2009). Second,\nan important generalization would be to allow mul-\ntiple mediators in the identiﬁcation analysis. This\nwill be particularly valuable since in many applica-\ntions researchers aim to test competing hypotheses\nabout alternative causal mechanisms via mediation\nanalysis.",
    "content_hash": "ae1fe085aede42a55deb09ecf696965bbc481a2119edff6613c5a970ae1d0648",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b0853247-e95e-4e83-9d73-2873486c0577",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Finally, equation (26) implies\nE(Yi(t,Mi(t′)))\n=\nZ Z\nE(Yi|Mi = m,Ti = t,Xi = x)\ndFMi|Ti=t′,Xi=x(m)dFXi(x).",
    "content_hash": "54888ef8165ec8f42f0aaa338cb230c6611f6224bae1f033e95269992b8f7329",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b639f419-7063-4620-8196-630a6f55fa77",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "In the case of bi-\nnary mediator and outcome, we can derive the fol-\nlowing sharp bounds using the result of (2009):\nmax\n\n\n\n−P001 −P011\n−P000 −P001 −P100\n−P011 −P010 −P110\n\n\n\n(29)\n≤¯δ(1) ≤min\n\n\n\nP101 + P111\nP000 + P100 + P101\nP010 + P110 + P111\n\n\n,\nmax\n\n\n\n−P100 −P110\n−P001 −P100 −P101\n−P110 −P011 −P111\n\n\n\n(30)\n≤¯δ(0) ≤min\n\n\n\nP000 + P010\nP010 + P011 + P111\nP000 + P001 + P101\n\n\n,",
    "content_hash": "96b8ccb6dc156fba61084036705b216b469fa0b4c10169faa0724fb32881f093",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "de15f02c-8361-4331-b936-f96d10a5fcfd",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Then,\nsubstituting σ3 = (˜ρσ1 −γσ2)/ρ into the above ex-\npression of σ2\n1 yields the following quadratic equa-\ntion: γ2 −2γ˜ρσ1/σ2 + σ2\n1(˜ρ2 −ρ2)/{σ2\n2(1 −ρ2)} =\n0. Solving this equation and using σ3 ≥0, we ob-\ntain the following desired expression: γ = σ1\nσ2 {˜ρ −\nρ\np\n(1 −˜ρ2)/(1 −ρ2)}. Thus, given a particular value\nof ρ, ¯δ(t) is identiﬁed. APPENDIX E: NONPARAMETRIC\nSENSITIVITY ANALYSIS\nWe consider a sensitivity analysis for the simple\nplug-in nonparametric estimator introduced in Sec-\ntion 4.2. Unfortunately, sensitivity analysis is not as\nstraightforward as the parametric settings. Here, we\nexamine the special case of binary mediator and out-\ncome where some progress can be made and leave\nthe development of sensitivity analysis in a more\ngeneral nonparametric case for future research. We begin by the nonparametric bounds on the\nACME without assuming equation (5) of the se-\nquential ignorability assumption.",
    "content_hash": "727e9e77ce158c5f5c048962b70474f03a7dbbe4fae42e3c6a4cf3daad97e4c3",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c3ba166e-165c-4906-816f-f67273d87be3",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "These mean in-\ndependence relationships (together with the law of\niterated expectations) imply\n0 = E(εi3(Ti,Mi(Ti))Mi)\n= E{εi3(Ti,Mi(Ti))(α2 + β2Ti + εi2(Ti))}\n= E{εi3(Ti,Mi(Ti))εi2(Ti))}. Thus, under Assumption 1, we have ρ = 0 ⇐⇒\nE{εi2(Ti)εi3(Ti,Mi(Ti))} = 0. APPENDIX D: PROOF OF THEOREM 4\nFirst, we write the LSEM in terms of equations (12)\nand (14). We omit possible pre-treatment confounders\nXi from the model for notational simplicity, although\nthe result below remains true even if such confounders\nare included. Since equation (4) implies E(εji|Ti) = 0\nfor j = 2,3, we can consistently estimate (α1,α2,β1,\nβ2), where α1 = α3 + α2γ and β1 = β3 + β2γ, as\nwell as (σ2\n1,σ2\n2, ˜ρ). Thus, given a particular value of\nρ, we have ˜ρσ1σ2 = γσ2\n2 + ρσ2σ3 and σ2\n1 = γ2σ2\n2 +\nσ2\n3 + 2γρσ2σ3. If ρ = 0, then γ = ˜ρσ1/σ2 provided\nthat σ2\n3 = σ2\n1(1 −˜ρ2) ≥0. Now, assume ρ ̸= 0.",
    "content_hash": "0f8fbef514aaf1a25af98e1e6ceff0b3afa3c9cb67207fedf6e79263877fc25b",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "50530a8d-1834-4bc9-b06d-2065fb9fcbe9",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Similarly, equation (5) implies εi3(t,m) ⊥Mi|Ti =\nt for all t and m, yielding E(εi3(Ti,Mi(Ti))|Ti =\nt,Mi = m) = E(εi3(t,m)|Ti = t) = E(εi3(t,m)) = 0\nfor any t and m where the second equality follows\nfrom equation (4). Thus, the parameters in equa-\ntions (12) and (13) are identiﬁed under Assump-\ntion 1. Finally, under Assumption 1 and the LSEM,\nwe can write E(Mi|Ti) = α2+β2Ti, and E(Yi|Mi,Ti) =\nα3 + β3Ti + γMi. Using these expressions and The-\norem 1, the ACME can be shown to equal β2γ. APPENDIX C: PROOF THAT ρ = 0 UNDER\nASSUMPTION 1\nFirst, as shown in Appendix B, Assumption 1 im-\nplies\nE(εi2(Ti)|Ti) = 0\nand\nE(εi3(Ti,Mi(Ti))|Ti,\nMi) = 0 where the (potential) error terms are de-\nﬁned in equations (27) and (28).",
    "content_hash": "b43736a16b0322d926696e33a1b6a7ffc137373c6a97fa8422b77cc70233bd13",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a8479134-f029-4a62-8813-9863078483c9",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "18\nK. IMAI, L. KEELE AND T. YAMAMOTO\nSubstituting this expression into the deﬁnition of\n¯δ(t) given by equations (1) and (2) yields the de-\nsired expression for the ACME. In addition, since\n¯τ = ¯ζ(t) + ¯δ(t′) for any t,t′ = 0,1 and t ̸= t′ under\nAssumption 1, the result for the average natural di-\nrect eﬀects is also immediate. APPENDIX B: PROOF OF THEOREM 2\nWe ﬁrst show that under Assumption 1 the model\nparameters in the LSEM are identiﬁed. Rewrite equa-\ntions (12) and (13) using the potential outcome no-\ntation as follows:\nMi(Ti) = α2 + β2Ti + εi2(Ti),\n(27)\nYi(Ti,Mi(Ti)) = α3 + β3Ti + γMi(Ti)\n(28)\n+ εi3(Ti,Mi(Ti)),\nwhere the following normalization is used: E(εi2(t)) =\nE(εi3(t,m)) = 0 for t = 0,1 and m ∈M. Then, equa-\ntion (4) of Assumption 1 implies εi2(t) ⊥Ti, yield-\ning E(εi2(Ti)|Ti = t) = E(εi2(t)) = 0 for any t = 0,1.",
    "content_hash": "eeaa7e80cbc6a7293ac1ccc2b191c252995a00d9a58c9a4c0a8025083c15cbdc",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e14b0234-e5d8-4f87-aa31-40ad3747039c",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "CAUSAL MEDIATION ANALYSIS\n19\nwhere Pymt ≡Pr(Yi = y,Mi = m|Ti = t) for all y,m,\nt ∈{0,1}. These bounds always contain zero, im-\nplying that the sign of the ACME is not identiﬁed\nwithout an additional assumption even in this spe-\ncial case. To construct a sensitivity analysis, we follow the\nstrategy of Imai and Yamamoto (2010) and ﬁrst ex-\npress the second assumption of sequential ignorabil-\nity using the potential outcomes notation as follows:\nPr(Yi(1,1) = y11,Yi(1,0) = y10,\nYi(0,1) = y01,Yi(0,0) = y00|Mi = 1,Ti = t′)\n= Pr(Yi(1,1) = y11,Yi(1,0) = y10,\n(31)\nYi(0,1) = y01,Yi(0,0) = y00|\nMi = 0,Ti = t′)\nfor all t′,ytm,∈{0,1}. The equality states that within\neach treatment group the mediator is assigned inde-\npendent of potential outcomes. We now consider the\nfollowing sensitivity parameter υ, which is the maxi-\nmum possible diﬀerence between the left- and right-\nhand side of equation (31). That is, υ represents the\nupper bound on the absolute diﬀerence in the pro-\nportion of any principal stratum that may exist be-\ntween those who take diﬀerent values of the media-\ntor given the same treatment status.",
    "content_hash": "c488ae677b3a50978697828ae80e4cbaa188d8fae8f603072bdc2acc4478ed61",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "46789b9f-41fb-401e-bd24-4719c0e0584c",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "For the pur-\npose of illustration, we dichotomize both the me-\ndiator and treatment variables using their sample\nmedians as cutpoints. Figure 3 shows the results of\nthis analysis. In each panel the solid curves represent\nthe sharp upper and lower bounds on the ACME\nfor diﬀerent values of the sensitivity parameter υ. The horizontal dashed lines represent the point es-\ntimates of ¯δ(1) (upper panel) and ¯δ(0) (lower panel)\nunder Assumption 1. This corresponds to the case\nwhere the sensitivity parameter is exactly equal to\nzero (i.e., υ = 0), so that equation (31) holds. The\nsharp bounds widen as we increase the value of υ,\nuntil they ﬂatten out and become equal to the no-\nassumption bounds given in equations (29) and (30). The results suggest that the point estimates of the\nACME are rather sensitive to the violation of the\nsequential ignorability assumption. For both ¯δ(1)\nand ¯δ(0), the upper bounds sharply increase as we\nincrease the value of υ and cross the zero line at\nsmall values of υ [0.019 for ¯δ(1) and 0.022 for ¯δ(0)]. This contrasts with the parametric sensitivity anal-\nyses reported in Section 6.2, where the estimates of\nthe ACME appeared quite robust to the violation\nof Assumption 1. Although the direct comparison",
    "content_hash": "102f241eab20b2a71a78c894596240ff83e99e1b570aac028b92e70c45987349",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e3cda9b0-6e47-4021-abf7-2f1f826b7db7",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Finally, note that the ACME can be written as\nthe following linear function of unknown parame-\nters πm1m0\ny11y10y01y00:\n¯δ(t) =\n1\nX\nm=0\n1\nX\ny1−t,m=0\n1\nX\ny1,1−m=0\n1\nX\ny0,1−m=0\n(34)\n1\nX\nm0=0\nπmm0\ny11y10y01y00 −\n1\nX\nm1=0\nπm1m\ny11y10y01y00\n! ,\nwhere one of the subscripts of π corresponding to\nytm is equal to 1. Then, given a ﬁxed value of sensi-\ntivity parameter υ, you can obtain the sharp bounds\non the ACME by numerically solving the linear opti-\nmization problem with the linear constraints implied\nby equations (32) and (33) as well as the following\nrelationship implied by the ignorability of the treat-\nment assignment:\nPymt =\n1\nX\ny1−t,m=0\n1\nX\nyt,1−m=0\n1\nX\ny1−t,1−m=0\n1\nX\nm1−t=0\nπm1m0\ny11y10y01y00\n(35)\nfor each y,m,t ∈{0,1}. In addition, we use the linear\nconstraint that all πm1m0\ny11y10y01y00 sum up to 1. We apply this framework to the media framing\nexample described in Sections 2 and 6.",
    "content_hash": "59f65bf22ce10b777e7850c7c5a9753fa92bfd28094bb59adedc0858899bf29d",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d4d0518f-3f7b-4116-b543-dc492c227069",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Thus, this pro-\nvides one way to parametrize the maximum degree\nto which the sequential ignorability can be violated. (Other, potentially more intuitive, parametrization\nare possible, but, as shown below, this parametriza-\ntion allows for easier computation of the bounds.)\nUsing the population proportion of each princi-\npal stratum, that is, πm1m0\ny11y10y01y00 ≡Pr(Yi(1,1) = y11,\nYi(1,0) = y10,Yi(0,1) = y01,Yi(0,0) = y00,Mi(1) =\nm1,Mi(0) = m0), we can write this diﬀerence as fol-\nlows:\nP1\nm0=0 π1m0\ny11y10y01y00\nP1\ny=0 Py11\n−\nP1\nm0=0 π0m0\ny11y10y01y00\nP1\ny=0 Py01\n(32)\n≤υ,\nP1\nm1=0 πm11\ny11y10y01y00\nP1\ny=0 Py10\n−\nP1\nm1=0 πm10\ny11y10y01y00\nP1\ny=0 Py00\n(33)\n≤υ,\nwhere υ is bounded between 0 and 1. Clearly, if and\nonly if υ = 0, the sequential ignorability assumption\nis satisﬁed.",
    "content_hash": "da117330eaf03691c7ee8bb6c3a22add187f3fee4e595c785daf56bcc56d8a81",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d285c442-4cc4-4606-8e4d-8e2ead7b3d7b",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "In contrast\nto the parametric sensitivity analysis reported in Section 6.2,\nthe estimates are shown to be rather sensitive to the violation\nof Assumption 1. VanderWeele and seminar participants at Columbia\nUniversity, Harvard University, New York Univer-\nsity, Notre Dame, University of North Carolina, Uni-\nversity of Colorado–Boulder, University of Pennsyl-\nvania and University of Wisconsin–Madison for use-\nful suggestions. The suggestions from the associate\neditor and anonymous referees signiﬁcantly improved\nthe presentation. Financial support from the Na-\ntional Science Foundation (SES-0918968) is acknowl-\nedged. REFERENCES\nAlbert, J. M. (2008). Mediation analysis via potential out-\ncomes models. Stat. Med. 27 1282–1304. MR2420158\nAngrist, J. D., Imbens, G. W. and Rubin, D. B. (1996). Identiﬁcation of causal eﬀects using instrumental variables\n(with discussion). J. Amer. Statist. Assoc. 91 444–455. Avin, C., Shpitser, I. and Pearl, J. (2005). Identiﬁability\nof path-speciﬁc eﬀects. In Proceedings of the International\nJoint Conference on Artiﬁcial Intelligence. Morgan Kauf-\nman, San Francisco, CA. MR2192340\nBaron, R. M. and Kenny, D. A. (1986). The moderator–\nmediator variable distinction in social psychological re-\nsearch: Conceptual, strategic, and statistical considera-\ntions.",
    "content_hash": "56e6b918aaed303de868bc091f91945156aa915b2836a44e7a0ab781000b72ea",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c9aff7fa-f7c2-4706-8e12-a28569630592",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "20\nK. IMAI, L. KEELE AND T. YAMAMOTO\nis diﬃcult because of diﬀerent parametrization and\nvariable coding, this stark diﬀerence illustrates the\npotential importance of parametric assumptions in\ncausal mediation analysis; a signiﬁcant part of iden-\ntiﬁcation power could in fact be attributed to such\nfunctional form assumptions as opposed to empirical\nevidence. ACKNOWLEDGMENTS\nA companion paper applying the proposed meth-\nods to various applied settings is available as Imai,\nKeele and Tingley (2009). The proposed methods\ncan be implemented via the easy-to-use software\nmediation (Imai et al. 2010), which is an R package\navailable at the Comprehensive R Archive Network\n(http://cran.r-project.org/web/packages/mediation). The replication data and code for this article are\navailable for download as Imai, Keele and Yamamoto\n(2010). We thank Brian Egleston, Adam Glynn, Guido\nImbens, Gary King, Dave McKinnon, Judea Pearl,\nMarc Ratkovic, Jas Sekhon, Dustin Tingley, Tyler\nFig. 3. Nonparametric sensitivity analysis for the media\nframing experiment. In each panel the solid curves show the\nsharp upper and lower bounds on the ACME as a function\nof the sensitivity parameter υ, which represents the degree of\nviolation of the sequential ignorability assumption. The hori-\nzontal dashed lines represent the point estimates of ¯δ(1) (upper\npanel) and ¯δ(0) (lower panel) under Assumption 1.",
    "content_hash": "32fecfe4111af7176b1d3d305690cdac79c98906e61aceaa108eb2fd1eb40a6e",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5f633683-e290-4091-bc28-85b20cd16285",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "B 69 199–215. MR2325272\nGlynn, A. N. (2010). The product and diﬀerence fallacies\nfor indirect eﬀects. Unpublished manuscript, Dept. Gov-\nernment, Harvard Univ. Goodman, L. A. (1960). On the exact variance of products. J. Amer. Statist. Assoc. 55 708–713. MR0117809\nGreen, D. P., Ha, S. E. and Bullock, J. G. (2010). Yes,\nbut what’s the mechanism? (don’t expect an easy answer). Journal of Personality and Social Psychology 98 550–558. Hafeman, D. M. and Schwartz, S. (2009). Opening the\nblack box: A motivation for the assessment of mediation. International Journal of Epidemiology 38 838–845.",
    "content_hash": "489e3659703c691eac07e5d20173f32b1fc40b8eb952b0c99d02af1b0236e14a",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "35f1e6e4-785f-4ff6-93a5-61e6188c4bfe",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "Imbens, G. W. (2003). Sensitivity to exogeneity assump-\ntions in program evaluation. American Economic Review\n93 126–132. Jo, B. (2008). Causal inference in randomized experiments\nwith mediational processes. Psychological Methods 13 314–\n336. Joffe, M. M., Small, D., Ten Have, T., Brunelli, S. and\nFeldman, H. I. (2008). Extended instrumental variables\nestimation for overall eﬀects. Int. J. Biostat. 4 Article 4. MR2399287\nJoffe, M. M., Small, D. and Hsu, C.-Y. (2007). Deﬁn-\ning and estimating intervention eﬀects for groups that\nwill develop an auxiliary outcome. Statist. Sci. 22 74–97. MR2408662\nJudd, C. M. and Kenny, D. A. (1981). Process analysis:\nEstimating mediation in treatment evaluations. Evaluation\nReview 5 602–619. Kraemer, H. C., Kiernan, M., Essex, M. and Kupfer,\nD. J. (2008). How and why criteria deﬁnig moderators\nand mediators diﬀer between the Baron & Kenny and\nMacArthur approaches. Health Psychology 27 S101–S108. Kraemer, H. C., Wilson, T., Fairburn, C. G. and Agras,\nW. S. (2002). Mediators and moderators of treatment ef-\nfects in randomized clinical trials.",
    "content_hash": "a550868a89660446dcd265daa3be5a54dfd0814f2ea29cb4cee84b192358a920",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d2fc2996-bec4-44c9-9afb-ea351c1b0c60",
    "source_id": "d7aff093-288c-43a3-944b-04a40c2ef2ba",
    "content": "CAUSAL MEDIATION ANALYSIS\n21\nHafeman, D. M. and VanderWeele, T. J. (2010). Alterna-\ntive assumptions for the identiﬁcation of direct and indirect\neﬀects. Epidemiology 21. To appear. Imai, K. and Yamamoto, T. (2010). Causal inference with\ndiﬀerential measurement error: Nonparametric identiﬁca-\ntion and sensitivity analysis. American Journal of Political\nScience 54 543–560. Imai, K., Keele, L. and Tingley, D. (2009). A general ap-\nproach to causal mediation analysis. Psychological Meth-\nods. To appear. Imai, K., Keele, L., Tingley, D. and Yamamoto, T. (2010). Causal mediation analysis using R. In Advances\nin Social Science Research Using R (H. D. Vinod, ed.). Lecture Notes in Statist. 196 129–154. Springer, New York. Imai, K., Keele, L. and Yamamoto, T. (2010). Repli-\ncation data for: Identiﬁcation, inference, and sensitiv-\nity analysis for causal mediation eﬀects. Available at\nhttp://hdl.handle.net/1902.1/14412. Imai, K., Tingley, D. and Yamamoto, T. (2009). Exper-\nimental designs for identifying causal mechanisms. Tech-\nnical report, Dept. Politics, Princeton Univ. Available at\nhttp://imai.princeton.edu/research/Design.html.",
    "content_hash": "f6b822d84fb0ba7af3cc8c9d3bde76897e0a7bc94f1893b7126bc4a6f1db8599",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "Kosuke Imai, Luke Keele and Teppei Yamamoto",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "89ae4a63-033c-434b-9d90-c175668027d1",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Exact p-values for network interference, 2015. Susan Athey, Raj Chetty, and Guido Imbens. Combining experimental and observational data:\ninternal and external validity. arXiv preprint, 2016a. Susan Athey, Raj Chetty, Guido Imbens, and Hyunseung Kang. Estimating treatment eﬀects\nusing multiple surrogates: The role of the surrogate score and the surrogate index, 2016b. Susan Athey, Guido Imbens, and Stefan Wager. Eﬃcient inference of average treatment eﬀects in\nhigh dimensions via approximate residual balancing. arXiv preprint arXiv:1604.07125, 2016c. [61]",
    "content_hash": "2bff89003322aeef0568426a576c074d02e1a488549c0cfef3e7c0198273ae7b",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "1f5caf2e-571c-4d7d-9650-cd88ca7b398a",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "American Economic Review, 93(-):113–132, 2003. Alberto Abadie and Guido W Imbens. Large sample properties of matching estimators for\naverage treatment eﬀects. Econometrica, 74(1):235–267, 2006. Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Synthetic control methods for com-\n[59]",
    "content_hash": "52d4c0d96cede6202553025833f121df12ccf28cc1b4ccf803beca3dee89c294",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "39875c24-7aff-4542-bd0a-5de959e7e1aa",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "settings (e.g., Manski [1993], Carrell et al. [2013]), workers in a labor market (Cr´epon et al. [2013]) or roommates in college (Sacerdote [2001]), or with general networks, where friends of\nfriends are not necessarily friends themselves (Christakis and Fowler [2007]). Sometimes it is\nmore reasonable to think of many disconnected networks, where distributional approximations\nrely on the number of networks getting large, versus a single connected network such as Facebook. It maybe reasonable in some cases to think of the links as undirected (symmetric), and in others\nas directed. These links can be binary, with links either present or not, or contain links of\ndiﬀerent strengths. This large set of scenarios has led to the literature becoming somewhat\nfractured and unwieldy. We will only touch on a subset of these problems in this review. 2.4.1\nModels for Peer Eﬀects\nBefore considering estimation strategies, it is useful to begin by considering models of the out-\ncomes in a setting with peer eﬀects. Such models have been proposed in the literature. A\nseminal paper in the econometric literature is Manski’s linear-in-means model (Manski [1993],\nBramoull´e and Fortin [2009], Goldsmith-Pinkham and Imbens [2013]). Manski’s original paper\nfocuses on the setting where the population is partioned into groups (e.g., classrooms), and peer\neﬀects are constant within the groups.",
    "content_hash": "67ae461c2996a73ad6c36e69d5a254e13e1fb57b75510038232afb0751050b3a",
    "location": null,
    "page_start": 1,
    "page_end": 21,
    "metadata": {
      "section": "Causal Eﬀects in Networks and Social Interactions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b11d5003-b3be-4c06-922a-aad280d13922",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Lawrence Wasserman. All of nonparametric statistics. Springer, 2007. Halbert White. A heteroskedasticity-consistent covariance matrix estimator and a direct test\nfor heteroskedasticity. Econometrica, 48(1):817–838, 1980. Richard Wyss, Allan Ellis, Alan Brookhart, Cynthia Girman, Michele Jonsson Funk, Robert\nLoCasale, and Til St urmer. The role ofprediction modeling in propensity score estimation: An\nevaluationof logistic regression, bcart, and the covariate-balancing propensity score. American\nJournal of Epidemiology, 180(6):645–655, 2014. Shu Yang, Guido Imbens, Zhanglin Cui, Douglas E. Faries, and Zbigniew Kadziola. Propen-\nsity score matching and subclassiﬁcation in observational studies with multi-level treatments. Biometrics, 0(0):–, 2016. Alwyn Young. Channelling ﬁsher: Randomization tests and the statistical insigniﬁcance of\nseemingly signiﬁcant experimental results. E, 0:0–0, 2015. Achim Zeileis, Torsten Hothorn, and Kurt Hornik. Model-based recursive partitioning. Journal\nof Computational and Graphical Statistics, 17(2):492–514, 2008. Jos´e R Zubizarreta. Stable weights that balance covariates for estimation with incomplete\noutcome data. Journal of the American Statistical Association, 110(511):910–922, 2015.",
    "content_hash": "e238e9f3ed0b3b67157e41e19f45c211d39a862cceba61f1368c7896507009f0",
    "location": null,
    "page_start": 1,
    "page_end": 75,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "caa63ba4-9049-4810-a7b0-4aea375b7a0c",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Journal of the American Statistical Assocation, 99, 2004. Guido Imbens. The role of the propensity score in estimating dose–response functions. Biometrika, 2000. Guido Imbens. Nonparametric estimation of average treatment eﬀects under exogeneity: A\nreview. Review of Economics and Statistics, 2004. Guido Imbens. Better late than nothing: Some comments on deaton (2009) and heckman and\nurzua (2009). Journal of Economic Literature, 2010. [68]",
    "content_hash": "21881227a88ee667ed33aef97e820ba32ecb8a4dcc08f28ced93f53fe13ff0c1",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "7d650ce4-3f9f-48b8-92d4-96c7ff491f64",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Keisuke Hirano, Guido Imbens, Geert Ridder, and Donald Rubin. Combining panels with\nattrition and refreshment samples. Econometrica, pages 1645–1659, 2001. P. Holland and S. Leinhardt. An exponential family of probability distributions for directed\ngraphs. Journal of the American Statistical Association, 76(373):33–50, 1981. Paul Holland. Statistics and causal inference. Journal of the American Statistical Association,\n81:945–970, 1986. V Joseph Hotz, Guido W Imbens, and Julie H Mortimer. Predicting the eﬃcacy of future\ntraining programs using past experiences at other locations. Journal of Econometrics, 125(1):\n241–270, 2005. Peter J Huber. The behavior of maximum likelihood estimates under nonstandard conditions. In Proceedings of the ﬁfth Berkeley symposium on mathematical statistics and probability,\nvolume 1, pages 221–233, 1967. Michael Hudgens and Elizabeth Halloran. Toward causal inference with interference. Journal\nof the American Statistical Association, pages 832–842, 2008. Kosuke Imai and Marc Ratkovic. Estimating treatment eﬀect heterogeneity in randomized\nprogram evaluation. The Annals of Applied Statistics, 7(1):443–470, 2013. Kosuke Imai and Marc Ratkovic. Covariate balancing propensity score. Journal of the Royal\nStatistical Society: Series B (Statistical Methodology), 76(1):243–263, 2014. Kosuke Imai and David Van Dyk. Causal inference with general treatment regimes: generalizing\nthe propensity score.",
    "content_hash": "55e8106c714e38bc2812cc0d48c2b4e5141884ff9e394a0b8dd83df922c236ed",
    "location": null,
    "page_start": 1,
    "page_end": 69,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "26098a2d-6f05-4ffd-b5b1-3d45e199f085",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Journal of\nEpidemiology and Community Health, 60(1):578–586, 2006. Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational\nand Graphical Statistics, 20(1), 2011. Keisuke Hirano and Guido Imbens. The propensity score with continuous treatments. Applied\nBayesian Modelling and Causal Inference from Missing Data Perspectives, 2004. [67]",
    "content_hash": "81154fa58cee5889288253ffa4f894677973da0c879548e6a1a14250fcd03edf",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d937bbee-9b4b-435f-a147-97d1769431a2",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Econometrica, pages 315–331, 1998. Jinyong Hahn, Petra Todd, and Wilbert Van der Klaauw. Identiﬁcation and estimation of\ntreatment eﬀects with a regression-discontinuity design. Econometrica, 69(1):201–209, 2001. [66]",
    "content_hash": "d6a0793dbbae7fc9bdf258d92a24cb2b58ef8105b707954dafcb19a18a400707",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e7f99b03-e1cb-4283-9489-5efe30792e5a",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Hugh A Chipman, Edward I George, and Robert E McCulloch. BART: Bayesian additive\nregression trees. The Annals of Applied Statistics, 4(1):266–298, 2010. Nicholas Christakis and James Fowler. The spread of obesity in a large social network over 32\nyears. The New England Journal of Medicine, (357):370–379, 2007. Nicholas A Christakis, James H Fowler, Guido W Imbens, and Karthik Kalyanaraman. An em-\npirical model for strategic network formation. Technical report, National Bureau of Economic\nResearch, 2010. Timothy Conley, Christian Hansen, and Peter Rossi. Plausibly exogenous. Review of Economics\nand Statistics, 94(1), 2012. Bruno Cr´epon, Esther Duﬂo, M. Gurgand, R. Rathelot, and P. Zamora. Do labor market policies\nhave displacement eﬀects? evidence from a clustered randomized experiment. Quarterly\nJournal of Economics, 128(2), 2013. Richard K Crump, V Joseph Hotz, Guido W Imbens, and Oscar A Mitnik. Dealing with limited\noverlap in estimation of average treatment eﬀects. Biometrika, page asn055, 2009. Angus Deaton. Instruments, randomization, and learning about development. Journal of\neconomic literature, 48(2):424–455, 2010. Rajeev H Dehejia and Sadek Wahba. Causal eﬀects in nonexperimental studies: Reevaluating\nthe evaluation of training programs. Journal of the American statistical Association, 94(448):\n1053–1062, 1999.",
    "content_hash": "e934c70f198dca52dab829909971d73db76c5b83f79228120d39fceeb7478ac5",
    "location": null,
    "page_start": 1,
    "page_end": 65,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "580be090-7923-453f-aab7-70a4822d11dc",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Susan Athey, Markus Mobius, and Jeno Pal. The eﬀect of aggregators on news consumption. working paper, 2016d. Abhijit Banerjee, Sylvain Chassang, and Erik Snowberg. Decision theoretic approaches to exper-\niment design and external validity. Technical report, National Bureau of Economic Research,\n2016. Colin B Begg and Denis HY Leung. On the use of surrogate end points in randomized trials. Journal of the Royal Statistical Society: Series A (Statistics in Society), 163(1):15–28, 2000. Paul A. Bekker. Alternative approximations to the distribution of instrumental variable esti-\nmators. Econometrica, 62(3):657–681, 1994. Alexandre Belloni, Victor Chernozhukov, Ivan Fern´andez-Val, and Christian Hansen. Program\nevaluation with high-dimensional data. Preprint, arXiv:1311.2645, 2013. Alexandre Belloni, Victor Chernozhukov, and Christian Hansen. Inference on treatment eﬀects\nafter selection among high-dimensional controls. The Review of Economic Studies, 81(2):\n608–650, 2014a. Alexandre Belloni, Victor Chernozhukov, and Christian Hansen. High-dimensional methods and\ninference on structural and treatment eﬀects. Journal of Economic Perspectives, 28(2):29–50,\n2014b. Marinho Bertanha and Guido Imbens. External validity in fuzzy regression discontinuity designs. 2015. Alina Beygelzimer and John Langford. The oﬀset tree for learning with partial labels.",
    "content_hash": "a06489b0f270763c1c46896bc081ae6250ec495b170cd206c6c63d8beca5a77b",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "1cde463e-d64b-4325-8fd2-7df315af7d72",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Graham [2008] focuses on a setting very similar to that of Manski’s linear-in-means model. He considers restrictions on the covariance matrix within peer groups implied by the model\nassuming homoskedasticity at the individual level. Bramoull´e and Fortin [2009] allows for a more\ngeneral network conﬁguration than Manski, and investigate the beneﬁts of such conﬁgurations\nfor identiﬁcation in the Manski-style linear-in-means model. Hudgens and Halloran [2008] start\ncloser to the Rubin Causal Model or potential outcome setup. Like Manski they focus on a\nsetting with a partitioned network. Following the treatment eﬀect literature they focus primarily\non the case with a binary treatment. Let Wi denote the treatment for individual i, and let Wi\ndenote the vector of treatments for the peer group for individual i. The starting point in the\nHudgens and Halloran [2008] set up is the potential outcome Yi(w), with restrictions placed on\nthe dependence of the potential outcomes on the full treatment vector w. Aronow and Samii\n[2013] allow for general networks and peer eﬀects, investigating the identifying power from\nrandomization. 2.4.2\nModels for Network Formation\nAnother part of the literature has focused on developing models for network formation. Such\nmodels are of interest in their own right, but they are also important for deriving asymptotic\napproximations based on large samples. Such approximations require the researcher to specify\nin what way the expanding sample would be similar to or diﬀerent from the current sample.",
    "content_hash": "a17868865708fa49b54032102d52fe4c9c0ab55183d38396865b0276c414243b",
    "location": null,
    "page_start": 1,
    "page_end": 22,
    "metadata": {
      "section": "Causal Eﬀects in Networks and Social Interactions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0eaf5a7d-6b48-4143-90d2-8a3a4ccdc7a7",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "[2009, 2015]. Stepping back from the details of the choice of norm for penalized regression, one might\nconsider why the penalty term is needed at all outside the case where there are more covariates\nthan observations. For smaller values of K, we can return to the question of what the goal\nis of the estimation procedure. Ordinary least squares is unbiased; it also minimizes the sum\nof squared residuals for a given sample of data. That is, it focuses on in-sample goodness-\nof-ﬁt. One can think of the term involving the penalty in (4.1) as taking into account the\n“over-ﬁtting” error, which corresponds to the expected diﬀerence between in-sample goodness\nof ﬁt and out-of-sample goodness of ﬁt. Once covariates are normalized, the magnitude of β\nis roughly proportional to the potential of the model to over-ﬁt. Although the gap between\nin-sample and out-of-sample ﬁt is by deﬁnition unobserved at the time the model is estimated,\nwhen λ is chosen by cross-validation, its value is chosen to balance in-sample and out-of-sample\nprediction in a way that minimizes mean-squared error on an independent dataset. Unlike many supervised machine learning methods, there is a large literature on the formal\n[47]",
    "content_hash": "99a07e32a535f325e413d4032d29717c858f43a882d2b2ea7600f4dc1aac0508",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "7e356558-4361-4be8-8af7-5053b29b83be",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "William R Shadish,\nThomas D Cook,\nand Donald T Campbell. Experimental and\nquasi-experimental designs for generalized causal inference. Houghton, Miﬄin and Company,\n2002. Christopher Skovron and Roc´ıo Titiunik. A practical guide to regression discontinuity designs\nin political science. American Journal of Political Science, 2015. Douglas Staiger and James H Stock. Instrumental variables regression with weak instruments. Econometrica, 65(3):557–586, 1997. Xiaogang Su, Chih-Ling Tsai, Hansheng Wang, David M Nickerson, and Bogong Li. Subgroup\nanalysis via recursive partitioning. The Journal of Machine Learning Research, 10:141–158,\n2009. Elie Tamer. Partial identiﬁcation in econometrics. Annual Review of Economics, 2(1):167–195,\n2010. D Thistlewaite and Donald Campbell. Regression-discontinuity analysis: An alternative to the\nex-post facto experiment. Journal of Educational Psychology, 51, 1960. Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal\nStatistical Society. Series B (Methodological), pages 267–288, 1996. Petra Todd and Kenneth I Wolpin. Using a social experiment to validate a dynamic behavioral\nmodel of child schooling and fertility: Assessing the impact of a school subsidy program in\nmexico. 2003. Wilbert Van Der Klaauw. Estimating the eﬀect of ﬁnancial aid oﬀers on college enrollment: A\nregression-discontinuity approach. International Economic Review, 43, 2002. Wilbert Van Der Klaauw.",
    "content_hash": "f7207f8f4d55307edbe6290b8565fb9832c055226cf4cd3571035a3b458e4520",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f4311a7c-372b-46dd-bc36-7fc01240bacc",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Guido Imbens. Instrumental variables: An econometricians perspective. Statistical Science,\n2014. Guido Imbens. Book review. Economic Journal, 2015a. Guido Imbens and Karthik Kalyanaraman. Optimal bandwidth choice for the regression dis-\ncontinuity estimator. Review of Economic Studies, 79(3), 2012. Guido Imbens and Thomas Lemieux. Regression discontinuity designs: A guide to practice. Journal of Econometrics, 142(2), 2008. Guido Imbens and Paul Rosenbaum. Randomization inference with an instrumental variable. Journal of the Royal Statistical Society, Series A, 168(1), 2005. Guido Imbens and Jeﬀrey Wooldridge. Recent developments in the econometrics of program\nevaluation. Journal of Economic Literature, 2009. Guido W Imbens. Sensitivity to exogeneity assumptions in program evaluation. The American\nEconomic Review, Papers and Proceedings, 93(2):126–132, 2003. Guido W Imbens. Matching methods in practice: Three examples. Journal of Human Resources,\n50(2):373–419, 2015b. Guido W Imbens and Joshua D Angrist. Identiﬁcation and estimation of local average treatment\neﬀects. Econometrica, 61, 1994. Guido W Imbens and Donald B Rubin. Causal Inference in Statistics, Social, and Biomedical\nSciences. Cambridge University Press, 2015. Guido W Imbens, Donald B Rubin, and Bruce I Sacerdote. Estimating the eﬀect of unearned\nincome on labor earnings, savings, and consumption: Evidence from a survey of lottery players.",
    "content_hash": "ab5c3fbdffebc80498dd189874035072e3a04c2de77d3c3542c2e6770e81bd5c",
    "location": null,
    "page_start": 1,
    "page_end": 70,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "396bc1ea-311c-4473-b644-63afc6953167",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "in the terminology of Section 3, for the results from a linear model. 2.3\nEstimating Average Treatment Eﬀects under Unconfoundedness\nin Settings with Multivalued Treatments\nMuch of the earlier econometric literature on treatment eﬀects focused on the case with binary\ntreatments. For a textbook discussion, see Imbens and Rubin [2015]. Here we discuss the results\nof the more recent multi-valued treatment eﬀect literature. In the binary treatment case, many\nmethods have been proposed for estimating the average treatment eﬀect. Here we focus on\ntwo of these methods, subclassiﬁcation with regression and and matching with regression, that\nhave been found to be eﬀective in the binary treatment case (Imbens and Rubin [2015]). We\ndiscuss how these can be extended to the multi-valued treatment setting without increasing the\ncomplexity of the estimators. In particular, the dimension reducing properties of a generalized\nversion of the propensity score can be maintained in the multi-valued treatment setting. 2.3.1\nSet Up\nTo set the stage, it is useful to start with the binary treatment case. The standard set up\npostulates the existence of two potential outcomes, Yi(0) and Yi(1). With the binary treatment\ndenoted by Wi ∈{0, 1}, the realized and observed outcome is\nY obs\ni\n= Yi(Wi) =\n\u001a Yi(0)\nif Wi = 0,\nYi(1)\nif Wi = 1. In addition to the treatment indicator and the outcome we may observe a set of pretreatment\nvariables denoted by Xi.",
    "content_hash": "a0f3ddc99b45383bb1a1500d3237c42774634eb0c4c1f09c729fa333f43aec5e",
    "location": null,
    "page_start": 1,
    "page_end": 17,
    "metadata": {
      "section": "in Settings with Multivalued Treatments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "43fe0e72-a7e3-46a7-8849-820fbb9360a5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "American Economic Review, pages 778–794, 2001. Matthew Jackson. Social and Economic Networks. Princeton University Press, 2010. Matthew Jackson and Asher Wolinsky. A strategic model of social and economic networks. Journal of Economic Theory, 71(1), 1996. [69]",
    "content_hash": "713837aae90d00e40c15127f38a2277b42fdc5334b3f4128c6bb3e6f9d8eab0f",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": null,
      "heading_level": null
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9ae3e44c-00b2-4f55-ac0c-19582959524b",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "of confounding variables have been controlled for. 4.2.1\nPropensity Score Methods\nOne strand of the literature has focused on estimators that directly involve the propensity score,\neither through weighting or matching. Such methods had been shown in the ﬁxed number of\ncovariates case to lead to semiparametrically eﬃcient estimators for the average treatment eﬀect,\ne.g., Hahn [1998], Hirano et al. [2001]. The speciﬁc implementations in those papers, relying on\nkernel or series estimation of the propensity score, would be unlikely to work in settings with\nmany covariates. In order to deal with many covariates, researchers have proposed estimating the propensity\nscore using random forests, boosting, or LASSO, and then use weights based on those esti-\nmates following the usual approaches from the existing literature (e.g., McCaﬀrey et al. [2004],\nWyss et al. [2014]). One concern with these methods is that even in settings with few covari-\nates the weighting and propensity matching methods have been found to be sensitive to the\nimplementation of the propensity score estimation. Minor changes in the speciﬁcation, e.g. using logit models versus probit models, can change the weights substantially for units with\npropensity score values close to zero or one, and thus lead to estimators that lack robustness. Although the modern nonparametric methods may improve the robustness somewhat compared\nto previous methods, the variability in the weights is not likely to improve with the presence of\nmany covariates.",
    "content_hash": "b3dd7a7fa7acdcd2fb787a971e1355b43af77039edcbef2eb410f13649379b27",
    "location": null,
    "page_start": 1,
    "page_end": 53,
    "metadata": {
      "section": "Machine Learning Methods for Average Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "029f7bbe-d388-4c98-98f6-aaf773bb0285",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "arXiv:1607.00699v1  [stat.ME]  3 Jul 2016\nThe State of Applied Econometrics - Causality and Policy\nEvaluation ∗\nSusan Athey†\nGuido W. Imbens‡\nJuly 2016\nAbstract\nIn this paper we discuss recent developments in econometrics that we view as important\nfor empirical researchers working on policy evaluation questions. We focus on three main\nareas, where in each case we highlight recommendations for applied work. First, we dis-\ncuss new research on identiﬁcation strategies in program evaluation, with particular focus\non synthetic control methods, regression discontinuity, external validity, and the causal\ninterpretation of regression methods. Second, we discuss various forms of supplementary\nanalyses to make the identiﬁcation strategies more credible. These include placebo anal-\nyses as well as sensitivity and robustness analyses. Third, we discuss recent advances in\nmachine learning methods for causal eﬀects. These advances include methods to adjust for\ndiﬀerences between treated and control units in high-dimensional settings, and methods\nfor identifying and estimating heterogenous treatment eﬀects.\nJEL Classiﬁcation: C14, C21, C52\nKeywords: Causality, Supplementary Analyses, Machine Learning, Treatment\nEﬀects, Placebo Analyses, Experiments\n∗We are grateful for comments .\n†Graduate School of Business, Stanford University, and NBER, athey@stanford.edu.\n‡Graduate School of Business, Stanford University, and NBER, imbens@stanford.edu.\n[1]",
    "content_hash": "a7dd43f2f34789fb97d77d5cd50338ebb19e58036e3740c47827004b7d980607",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1607.00699v1  [stat.ME]  3 Jul 2016",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "679c7280-c7f0-46e0-8cbf-ee8a6021ca69",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "each individual tree is not pruned, but typically is “fully grown” up to some minimum leaf size. By averaging distinct predictive trees, the discontinuities of regression trees are smoothed out,\nand each unit receives a fully personalized prediction. Although the details of the construction of random forests are complex and look quite dif-\nferent than standard econometric methods, [Wager and Athey, 2015] argue that random forests\nare closely related to other non-parameteric methods such as k-nearest-neighbor algorithms and\nkernel regression. The prediction for each point is a weighted average of nearby points, since\neach underlying regression tree makes a prediction based on a simple average of nearby points,\nequally weighted. The main conceptual diﬀerence between random forests and the simplest ver-\nsions of nearest neighbor and kernel algorithms is that there is a data-driven approach to select\nwhich covariates are important for determining what data points are “nearby” a given point. However, using the data to select the model also comes at a cost, in that the predictions of the\nrandom forest are asymptotically bias-dominated. Recently, Wager and Athey [2015] develop a modiﬁcation of the random forest where the\npredictions are asymptotically normal and centered around the true conditional expectation\nfunction, and also propose a consistent estimator for the asymptotic variance, so that conﬁdence\nintervals can be constructed. The most important deviation from the standard random forest is\nthat two subsamples are used to construct each regression tree, one to construct the partition of\nthe covariate space, and a second to estimate the sample mean in each leaf.",
    "content_hash": "468a30b8b37296a56d569a73b4ed7fd8ad646f2b83780f05b839e768d3e96d58",
    "location": null,
    "page_start": 1,
    "page_end": 51,
    "metadata": {
      "section": "Prediction Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ccaa7387-8d18-4e8c-99c8-503321db3338",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "They\nperform well in prediction contests; for example, in a recent economics paper (Glaeser et al. [2016]) on crowd-sourcing predictive algorithms for city governments through contests, the win-\nning algorithm was a random forest. One way to think about random forests is that they are are an example of “model averaging.”\nThe prediction of a random forest is constructed as the average of hundreds or thousands of\ndistinct regression trees. The regression trees diﬀer from one another for several reasons. First,\neach tree is constructed on a distinct training sample, where the samples are selected by either\nbootstrapping or subsampling. Second, at each potential split in constructing the tree, the\nalgorithm considers a random subset of covariates as potential variables for splitting. Finally,\n[49]",
    "content_hash": "b657d6410b13a592122eb227f2dbcb809a408423a087a5df1f33d2641ce6fe41",
    "location": null,
    "page_start": 1,
    "page_end": 50,
    "metadata": {
      "section": "Prediction Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "812a8b95-a1a1-4bf6-be25-82ce1d1cc67e",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "H. Djebbaria Bramoull´e, Y. and B. Fortin. Identiﬁcation of peer eﬀects through social networks. Journal of Econometrics, 150(1):41–55, 2009. Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen. Classiﬁcation and\nRegression Trees. CRC press, 1984. Christian Brinch, Magne Mogstad, and Matthew Wiswall. Beyond late with a discrete instru-\nment: Heterogeneity in the quantity-quality interaction in children. 2015. S. Calonico, Matias Cattaneo, and Rocio Titiunik. Robust nonparametric conﬁdence intervals\nfor regression-discontinuity designs. Econometrica, 82(6), 2014a. S. Calonico, Matias Cattaneo, and Rocio Titiunik. Robust data-driven inference in the\nregression-discontinuity design. Stata Journal, 2014b. David Card. The impact of the mariel boatlift on the miami labor market. Industrial and Labor\nRelation, 43(2):–, 1990. David Card, David Lee, Z Pei, and Andrea Weber. Inference on causal eﬀects in a generalized\nregression kink design. Econometrica, 83(6), 2015. Scott Carrell, Bruce Sacerdote, and James West. From natural variation to optimal policy? the\nimportance of endogenous peer group formation. Econometrica, 81(3), 2013. Mattias Cattaneo.",
    "content_hash": "8d654579799311a7f6f4c107fed4d4975249bf4ca892756f14de76457e292904",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "62e047f9-c813-4ab5-aeee-e7eb6d614456",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "will be tempted to search for heterogeneity, and may instead end up with spurious ﬁndings. This problem is more severe when there are many covariates. 4.3.1\nMultiple Hypothesis Testing\nOne approach to this problem is to exhaustively search for treatment eﬀect heterogeneity and\nthen correct for issues of multiple testing. By multiple testing, we mean the problem that arises\nwhen a researcher considers a large number of statistical hypotheses, but analyzes them as if\nonly one had been considered. This can lead to “false discovery,” since across many hypothesis\ntests, we expect some to be rejected even if the null hypothesis is true. To address this problem, List et al. [2016] propose to discretize each covariate, and then loop\nthrough the covariates, testing whether the treatment eﬀect is diﬀerent when the covariate is low\nversus high. Since the number of covariates may be large, standard approaches to correcting\nfor multiple testing may severely limit the power of a (corrected) test to ﬁnd heterogeneity. List et al. [2016] propose an approach based on bootstrapping that accounts for correlation\namong test statistics; this approach can provide substantial improvements over standard multiple\ntesting approaches when the covariates are highly correlated, since dividing the sample according\nto each of two highly correlated covariates results in substantially the same division of the data.",
    "content_hash": "095cc55cc3a588523c8c5a77326228d1de681c392c6f36150115441c8d61ce2d",
    "location": null,
    "page_start": 1,
    "page_end": 56,
    "metadata": {
      "section": "Heterogenous Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f2224171-51ca-4126-bed9-3747c82b13f7",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "has led to substantial improvements in our understanding of these methods. 2.2\nSynthetic Control Methods and Diﬀerence-In-Diﬀerences\nDiﬀerence-In-Diﬀerences (DID) methods have become an important tool for empirical researchers. In the basic setting there are two or more groups, at least one treated and one control, and we\nobserve (possibly diﬀerent) units from all groups in two or more time periods, some prior to\nthe treatment and some after the treatment. The diﬀerence between the treatment and control\ngroups post treatment is adjusted for the diﬀerence between the two groups prior to the treat-\nment. In the simple DID case these adjustments are linear: they take the form of estimating the\naverage treatment eﬀect as the diﬀerence in average outcomes post treatment minus the diﬀer-\nence in average outcomes pre treatment. Here we discuss two important recent developments,\nthe synthetic control approach and the nonlinear changes-in-changes method. 2.2.1\nSynthetic Control Methods\nArguably the most important innovation in the evalulation literature in the last ﬁfteen years is\nthe synthetic control approach developed by Abadie et al. [2010, 2014b] and Abadie and Gardeazabal\n[2003]. This method builds on diﬀerence-in-diﬀerences estimation, but uses arguably more at-\ntractive comparisons to get causal eﬀects. We discuss the basic Abadie et al. [2010] approach,\nand highlight alternative choices and restrictions that may be imposed to further improve the\nperformance of the methods relative to diﬀerence-in-diﬀerences estimation methods. We observe outcomes for a number of units, indexed by i = 0, . . . , N, for a number of\nperiods indexed by t = 1, . . . , T.",
    "content_hash": "fa7d872521ba9e47714fae51cf6e4295b0a9a7e089047e24a1bbef2648f03c13",
    "location": null,
    "page_start": 1,
    "page_end": 13,
    "metadata": {
      "section": "Synthetic Control Methods and Diﬀerence-In-Diﬀerences",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0c554d02-6fb5-4bfd-82a8-3edebd4eeb36",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "consider the classic diﬀerence-in-diﬀerences study by Card [1990]. Card is interested in the\neﬀect of the Mariel boatlift, which brought Cubans to Miami, on the Miami labor market, and\nspeciﬁcally on the wages of low-skilled workers. He compares the change in the outcome of\ninterest, for Miami, to the corresponding change in a control city. He considers various possible\ncontrol cities, including Houston, Petersburg, Atlanta. The synthetic control idea is to move away from using a single control unit or a simple\naverage of control units, and instead use a weighted average of the set of controls, with the\nweights chosen so that the weighted average is similar to the treated unit in terms of lagged\noutcomes and covariates. In other words, instead of choosing between Houston, Petersburg or\nAtlanta, or taking a simple average of outcomes in those cities, the synthetic control approach\nchooses weights λh, λp, and λa for Houston, Petersburg and Atlanta respectively, so that λh ·\nYht + λp · Ypt + λa · Yat is close to Ymt (for Miami) for the pre-treatment periods t = 1, . . . , T0,\nas well as for the other pretreatment variables (e.g., Peri and Yasenov [2015]). This is a very\nsimple, but very useful idea. Of course, if pre-boatlift wages are higher in Houston than in\nMiami, and higher in Miami than in Atlanta, it would make sense to compare Miami to the\naverage of Houston and Atlanta rather than to Houston or Atlanta.",
    "content_hash": "97cc1c5c60ac796a9e0aa782f9c4a7d2bfe96ebe2fd3e2bf544becb50958511e",
    "location": null,
    "page_start": 1,
    "page_end": 14,
    "metadata": {
      "section": "Synthetic Control Methods and Diﬀerence-In-Diﬀerences",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9734d830-5a6a-439e-83d3-fa02120073c3",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "subject to the restrictions that the λi are non-negative and summ up to one. Doudchenko and Imbens [2016] point out that one can view the question of estimating the\nweights in the Abadie-Diamond-Hainmueller synthetic control method diﬀerently. Starting with\nthe case without covariates and only lagged outcomes, one can consider the regression function\nY0t =\nN\nX\ni=1\nλi · Yit + εt,\nwith T0 units and N regressors. The absence of the covariates is rarely important, as the ﬁt\ntypically is driven by matching up the lagged outcomes rather than matching the covariates. Estimating this regression by least squares is typically not possible because the number of\nregressors N (the number of control units) is often larger than, or the same order of magnitude\nas, the number of observations (the number of time periods T0). We therefore need to regularize\nthe estimates in some fashion or another. There are a couple of natural ways to do this. Abadie et al. [2010] impose the restriction that the weights λi are non-negative and add up\nto one. That often leads to a unique set of weights. However, there are alternative ways to\nregularize the estimates. In fact, both the restrictions that Abadie et al. [2010] impose may\nhurt performance of the model. If the unit is on the extreme end of the distribution of units,\nallowing for weights that sum up to a number diﬀerent from one, or allowing for negative weights\nmay improve the ﬁt.",
    "content_hash": "3e46de5f12b452529050b0a7c2688e9eb3a4dcd4b41af180c37f051833a399ab",
    "location": null,
    "page_start": 1,
    "page_end": 15,
    "metadata": {
      "section": "Synthetic Control Methods and Diﬀerence-In-Diﬀerences",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "8f7369c6-5f83-46f1-9a5d-1bb0b8dc4865",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "1\nIntroduction\nThis article synthesizes recent developments in econometrics that may be useful for researchers\ninterested in estimating the eﬀect of policies on outcomes. For example, what is the eﬀect of the\nminimum wage on employment? Does improving educational outcomes for some students spill\nover onto other students? Can we credibly estimate the eﬀect of labor market interventions with\nobservational studies? Who beneﬁts from job training programs? We focus on the case where\nthe policies of interest had been implemented for at least some units in an available dataset,\nand the outcome of interest is also observed in that dataset. We do not consider here questions\nabout outcomes that cannot be directly measured in a given dataset, such as consumer welfare\nor worker well-being, and we do not consider questions about policies that have never been\nimplemented. The latter type of question is considered in a branch of applied work referred to\nas “structural” analysis; the type of analysis considered in this review is sometimes referred to\nas “reduced-form,” or “design-based,” or “causal” methods. The gold standard for drawing inferences about the eﬀect of a policy is the randomized\ncontrolled experiment; with data from a randomized experiment, by construction those units\nwho were exposed to the policy are the same, in expectation, as those who were not, and it\nbecomes relatively straightforward to draw inferences about the causal eﬀect of a policy. The\ndiﬀerence between the sample average outcome for treated units and control units is an unbiased\nestimate of the average causal eﬀect. Although digitization has lowered the costs of conducting\nrandomized experiments in many settings, it remains the case that many policies are expensive to\ntest experimentally.",
    "content_hash": "af7959ac226c2a6ed2613d4feebfabe6045a6d836fb2a12b6c3221e5e4aca68b",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "Introduction",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "168d3476-c79d-42d8-8f35-fdec30c5141d",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In other cases, large-scale experiments may not be politically feasible. For\nexample, it would be challenging to randomly allocate the level of minimum wages to diﬀerent\nstates or metropolitan areas in the United States. Despite the lack of such randomized controlled\nexperiments, policy makers still need to make decisions about the minimum wage. A large share\nof the empirical work in economics about policy questions relies on observational data–that is,\ndata where policies were determined in a way other than random assignment. But drawing\ninferences about the causal eﬀect of a policy from observational data is quite challenging. To understand the challenges, consider the example of the minimum wage. It might be the\ncase that states with higher costs of living, as well as more price-insensitive consumers, select\nhigher levels of the minimum wage. Such states might also see employers pass on higher wage\n[1]",
    "content_hash": "f622b9718cd5789da35154b61f0613330cc8d1fefe311ed15aaa03edb97ece48",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "Introduction",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "653c5990-bb5c-45b8-bb7e-d1c3d20d215f",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "costs to consumers without losing much business. In contrast, states with lower cost of living\nand more price-sensitive consumers might choose a lower level of the minimum wage. A naive\nanalysis of the eﬀect of a higher minimum wage on employment might compare the average\nemployment level of states with a high minimum wage to that of states with a low minimum\nwage. This diﬀerence is not a credible estimate of the causal eﬀect of a higher minimum wage:\nit is not a good estimate of the change in employment that would occur if the low-wage state\nraised its minimum wage. The naive estimate would confuse correlation with causality. In\ncontrast, if the minimum wages had been assigned randomly, the average diﬀerence between\nlow-minimum-wage states and high-minimum-wage states would have a causal interpretation. Most of the attention in the econometrics literature on reduced-form policy evaluation focuses\non issues surrounding separating correlation from causality in observational studies, that is,\nwith non-experimental data. There are several distinct strategies for estimating causal eﬀects\nwith observational data. These strategies are often referred to as “identiﬁcation strategies,” or\n“empirical strategies” (Angrist and Krueger [2000]) because they are strategies for “identifying”\nthe causal eﬀect. We say that a causal eﬀect is “identiﬁed” if it can be learned when the data\nset is suﬃciently large. Issues of identiﬁcation are distinct from issues that arise because of\nlimited data. In Section 2, we review recent developments corresponding to several diﬀerent\nidentiﬁcation strategies.",
    "content_hash": "f4461fa152d93e1cbb39a78b5f57bdb22751b415e3a178a5a135ddaeae11d3c8",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "Introduction",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "4210927c-7eb4-4c7c-bfbd-02d665fc75c5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "An example of an identiﬁcation strategy is one based on “regression\ndiscontinuity.” This type of strategy can be used in a setting when allocation to a treatment\nis based on a “forcing” variable, such as location, time, or birthdate being above or below a\nthreshold. For example, a birthdate cutoﬀmay be used for school entrance or for the decision\nof whether a child can legally drop out of school in a given academic year; and there may be\ngeographic boundaries for assigning students to schools or patients to hospitals. The identifying\nassumption is that there is no discrete change in the characteristics of individuals who fall on\none side or the other of the threshold for treatment assignment. Under that assumption, the\nrelationship between outcomes and the forcing variable can be modeled, and deviations from the\npredicted relationship at the treatment assignment boundary can be attributed to the treatment. Section 2 also considers other strategies such as synthetic control methods, methods designed\nfor networks settings, and methods that combine experimental and observational data. In Section 3 we discuss what we refer to in general as supplementary analyses. By supple-\nmentary analyses we mean analyses where the focus is on providing support for the identiﬁcation\n[2]",
    "content_hash": "888c0e23c81d26199ff3debdec86a591341ca26b04df22d288b67f96a1389cc2",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "Introduction",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "68d090af-598c-4378-9880-859060f59a88",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "strategy underlying the primary analyses, on establishing that the modeling decisions are ade-\nquate to capture the critical features of the identiﬁcation strategy, or on establishing robustness\nof estimates to modeling assumptions. Thus the results of the supplementary analyses are in-\ntended to convince the reader of the credibility of the primary analyses. Although these analyses\noften involve statistical tests, the focus is not on goodness of ﬁt measures. Supplementary anal-\nyses can take on a variety of forms, and we discuss some of the most interesting ones that have\nbeen proposed thus far. In our view these supplementary analyses will be of growing impor-\ntance for empirical researchers. In this review, our goal is to organize these analyses, which may\nappear to be applied unsystematically in the empirical literature, or may have not received a\nlot of formal attention in the econometrics literature. In Section 4 we discuss brieﬂy new developments coming from what is referred to as the\nmachine learning literature. Recently there has been much interesting work combining these\npredictive methods with causal analyses, and this is the part of the literature that we put\nspecial emphasis on in our discussion. We show how machine learning methods can be used to\ndeal with datasets with many covariates, and how they can be used to enable the researcher to\nbuild more ﬂexible models. Because many common identiﬁcation strategies rest on assumptions\nsuch as the ability of the researcher to observe and control for confounding variables (e.g.",
    "content_hash": "80064a1d0c3444dbf41096bd80e92e1a30cec871215d04ea956abc9eb3b87162",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Introduction",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "65e711bd-bc68-4b81-9b44-dd8eab6690e6",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "the\nfactors that aﬀect treatment assignment as well as outcomes), or to ﬂexibly model the factors\nthat aﬀect outcomes in the absence of the treatment, machine learning methods hold great\npromise in terms of improving the credibility of policy evaluation, and they can also be used to\napproach supplementary analyses more systematically. As the title indicates, this review is limited to methods relevant for policy analysis, that is,\nmethods for causal eﬀects. Because there is another review in this issue focusing on structural\nmethods, as well as one on theoretical econometrics, we largely refrain from discussing those ar-\neas, focusing more narrowly on what is sometimes referred to as reduced-form methods, although\nwe prefer the terms causal or design-based methods, with an emphasis on recommendations for\napplied work. The choices for topics within this area is based on our reading of recent research,\nincluding ongoing work, and we point out areas where we feel there are interesting open research\nquestions. This is of course a subjective perspective. [3]",
    "content_hash": "3e852eaad7340fdb60229e08ae431cc998975c158c9dd7c4b61a5b430368588f",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Introduction",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "888e4660-a3b0-412e-9b7f-716e0396ecc3",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "2\nNew Developments in Program Evaluation\nThe econometric literature on estimating causal eﬀects has been a very active one for over three\ndecades now. Since the early 1990s the potential outcome, or Neyman-Rubin Causal Model,\napproach to these problems has gained substantial acceptance as a framework for analyzing\ncausal problems. (We should note, however, that there is a complementary approach based on\ngraphical models (e.g., Pearl [2000]) that is widely used in other disciplines, though less so in\neconomics.) In the potential outcome approach, there is for each unit i, and each level of the\ntreatment w, a potential outcome Yi(w), that describes the level of the outcome under treatment\nlevel w for that unit. In this perspective, causal eﬀects are comparisons of pairs of potential\noutcomes for the same unit, e.g., the diﬀerence Yi(w′) −Yi(w). Because a given unit can only\nreceive one level of the treatment, say Wi, and only the corresponding level of the outcome,\nY obs\ni\n= Yi(Wi) can be observed, we can never directly observe the causal eﬀects, which is what\nHolland [1986] calls the “fundamental problem of causal inference.” Estimates of causal eﬀects\nare ultimately based on comparisons of diﬀerent units with diﬀerent levels of the treatment. A large part of the causal or treatment eﬀect literature has focused on estimating average\ntreatment eﬀects in a binary treatment setting under the unconfoundedness assumption (e.g.,\nRosenbaum and Rubin [1983a]),\nWi ⊥⊥\n\u0010\nYi(0), Yi(1)\n\u0011 \f\f\f Xi.",
    "content_hash": "3678ec7ed0cbde014203ca06ef3fc3e90bd65f50740c3ae48753ae6319f82689",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "New Developments in Program Evaluation",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "963b585c-594d-42ff-9895-c86e605c3912",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Under this assumption, associational or correlational relations such as E[Y obs\ni\n|Wi = 1, Xi =\nx] −E[Y obs\ni\n|Wi = 0, Xi = x] can be given a causal interpretation as the average treatment\neﬀect E[Yi(1) −Yi(0)|Xi = x]. The literature on estimating average treatment eﬀects under\nunconfoundedness is by now a very mature literature, with a number of competing estima-\ntors and many applications. Some estimators use matching methods, some rely on weighting,\nand some involve the propensity score, the conditional probability of receiving the treatment\ngiven the covariates, e(x) = pr(Wi = 1|Xi = x). There are a number of recent reviews of\nthe general literature (Imbens [2004], Imbens and Rubin [2015], and for a diﬀerent perspective\nHeckman and Vytlacil [2007a,b]), and we do not review it in its entirety in this review. However,\none area with continuing developments concerns settings with many covariates, possibly more\nthan there are units. For this setting connections have been made with the machine learning and\n[4]",
    "content_hash": "21db6401be7387b46c3a2603087a8604cac77118c6c30c4bcd9013f82b0fd79d",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "New Developments in Program Evaluation",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c5b54760-1ea6-4df9-9d36-4c16d6c064c7",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "See Bekker [1994], Staiger and Stock [1997], Chamberlain and Imbens\n[2004] for speciﬁc contributions, and Andrews and Stock [2006] for a survey. We also do not\ndiscuss in detail bounds and partial identiﬁcation analyses. Since the work by Manski (e.g.,\nManski [1990]) these have received a lot of interest, with an excellent recent review in Tamer\n[2010]. 2.1\nRegression Discontinuity Designs\nA regression discontinuity design is a research design that exploits discontinuities in incentives\nto participate in a treatment to evaluate the eﬀect of these treatment. 2.1.1\nSet Up\nIn regression discontinuity designs, we are interested in the causal eﬀect of a binary treatment or\nprogram, denoted by Wi. The key feature of the design is the presence of an exogenous variable,\n[5]",
    "content_hash": "2754cd8b142fc857325c5cf8337f85da42e3e596b0b98d2e30aa82aa0d667df7",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "New Developments in Program Evaluation",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a89d64cf-2c0b-42e2-a714-ce3bac0fdd79",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "big data literatures. We review these new developments in Section 4.2. In the context of many\ncovariates there has also been interesting developments in estimating heterogenous treatment\neﬀects; we cover this literature in Section 4.3. We also discuss, in Section 2.3, settings with\nunconfoundedness and multiple levels for the treatment. Beyond settings with unconfoundedness we discuss issues related to a number of other iden-\ntiﬁcation strategies and settings. In Section 2.1, we discuss regression discontinuity designs. Next, we discuss synthetic control methods as developed in the Abadie et al. [2010], which we\nbelieve is one the most important development in program evaluation in the last decade. In\nSection 2.4 we discuss causal methods in network settings. In Section 2.5 we draw attention to\nsome recent work on the causal interpretation of regression methods. We also discuss external\nvalidity in Section 2.6, and ﬁnally, in Section 2.7 we discuss how randomized experiments can\nprovide leverage for observational studies. In this review we do not discuss the recent literature on instrumental variables. There\nare two major strands of that by now fairly mature literature. One focuses on heterogenous\ntreatment eﬀects, with a key development the notion of the local average treatment eﬀect\n(Imbens and Angrist [1994], Angrist et al. [1996]). This literature has recently been reviewed in\nImbens [2014]. There is also a separate literature on weak instruments, focusing on settings with\na possibly large number of instruments and weak correlation between the instruments and the\nendogenous regressor.",
    "content_hash": "700468c06f1711bc30e7072bfbb5dfda4988a26f55570270067cf50490f0dfeb",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "New Developments in Program Evaluation",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "091c2a58-143d-4595-a9f7-e0dd49f0af45",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In the FRD case, the interpretation of the estimand is the\naverage eﬀect for compliers at the threshold (i.e., individuals at the threshold whose treatment\nstatus would have changed had they been on the other side of the threshold) [Hahn et al., 2001]. 2.1.2\nEstimation and Inference\nIn the general FRD case, the estimand τ rd has four components, each of them the limit of the\nconditional expectation of a variable at a particular value of the forcing variable. We can think of\nthis, after splitting the sample by whether the value of the forcing variable exceeds the threshold\nor not, as estimating the conditional expectation at a boundary point. Researchers typically wish\nto use ﬂexible (e.g., semiparametric or nonparametric) methods for estimating these conditional\nexpectations. Because the target in each case is the conditional expectation at a boundary point,\nsimply diﬀerencing average outcomes close to the threshold on the right and on the left leads to\nan estimator with poor properties, as stressed by Porter [2003]. As an alternative Porter [2003]\nsuggested “local linear regression,” which involves estimating linear regressions of outcomes on\nthe forcing variable separately on the left and the right of the threshhold, weighting most heavily\n[6]",
    "content_hash": "c536b5bf7ef90f16a19b2fb838e36c3864cbccf4ef95bee984ac78dbab3d1f61",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "54e4fcb1-f74d-47ae-9f28-dd0949759061",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "the forcing variable, denoted by Xi, such that at a particular value of this forcing variable, the\nthreshold c, the probability of participating in the program or being exposed to the treatment\nchanges discontinuously:\nlim\nx↑c pr(Wi = 1|Xi = x) ̸= lim\nx↓c pr(Wi = 1|Xi = x). If the jump in the conditional probability is from zero to one, we have a sharp regression\ndiscontinuity (SRD) design; if the magnitude of the jump is less than one, we have a fuzzy\nregression discontinuity (FRD) design. The estimand is the discontinuity in the conditional\nexpectation of the outcome at the threshold, scaled by the discontinuity in the probability of\nreceiving the treatment:\nτ rd = limx↓c E[Yi|Xi = x] −limx↑c E[Yi|Xi = x]\nlimx↓c E[Wi|Xi = x] −limx↑c E[Wi|Xi = x]. In the SRD case the denominator is equal to one, and we just focus on the discontinuity of the\nconditional expectation of the outcome given the forcing variable at the threshold. In that case,\nunder the assumption that the individuals just to the right and just to the left of the threshold\nare comparable, the estimand has an interpretation as the average eﬀect of the treatment for\nindividuals close to the threshold.",
    "content_hash": "08e49dd7059c3513f5af2336d4bccee31a02c892aeaaceb4e93be33b0622590e",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b634678c-ac45-4f3b-b3a9-94f87114ee79",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "observations close to the threshold, and then taking the diﬀerence between the predicted values\nat the threshold. This local linear estimator has substantially better ﬁnite sample properties\nthan nonparametric methods that do not account for threshold eﬀects, and it has become the\nstandard. There are some suggestions that using local quadratic methods may work well given\nthe current technology for choosing bandwidths (e.g., Calonico et al. [2014a]). Some applications\nuse global high order polynomial approximations to the regression function, but there has been\nsome criticism of this practice. Gelman and Imbens [2014] argue that in practice it is diﬃcult\nto choose the order of the polynomials in a satisfactory way, and that conﬁdence intervals based\non such methods have poor properties. Given a local linear estimation method, a key issue is the choice of the bandwidth, that is,\nhow close observations need to be to the threshold. Conventional methods for choosing optimal\nbandwidths in nonparametric estimation, e.g., based on cross-validation, look for bandwidths\nthat are optimal for estimating the entire regression function, whereas here the interest is solely\nin the value of the regression function at a particular point. The current state of the literature\nsuggests choosing the bandwidth for the local linear regression using asymptotic expansions of\nthe estimators around small values for the bandwidth. See Imbens and Kalyanaraman [2012]\nand Cattaneo [2010] for further discussion. In some cases, the discontinuity involves multiple exogenous variables.",
    "content_hash": "28ad0ca6371c19607f94febbfd3cc9299fe9aaf49147fad69fa6f5b867c9bee7",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ee3f1b1e-455c-4e74-ad80-4f56627e9363",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "For example, in\nJacob and Lefgren [2004] and Matsudaira [2008], the focus is on the causal eﬀect of attending\nsummer school. The formal rule is that students who score below a threshold on either a language\nor a mathematics test are required to attend summer school. Although not all the students who\nare required to attend summer school do so (so that this a fuzzy regression discontinuity design),\nthe fact that the forcing variable is a known function of two observed exogenous variables makes\nit possible to estimate the eﬀect of summer school at diﬀerent margins. For example, one can\nestimate of the eﬀect of summer school for individuals who are required to attend summer school\nbecause of failure to pass the language test, and compare this with the estimate for those who\nare required because of failure to pass the mathematics test. Even more than the presence\nof other exogenous variables, the dependence of the threshold on multiple exogenous variables\nimproves the ability to detect and analyze heterogeneity in the causal eﬀects. [7]",
    "content_hash": "532ea040a3fc3808073b0ceba5b02c7f732286f45beed0189d1faac4b19103ab",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d070a162-795f-4663-a9ba-67ac390d16d8",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "[74]",
    "content_hash": "5fb6526ed57d24d42814dce7d26e69870bf77750eb91ecb11501b8d9a4781e51",
    "location": null,
    "page_start": 8,
    "page_end": 75,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "4f0d3ec5-0416-42a6-96e2-094e6aa27bfe",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "The solid line shows the set of values for the mathematics and reading scores that would\nrequire the students to participate in the summer program. The area enclosed by the dashed\nline contains all the students within the bandwidth from the threshold. We can partition the sample into students with relatively high reading scores (above the\nthreshold plus the Imbens-Kalyanaraman bandwidth), who could only be in the summer program\nbecause of their mathematics score, students with relatively high mathematics scores (above the\nthreshold plus the bandwidth) who could only be in the summer program because of their\nreading score, and students with low mathematics and reading scores (below the threshold plus\nthe bandwidth). Rows 2-4 present estimates for these separate subsamples. We ﬁnd that there\nis relatively little evidence of heterogeneity in the estimates of the program. The last row demonstrates the importance of using local linear rather than standard kernel\n(local constant) regressions. Using the same bandwidth, but using a weighted average of the\noutcomes rather than a weighted linear regression, leads to an estimate equal to -0.15: rather\n[8]",
    "content_hash": "e711b2fe64a76e73911908ceb47611fe19e461b5c32368c5397661a908693a6c",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "1b6a2d98-16a7-45df-bdce-ce51209ed1fd",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "2.1.3\nAn Illustration\nLet us illustrate the regression discontinuity design with data from Jacob and Lefgren [2004]. Jacob and Lefgren [2004] use administrative data from the Chicago Public Schools which in-\nstituted in 1996 an accountability policy that tied summer school attendance and promotional\ndecisions to performance on standardized tests. We use the data for 70,831 third graders in years\n1997-99. The rule was that individuals score below the threshold (2.75 in this case) on either\na reading or mathematics score before the summer were required to attend summer school. It\nshould be noted that the initial scores range from 0 to 6.8, with increments equal to 0.1. The\noutcome variable Y obs\ni\nis the math score after the summer school, normalized to have variance\none. Out of the 70,831 third graders, 15,846 score below the threshold on the mathematics test,\n26,833 scored below the threshold on the reading test, 12,779 score below the threshold on both\ntests, and 29,900 scored below the threshold on at least one test. Table 1 presents some of the results. The ﬁrst row presents an estimate of the eﬀect on\nthe mathematics test, using for the forcing variable the minimum of the initial mathematics\nscore and the initial reading score. We ﬁnd that the program has a substantial eﬀect. Figure\n1 shows which students contribute to this estimate. The ﬁgure shows a scatterplot of 1.5% of\nthe students, with uniform noise added to their actual scores to show the distribution more\nclearly.",
    "content_hash": "af57d56b86ae5d15de6ad7aabe3e07d03635d0b0a90807d02c1c80f011368ac9",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "98c5d434-bba4-4400-8686-8e04e7aae400",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "The forcing variable is\na lagged earnings variable that determines unemployment beneﬁts. A simple rule would be that\nunemployment beneﬁts are a ﬁxed percentage of last year’s earnings, up to a maximum. Thus\nthe unemployment beneﬁt, as a function of the forcing variable, is a continuous, piecewise linear\nfunction. Now suppose we are interested in the causal eﬀect of an increase in the unemployment\nbeneﬁts on the duration of unenmployment spells. Because the beneﬁts are a deterministic\nfunction of lagged earnings, direct comparisons of individuals with diﬀerent levels of beneﬁts are\n[9]",
    "content_hash": "326ff7845b0a03caf5e36417857fd403c8be4482b4f20c9d616a3a242c670261",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c3a70037-a8dc-492e-96ec-6bc6953bc982",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "than beneﬁting from the summer school, this estimate counterintuitively suggests that the sum-\nmer program hurts the students in terms of subsequent performance. This bias that leads to\nthese negative estimates is not surprising: the students who participate in the program are on\naverage worse in terms of prior performance than the students who do not participate in the\nprogram, even if we only use information for students close to the threshold. Table 1: Regression Discontinuity Designs: The Jacob-Lefgren Data\nOutcome\nSample\nEstimator\nEstimate\n(s.e.)\nIK Bandwidth\nMath\nAll\nLocal Linear\n0.18\n(0.02)\n0.57\nMath\nReading > 3.32\nLocal Linear\n0.15\n(0.02)\n0.57\nMath\nMath > 3.32\nLocal Linear\n0.17\n(0.03)\n0.57\nMath\nMath and Reading < 3.32\nLocal Linear\n0.19\n(0.02)\n0.57\nMath\nAll\nLocal Constant\n-0.15\n(0.02)\n0.57\n2.1.4\nRegression Kink Designs\nOne of the most interesting recent developments in the area of regression discontinuity designs is\nthe generalization to discontinuities in derivatives, rather than levels, of conditional expectations. The ﬁrst discussions of these regression kink designs are in Nielsen et al. [2010], Card et al. [2015], Dong [2014]. The basic idea is that at a threshold for the forcing variable, the slope of\nthe outcome function (as a function of the forcing variable) changes, and the goal is to estimate\nthis change in slope. To make this clearer, let us discuss the example in Card et al. [2015].",
    "content_hash": "e10d7579b5fad5768351d77e3f313276331f204769ab7be9b0edb108accae08f",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "659af79e-1af2-4105-949e-19588f0732dc",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "confounded by diﬀerences in lagged earnings. However, at the threshold, the relation between\nbeneﬁts and lagged earnings changes. Speciﬁcally, the derivative of the beneﬁts with respect\nto lagged earnings changes. If we are willing to assume that in the absence of the kink in the\nbeneﬁt system, the derivative of the expected duration would be smooth in lagged earnings,\nthen the change in the derivative of the expected duration with respect to lagged earnings is\ninformative about the relation between the expected duration and the beneﬁt schedule, similar\nto the identiﬁcation in a regular regression discontinuity design. To be more precise, suppose the beneﬁts as a function of lagged earnings satisfy\nBi = b(Xi),\nwith b(x) known and continuous, with a discontinuity in the ﬁrst derivative at x = c. Let b′(v)\ndenote the derivative, letting b′(c+) and b′(c−) denote the derivatives from the right and the\nleft at x = c. If the beneﬁt schedule is piecewise linear, we would have\nBi = β0 + β1−· (Xi −c),\nXi < c,\nBi = β0 + β1+ · (Xi −c),\nXi ≥c. This relationship is deterministic, making this a sharp regression kink design. Here, as before, c\nis the threshold. The forcing variable Xi is lagged earnings, Bi is the unemployment beneﬁt that\nan individual would receive.",
    "content_hash": "96b6c09b72a36a72cb6c43947ccd5880d58bfa0fa0f06ff67b10bcd76c521da4",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "6e0d6014-4d0f-411d-8be4-21797d79ccc5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "As a function of the beneﬁts b, the logarithm of the unemployment\nduration, denoted by Yi, is assumed to satisfy\nYi(b) = α + τ · ln(b) + εi. Let g(x) = E[Yi|Xi = x] be the conditional expectation of Yi given Xi = x, with derivative g′(x). The derivative is assumed to exist everywhere other than at x = c, where the limits from the\nright and the left exist. The idea is to characterize τ as\nτ = limx↓c g′(x) −limx↑c g′(x)\nlimx↓c b′(x) −limx↑c b′(x) . Card et al. [2015] propose estimating τ by ﬁrst estimating g(x) by local linear or local quadratic\nregression around the threshold. We then divide the diﬀerence in the estimated derivative from\nthe right and the left by the diﬀerence in the derivatives of b(x) from the right and the left at\nthe threshold. [10]",
    "content_hash": "0ef361ae903f644d88eb7626e5b976b93cb1d0cffc35716f694889e012fd72a7",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5a9be47f-324f-4d4a-bbc5-d1ae67aa28c0",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In some cases, the relationship between Bi and Xi is not deterministic, making it a fuzzy\nregression kink design. In the fuzzy version of the regression kink design, the conditional expec-\ntation of Bi given Xi is estimated using the same approach to get an estimate of the change in\nthe derivative at the threshold. 2.1.5\nSummary of Recommendations\nThere are some speciﬁc choices to be made in regression discontinuity analyses, and here we pro-\nvide our recommendations for these choices. We recommend using local linear or local quadratic\nmethods (see for details on the implementation Hahn et al. [2001], Porter [2003], Calonico et al. [2014a]) rather than global polynomial methods. Gelman and Imbens [2014] present a detailed\ndiscussion on the concerns with global polynomial methods. These local linear methods require\na bandwidth choice. We recommend the optimal bandwidth algorithms based on asymptotic ar-\nguments involving local expansions discssed in Imbens and Kalyanaraman [2012], Calonico et al. [2014a]. We also recommend carrying out supplementary analyses to assess the credibility of the\ndesign, and in particular to test for evidence of manipulation of the forcing variable. Most impor-\ntant here is the McCrary test for discontinuities in the density of the forcing variable (McCrary\n[2008]), as well as tests for discontinuities in average covariate values at the threshold. We\ndiscuss examples of these in the section on supplementary analyses (Section 3.4).",
    "content_hash": "00d00d638640d5c949cd04e5e63ed6a38f3080534b09e9ffc353ab14e2c9ea32",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "cdd61c9b-8d83-4178-b241-481143247847",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "We also rec-\nommend researchers to investigate external validity of the regression discontinuity estimates by\nassessing the credibility of extrapolations to other subpopulations (Bertanha and Imbens [2015],\nAngrist and Rokkanen [2015], Angrist and Fernandez-Val [2010], Dong and Lewbel [2015]). See\nSection 2.6 for more details. 2.1.6\nThe Literature\nRegression Discontinuity Designs have a long history, going back to work in psychology in the\nﬁfties by Thistlewaite and Campbell [1960], but the methods did not become part of the main-\nstream economics literature until the early 2000s (with Goldberger [1972, 2008] an exception). Early applications in economics include Black [1999] Angrist and Lavy [1999], Van Der Klaauw\n[2002], Lee [2008]. Recent reviews include Imbens and Lemieux [2008], Lee and Lemieux [2010],\nVan Der Klaauw [2008], Skovron and Titiunik [2015]. More recently there have been many ap-\nplications (e.g., Edenstein et al. [2016]) and a substantial amount of new theoretical work which\n[11]",
    "content_hash": "1ad5e31bfcd5c6ddfd380a5b397578aa1815364663ac66d9fd2552c92c307b93",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b0bef46f-6e1e-47b9-abb1-35d2c6fdd099",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "There is a single unit, say unit 0, who was exposed to the\ncontrol treatment during periods 1, . . . , T0 and who received the active treatment, starting in\nperiod T0 + 1. For ease of exposition let us focus on the case with T = T0 + 1 so there is only\na single post-treatment period. All other units are exposed to the control treatment for all\nperiods. The number of control units N can be as small as 1, and the number of periods T can\nbe as small as 2. We may also observe exogenous ﬁxed covariates for each of the units. The\nunits are often aggregates of individuals, say states, or cities, or countries. We are interested in\nthe causal eﬀect of the treatment for this unit, Y0T(1) −Y0T(0). The traditional DID approach would compare the change for the treated unit (unit 0) between\nperiods t and T, for some t < T, to the corresponding change for some other unit. For example,\n[12]",
    "content_hash": "3a3785542763645f0a211fd7766ec6947e5a3a961f2a03db3db3f6e189268aaf",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "Synthetic Control Methods and Diﬀerence-In-Diﬀerences",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9b0e9bbc-fe5e-48f8-a2f8-b8ea4d089384",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "The simplicity of the idea,\nand the obvious improvement over the standard methods, have made this a widely used method\nin the short period of time since its inception. The implementation of the synthetic control method requires a particular choice for estimat-\ning the weights. The original paper Abadie et al. [2010] restricts the weights to be non-negative\nand requires them to add up to one. Let K be the dimension of the covariates Xi, and let Ωbe\nan arbitrary positive deﬁnite K × K matrix. Then let λ(Ω) be the weights that solve\nλ(Ω) = arg min\nλ\nX0 −\nN\nX\ni=1\nλi · Xi\n!′\nΩ\nX0 −\nN\nX\ni=1\nλi · Xi\n! . Abadie et al. [2010] choose the weight matrix Ωthat minimizes\nT0\nX\nt=1\nY0t −\nN\nX\ni=1\nλi(Ω) · Yit\n!2\n. If the covariates Xi consist of the vector of lagged outcomes, this estimate amounts to minimizing\nT0\nX\nt=1\nY0t −\nN\nX\ni=1\nλi · Yit\n!2\n,\n[13]",
    "content_hash": "c6357f44b21f657626544960a9c747f85929b91d72290a788f6880f6baf4118a",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "Synthetic Control Methods and Diﬀerence-In-Diﬀerences",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "955f55ba-97b5-4b2f-b9fa-912d11b4a327",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "We can do so by using alternative regularization methods such as best\nsubset regression, or LASSO (see Section 4.1.1 for a description of LASSO) where we add a\npenalty proportional to the sum of the weights. Doudchenko and Imbens [2016] explore such\napproaches. 2.2.2\nNonlinear Diﬀerence-in-Diﬀerence Models\nA commonly noted concern with diﬀerence-in-diﬀerence methods is that functional form as-\nsumptions play an important role. For example, in the extreme case with only two groups and\ntwo periods, it is not clear whether the change over time should be modeled as the same for the\ntwo groups in terms of levels of outcomes, or in terms of percentage changes in outcomes. If the\ninitial period mean outcome is diﬀerent across the two groups, the two diﬀerent assumptions\ncan give diﬀerent answers in terms of both sign and magnitude. In general, a treatment might\naﬀect both the mean and the variance of outcomes, and the impact of the treatment might vary\nacross individuals. [14]",
    "content_hash": "41a69358eb6e4f1de30f6f0f34c1f1f89409d1fd58e8455213cb216f1882982f",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "Synthetic Control Methods and Diﬀerence-In-Diﬀerences",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "664310a8-21da-4e93-9b0f-98df28e06f8c",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "First is that the potential outcome in\nthe absence of the treatment can be written as a monotone function of an unobservable Ui and\ntime: Ygti(0) = h(Ui, t). Note that the function does not depend directly on g, so that diﬀerences\nacross groups are attributed to diﬀerences in the distribution of Ui across groups. Second, the\nfunction h is strictly increasing. This is not a restrictive assumption for a single time period,\nbut it is restrictive when we require it to hold over time, in conjunction with a third assumption,\nnamely that the distribution of Ui is stable over time within each group. The ﬁnal assumption\nis that the support of Ui for the treatment group is contained in the support of Ui for the control\ngroup. Under these assumptions, the distribution of YB2i(0) is identiﬁed, with the formula for\nthe distribution given as follows:\nPr(YB2i(0) ≤y) = FB1(F (−1)\nA1 (FA2(y))). Athey and Imbens [2006] show that an estimator based on the empirical distributions of the\nobserved outcomes is eﬃcient and discuss extensions to discrete outcome settings. The nonlinear diﬀerence-in-diﬀerence model can be used for two distinct purposes. First,\nthe distribution is of direct interest for policy, beyond the average treatment eﬀect. Further, a\nnumber of authors have used this approach as a robustness check, i.e., a supplementary analysis\n[15]",
    "content_hash": "438304a0ced8078646336c5afb7dc42dff263d726bcad64d2dd59b5f04c767aa",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "Synthetic Control Methods and Diﬀerence-In-Diﬀerences",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e5bc772a-f2dd-4c02-8c1f-7332c8fb894c",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "For the case where the data includes repeated cross-sections of individuals (that is, the data\ninclude individual observations about many units within each group in two diﬀerent time periods,\nbut the individuals can not be linked across time periods or may come from a distinct sample),\nAthey and Imbens [2006] propose a non-linear diﬀerence-in-diﬀerence model which they refer to\nas the changes-in-changes model that does not rely on functional form assumptions. Modifying the notation from the last subsection, we now imagine that there are two groups,\ng ∈{A, B}, where group A is the control group and group B is the treatment group. There are\nmany individuals in each group with potential outcomes denoted Ygti(w). We observe Ygti(0)\nfor a sample of units in both groups when t = 1, and for group A when t = 2; we observe\nYgti(1) for group B when t = 2. Denote the distribution of the observed outcomes in group g\nat time t by Fgt(·). We are interested in the distribution of treatment eﬀects for the treatment\ngroup in the second period, YB2i(1) −YB2i(0). Note that the distribution of YB2i(1) is directly\nestimable, while the counterfactual distribution of YB2i(0) is not, so the problem boils down to\nlearning the distribution of YB2i(0), based on the distributions of YB1i(0), YA2i(0), and YA1i(0). Several assumptions are required to accomplish this.",
    "content_hash": "4337253507cc4921c476543753d8fab8fd6b015d6fd306b2da034411245bc9bc",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "Synthetic Control Methods and Diﬀerence-In-Diﬀerences",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "abf3249f-1c4e-4695-9132-11498d3aabad",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Following Rosenbaum and Rubin [1983a] a large literature focused on\nestimation of the population average treatment eﬀect τ = E[Yi(1) −Yi(0)], under the uncon-\nfoundedness assumption that\nWi ⊥⊥\n\u0010\nYi(0), Yi(1)\n\u0011 \f\f\f Xi. In combination with overlap, requiring that the propensity score e(x) = pr(Wi = 1|Xi = x),\nis strictly between zero and one, the researcher can estimate the population average treatment\neﬀect by adjusting the diﬀerences in outcomes by treatment status for diﬀerences in the pre-\ntreatment variables:\nτ = E\nh\nE[Y obs\ni\n|Xi, Wi = 1] −E[Y obs\ni\n|Xi, Wi = 0]\ni\n. [16]",
    "content_hash": "cb3d3c62285ca073469705097e2de23faf22344edbd38bdc7f77e578517bd565",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "in Settings with Multivalued Treatments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "eba41938-20f8-4b4e-a0ee-1443abd099db",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "One natural set of estimands\nis the average treatment eﬀect if all units were switched from treatment level w1 to treatment\nlevel w2:\nτw1,w2 = E[Yi(w2) −Yi(w1)]. (2.1)\nTo estimate estimands corresponding to uniform policies such as (2.1), it is not suﬃcient to\ntake all the units with treatment levels w1 or w2 and use methods for estimating treatment\neﬀects in a binary setting. The latter strategy would lead to an estimate of τ ′\nw1,w2 = E[Yi(w2) −\nYi(w1)|Wi ∈{w1, w2}], which diﬀers in general from τw1,w2 because of the conditioning. Focusing\non unconditional average treatment eﬀects like τw1,w2 maintains transitivity: τw1,w2 + τw2,w3 =\nτw1,w3, which would not necessarily be the case for τ ′\nw1,w2. There are other possible estimands,\nbut we do not discuss alternatives here. A key ﬁrst step is to note that this estimand can be written as the diﬀerence in two marginal\nexpectations: τw1,w2 = E[Yi(w2)] −E[Yi(w1)], and that therefore identiﬁcation of marginal ex-\npectations such as E[Yi(w)] is suﬃcient for identiﬁcation of average treatment eﬀects. [17]",
    "content_hash": "c472fbee0ae9986cb3e7643247facc5a45112fe4d2b4a3ee376db361b74c3758",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "in Settings with Multivalued Treatments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "07e46fb7-5f50-4097-a3ec-8bb5d76240e2",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In that case many estimation strategies have been developed, relying on regression Hahn [1998],\nmatching Abadie and Imbens [2006], inverse propensity weighting Hirano et al. [2001], subclassi-\nﬁcation Rosenbaum and Rubin [1983a], as well as doubly robust methods Robins and Rotnitzky\n[1995], Robins et al. [1995]. Rosenbaum and Rubin [1983a] established a key result that under-\nlies a number of these estimation strategies: unconfoundedness implies that conditional on the\npropensity score, the assignment is independent of the potential outcomes:\nWi ⊥⊥\n\u0010\nYi(0), Yi(1)\n\u0011 \f\f\f e(Xi). In practice the most eﬀective estimation methods appear to be those that combine some covari-\nance adjustment through regression with a covariate balancing method such as subclassiﬁcation,\nmatching, or weighting based on the propensity score (Imbens and Rubin [2015]). Substantially less attention has been paid to the case where the treatment takes on multiple\nvalues. Exceptions include Imbens [2000], Lechner [2001], Imai and Van Dyk [2004], Cattaneo\n[2010], Hirano and Imbens [2004] and Yang et al. [2016]. Let W = {0, 1, . . . , T} be the set of\nvalues for the treatment. In the multivalued treatment case, one needs to be careful in deﬁning\nestimands, and the role of the propensity score is subtly diﬀerent.",
    "content_hash": "f39eca14e163ee573bdc4df158d5a1ff12f2d5d080f423d4f23cb327ac6d559c",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "in Settings with Multivalued Treatments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "01b4720e-81db-45e5-a174-e5797b8dbff6",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Now suppose that a generalized version of unconfoundedness holds:\nWi ⊥⊥\n\u0010\nYi(0), Yi(1), . . . , Yi(T)\n\u0011 \f\f\f Xi. There is no scalar function of the covariates that maintains this conditional independence re-\nlation. In fact, with T treatment levels one would need to condition on T −1 functions of the\ncovariates to make this conditional independence hold. However, unconfoundedness is in fact\nnot required to enjoy the beneﬁts of the dimension-reducing property of the propensity score. Imbens [2000] introduces a concept, called weak unconfoundedness, which requires only that the\nindicator for receiving a particular level of the treatment and the potential outcome for that\ntreatment level are conditionally independent:\n1Wi=w ⊥⊥Yi(w)\nXi,\nfor all w ∈{0, 1, . . . , T}. Imbens [2000] shows that weak uncnfoundedness implies similar dimension reduction proper-\nties as are available in the binary treatment case. He further introduced the concept of the\ngeneralized propensity score:\nr(w, x) = pr(Wi = w|Xi = x). Weak unconfoundedness implies that, for all w, it is suﬃcient for the removal of systematic\nbiases to condition on the generalized propensity score for that particular treatment level:\n1Wi=w ⊥⊥Yi(w)\nr(w, Xi). This in turn can be used to develop matching or propensity score subclassiﬁcation strategies as\noutlined in Yang et al. [2016].",
    "content_hash": "9aee66718c7c88b17c817dbbcb2c3365fc9051896b5a109d9208c5e59dd65563",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "in Settings with Multivalued Treatments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "19e5902e-30e8-4ca0-bd10-a4c60f42bb6a",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "This approach relies on the equality E[Yi(w)] = E\nh\nE[Y obs\ni\n|Xi, Wi =\nw]\ni\n. As shown in Yang et al. [2016], it follows from weak unconfoundedness that\nE[Yi(w)] = E\nh\nE[Y obs\ni\n|r(w, Xi), Wi = w]\ni\n. To estimate E[Yi(w)], divide the sample into J sublasses based on the value of r(w, Xi), with\nBi ∈{1, . . . , J} denoting the subclass. We estimate µj(w) = E[Yi(w)|Bi = j] as the average\nof the outcomes for units with Wi = w and Bi = j. Given those estimates, we estimate\nµ(w) = E[Yi(w)] as a weighted average of the ˆµj(w), with weights equal to the fraction of units\n[18]",
    "content_hash": "58eb3b3399d56fed67d447d02245caaa23d48f4f6a93fa3af435481da83b3639",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "in Settings with Multivalued Treatments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "72d47792-f548-4ff0-be24-389719cdb693",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In general, the questions in this literature focus on causal eﬀects in settings where units, often\nindividuals, interact in a way that makes the no-interference or sutva (Rosenbaum and Rubin\n[1983a], Imbens and Rubin [2015]) assumptions that are routinely made in the treatment eﬀect\nliterature implausible. Settings of interest include those where the possible interference is simply\na nuisance, and the interest continuous to be in causal eﬀects of treatments assigned to a\nparticular unit on the outcomes for that unit. There are also settings where the interest is in\nthe magnitude of the interactions, or peer eﬀects, that is, in the eﬀects of changing treatments\nfor one unit on the outcomes of other units. There are settings where the network (that is,\nthe set of links connecting the individuals) is ﬁxed exogenously, and some where the network\nitself is the result of a possibly complex set of choices by individuals, possibly dynamic and\npossibly aﬀected by treatments. There are settings where the population can be partitioned into\nsubpopulations with all units within a subpopulation connected, as, for example, in classroom\n[19]",
    "content_hash": "12f7413f84f53e8bf94ab5c46bdd9c566004fd18c6a84e40b31e0f5fca7679cb",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "Causal Eﬀects in Networks and Social Interactions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "6d8252b5-1dd6-4dc6-b4e7-3abc3ccf6e22",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "in subclass j. The idea is not to ﬁnd subsets of the covariate space where we can interpret the\ndiﬀerence in average outcomes by all treatment levels as estimates of causal eﬀects. Instead we\nﬁnd subsets where we can estimate the marginal average outcome for a particular treatment\nlevel as the conditional average for units with that treatment level, one treatment level at a\ntime. This opens up the way for using matching and other propensity score methods developed\nfor the case with binary treatments in settings with multivalued treatments, irrespective of the\nnumber of treatment levels. A separate literature has gone beyond the multi-valued treatment setting to look at dy-\nnamic treatment regimes. With few exceptions most of these studies appear in the biostatistical\nliterature: see Hern´an and Robins [2006] for a general discussion. 2.4\nCausal Eﬀects in Networks and Social Interactions\nAn important area that has seen much novel work in recent years is that on peer eﬀects and\ncausal eﬀects in networks. Compared to the literature on estimating average causal eﬀects\nunconfoundedness without interference, the literature has not focused on a single setting; rather,\nthere are many problems and settings with interesting questions. Here, we will discuss some\nof the settings and some of the progress that has been made. However, this review will be\nbrief, and incomplete, because this continues to be a very active area, with work ranging from\neconometrics (Manski [1993]) to economic theory (Jackson [2010]).",
    "content_hash": "79adcd91e12b374ad9cce6f427f420dc1aa709e2efed568d4adc9400f7a687f3",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "Causal Eﬀects in Networks and Social Interactions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "825814ee-8498-443f-8882-e11fffab589f",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "The basic model speciﬁcation is\nYi = β0 + βY · Y i + β′\nXXi + β′\nXXi + β′\nZZi + εi,\nwhere i indexes the individual. Here Yi is the outcome for individual i, Y i is the average outcome\nfor individuals in the peer group for individual i, Xi is a set of exogenous characteristics of\nindividual i, Xi is the average value of the characteristics in individual i’s peer group, and Zi\nare group characteristics that are constant for all individuals in the same peer group. Manski\nconsiders three types of peer eﬀects. Outcomes for individuals in the same group may be\ncorrelated because of a shared environment. These eﬀects are called correlated peer eﬀects, and\ncaptured by the coeﬃcient on Zi. Next are the exogenous peer eﬀects, captured by the coeﬃcient\non the group average Xi of the exogenous variables. The third type is the endogenous peer\neﬀect, captured by the coeﬃcient on the group average outcomes Y i. Manski concludes that\nidentiﬁcation of these eﬀects, even in the linear model setting, relies on very strong assumptions\nand is unrealistic in many settings. In subsequent empirical work, researchers have often ruled\nout some of these eﬀects in order to identify others. [20]",
    "content_hash": "b13a0a0060e64389d734a2859044b596fb7a42de5b37b3aad3e37087d4a61fbd",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "Causal Eﬀects in Networks and Social Interactions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "88f6b74b-0836-4c24-9385-e07615c58575",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "For\nexample, it would require the researcher to be speciﬁc in the way the additional units would be\nlinked to current units or other new units. There is a wide range of models considered, with some models relying more heavily on opti-\nmizing behavior of individuals, and others using more statistical models. See Goldsmith-Pinkham and Imbens\n[2013], Christakis et al. [2010], Mele [2013], Jackson [2010], Jackson and Wolinsky [1996] for\nsuch network models in economics, and Holland and Leinhardt [1981] for statistical models. Chandrasekhar and Jackson develops a model for network formation and develops a correspond-\ning central limit theorem in the presence of correlation induced by network links. Chandrasekhar\nsurveys the econometrics of network formation. [21]",
    "content_hash": "c7a3f8b267b1d73fbad73dcb4917a9a45dae767b8dc9784d5b7025efaa89d046",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "Causal Eﬀects in Networks and Social Interactions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "7d428159-860e-498a-ac54-506746b12dac",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "2.4.3\nExact Tests for Interactions\nOne challenge in testing hypotheses about peer eﬀects using methods based on standard asymp-\ntotic theory is that when individuals interact (e.g., in a network), it is not clear how interactions\namong individuals would change as the network grows. Such a theory would require a model\nfor network formation, as discussed in the last subsection. This motivates an approach that\nallows us to test hypotheses without invoking large sample properties of test statistics (such as\nasymptotic normality). Instead, the distributions of the test statistics are based on the random\nassignments of the treatment, that is, the properties of the tests are based on randomization in-\nference. In randomization inference, we approximate the distribution of the test statistic under\nthe null hypothesis by re-calculating the test statistic under a large number of alternative (hypo-\nthetical) treatment assignment vectors, where the alternative treatment assignment vectors are\ndrawn from the randomization distribution. For example, if units were independently assigned\nto treatment status with probability p, we re-draw hypothetical assignment vectors with each\nunit assigned to treatment with probability p. Of course, re-calculating the test statistic requires\nknowing the values of units’ outcomes. The randomization inference approach is easily applied\nif the null hypothesis of interest is “sharp”: that is, the null hypothesis speciﬁes what outcomes\nwould be under all possible treatment assignment vectors.",
    "content_hash": "148663ba3fb6463a19d892a7acaa82dff74c42ebb77cd0804dd38a65dcb60bf9",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "Causal Eﬀects in Networks and Social Interactions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "08c78a18-39cc-4777-8d93-16ac386cb0f2",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "If the null hypothesis is that the\ntreatment has no eﬀect on any units, this null is sharp: we can infer what outcomes would have\nbeen under alternative treatment assignment vectors, in in particular, outcomes would be the\nsame as the realized outcomes under the realized treatment vector. More generally, however, randomization inference for tests for peer eﬀects is more compli-\ncated than in settings without peer eﬀects because the null hypotheses are often not sharp. Aronow [2012], Athey et al. [2015] develop methods for calculating exact p-values for general\nnull hypotheses on causal eﬀects in a single connected network, allowing for peer eﬀects. The\nbasic case Aronow [2012], Athey et al. [2015] consider is that where the null hypothesis rules out\npeer eﬀects but allows for direct (own) eﬀects of a binary treatment assigned randomly at the\nindividual level. Given that direct eﬀects are not speciﬁed under the null, individual outcomes\nare not known under alternative treatment assignment vectors, and so the null is not sharp. To\naddress this problem, Athey et al. [2015] introduce the notion of an artiﬁcial experiment that\ndiﬀers from the actual experiment. In the artiﬁcial experiment, some units have their treatment\n[22]",
    "content_hash": "13a079d0f81bcc81101e71bc608f2fe767929aa40716efa98dcf9067d9252bec",
    "location": null,
    "page_start": 23,
    "page_end": 23,
    "metadata": {
      "section": "Causal Eﬀects in Networks and Social Interactions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "283d1fe8-be5c-4f52-bc2f-c3c3eebf5962",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In many models\npeer eﬀects are restricted so that all peers have equal eﬀects on an individual’s outcome. It\nmay be more realistic to allow eﬀects of highly connected individuals, or closer friends, to be be\ndiﬀerent from those of less connected or more distant friends. Such hypotheses can be tested in\nthis framework. 2.5\nRandomization Inference and Causal Regressions\nIn recent empirical work, data from randomized experiments are often analyzed using conven-\ntional regression methods. Some researchers have raised concerns with the regression approach in\nsmall samples (Freedman [2006, 2008], Young [2015], Athey and Imbens [2016], Imbens and Rubin\n[2015]), but generally such analyses are justiﬁed at least in large samples, even in settings with\nmany covariates (Bloniarz et al. [2016], Du et al. [2016]). There is an alternative approach to\nestimation and inference, however, that does not rely on large sample approximations, using\napproximations for the distribution of estimators induced by randomization. Such methods,\n[23]",
    "content_hash": "53240ddf402c9fde0f2d5d2d9dd4d9dec01dffd77c32d4a1466db0217f8d80a1",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "Randomization Inference and Causal Regressions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f4ba8244-8589-422c-9a21-7d37d73b76d2",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "assignments held ﬁxed, and we randomize over the remaining units. Thus, the randomization dis-\ntribution is replaced by a conditional randomization distribution, where treatment assignments\nof some units are re-randomized conditional on the assignment of other units. By focusing on\nthe conditional assignment given a subset of the overall space of assignments, and by focusing on\noutcomes for a subset of the units in the original experiment, they create an artiﬁcial experiment\nwhere the original null hypothesis that was not sharp in the original experiment is now sharp. To be speciﬁc, the artiﬁcial experiments starts by designating an arbitrary set of units to be\nfocal. The test statistics considered depend only on outcomes for these focal units. Given the\nfocal units, the set of assignments that, under the null hypothesis of interest, does not change\nthe outcomes for the focal units is derived. The exact distribution of the test statistic can then\nbe inferred for such test statistics under that conditional randomization distribution under the\nnull hypothesis considered. Athey et al. [2015] extend this idea to a large class of null hypotheses. This class includes\nhypotheses restricting higher order peer eﬀects (peer eﬀects from friends-of-friends) while allow-\ning for the presence of peer eﬀects from friends. It also includes hypotheses about the validity of\nsparsiﬁcation of a dense network, where the question concerns peer eﬀects of friends according\nto the pre-sparsiﬁed network while allowing for peer eﬀects of the sparsiﬁed network. Finally,\nthe class also includes null hypotheses concerning the exchangeability of peers.",
    "content_hash": "c77e514e4428889f97262c9512040905d08e2bf2373e53de762ed2a8c2095e56",
    "location": null,
    "page_start": 24,
    "page_end": 24,
    "metadata": {
      "section": "Causal Eﬀects in Networks and Social Interactions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "55e7dd58-70e3-4760-b7f8-63257c1493c5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "which go back to Fisher [1925, 1935], Neyman [1923/1990, 1935], clarify how the act of random-\nization allows for the testing for the presence of treatment eﬀects and the unbiased estimation of\naverage treatment eﬀects. Traditionally these methods have not been used much in economics. However, recently there has been some renewed interest in such methods. See for example\nImbens and Rosenbaum [2005], Young [2015], Athey and Imbens [2016]). In completely ran-\ndomized experiments these methods are often straightforward, although even there analyses\ninvolving covariates can be more complicated. However, the value of the randomization perspective extends well beyond the analysis of\nactual experiments. It can shed light on the interpretation of observational studies and the\ncomplications arising from ﬁnite population inference and clustering. Here we discuss some of\nthese issues and more generally provide an explicitly causal perspective on linear regression. Most textbook discussions of regression specify the regression function in terms of a dependent\nvariable, a number of explanatory variables, and an unobserved component, the latter often\nreferred to as the error term:\nYi = β0 +\nK\nX\nk=1\nβk · Xik + εi. Often the assumption is made that in the population the units are randomly sampled from,\nthe unobserved component εi is independent of, or uncorrelated with, the regressors Xik. The\nregression coeﬃcients are then estimated by least squares, with the uncertainty in the estimates\ninterpreted as sampling uncertainty induced by random sampling from the large population. This approach works well in many cases.",
    "content_hash": "4a99137da3451722282587cdc61fdbc5eedd32db97e6289b435cc55ef6687eeb",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "Randomization Inference and Causal Regressions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a37411c3-4f17-4a4b-b262-79c79e6d08ab",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In analyses using data from the public use surveys\nsuch as the Current Population Survey or the Panel Study of Income Dynamics it is natural\nto view the sample at hand as a random sample from a large population. In other cases this\nperspective is not so natural, with the sample not drawn from a well-deﬁned population. This\nincludes convenience samples, as well as settings where we observe all units in the population. In those cases it is helpful to take an explictly causal perspective. This perspective also clariﬁes\nhow the assumptions underlying identiﬁcation of causal eﬀects relate to the assumptions often\nmade in least squares approaches to estimation. Let us separate the covariates Xi into a subset of causal variables Wi and the remainder,\nviewed as ﬁxed characteristics of the units. For example, in a wage regression the causal variable\nmay be years of education and the characteristics may include sex, age, and parental background. [24]",
    "content_hash": "0561497d58ff7d03386201ca7418f96dd1e79eea31c5f087bc5539638578e173",
    "location": null,
    "page_start": 25,
    "page_end": 25,
    "metadata": {
      "section": "Randomization Inference and Causal Regressions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "168a02ed-690b-49e8-95d8-cb50a954f6cf",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "This extends very easily to the case where Wi is binary and completely randomly as-\nsigned but there are other regressors included in the regression function. As Lin [2013] and\nImbens and Rubin [2015] show there is no need for assumptions about the relation of those\nregressors to the outcome, as long as the cause Wi is randomly assigned. Abadie et al. [2014a]\nextend this to the case where the cause is multivalued, possibly continuous, and the charac-\nteristics Zi are allowed to be generally correlated with the cause Wi. Aronow and Samii [2013]\ndiscuss the interpretation of the regression estimates in a causal framework. Abadie et al. [2016]\ndiscuss extensions to settings with clustering where the need for clustering adjustments in stan-\ndard errors arises from the clustered assignment of the treatment rather than through clustered\nsampling. [25]",
    "content_hash": "0a79cfaa6604b03e325957811cc0aa79f79e5394b9849f29fb7231b9b0fa42b6",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "External Validity",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0753649a-0067-4e8c-8979-af7680ce97bf",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Using the potential outcomes perspective we can interpret Yi(w) as the outcome corresponding to\na level of the treatment w for unit or individual i. Now suppose that for all units i the function\nYi(·) is linear in in its argument, with a common slope coeﬃcient, but a variable intercept,\nYi(w) = Yi(0) + βW · w. Now write Yi(0), the outcome for unit i given treatment level 0 as\nYi(0) = β0 + β′\nZZi + εi,\nwhere β0 and βZ are the population best linear predictor coeﬃcients. This representation of\nYi(0) is purely deﬁnitional and does not require assumptions on the population. Then we can\nwrite the model as\nYi(w) = β0 + βW · w + β′\nZZi + εi,\nand the realized outcome as\nYi = β0 + βW · Wi + β′\nZZi + εi. Now we can investigate the properties of the least squares estimator ˆβW for βW, where the\ndistribution of ˆβW is generated by the assignment mechanism for the Wi. In the simple case\nwhere there are no characteristics Zi and the cause Wi is a binary indicator, the assumption\nthat the cause is completely randomly assigned leads to the conventional Eicker-Huber-White\nstandard errors (Eicker [1967], Huber [1967], White [1980]). Thus, in that case viewing the\nrandomness as arising from the assignment of the causes rather than as sampling uncertainty\nprovides a coherent way of interpreting the uncertainty.",
    "content_hash": "d5003b69ef8f420e416f0f0b98d5f51d0d6f9282a0e25304a2b28dc8beb7b20b",
    "location": null,
    "page_start": 26,
    "page_end": 26,
    "metadata": {
      "section": "Randomization Inference and Causal Regressions",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9f8c48a9-e6de-406d-b15e-f70a81d8a3f1",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "2.6\nExternal Validity\nOne concern that has been raised in many studies of causal eﬀects is that of external validity. Even if a causal study is done carefully, either in analysis or by design, so that the internal\nvalidity of such a study is high, there is often little guarantee that the causal eﬀects are valid\nfor populations or settings other than those studied. This concern has been raised particularly\nforcefully in experimental studies where the internal validity is guaranteed by design. See for\nexample the discussion in Deaton [2010], Imbens [2010] and Manski [2013]. Traditionally, there\nhas been much emphasis on internal validity in studies of causal eﬀects, with some arguing for\nthe primacy of internal validity. Some have argued that without internal validity, little can be\nlearned from a study (Shadish et al. [2002], Imbens [2015a]). Recently, however, Deaton [2010],\nManski [2013], Banerjee et al. [2016] have argued that external validity should receive more\nemphasis. Some recent work has taken concerns with external validity more seriously, proposing a\nvariety of approaches that directly allow researchers to assess the external validity of esti-\nmators for causal eﬀects. A leading example concerns settings with instrumental variables\nwith heterogenous treatment eﬀects (e.g., Angrist [2004], Angrist and Fernandez-Val [2010],\nDong and Lewbel [2015], Angrist and Rokkanen [2015], Bertanha and Imbens [2015], Kowalski\n[2015], Brinch et al. [2015]).",
    "content_hash": "dee8c249d2b4fb8b97be7a94804c95c088375fec9e38885666aaa0f68a22c603",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "External Validity",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c25f3699-613f-46fe-b3e0-ff1498f49d5d",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In the modern literature with heterogenous treatment eﬀects the\ninstrumental variables estimator is interpreted as an estimator of the local average treatment\neﬀect, the average eﬀect of the treatment for the compliers, that is, individuals whose treatment\nstatus is aﬀected by the instrument. In this setting, the focus has been on whether the instru-\nmental variables estimates are relevant for the entire sample, that is, have external validity, or\nonly have local validity for the complier subpopulation. In that context, Angrist [2004] suggests testing whether the diﬀerence in average outcomes\nfor always-takers and never-takers is equal to the average eﬀect for compliers. In this context,\na Hausman test [Hausman, 1978] for equality of the ordinary least squares estimate and an\ninstrumental variables estimate can be interpreted as testing whether the average treatment\neﬀect is equal to the local average treatment eﬀect; of course, the ordinary least squares es-\ntimate only has that interpretation if unconfoundedness holds. Bertanha and Imbens [2015]\nsuggest testing a combination of two equalities, ﬁrst that the average outcome for untreated\n[26]",
    "content_hash": "b63e7e69f33ab9d97c92e5ab0001507a796b15677c76e0085b396b4206d987cd",
    "location": null,
    "page_start": 27,
    "page_end": 27,
    "metadata": {
      "section": "External Validity",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "4a92615c-0437-4284-9e5e-cf8f47a23ed3",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "compliers is equal to the average outcome for never-takers, and second, that the average out-\ncome for treated compliers is equal to the average outcome for always-takers. This turns out\nto be equivalent to testing both the null hypothesis suggested by Angrist [2004] and the Haus-\nman null. Angrist and Fernandez-Val [2010] consider extrapolating local average treatment\neﬀects by exploiting the presence of other exogenous covariates. The key assumption in the\nAngrist and Fernandez-Val [2010] approach, “conditional eﬀect ignorability,” is that conditional\non these additional covariates the average eﬀect for compliers is identical to the average eﬀect\nfor never-takers and always-takers. In the context of regression discontinuity designs, and especially in the fuzzy regression\ndiscontinuity setting, the concerns about external validity are especially salient. In that set-\nting the estimates are in principle valid only for individuals with values of the forcing variable\nequal to, or close to, the threshold at which the probability of receipt of the treatment changes\ndiscontinuously. There have been a number of approaches to assess the plausibility of gener-\nalizing those local estimates to other parts of the population. The focus and the applicability\nof the various methods to assess external validity varies. Some of them apply to both sharp\nand fuzzy regression discontinuity designs, and some apply only to fuzzy designs. Some require\nthe presence of additional exogenous covariates, and others rely only on the presence of the\nforcing variable.",
    "content_hash": "afebf61dc6867818b228ad87da34c976a25003223b11cd2e458898df9e9f9047",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "External Validity",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e9f36ab4-540f-4ebb-8746-ed00d22923c5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Dong and Lewbel [2015] observe that in general, in regression discontinuity\ndesigns with a continuous forcing variable, one can estimate the magnitude of the discontinuity\nas well as the magnitude of the change in the ﬁrst derivative of the regression function, or even\nhigher order derivatives. Under assumptions about the smoothness of the two conditional mean\nfunctions, knowing the higher order derivatives allows one to extrapolate away from values of\nthe forcing variable close to the threshold. This method apply both in the sharp and in the\nfuzzy regression discontinuity design. It does not require the presence of additional covariates. In another approach, Angrist and Rokkanen [2015] do require the presence of additional ex-\nogenous covariates. They suggest testing whether whether conditional on these covariates, the\ncorrelation between the forcing variable and the outcome vanishes. This would imply that the\nassignment can be thought of as unconfounded conditional on the additional covariates. Thus\nit would allow for extrapolation away from the threshold. Like the Dong-Lewbel approach, the\nAngrist-Rokkanen methods apply both in the case of sharp and fuzzy regression discontinuity\ndesigns. Finally, Bertanha and Imbens [2015] propose an approach requiring a fuzzy regression\n[27]",
    "content_hash": "8138ab35557c7e7bbdda4cf52176262de7b9b3ad37f46c0bc85adce4beaeac08",
    "location": null,
    "page_start": 28,
    "page_end": 28,
    "metadata": {
      "section": "Leveraging Experiments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c9422c5e-8f54-4f34-88e8-21e093c2909a",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Instead an intermediate outcome\nwas observed. In a second study, both the intermediate outcome and the primary outcome were\nobserved. In both studies there may be additional pretreatment variables observed and possibly\nthe treatment indicator. These two examples do not exhaust the set of possible settings where researchers can leverage\nexperimental data more eﬀectively, and this is likely to be an area where more research is fruitful. [28]",
    "content_hash": "52e5c8b8b32383080c2e62518f07b8085838e10e2d2e23ca4a3c9037192d7230",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "Leveraging Experiments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "feefd134-63c5-4af1-8db3-29212d2c1be4",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "discontinuity design. They suggest testing for continuity of the conditional expectation of the\noutcome conditional on the treatment and the forcing variable, at the threshold, adjusted for\ndiﬀerences in the covariates. 2.7\nLeveraging Experiments\nRandomized experiments are the most credible design to learn about causal eﬀects. However,\nin practice there are often reasons that researchers cannot conduct randomized experiments to\nanswer the causal questions of interest. They may be expensive, or they may take too long to\ngive the researcher the answers that are needed now to make decisions, or there may be ethical\nobjections to experimentation. As a result, we often rely on a combination of experimental\nresults and observational studies to make inferences and decisions about a wide range of ques-\ntions. In those cases we wish to exploit the beneﬁts of the experimental results, in particular\nthe high degree of internal validity, in combination with the external validity and precision from\nlarge scale representative observational studies. At an abstract level, the observational data\nare used to estimate rich models that allow one to answer many questions, but the model is\nforced to accommodate the answers from the experimental data for the limited set of questions\nthe latter can address. Doing so will improve the answers from the observational data without\ncompromising their ability to answer more questions. Here we discuss two speciﬁc settings where experimental studies can be leveraged in combina-\ntion with observational studies to provide richer answers than either of the designs could provide\non their own. In both cases, the interest is in the average causal eﬀect of a binary treatment\non a primary outcome. However, in the experiment the primary outcome was not observed and\nso one cannot directly estimate the average eﬀect of interest.",
    "content_hash": "7f47ba7eed99b167677bd686b5f0319dcf12f48d884fe9ee13c1836ded52dcc3",
    "location": null,
    "page_start": 29,
    "page_end": 29,
    "metadata": {
      "section": "Leveraging Experiments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ba8f0a41-2b71-4133-abe6-70f567f447f5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "2.7.1\nSurrogate Variables\nIn the ﬁrst setting, studied in Athey et al. [2016b], in the second sample the treatment indicator\nis not observed. In this case researchers may wish to use the intermediate variable, denoted Si, as\na surrogate. Following Prentice [1989], Begg and Leung [2000], Frangakis and Rubin [2002], the\nkey condition for an intermediate variable to be a surrogate is that in the experimental sample,\nconditional the surrogate and observed covariates, the (primary) outcomes and the treatment\nare independent: Yi ⊥⊥Wi|(Si, Xi). There is a long history of attempts to use intermediate\nhealth measures in medical trials as surrogates (Prentice [1989]). The results are mixed, with\nthe condition often not satisﬁed in settings where it could be tested. However, many of these\nstudies use low-dimensional surrogates. In modern settings there is often a large number of\nintermediate variables recorded in administrative data bases that lie on or close to the causal\npath between the treatment and the primary outcome. In such cases it may be more plausible\nthat the full set of surrogate variables satisﬁes at least approximately the surrogacy condition. For example, suppose an internet company is considering a change to the user experience on\nthe company’s website. They are interested in the eﬀect of that change on the user’s engagement\nwith the website over a year long period. They carry out a randomized experiment over a\nmonth, where they measure details about the user’s engagement, including the number of visits,\nwebpages visited, and the length of time spent on the various webpages.",
    "content_hash": "43bc86f49322fd8dcec7674461194a90934912a348925cf979237f4b008441d6",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "Leveraging Experiments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "8d4bb167-947e-44ec-92bf-42780ea61913",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In addition, they may\nhave historical records on user characteristics including past engagement, for a large number of\nusers. The combination of the pretreatment variables and the surrogates may be suﬃciently rich\nso that conditional on the combination the primary outcome is independent of the treatment. Given surrogacy, and given comparability of the observational and experimental sample\n(which requires that the conditional distribution of the primary outcome given surrogates and\npretreatment variables is the same in the experimental and observational sample), Athey et al. [2016b] develop two methods for estimating the average eﬀect. The ﬁrst corresponds to estimat-\ning the relation between the outcome and the surrogates in the observational data and using\nthat to impute the missing outcomes in the experimental sample. The second corresponds to\nestimating the relation between the treatment and the surrogates in the experimental sample\nand use that to impute the treatment indicator in the observational sample. They also derive\nthe biases from violations of the surrogacy assumption. [29]",
    "content_hash": "c34b73ead72fb5d9fe39a6e0a8d2b2fa28d757845b4e82368f390506800b3468",
    "location": null,
    "page_start": 30,
    "page_end": 30,
    "metadata": {
      "section": "Leveraging Experiments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2604397e-9d14-4b6c-bfb0-0aade065233f",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "The researcher may be interested in combining\nthese experiments to obtain more eﬃcient estimates, predicting the eﬀect of a treatment in\nanother population, or estimating the eﬀect of a treatment with diﬀerent characteristics. Such\ninferences are not validated by the design of the experiments, but the experiments are important\nin making such inferences more credible. These issues are related to external validity concerns,\nbut include more general eﬀorts to decompose experimentally estimated eﬀects into components\nthat can inform decisions on related treatments. In the treatment eﬀect literature aspects of\nthese problems have been studied in Hotz et al. [2005], Imbens [2010], Allcott [2015]. They have\nalso received some attention in the literature on structural modeling, where the experimental\ndata are used to anchor aspects of the structural model, e.g., Todd and Wolpin [2003]. [30]",
    "content_hash": "369affcbd6447e32517373706faae3c3f4c751ffc2ddb82bda70b5dc828d4a8d",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "Supplementary Analyses",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "444587e0-f86a-44fe-9974-3b31a070104f",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "2.7.2\nExperiments and Observational Studies\nIn the second setting, studied in Athey et al. [2016a], the researcher again has data from a ran-\ndomized experiment containing information on the treatment and the intermediate variables, as\nwell as pretreatment variables. In the observational study the researcher now observes the same\nvariables plus the primary outcome. If in the observational study unconfoundedness (selection-\non-observables) were to hold, the researcher would not need the experimental sample, and could\nsimply estimate the average eﬀect of the treatment on the primary outcome by adjusting for dif-\nferences between treated and control units in pretreatment variables. However, one can compare\nthe estimates of the average eﬀect on the intermediate outcomes based on the observational sam-\nple, after adjusting for pretreatment variables, with those from the experimental sample. The\nlatter are known to be consistent, and so if one ﬁnds substantial and statistically signiﬁcant\ndiﬀerences, unconfoundedness need not hold. For that case Athey et al. [2016a] develop meth-\nods for adjusting for selection on unobservables exploiting the observations on the intermediate\nvariables. 2.7.3\nMultiple Experiments\nAn issue that has not received as much attention, but provides fertile ground for future work\nconcerns the use of multiple experiments. Consider a setting where a number of experiments were\nconducted. The experiments may vary in terms of the population that the sample is drawn from,\nor in the exact nature of the treatments included.",
    "content_hash": "8e73dbbadec7e1d7cf682982755d517e18e22fd9b034a4eae9b725b3b4d40064",
    "location": null,
    "page_start": 31,
    "page_end": 31,
    "metadata": {
      "section": "Leveraging Experiments",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "3afa5794-d8f0-4a19-9157-4d7044b4124d",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "3\nSupplementary Analyses\nOne common feature of much of the empirical work in the causal literature is the use of what\nwe call here supplementary analyses. We want to contrast supplementary analyses with primary\nanalyses whose focus is on point estimates of the primary estimands and standard errors thereof. Instead, the point of the supplementary analyses is to shed light on the credibility of the primary\nanalyses. They are intended to probe the identiﬁcation strategy underlying the primary analyses. The results of these supplementary analyses is not to end up with a better estimate of the eﬀect\nof primary interest. The goal is also not to directly select among competing statistical models. Rather, the results of the supplementary analyses either enhance the credibility of the primary\nanalyses or cast doubts on them, without necessarily suggesting alternatives to these primary\nanalyses (although sometimes they may). The supplementary analyses are often based on careful\nand creative examinations of the identiﬁcation strategy. Although at ﬁrst glance, this creativity\nmay appear application-speciﬁc, in this section we try to highlight some common themes. In general, the assumptions behind the identiﬁcation strategy often have implications for the\ndata beyond those exploited in the primary analyses, and these implications are the focus of\nthe supplementary analyses. The supplementary analyses can take on a variety of forms, and\nwe are currently not aware of a comprehensive survey to date. Here we discuss some examples\nfrom the empirical and theoretical literatures and draw some general conclusions in the hope of\nproviding some guidance for future work. This is a very active literature, both in theoretical and\nempirical studies, and it is likely that the development of these methods will continue rapidly.",
    "content_hash": "ab61b18dda74e18c21e6ebcc8a0db5bf327ecac9b8681b50575904a422f5a0c7",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "Supplementary Analyses",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "40399527-6fad-443f-b3b8-c858ae590cb6",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "The assumptions underlying identiﬁcation strategies can typically be stated without reference\nto functional form assumptions or estimation strategies. For example, unconfoundedness is a\nconditional independence assumption. There are variety of estimation strategies that exploit the\nunconfoundedness assumption. Supplementary analyses may attempt to establish the credibility\nof the underlying independence assumption; or, they may jointly establish the credibility of the\nunderlying assumption and the speciﬁc estimation approach used for the primary analysis. In Section 3.1 we discuss one of the most common forms of supplementary analyses, placebo\nanalysis, where pseudo causal eﬀects are estimated that are known to be equal to zero. In Section\n3.2 we discuss sensitivity and robustness analyses that assess how much estimates of the primary\nestimands can change if we weaken the critical assumptions underlying the primary analyses. [31]",
    "content_hash": "3f07e3f28fe895485eafd1f189bd0a7b5b23e872861ec85b1f7ca726124771a7",
    "location": null,
    "page_start": 32,
    "page_end": 32,
    "metadata": {
      "section": "Placebo Analyses",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d7030d66-5bde-4e86-915c-101a82331bb7",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In Section 3.3 we discuss some recent work on understanding the identiﬁcation of key model\nestimates by linking model parameters to summary statistics of the data. In Section 3.4 we disuss\na particular supplementary analysis that is speciﬁc to regression discontinuity analyses. In this\ncase the focus is on the continuity of the density of an exogenous variable, with a discontinuity\nas the threshold for the regression discontinuity analyses evidence of manipulation of the forcing\nvariable. 3.1\nPlacebo Analyses\nThe most widely used of the supplementary analyses is what is often referred to as a placebo\nanalysis. In this case the researcher replicates the primary analysis with the outcome replaced\nby a pseudo outcome that is known not to be aﬀected by the treatment. Thus, the true value\nof the estimand for this pseudo outcome is zero, and the goal of the supplementary analysis is\nto assess whether the adjustment methods employed in the primary analysis when applied to\nthe pseudo outcome lead to estimates that are close to zero, taking into account the statistical\nuncertainty. Here we discuss some settings where such analyses, in diﬀerent forms, have been\napplied, and provide some general guidance. Although these analyses often take the form of\nestimating an average treatment eﬀect and testing whether that is equal to zero, underlying the\napproach is often conditional independence relation. In this review we highlight the fact that\nthere is typically more to be tested than simply a single average treatment eﬀect. 3.1.1\nLagged Outcomes\nOne type of placebo test relies on treating lagged outcomes as pseudo outcomes. Consider, for\nexample, the lottery data set assembled by Imbens et al. [2001], which studies participants in\nthe Massachusetts state lottery.",
    "content_hash": "fc4d27a1d339048970e1ba06b14864a6fc6bb11acfb8a33b943739bf423a419f",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "Placebo Analyses",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2ea53033-1ba1-4ae8-b6ec-f965c4cb9b59",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "The treatment of interest is an indicator for winning a big prize\nin the lottery (with these prizes paid out over a twenty year period), with the control group\nconsisting of individuals who won one small, one-time prizes. The estimates of the average\ntreatment eﬀect rely on an unconfoundedness assumption, namely that the lottery prize is as\ngood as randomly assigned after taking out associations with some pre-lottery variables:\nWi ⊥⊥\n\u0010\nYi(0), Yi(1)\n\u0011\f\f\f Xi. (3.1)\n[32]",
    "content_hash": "0f8c0ec1a72d7cd5dfff01a493cde0729224ada466a998a8f6a7077a7efd3e13",
    "location": null,
    "page_start": 33,
    "page_end": 33,
    "metadata": {
      "section": "Placebo Analyses",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a302b912-a8f2-4b37-855b-d0b9b7f305d2",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Table 2: Lagged as Pseudo-Outcomes in the Lottery Data\nOutcome\nest\n(s.e.)\nPseudo Outcome: Y−1,i\n-0.53\n(0.78)\nActual Outcome: Y obs\ni\n-5.74\n(1.40)\nThe pre-treatment variables include six years of lagged earnings as well as six individual charac-\nteristics (including education measures and gender). Unconfoundedness is plausible here because\nwhich ticket wins the lottery is random, but because of a 50% response rate, as well as diﬀerences\nin the rate at which individuals buy lottery tickets, there is no guarantee that this assumption\nholds. To assess the assumption it is useful to estimate the same regression function with pre-\nlottery earnings as the outcome, and the indicator for winning on the right hand side with the\nsame set of additional exogenous covariates. Formally, we partition the vector of covariates Xi\ninto two parts, a (scalar) pseudo outcome, denoted by Xp\ni , and the remainder, denoted by Xr\ni ,\nso that Xi = (Xp\ni , Xr\ni ). We can then test the conditional independence relation\nWi ⊥⊥Xp\ni\nXr\ni . (3.2)\nWhy is testing this conditional independence relation relevant for assessing unconfoundedness\nin (3.1)? There are two conceptual steps. One is that the pseudo outcome Xp\ni is viewed as\na proxy for one or both of the potential outcomes. Second, it relies on the notion that if\nunconfoundedness holds given the full set of pretreatment variables Xi, it is plausible that it\nalso holds given the subset Xr\ni .",
    "content_hash": "d887fb4248d2ad9fc8353a17b046522715896fafd9290f08e2571865e3e05735",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "Placebo Analyses",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "74d90dd7-280a-432e-b58e-6a663b44e831",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In the lottery application, taking Xp\ni to be earnings in the\nyear prior to winning or not, both steps appear plausible. Results for this analysis are in Table\n2. Using the actual outcome we estimate that winning the lottery (with on average a $20,000\nyearly prize), reduces average post-lottery earnings by $5,740, with a standard error of $1,400. Using the pseudo outcome we obtain an estimate of minus $530, with a standard error of $780. In Table 3, we take this one step further by testing the conditional independence relation\nin (3.2) more fully. We do this by testing the null of no average diﬀerence for two functions\nof the pseudo-outcome, namely the actual level and an indicator for the pseudo-outcome being\n[33]",
    "content_hash": "13c4150b766b409ce81175d830b94175b71e2b7ef762b3d7091cfac1e0b374b7",
    "location": null,
    "page_start": 34,
    "page_end": 34,
    "metadata": {
      "section": "Placebo Analyses",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a022bca7-2c14-4982-b900-bdc27b426df5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Table 3: Testing Conditional Independence of Lagged Outcomes and the Treat-\nment in the Lottery Data\nPseudo\nSubpopulation\nest\n(s.e.)\nOutcome\n1{Y−1,i=0}\nY−2,i = 0\n-0.07\n(0.78)\n1{Y−1,i=0}\nY−2,i > 0\n0.02\n(0.02)\nY−1,i\nY−2,i = 0\n-0.31\n(0.30)\nY−1,i\nY−2,i > 0\n0.05\n(0.06)\nstatistic\np-value\nCombined Statistic\n(chi-squared, dof 4)\n2.20\n0.135\npositive. Moreover we test this separately for individuals with positive earnings two years prior\nto the lottery and individuals with zero earnings two years prior to the lottery. Combining\nthese four tests in a chi-squared statistic leads to a p-value of 0.135. Overall these analyses are\nsupportive of unconfoundedness holding in this study. Using the same approach with the LaLonde [1986] data that are widely used in the eval-\nuation literature (e.g., Heckman and Hotz [1989], Dehejia and Wahba [1999], Imbens [2015b]),\nthe results are quite diﬀerent. Here we use 1975 earnings as the pseudo-outcome, leaving us\nwith only a single pretreatment year of earnings to adjust for the substantial diﬀerence between\nthe trainees and comparison group from the CPS. Now, as reported in Table 4, the adjusted\ndiﬀerences between trainees and CPS controls remain substantial, casting doubt on the uncon-\nfoundedness assumption.",
    "content_hash": "e31dd9c855e4a4be919199fa841fb28935121dd49e91fdccf08fc1a8a1b91898",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "Placebo Analyses",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2ac6e52d-0cbf-4be7-ae00-ab5c3f142643",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Again we ﬁrst test whether the simple average diﬀerence in adjusted\n1975 earnings is zero. Then we test whether both the level of 1975 earnings and the indicator for\npositive 1975 earnings are diﬀerent in the two groups, separately for individuals with zero and\npositive 1974 earnings. The null is rejected, casting doubt on the unconfoundedness assumption\n(together with the approach for controlling for covariates, in this case subclassiﬁcation). [34]",
    "content_hash": "c939a14168d21df8f22420335a885c4d6f7692be61397b3a4eebe89f968ccc63",
    "location": null,
    "page_start": 35,
    "page_end": 35,
    "metadata": {
      "section": "Placebo Analyses",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "3a02230c-57af-42e3-ba35-952ea06bdaa5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Note that formally, we do\nnot need this conditional independence to hold, and if it were to fail one might be tempted\nto simply adjust for it in a regression analysis. However, the presence of such a discontinuity\nmay be diﬃcult to explain in a regression discontinuity design, and adjusted estimates would\ntherefore not have much credibility. The discontinuity might be interpreted as evidence for an\nunobserved confounder whose distribution changes at the boundary, one which might also be\ncorrelated with the outcome of interest. Let us illustrate this with the Lee election data (Lee [2008]). Lee [2008] is interested in\nestimating the eﬀect of incumbency on electoral outcomes. The treatment is a Democrat win-\n[35]",
    "content_hash": "40ac152ccf781d238ce65da84cd56d4209c72c44c8a4c8ebb3b2e68fd8fa6c44",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "Placebo Analyses",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d28f2381-7094-4049-a007-6a0fa5add58d",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Table 4: Lagged Earnings as a Pseudo-Outcome in the Lalonde Data\np-value\nearnings 1975:\n-0.90\n(0.33)\n0.006\nchi-squared test\n53.8\n(dof=4)\n< 0.001\n3.1.2\nCovariates in Regression Discontinuity Analyses\nAs a second example, consider a regression discontinuity design. Covariates typically play only a\nminor role in the primary analyses there, although they can improve precision (Imbens and Lemieux\n[2008], Calonico et al. [2014a,b]). The reason is that in most applications of regression discon-\ntinuity designs, the covariates are uncorrelated with the treatment conditional on the forcing\nvariable being close to the threshold. As a result, they are not required for eliminating bias. However, these exogenous covariates can play an important role in assessing the plausibility\nof the design. According to the identiﬁcation strategy, they should be uncorrelated with the\ntreatment when the forcing variable is close to the threshold. However, there is nothing in the\ndata that guarantees that this holds. We can therefore test this conditional independence, for\nexample by using a covariate as the pseudo outcome in a regression discontinuity analysis. If\nwe were to ﬁnd that the conditional expectation of one of the covariates is discontinuous at\nthe threshold, it would cast doubt on the identiﬁcation strategy.",
    "content_hash": "ff590227ca195b5387934acd4bc6beccda2c56a1f4622a51fc37962cf11c386d",
    "location": null,
    "page_start": 36,
    "page_end": 36,
    "metadata": {
      "section": "Placebo Analyses",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a5a935f2-1359-4d86-9c7f-e4d5e9d7c17c",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "ning a congressional election, and the forcing variable is the Democratic vote share minus the\nRepublication vote share in the current election. We look at an indicator for winning the next\nelection as the outcome. As a pretreatment variable, we consider an indicator for winning the\nprevious election to the one that deﬁnes the forcing variable. Table 5 presents the results, based\non the Imbens-Kalyanaraman bandwidth, where we use local linear regression (weighted with a\ntriangular kernel to account for boundary issues). The estimates for the actual outcome (win-\nTable 5: Winning a Previous Election as a Pseudo-Outcome in Election Data\nDemocrat Winning Next Election\n0.43\n(0.03)\n0.26\nDemocrat Winning Previous Election\n0.03\n(0.03)\n0.19\nning the next election) are substantially larger than those for the pseudo outcome (winning the\nprevious election), where we cannot reject the null hypothesis that the eﬀect on the pseudo\noutcome is zero. 3.1.3\nMultiple Control Groups\nAnother example of the use of placebo regressions is Rosenbaum et al. [1987] (see also Heckman and Hotz\n[1989], Imbens and Rubin [2015]). Rosenbaum et al. [1987] is interested in the causal eﬀect of a\nbinary treatment and focuses on a setting with multiple comparison groups. There is no strong\nreason to believe that one of the comparison groups is superior to another. Rosenbaum et al. [1987] proposes testing equality of the average outcomes in the two comparison groups after\nadjusting for pretreatment variables.",
    "content_hash": "5617941f6dc38aef11ab5da180b5a1264be63012d7ce51da245e33ed7148e919",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "Placebo Analyses",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a572e928-2147-4e99-9d34-0bf74eee6b83",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "If one ﬁnds that there are substantial diﬀerences left after\nsuch adjustments, it shows that at least one of the comparison groups is not valid, which makes\nthe use of either of them less credible. In applications to evaluations of labor market programs\none might implement such methods by comparing individuals who are eligible but choose not to\nparticipate, to individuals who are not eligible. The biases from evaluations based on the ﬁrst\n[36]",
    "content_hash": "c0541c103d07750bd8747de1c9f3a27a4e265dd3b3b8ad98201b6c70ff4e7498",
    "location": null,
    "page_start": 37,
    "page_end": 37,
    "metadata": {
      "section": "Robustness and Sensitivity",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "66b701d7-e4ce-4309-8d2b-c122a30a1c1c",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "These alternative speciﬁcations are not intended to be\ninterpreted as statistical tests of the validity of the preferred model, rather they are intended to\nconvey that the substantive results of the preferred speciﬁcation are not sensitive to some of the\nchoices in that speciﬁcation. These alternative speciﬁcations may involve diﬀerent functional\nforms of the regression function, or diﬀerent ways of controlling for diﬀerences in subpopulations. Recently there has been some work trying to make these eﬀorts at assessing robustness more\nsystematic. Athey and Imbens [2015] propose an approach to this problem. We can illustrate the ap-\nproach in the context of regression analyses, although it can also be applied to more complex\nnonlinear or structural models. In the regression context, suppose that the object of interest is\n[37]",
    "content_hash": "0a8b3b41852c4842454d7d6edc6943eac0ce25cfee093c50c82b68de9b5b0ee6",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "Robustness and Sensitivity",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "13e31b36-f02f-4ef6-9dac-55526a6f1130",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "control group might correspond to diﬀerences in motivation, whereas evaluations based on the\nsecond control group could be biased because of direct associations between eligibility criteria\nand outcomes. Note that one can also exploit the presence of multiple control groups by comparing esti-\nmates of the actual treatment eﬀect based on one comparison group to that based on a second\ncomparison group. Although this approach seems appealing at ﬁrst glance, it is in fact less\neﬀective than direct comparisons of the two comparison groups because comparing treatment\neﬀect estimates involves the data for the treatment group, whose outcomes are not relevant for\nthe hypothesis at hand. 3.2\nRobustness and Sensitivity\nAnother form of supplementary analyses focuses on sensitivity and robustness measures. The\nclassical frequentist statistical paradigm suggests that a researcher speciﬁes a single statistical\nmodel. The researcher then estimates this model on the data, and reports estimates and standard\nerrors. The standard errors and the corresponding conﬁdence intervals are valid given under\nthe assumption that the model is correctly speciﬁed, and estimated only once. This is of course\nfar from common practice, as pointed out, for example, in Leamer [1978, 1983]. In practice\nresearcher consider many speciﬁcations and perform various speciﬁcation tests before settling\non a preferred model. Not all the intermediate estimation results and tests are reported. A common practice in modern empirical work is to present in the ﬁnal paper estimates of the\npreferred speciﬁcation of the model, in combination with assessments of the robustness of the\nﬁndings from this preferred speciﬁcation.",
    "content_hash": "fc2dfa7479abd51d2f28c4d5e7f8a004f59a8021070c74499d603daddabe4ea7",
    "location": null,
    "page_start": 38,
    "page_end": 38,
    "metadata": {
      "section": "Robustness and Sensitivity",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5f98e7b4-6fa7-4b2d-8e16-4d3033efaa2f",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "If there are many covariates,\nsome form of dimensionality reduction may be appropriate prior to estimating the robustness\nmeasure. Reﬁnements and improvements on this approach is an interesting direction for future\nwork. Another place where it is natural to assess robustness is in estimation of average treatment\neﬀects E[Yi(1) −Yi(0)] under unconfoundedness or selection on observables,\nWi ⊥⊥\n\u0010\nYi(0), Yi(1)\n\u0011 \f\f\fXi. The theoretical literature has developed many estimators in the setting with unconfoundedness. Some rely on estimating the conditional mean, E[Yi|Xi, Wi], some rely on estimating the propen-\nsity score E[Wi|Xi], while others rely on matching on the covariates or the propensity score. See\nImbens and Wooldridge [2009] for a review of this literature. We believe that researchers should\n[38]",
    "content_hash": "b92b3c487f38423076043df55c9ba38a415024ca3273b966fdb1537b084d2060",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "Robustness and Sensitivity",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "6acfd4c7-deee-496d-bca5-8cfa403f40a6",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "a particular regression coeﬃcient that has an interpretation as a causal eﬀect. For example, in\nthe preferred speciﬁcation\nE[Yi|Xi, Zi] = β0 + βW · Wi + β′\nZZi,\nthe interest may be in βW, the coeﬃcient on Wi. They then suggest considering a set of diﬀerent\nspeciﬁcations based on splitting the sample into two subsamples, with Xi ∈{0, 1} denoting the\nsubsample, and in each case estimating\nE[Yi|Wi, Zi, Zi = z] = β0x + βW x · Wi + β′\nZxZi. The original causal eﬀect is then estimated as ˜βW = X · ˆβW 1 + (1 −X) · ˆβW 0. If the original\nmodel is correct, the augmented model still leads to a consistent estimator for the estimand. Athey and Imbens [2015] suggest splitting the original sample once for each of the elements of the\noriginal covariate vector Zi, and splitting at a threshold that optimizes ﬁt by minimizing the sum\nof squared residuals. Note that the focus is not on ﬁnding an alternative speciﬁcation that may\nprovide a better ﬁt; rather, it is on assessing whether the estimate in the original speciﬁcation\nis robust to a range of alternative speciﬁcations. They suggest reporting the standard deviation\nof the ˜βW over the set of sample splits, rather than the full set of estimates for all sample splits. This approach has some weaknesses, however. For example, adding irrelevant covariates to the\nprocedure might decrease the standard deviation of estimates.",
    "content_hash": "a11889980ef0841ede6fae15c1c73d58db5b7b9780c08faa252a216faffead89",
    "location": null,
    "page_start": 39,
    "page_end": 39,
    "metadata": {
      "section": "Robustness and Sensitivity",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b6fc74d9-5002-46a7-afa4-eca7fbc5ec3d",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Imbens [2003] builds on the Rosenbaum\nand Rubin approach by developing a data-driven way to obtain a set of correlations between\nthe unobserved covariates and treatment and outcome. Speciﬁcally he suggests relating the\nexplanatory power of the unobserved covariate to that of the observed covariates in order to\ncalibrate the magnitude of the eﬀects of the unobserved components. Altonji et al. [2008] and Oster [2015] focus on the correlation between the unobserved com-\nponent in the relation between the outcome and the treatment and observed covariates, and the\nunobserved component in the relation between the treatment and the observed covariates. In\nthe absence of functional form assumptions this correlation is not identiﬁed. Altonji et al. [2008]\nand Oster [2015] therefore explore the sensitivity to ﬁxed values for this correlation, ranging from\nthe case where the correlation is zero (and the treatment is exogenous), to an upper limit, chosen\nto match the correlation found between the observed covariates in the two regression functions. Oster [2015] takes this further by developing estimators based on this equality. What makes\n[39]",
    "content_hash": "c2db3eba6197c7a23f61107a112d0d0cfe910dc93637d148f3fde3b68d036c82",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "Identiﬁcation and Sensitivity",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2e92e698-01d6-4656-a287-6985cfa03ccc",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "not rely on a single method, but report estimates estimation based on a variety of methods to\nassess robustness. Arkhangelskiy and Drynkin [2016] studies sensitivity of the estimates of the parameters of\ninterest to misspeciﬁcation of the model governing the nuisance parameters. Another way\nto assess robustness is to use the partial indentiﬁcation or bounds literature originating with\nManski [1990]. See Tamer [2010] for a recent review. In combination with reporting estimates\nbased on the preferred speciﬁcation that may lead to point identiﬁcation, it may be useful to\ncombine that with reporting ranges based substantially weaker assumptions. Coming at the\nsame problem as the bounds approach, but from the opposite direction, Rosenbaum and Rubin\n[1983b], Rosenbaum [2002] suggest sensitivity analyses. Here the idea is to start with a re-\nstrictive speciﬁcation, and to assess the changes in the estimates that result from small to\nmodest relaxations of the key identifying assumptions such as unconfoundedness. In the con-\ntext Rosenbaum and Rubin [1983b] consider, that of estimating average treatment eﬀects under\nselection on observables, they allow for the presence of an unobserved covariate that should\nhave been adjusted for in order to estimate the average eﬀect of interest. They explore how\nstrong the correlation between this unobserved covariate and the treatment and the correlation\nbetween the unobserved covariate and the potential outcomes would have to be in order the\nsubstantially change the estimate for the average eﬀect of interest. A challenge is how to make a\ncase that a particular correlation is substantial or not.",
    "content_hash": "0505bde73962dc4cd0385547beb5a1ac94df1ca8020e5f5d56d9f032f07c1ad6",
    "location": null,
    "page_start": 40,
    "page_end": 40,
    "metadata": {
      "section": "Robustness and Sensitivity",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "36c257f7-a853-4bf3-aa94-e62e38c1573e",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "this approach very useful is that for a general set of models it provides the researcher with a\nsystematic way of doing the sensitivity analyses that are routinely, but often in an unsystematic\nway, done in empirical work. 3.3\nIdentiﬁcation and Sensitivity\nGentzkow and Shapiro [2015] take a diﬀerent approach to sensitivity. They propose a method\nfor highlighting what statistical relationships in a dataset are most closely related to parameters\nof interest. Intuitively, the idea is that covariation between particular sets of variables may deter-\nmine the magnitude of model estimates. To operationalize this, they investigate in the context\nof a given model, how the key parameters relate to a set of summary statistics. These summary\nstatistics would typically include easily interpretable functions of the data such as correlations\nbetween subsets of variables. Under mild conditions, the joint distribution of the model param-\neters and the summary statistics should be jointly normal in large samples. If the summary\nstatistics are in fact asymptotically suﬃcient for the model parameters, the joint distribution\nof the parameter estimates and the summary statistics will be degenerate. More typically the\njoint normal distribution will have a covariance matrix with full rank. Gentzkow and Shapiro\n[2015] discuss how to interpret the covariance matrix in terms of sensitivity of model parameters\nto model speciﬁcation. Gentzkow and Shapiro [2015] focus on the derivative of the conditional\nexpectation of the model parameters with respect to the summary statistics to assess how impor-\ntant particular summary statistics are for determining the parameters of interest. More broadly,\ntheir approach is related to proposals by Conley et al. [2012], Chetty [2009] in diﬀerent settings.",
    "content_hash": "0899523dc8e9f5e3d92ae6a07d8467f3b62f8c02819e7d0304af40f7cb87f604",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "Supplementary Analyses in Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b3e89d76-4892-49c2-ae55-b81f84c0b6fc",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "3.4\nSupplementary Analyses in Regression Discontinuity Designs\nOne of the most interesting supplementary analyses is the McCrary test in regression disconti-\nnuity designs (McCrary [2008], Otsu et al. [2013]). What makes this analysis particularly inter-\nesting is the conceptual distance between the primary analysis and the supplementary analysis. The McCrary test assesses whether there is a discontinuity in the density of the forcing variable\nat the threshold. If the forcing variable is denoted by Xi, with density fX(·), and the threshold\nc, the null hypothesis underlying the McCrary test is\nH0 : lim\nx↑c fX(x) = lim\nx↓c fX(x),\n[40]",
    "content_hash": "f20ac657ecc65dc6b60d57e7ffbfaabff2643b78e51929c6e8061f455df88ab2",
    "location": null,
    "page_start": 41,
    "page_end": 41,
    "metadata": {
      "section": "Supplementary Analyses in Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5090321f-b365-49fa-a7de-3eb5080a1780",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "with the alternative hypothesis that there is a discontinuity in the density of the forcing variable\nat the threshold. In a conventional analysis, it is unusual that the marginal distribution of a\nvariable that is assumed to be exogenous is of any interest to the researcher: often the entire\nanalysis is conducted conditional on such regressors. Why is this marginal distribution of interest in this setting? The reason is that the identi-\nﬁcation strategy underlying regression discontinuity designs relies on the assumption that units\njust to the left and just to the right of the threshold are comparable. The assumption underling\nregression discontinuity designs is that it was as good as random on which side of the threshold\nthe units were placed, and implicitly, that there is nothing special about the threshold in that\nregard. That argument is diﬃcult to reconcile with the ﬁnding that there are substantially\nmore units just to the left than just to the right of the threshold. Again, even though such an\nimbalance is easy to take into account in the estimation, it is the very presence of the imbalance\nthat casts doubt on the entire approach. In many cases where one would ﬁnd such an imbal-\nance it would suggest that the forcing variable is not a characteristic exogenously assigned to\nindividuals, rather that it is something that is manipulated by someone with knowledge of the\nimportance of the value of the forcing variable for the treatment assignment. The classic example is that of an educational regression discontinuity design where the forcing\nvariable is a test score. If the teacher or individual grading the test is aware of the importance\nof exceeding the threshold, they may assign scores diﬀerently than if there were not aware of\nthis.",
    "content_hash": "4df11f4ce66f28262badef79a766921427cbdb96290d6ea83294ac33a7dc4c04",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "Supplementary Analyses in Regression Discontinuity Designs",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "030a2d64-2f34-41d7-a0d8-6b5cd76be36f",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "If there was such manipulation of the score, there would likely be a discontinuity in the\ndensity of the forcing variable at the threshold: there would be no reason to change the grade\nfor an individual scoring just above the threshold. Let us return to the Lee election data to illustrate this. For these data the estimated\ndiﬀerence in the density at the threshold is 0.10 (with the level of the density around 0.90), with\na standard error of 0.08, showing there is little evidence of a discontinuity in the density at the\nthreshold. 4\nMachine Learning and Econometrics\nIn recent years there have been substantial advances in ﬂexible methods for analyzing data in\ncomputer science and statistics, a literature that is commonly referred to as the “machine learn-\n[41]",
    "content_hash": "3b2bb8a388bb3f79823105a1a32b3cd903779e0dad246fed2fa860bacf6bea60",
    "location": null,
    "page_start": 42,
    "page_end": 42,
    "metadata": {
      "section": "Machine Learning and Econometrics",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "38c5526f-8592-4c29-989d-e7a9cd954786",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "[2016d] for an example where unsupervised\nlearning is used to put newspaper articles into topics). The unsupervised learning literature\ndoes have some connections with the statistics literature, for example, for estimating mixture\ndistributions; principal-components analysis is another method that has been used in the social\nsciences historically, and that falls under the umbrella of unsupervised learning. “Supervised” machine learning focuses primarily on prediction problems: given a “training\ndataset” with data on an outcome Yi, which could be discrete or continuous, and some covariates\nXi, the goal is to estimate a model for predicting outcomes in a new dataset (a “test” dataset)\nas a function of Xi. The typical assumption in these methods is that the joint distribution of Xi\nand Yi is the same in the training and the test data. Note that this diﬀers from the goal of causal\ninference in observational studies, where we observe data on outcomes and a treatment variable\nWi, and we wish to draw inferences about potential outcomes. Implicitly, causal inference has\nthe goal of predicting outcomes for a (hypothetical, or counterfactual) test dataset where, for\nexample, the treatment is set to 1 for all units. Letting Y obs\ni\n= Yi(Wi), by construction, the\njoint distribution of Wi and Y obs\ni\nin the training data is diﬀerent than what it would be in a test\n[42]",
    "content_hash": "f62d850e8f0a1f9e80218de1adc17dceedd6ee4e60d16cfcddb6f600879c68bf",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "Machine Learning and Econometrics",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ad45469b-4b44-4dc6-ad29-d25295b97f1a",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "ing” literature. These methods have made only limited inroads into the economics literature,\nalthough interest has increased substantially very recently. There are two broad categories of\nmachine learning, “supervised” and “unsupervised” learning. “Unsupervised learning” focuses\non methods for ﬁnding patterns in data, such as groups of similar items. In the parlance of this\nreview, it focuses on reducing the dimensionality of covariates in the absence of outcome data. Such models have been applied to problems like clustering images or videos, or putting text\ndocuments into groups of similar documents. Unsupervised learning can be used as a ﬁrst step\nin a more complex model. For example, instead of including as covariates indicator variables\nfor whether a unit (a document) contains each of a very large set of words in the English lan-\nguage, unsupervised learning can be used to put documents into groups, and then subsequent\nmodels could use as covariates indicators for whether a document belongs to one of the groups. The number of groups might be much smaller than the number of words that appears in all of\nthe documents, and so unsupervised learning is a method to reduce the dimensionality of the\ncovariate space. We do not discuss unsupervised learning further here, beyond simply noting\nthat the method can potentially be quite useful in applications involving text, images, or other\nvery high-dimensional data, even though they have not had too much use in the economics\nliterature so far (for an exception, see Athey et al.",
    "content_hash": "a30492e0acfb183fd5c57867c0d476b5fbe9f623f3b22f63c69e87b2a3b1e02a",
    "location": null,
    "page_start": 43,
    "page_end": 43,
    "metadata": {
      "section": "Machine Learning and Econometrics",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "789f89cb-be0a-4784-a000-814c8c4cf816",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "dataset where Wi = 1 for all units. Kleinberg et al. [2015] argue that many important policy\nproblems are fundamentally prediction problems; see also the review article in this volume. In\nthis review, we focus primarily on problems of causal inference, showing how supervised machine\nlearning methods can be used to improve the performance of causal analysis, particularly in cases\nwith many covariates. We also highlight a number of diﬀerences in focus between the supervised machine learning\nliterature and the econometrics literature on nonparametric regression. A leading diﬀerence is\nthat the supervised machine learning literature focuses on how well a prediction model does in\nminimizing the mean-squared error of prediction in an independent test set, often without much\nattention to the asymptotic properties of the estimator. The focus on minimizing mean-squared\nerror on a new sample implies that predictions will make a bias-variance tradeoﬀ; successful\nmethods allow for bias in estimators (for example, by dampening model parameters towards\nthe mean) in order to reduce the variance of the estimator. Thus, predictions from machine\nlearning methods are not typically unbiased, and estimators may not be asymptotically normal\nand centered around the estimand. Indeed, the machine learning literature places much less\n(if any) emphasis on asymptotic normality, and when theoretical properties are analyzed, they\noften take the forms of worst-case bounds on risk criteria. A closely related diﬀerence between many (but not all) econometric approaches and super-\nvised machine learning is that many supervised machine learning methods rely on data-driven\nmodel selection, most commonly through cross-validation, to choose “tuning” parameters.",
    "content_hash": "10a16b8adc552a5529e292ccc9a6b9c32670341e1888c1ce3755baf7e7eae61b",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "Machine Learning and Econometrics",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "e7e963c2-75b1-452d-a61a-e8935cc09758",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Tun-\ning parameters may take the form of a penalty for model complexity, or in the case of a kernel\nregression, a bandwidth. For the supervised learning methods typically the sample is split into\ntwo samples, a training sample and a test sample, where for example the test sample might have\n10% of observations. The training sample is itself partitioned into a number of subsamples,\nor cross-validation samples, say m = 1, .., M, where commonly M = 10. For each subsample\nm = 1, . . . , M, the cross-validation sample m is set aside. The remainder of the training sample\nis used for estimation. The estimation results are then used to predict outcomes for the left-out\nsubsample m. The sum of squared residuals for these M subsamples sample are added up. Keep-\ning ﬁxed the partition, the process is repeated for many diﬀerent values of a tuning parameter. The ﬁnal choice of tuning parameter is the one that minimizes the sum of the squared residuals\nin the cross-validation samples. Cross-validation has been used for kernel regressions within the\n[43]",
    "content_hash": "3d5769d6e3647f6156525aefa3c24171429bdfbb55aae39c483119cb2a161e3f",
    "location": null,
    "page_start": 44,
    "page_end": 44,
    "metadata": {
      "section": "Machine Learning and Econometrics",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c1c525cf-4060-4438-b2a3-c0dbe57c350e",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "econometrics literature; in that literature, the convention is often to set M equal to the size\nof the training sample minus one; that is, researchers often do “leave-one-out” cross-validation. In the machine learning literature, the sample sizes are often much larger and estimation may\nbe more complex, so that the computational burden of leave-one-out may be too high. Thus,\nthe convention is to use 10 cross-validation samples. Finally, after the model is “tuned” (that\nis, the tuning parameter is selected), the researcher re-estimates the model using the chosen\ntuning parameter and the entire training dataset. Ultimate model performance is assessed by\ncalculating the mean-squared error of model predictions (that is, the sum of squared residuals)\non the held-out test sample, which was not used at all for model estimation or tuning. This\nﬁnal step is uncommon in the traditional econometrics literature, where the emphasis is more\non eﬃcient estimation and asymptotic properties. One way to think about cross-validation is that it is tuning the model to best achieve its\nultimate goal, which is prediction quality on a new, independent test set. Since at the time\nof estimation, the test set is by deﬁnition not available, cross-validation mimics the process of\nﬁnding a tuning parameter which maximizes goodness of ﬁt on independent samples, since for\neach m, a model is trained on one sample and evaluated on an independent sample (sample m). The complement of m in the training sample is smaller than the ultimate training sample will\nbe, but otherwise cross-validation mimics the ultimate exercise.",
    "content_hash": "e79d923f55b42772e351dd3a042ea986a10f59204b162693a8a0eca30993da85",
    "location": null,
    "page_start": 45,
    "page_end": 45,
    "metadata": {
      "section": "Machine Learning and Econometrics",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "da649fff-d338-4c84-8a10-8a2c4d92af2e",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "When the tuning parameter\nrepresents model complexity, cross-validation can be thought of as optimizing model complexity\nto balance bias and variance for the estimator. A complex model will ﬁt very well on the sample\nused to estimate the model (good in-sample ﬁt), but possibly at the cost of ﬁtting poorly on\na new sample. For example, a linear regression with as many parameters as observations ﬁts\nperfectly in-sample, but may do very poorly on a new sample, due to what is referred to as\n“over-ﬁtting.”\nThe fact that model performance (in the sense of predictive accuracy on a test set) can be\ndirectly measured makes it possible to meaningfully compare predictive models, even when their\nasymptotic properties are not understood. It is perhaps not surprising that enormous progress\nhas been made in the machine learning literature in terms of developing models that do well\n(according to the stated criteria) in real-world datasets. Here, we brieﬂy review some of the\nsupervised machine learning methods that are most popular and also most useful for causal\ninference, and relate them to methods traditionally used in the economics and econometrics\n[44]",
    "content_hash": "a2fe34f522700f7e72bf6828661fbbed3f4275b9972c22d365dca25377bf03e1",
    "location": null,
    "page_start": 45,
    "page_end": 45,
    "metadata": {
      "section": "Prediction Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a42ef63e-9a8f-4ddc-8a74-205d3f82acaa",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "N\nX\ni=1\nK\n\u0012Xi −x\nh\n\u0013\n,\nfor some kernel function K(·), sometimes a normal kernel K(x) = exp(−x2/2), or bounded\nkernel such as the uniform kernel K(x) = 1|x|≤1. The properties of such kernel estimators are\nwell established, and known to be poor when the dimension of Xi is high. To see why, note that\nwith many covariates, the nearest observations across a large number of dimensions may not be\nparticularly close in any given dimension. Other alternatives for nonparametric regression include series regression where g(x) is ap-\nproximated by the sum of a set of basis functions, g(x) = PK\nk=0 βk · hk(x), for example polyno-\nmial basis functions, hk(x) = xk (although the polynomial basis is rarely an attractive choice\n[45]",
    "content_hash": "1dcbed5873d75444a9a058a301587e14f14522253f4d562351d5dfff5d698583",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "Prediction Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "3dc1646c-69eb-4568-bc7e-1586d1bc2084",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "literatures. We then describe some of the recent literature combining machine learning and\neconometrics for causal inference. 4.1\nPrediction Problems\nThe ﬁrst problem we discuss is that of nonparametric estimation of regression functions. The\nsetting is one where we have observation for a number of units on an outcome, denoted by Yi for\nunit i, and a vector of features, covariates, exogenous variables, regressors or predictor variables,\ndenoted by Xi. The dimension of Xi may be large, both relative to the number of units and in\nabsolute terms. The target is the conditional expectation\ng(x) = E[Yi|Xi = x]. For this setting, the traditional methods in econometrics are based on kernel regression or\nnearest neighbor methods (H¨ardle [1990], Wasserman [2007]). In “K-nearest-neighbor” or KNN\nmethods, ˆg(x) is the sample average of the K nearest observations to x in Euclidean distance. K is a tuning parameter; when applied in the supervised machine learning literature, K might\nbe chosen through cross-validation to minimize mean-squared error on independent test sets. In\neconomics, where bias-reduction is often paramount, it is more common to use a small number\nfor K. Kernel regression is similar, but a weighting function is used to weight observations\nnearby to x more heavily than those far away. Formally, the kernel regression the estimator\nˆg(x) has the form\nˆg(x) =\nN\nX\ni=1\nYi · K\n\u0012Xi −x\nh\n\u0013 .",
    "content_hash": "006342cc02de9e6c0cf40a71a3c02a9a4acf6308d2af81599a70db193a31d2b0",
    "location": null,
    "page_start": 46,
    "page_end": 46,
    "metadata": {
      "section": "Prediction Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "ea49e9a1-09c6-43ab-abc8-1f6f178b3c11",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "in practice). These methods do have well established properties (Newey and McFadden [1994]),\nincluding asymptotic normality, but they do not work well in high-dimensional cases. 4.1.1\nPenalized Regression\nOne of the most important methods in the supervised machine learning literature is the class\nof penalized regression models, where one of the most popular members of this class is LASSO\n(Least Absolute Shrinkage and Selection Operator, Tibshirani [1996], Hastie et al. [2009, 2015]). This estimator imposes a linear model for outcomes as a function of covariates and attempts to\nminimize an objective that includes the sum of square residuals as in ordinary least squares, but\nalso adds on an additional term penalizing the magnitude of regression parameters. Formally,\nthe objective function for these penalized regression models, after demeaning the covariates and\noutcome, and standardizing the variance of the covariates, can be written as\nmin\nβ1,...,βK\nN\nX\ni=1\n\u0010\nYi −\nK\nX\nk=1\nβk · Xik\n\u00112\n+ λ · ∥β∥,\n(4.1)\nwhere ∥·∥is a general norm. The standard practice is to select the tuning parameter λ through\ncross-validation. To interpret this, note that if we take λ = 0, we are back in the least squares\nworld, and obtain the ordinary least squares estimator. However, the ordinary least squares\nestimator is not unique if there are more regressors than units, K > N.",
    "content_hash": "8ec759b9ab99d4ce36107855fb23df8d6bdf850e7a039090d3d70043a74f8b2c",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "Prediction Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a63949cd-b8e0-41df-acb8-e74df209a20d",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Positive values for λ\nregularize this problem, so that the solution to the LASSO minimization problem is well deﬁned\neven if K > N. With a positive value for λ, there are a number of interesting choices for the\nnorm. A key feature is that for some choices of the norm, the algorithm leads to some of the\nβk to be exactly zero, leading to a sparse model. For example, the L0 norm ∥β∥= PK\nk=1 1βk̸=0\nleads to optimal subset selection: the estimator selects some of the βk to be exactly zero, and\nestimates the remainder by ordinary least squares. Another interesting choice is the L2 norm,\n∥β∥= PK\nk=1 β2\nk , which leads to ridge regression: all βk are shrunk smoothly towards zero, but\nnone are set equal to zero. In that case there is a very close connection to Bayesian estimation. If we specify the prior distribution on the βk to be Gaussian centered at zero, with variance equal\nto λ, the estimator for β is equal to the posterior mean. Perhaps the most important case is\n∥β∥= PK\nk=1 |βk|. In that case some of the βk will be estimated to be exactly equal to zero, and\nthe remainder will be shrunk towards zero. This is the LASSO (Tibshirani [1996], Hastie et al. [2009, 2015]). The value of the tuning parameter λ is typically choosen by cross-validation. [46]",
    "content_hash": "73d83a4ea23145942bdafd01327c00d9361312829cee1f4df963ab7f11a63d7f",
    "location": null,
    "page_start": 47,
    "page_end": 47,
    "metadata": {
      "section": "Prediction Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5a30da47-c0ad-4086-9ca1-437cdc685035",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Consider the choice between LASSO and ridge regression. From a Bayesian perspective,\nboth can be interpreted as putting independent prior distributions on all the βk, with in one\ncase the prior distributions being normal and in the other case the prior distributions being\nLaplace. There appears to be little reason to favor one rather than the other conceptually. Tibshirani [1996] in the original LASSO paper discusses scenarios where LASSO performs better\n(many of the βk equal or very close to zero, and a few that are large), and some where ridge\nregression performs better (all βk small, but not equal to zero). The more important diﬀerence\nis that LASSO leads to a sparse model. This can make it easier to interpret and discuss\nthe estimated model, even if it does not perform any better in terms of prediction than ridge\nregression. Researchers should ask themselves whether the sparsity is important in their actual\napplication. If the model is simply used for prediction, this feature of LASSO may not be of\nintrinsic importance. Computationally eﬀective algorithms have been developed that allow for\nthe calculation of the LASSO estimates in large samples with many regressors. One important extension that has become popular is to combine the ridge penalty term that\nis proportional to (PK\nk=1 |βk|2) with the LASSO penalty term that is proportional to PK\nk=1 |βk|\nin what is called an elastic net (Hastie et al. [2009, 2015]). There are also many extensions of\nthe basic LASSO methods, allowing for nonlinear regression (e.g., logistic regression models) as\nwell as selection of groups of parameters, see Hastie et al.",
    "content_hash": "73cf2fb3f3ddc1fa5c146da61513c99ac224f4735f5df2f282b50c2c66b91745",
    "location": null,
    "page_start": 48,
    "page_end": 48,
    "metadata": {
      "section": "Prediction Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d97ad5a7-c48b-488f-9e39-0073136167cb",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Initially the sum of squared residuals is PN\ni=1(Yi−Y )2. We can split the sample by\nXi1 < c versus Xi1 ≥c, or we can split it by Xi2 < c versus Xi2 ≥c. We look for the split (either\nsplitting by Xi1 or by Xi2, and the choice of c) that minimizes the sum of squared residuals. After the ﬁrst split we look at the two subsets (the two leaves of the tree), and we consider the\nnext split for each of the two subsets. At each stage there will be a split (typically unique) that\nreduces the sum of squared residuals the most. In the simplest version of a regression tree we\nwould stop once the reduction in the sum of squared residuals is below some threshold. We can\nthink of this as adding a penalty term to the sum of squared residuals that is proportional to\nthe number of leaves. A more sophisticated version of the regression trees ﬁrst builds (grows) a\nlarge tree, and then prunes leaves that have little impact on the sum of squared residuals. This\navoids the problem that a simple regression tree may miss splits that would lead to subsequent\nproﬁtable splits if the initial split did not improve the sum of squared residuals suﬃciently. In\n[48]",
    "content_hash": "7baa5c6a98c7f9734df12f828ae649a14b8b1d2a17eb7edcb35bc9d6b13a07b6",
    "location": null,
    "page_start": 49,
    "page_end": 49,
    "metadata": {
      "section": "Prediction Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "155a3edc-fc89-4a04-b5e8-d96840187de1",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "asymptotic properties of the LASSO; this may make the LASSO more attractive as an empirical\nmethod in economics. Under some conditions standard least squares conﬁdence intervals based\ningoring the variable selection feature of the LASSO are valid. The key condition is that the\ntrue value for many of the regressors is in fact exactly equal to zero, with the number of non-zero\nparameter values increasing very slowly with the sample size. See Hastie et al. [2009, 2015]. This\ncondition is of course unlikely to hold exactly in applications. LASSO is doing data-driven model\nselection, and ignoring the model selection for inference as suggested by the theorems based on\nthese sparsity assumptions may lead to substantial under-coverage for conﬁdence intervals in\npractice. In addition, it is important to recognize that regularized regression models reward\nparsimony: if there are several correlated variables, LASSO will prefer to put more weight on\none and drop the others. Thus, individual coeﬃcients should be interpreted with caution in\nmoderate sample sizes or when sparsity is not known to hold. 4.1.2\nRegression Trees\nAnother important class of methods for prediction that is only now beginning to make inroads\ninto the economics literature is regression trees and its generalizations. The classic reference for\nregression trees is Breiman et al. [1984]. Given sample with N units and a set of regressors Xi,\nthe idea is to sequentially partition the covariate space into subspaces in a way that reduces the\nsum of squared residuals as much as possible. Suppose, for example, that we have two covariates\nXi1 and Xi2.",
    "content_hash": "75535b9acfd4c5ba99ebdc558199defc7a1bd7a4c25f6c238ca1f20d88f264e5",
    "location": null,
    "page_start": 49,
    "page_end": 49,
    "metadata": {
      "section": "Prediction Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "51770a6b-7edf-4db1-a5ab-9fa15c0a9a00",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "both cases a key tuning parameter is the penalty term on the number of leaves. The standard\napproach in the literature is to choose that through crossvalidation, similar to that discussed in\nthe LASSO section. There is relatively little asymptotic theory on the properties of regression trees. Even estab-\nlishing consistency for the simple version of the regression tree, let alone inferential results that\nwould allow for the construction of conﬁdence intervals is not straightforward. A key problem\nin establishing such properties is that the estimated regression function is a non-smooth step\nfunction. We can compare regression trees to common practices in applied work of capturing nonlin-\nearities in a variable by discretizing the variable, for example, by dividing it into deciles. The\nregression tree uses the data to determine the appropriate “buckets” for discretization, thus po-\ntentially capturing the underlying nonlinearities with a more parsimonious form. On the other\nhand, the regression tree has diﬃculty when the underlying functional form is truly linear. Regression trees are generally dominated by other, more continuous models when the only\ngoal is prediction. Regression trees are used in practice due to their simplicity and interpretabil-\nity. Within a partition, the prediction from a regression tree is simply a sample mean. Simply\nby inspecting the tree (that is, describing the partition), it is straightforward to understand why\na particular observation received the prediction it did. 4.1.3\nRandom Forests\nRandom forests are one of the most popular supervised machine learning methods, known for\ntheir reliable “out-of-the-box” performance that does not require a lot of model tuning.",
    "content_hash": "b874c799896cb28c84668de421602367136a0883758a7c0cf3881b8577a04561",
    "location": null,
    "page_start": 50,
    "page_end": 50,
    "metadata": {
      "section": "Prediction Problems",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "2432dad9-f40a-45e7-accb-6e4cce7a9383",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "This sample splitting\napproach ensures that the estimates from each component tree in the forest are unbiased, so that\nthe predictions of the forest are no longer asymptotically bias-dominated. Although asymptotic\nnormality may not be crucial for pure prediction problems, when the random forest is used as\na component of estimation of causal eﬀects, such properties play a more important role, as we\nshow below. 4.1.4\nBoosting\nA general way to improve simple machine learning methods is boosting. We discuss this in the\ncontext of regression trees, but its application is not limited to such settings. Consider a very\nsimple algorithm for estimating a conditional mean, say a tree with only two leaves. That is, we\nonly split the sample once, irrespective of the number of units or the number of features. This\nis unlikely to lead to a very good predictor. The idea behind boosting is to repeatedly apply\n[50]",
    "content_hash": "145720672a9d433d690978a2894a8d6aeff80f321cef4ab4c6273ed9cfd06ad3",
    "location": null,
    "page_start": 51,
    "page_end": 51,
    "metadata": {
      "section": "Machine Learning Methods for Average Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "1035ad98-c7ad-4885-823c-b9d668452b09",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "4.2\nMachine Learning Methods for Average Causal Eﬀects\nThere is a large literature on estimating treatment eﬀects in settings with selection on observ-\nables, or unconfoundedness. This literature has largely focused on the case with a ﬁxed and\nmodest number of covariates. In practice, in order to make the critical assumptions more plausi-\nble, the number of pretreatment variables may be substantial. In recent years, researchers have\nintroduced machine learning methods into this literature to account for the presence of many\ncovariates. In many cases, the newly proposed estimators closely mimic estimators developed\nin the literature with a ﬁxed number of covariates. From a conceptual perspective, being able\nto ﬂexibly control for a large number of covariates may make an estimation strategy much more\nconvincing, particularly if the identiﬁcation assumptions are only plausible once a large number\n[51]",
    "content_hash": "5a846cd98683ed61999bf911c2f64b0841b19691cbb3fea8a6af6e2cdede95d2",
    "location": null,
    "page_start": 52,
    "page_end": 52,
    "metadata": {
      "section": "Machine Learning Methods for Average Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "18ec2767-7e35-4139-83ea-69741b815c3f",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "this naive method. After the ﬁrst application we calculate the residuals. We then apply the\nsame method to the residuals instead of the original outcomes. That is, we again look for the\nsample split that leads to the biggest reduction in the sum of squared residuals. We can repeat\nthis many times, each time applying the simple single split regression tree to the residuals from\nthe previous stage. If we apply this simple learner many times, we can approximate the regression function in a\nfairly ﬂexible way. However, this does not lead to an accurate approximation for all regression\nfunctions. By limiting ourselves to a naive learner that is a single split regression tree we can only\napproximate additive regression functions, where the regression function is the sum of functions\nof one of the regressors at a time. If we want to allow for interactions between pairs of the basic\nregressors we need to start with a simple learner that allows for two splits rather than one. 4.1.5\nSuper Learners and Ensemble methods\nOne theme in the supervised machine learning literature is that model averaging often performs\nvery well; many contests such as those held by Kaggle are won by algorithms that average many\nmodels. Random forests use a type of model averaging, but all of the models that are averaged\nare in the same family. In practice, performance can be better when many diﬀerent types of\nmodels are averaged. The idea of Super Learners in Van der Laan et al. [2007] is to use model\nperformance to construct weights, so that better performing models receive more weight in the\naveraging.",
    "content_hash": "bc2e581242e17cf8e239a7feddea6ed10ce47bdedec6f7262484d21373802392",
    "location": null,
    "page_start": 52,
    "page_end": 52,
    "metadata": {
      "section": "Machine Learning Methods for Average Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d6021063-7e5a-4d92-85ba-7f4ca52d06b4",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Thus, procedures such as “trimming” the data to eliminate extreme values of\nthe estimated propensity score (thus changing the estimand as in [Crump et al., 2009]) remain\nimportant. 4.2.2\nRegularized Regression Methods\nBelloni et al. [2014a,b, 2013] focus on regression estimators for average treatment eﬀects. For\nease of exposition, suppose one is interested in the average eﬀect for the treated, and so the\nproblem is to estimate E[Y (0)|Wi = 1]. Under unconfoundedness this is equal to E[E[Y obs\ni\n|Wi =\n0, Xi]|Wi = 1]. Suppose we model E[Y obs\ni\n|Xi = x, Wi = 0] as x′βc. Belloni et al. [2014a] point\nout that estimating βc using lasso leads to estimators for average treatment eﬀects with poor\nproperties. Their insight is that the objective function for LASSO (which is purely based on\npredicting outcomes) leads the LASSO to select covariates that are highly correlated with the\n[52]",
    "content_hash": "f06f30c067b80964d48bab9a4aca70c925cb5836dfc5d4099574f27121c47865",
    "location": null,
    "page_start": 53,
    "page_end": 53,
    "metadata": {
      "section": "Machine Learning Methods for Average Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "6b661244-258d-46fd-9e9b-cfacc1d64b5c",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "outcome; but the objective fails to prioritize covariates that are highly correlated with the treat-\nment but only weakly correlated with outcomes. Such variables are potential confounders for the\naverage treatment eﬀect, and omitting them leads to bias, even if they are not very important\nfor predicting unit-level outcomes. This highlights a general issue with interpreting individual\ncoeﬃcients in a LASSO: because the LASSO objective focuses on prediction of outcomes rather\nthan unbiased estimation, individual parameter estimates should be interpreted with caution. LASSO penalizes the inclusion of covariates, and some will be omitted in general; LASSO will\nfavor a more parsimonious functional form, where if two covariates are correlated, only one will\nbe included, and its parameter estimate will reﬂect the eﬀects of both the included and omitted\nvariables. Thus, in general LASSO coeﬃcients should not be given a causal interpretation. Belloni et al. [2013] propose a modiﬁcation of the LASSO that addresses these concerns and\nrestores the ability of LASSO to produce valid causal estimates. They propose a double selection\nprocedure, where they use LASSO ﬁrst to select covariates that are correlated with the outcome,\nand then again to select covariates that are correlated with the treatment. In a ﬁnal ordinary\nleast squares regression they include the union of the two sets of covariates, greatly improving\nthe properties of the estimators for the average treatment eﬀect. This approach accounts for\nomitted variable bias that would otherwise appear in a standard LASSO. Belloni et al.",
    "content_hash": "e44ccb39b9ed9be533edcdac392a556adb67c6cd433710f9d6824c5758d318db",
    "location": null,
    "page_start": 54,
    "page_end": 54,
    "metadata": {
      "section": "Machine Learning Methods for Average Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "551097ac-3af4-4b9b-b11a-05424bae0758",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "[2014b]\nillustrate the magnitude of the bias that can occur in real-world datasets from failing to account\nfor this issue. More broadly, these papers highlight the distinction between predictive modeling\nand estimation of causal eﬀects. 4.2.3\nBalancing and Regression\nAn alternative line of research has focused on ﬁnding weights that directly balance covariates\nor functions of the covariates between treatment and control groups, so that once the data has\nbeen re-weighted, it mimics more closely a randomized experiment. In the earlier literature\nwith few covariates, this approach has been developed in Hainmueller [2012], Graham et al. [2012, 2016]. More recently these ideas have also been applied to the many covariates case in\nZubizarreta [2015], Imai and Ratkovic [2014]. Athey et al. [2016c] develop an estimator that\ncombines the balancing with regression adjustment, in the spirit of the double robust estimators\nproposed by Robins and Rotnitzky [1995], Robins et al. [1995], Kang and Schafer [2007]. The\nidea is that, in order to predict the counterfactual outcomes that the treatment group would have\n[53]",
    "content_hash": "9fd9092adea2efc5f05ed34bbdcb36d6a24e92e8d279f5899e5bde85e5773215",
    "location": null,
    "page_start": 54,
    "page_end": 54,
    "metadata": {
      "section": "Heterogenous Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "19655ff0-7b8f-475b-bdb0-46af886d136a",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "had in the absence of the treatment, it is necessary to extrapolate from control observations. By\nrebalancing the data, the amount of extrapolation required to account for diﬀerences between\nthe two groups is reduced. To capture remaining diﬀerences, regularized regression can be used\nto model outcomes in the absence of the treatment. The general form of the Athey et al. [2016c] estimator for the expected control outcome for\nthe treated, that is, µc = E[Yi(0)|Wi = 1] = E[Yi|Xi = x, Wi = 0], is\nˆµc = Xt · ˆβc +\nX\ni:Wi=0\nγi\n\u0010\nY obs\ni\n−Xi · ˆβc\n\u0011\n. They suggest estimating ˆβc using LASSO or elastic net, in a regression of Y obs\ni\non Xi using the\ncontrol units. They suggest choosing the weights γi as the solution to\nγ = arg min\nγ\n(1 −ζ) ∥γ∥2\n2 + ζ\nXt −X⊤\nc γ\n2\n∞subject to\nX\nγi = 1, γi ≥0. This objective function balances the bias coming from imbalance between the covariates in the\ntreated subsample and the weighted control subsample and the variance from having excessively\nvariable weights. They suggest using ζ = 1/2. Unlike methods that rely on directly estimating\nthe treatment assignment process (e.g. the propensity score), the method controls bias even\nwhen the process determining treatment assignment cannot be represented with a sparse model.",
    "content_hash": "77b5a7029cb04ecf0f04a123141ec553ca413426ba6d61b72ffc78b803569e7c",
    "location": null,
    "page_start": 55,
    "page_end": 55,
    "metadata": {
      "section": "Heterogenous Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "9f0b18ea-364c-470f-a715-fc305311b22a",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "4.3\nHeterogenous Causal Eﬀects\nA diﬀerent problem is that of estimating the average eﬀects of the treatment for each value\nof the features, that is, the conditional average treatment eﬀect (CATE) τ(x) = E[Yi(1) −\nYi(0)|Xi = x]. This problem is highly relevant as a step towards assigning units to optimal\ntreatments. If all costs and beneﬁts of the treatment are incorporated in the measured outcomes,\nunderstanding the set of covariates where CATE is positive all that matters for determining\ntreatment assignment; in contrast, if the policy might be applied in diﬀerent settings with\nadditional costs or beneﬁts that might be diﬀerent than those in the training data, or if the\nanalyst wants to also gain insight about treatment eﬀect heterogeneity,\nThe concern is that searching over many covariates and subsets of the covariate space may\nlead to spurious ﬁndings of treatment eﬀect diﬀerences. Indeed, in medicine (e.g. for clinical\ntrials), pre-analysis plans must be registered in advance to avoid the problem that researchers\n[54]",
    "content_hash": "a54e202bdd47e5c9da377cb7f6e449c7409f43eb04c8824227885a09c1320871",
    "location": null,
    "page_start": 55,
    "page_end": 55,
    "metadata": {
      "section": "Heterogenous Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "428382b6-56dd-4160-9832-cd42f66b489c",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "A drawback of this approach is that the researcher must specify in advance all of the hy-\npotheses to be tested; alternative ways to discretize covariates, and ﬂexible interactions among\ncovariates, may not be possible to fully explore. A diﬀerent approach is to adapt machine\nlearning methods to discover particular forms of heterogeneity, as we discuss in the next section. 4.3.2\nSubgroup Analysis\nIn some settings, it is useful to identify subgroups that have diﬀerent treatment eﬀects. One\nexample is where eligibility for a government program is determined according to various criteria\nthat can be represented in a decision tree, or when a doctor uses a decision tree to determine\nwhether to prescribe a drug to a patient. Another example is when an algorithm uses a simple\nlookup table to determine which type of user interface, oﬀer, email solicitation, or ranking of\nsearch results to provide to a user. Subgroup analysis has long been used in medical studies\n([Foster et al., 2011]), but it is often subject to criticism due to concerns of multiple testing\n([Assmann et al., 2000]). [55]",
    "content_hash": "f40ceb55fc997108bed7bb62f56d2cea0b37f86520a8a2a740238f29682ae82f",
    "location": null,
    "page_start": 56,
    "page_end": 56,
    "metadata": {
      "section": "Heterogenous Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "51ddcac9-2c57-4f52-be0b-ace215232d7e",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Athey and Imbens [forthcoming] develops a method that they call “causal trees.”\nThe\nmethod is based on regression trees, and its goal is to identify a partition of the covariate\nspace into subgroups based on treatment eﬀect heterogeneity. The output of the method is a\ntreatment eﬀect and a conﬁdence interval for each subgroup. The approach diﬀers from standard\nregression trees in several ways. First, it uses a diﬀerent criterion for building the tree: rather\nthan focusing on improvements in mean-squared error of the prediction of outcomes, it focuses\non mean-squared error of treatment eﬀects. Second, the method relies on “sample splitting” to\nensure that conﬁdence intervals have nominal coverage, even when the number of covariates is\nlarge. In particular, half the sample is used to determine the optimal partition of the covariates\nspace (the tree structure), while the other half is used to estimate treatment eﬀects within the\nleaves. Athey and Imbens [forthcoming] highlight the fact that the criteria used for tree construction\nand cross-validation should diﬀer when the goal is to estimate treatment eﬀect heterogeneity\nrather than heterogeneity in outcomes; the factors that aﬀect the level of outcomes might be\nquite diﬀerent from those that aﬀect treatment eﬀects. To operationalize this, the criteria used\nfor sample splitting and cross-validation must confront two problems. First, unlike individual\noutcomes, the treatment eﬀect is not observed for any individual in the dataset.",
    "content_hash": "c5fdc034fb348b3309ac549e4910b2d5210090cbaf5a18ddf2de24554365ef01",
    "location": null,
    "page_start": 57,
    "page_end": 57,
    "metadata": {
      "section": "Heterogenous Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "193456c2-f6f7-45b8-b910-7e0512e18833",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Thus, it is not\npossible to directly calculate a sample average of the mean-squared error of treatment eﬀects,\nas this criterion is infeasible:\n−1\nN\nN\nX\ni=1\n\u0010\nτi −ˆτ(Xi)\n\u00112\n. (4.2)\nHowever, the approach exploits the fact that the regression tree makes the same prediction\nwithin each leaf. Thus, the estimator ˆτ is constant within a leaf, and so the infeasible mean-\nsquared error criterion can be estimated, since it depends only on averages of τi within leaves. The second issue is that the criteria are adapted to anticipate the fact that the model will be re-\nestimated with an independent data set. The modiﬁed criterion rewards a partition that creates\ndiﬀerentiation in estimated treatment eﬀects, but penalizes a partition where the estimated\ntreatment eﬀects have high variance, for example due to small sample size. Although the sample-splitting approach may seem extreme–ultimately only half the data\nis used for estimating treatment eﬀects–it has several advantages. One is that the conﬁdence\nintervals are valid no matter how many covariates are used in estimation. The second is that\n[56]",
    "content_hash": "62cfaf35beb86cf7120a3bde6465fc2d86cc46df01769dba4fc737f23557d145",
    "location": null,
    "page_start": 57,
    "page_end": 57,
    "metadata": {
      "section": "Heterogenous Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "3f0e05e1-a071-4e0c-a024-e21c234fd342",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "the researcher is free to estimate a more complex model in the second part of the data–the\npartition can be used to create covariates and motivate interactions in a more complex model,\nfor example if the researcher wishes to include ﬁxed eﬀects in the model, or model diﬀerent\ntypes of correlation in the error structure. Other related approaches include Su et al. [2009] and Zeileis et al. [2008], who propose statis-\ntical tests as criteria in constructing partitions. Neither of these approaches address the issue of\nconstructing valid conﬁdence intervals using the results of the partitions, but Athey and Imbens\n[forthcoming] combines their approaches with sample splitting in order to obtain valid conﬁ-\ndence intervals on treatment eﬀects. The approach of Zeileis et al. [2008] is more general than\nthe problem of estimating treatment eﬀect heterogeneity: this paper proposes estimating a po-\ntentially rich model within each leaf of the tree, and the criterion for splitting a leaf of the tree\nis a statistical test based on whether the split improves goodness of ﬁt of the model. 4.3.3\nPersonalized Treatment Eﬀects\nWager and Athey [2015] propose a method for estimating heterogeneous treatment eﬀects based\non random forests. Rather than rely on the standard random forest model, which focuses on\nprediction, Wager and Athey [2015] build random forests where each component tree is a causal\ntree [Athey and Imbens, forthcoming].",
    "content_hash": "ae5dff2a794ddb10e40e52df5cf52193a6453413ed9fc4d16e23a91559040c3c",
    "location": null,
    "page_start": 58,
    "page_end": 58,
    "metadata": {
      "section": "Heterogenous Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "8d0c0115-cdf6-4e8b-82a5-d6f2688526e6",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Relative to a causal tree, which identiﬁes a partition and\nestimates treatment eﬀects within each element of the partition, the causal forest leads to smooth\nestimates of τ(x). This type of method is more similar to a kernel regression, nearest-neighbor\nmatching, or other fully non-parametric methods, in that a distinct prediction is provided for\neach value of x. Building on their work for prediction-based random forests, Wager and Athey\n[2015] show that the predictions from causal forests are asymptotically normal and centered on\nthe true CATE for each x, since causal trees make use of sample splitting. They also propose\nan estimator for the variance, so that conﬁdence intervals can be obtained. Relative to existing\nmethods from econometrics, the random forest has been widely documented to perform well (for\nprediction problems) in a variety of settings with many covariates; and a particular advantage\nover methods such as nearest neighbor matching is that the random forest is resilient in the face\nof many covariates that have little eﬀect. These covariates are simply not selected for splitting\nwhen determining the partition. In contrast, nearest neighbor matching deteriorates quickly\nwith additional irrelevant covariates. [57]",
    "content_hash": "977a777b1ac15e2bdef1ab0e45d447584429393e857144fec597bd589c3b1595",
    "location": null,
    "page_start": 58,
    "page_end": 58,
    "metadata": {
      "section": "Heterogenous Causal Eﬀects",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5b12db79-07c2-4033-89e7-3a258f27a948",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "An alternative approach, closely related, is based on Bayesian Additive Regression Trees\n(BART) [Chipman et al., 2010]. Hill [2011] and Green and Kern [2012] apply these methods to\nestimate heterogeneous treatment eﬀects. BART is essentially a Bayesian version of random\nforests. Large sample properties of this method are unknown, but it appears to have good\nempirical performance in applications. Another approach is based on the LASSO [Imai and Ratkovic, 2013]. This approach esti-\nmates a LASSO model with the treatment indicator interacted with covariates, and uses LASSO\nas a variable selection algorithm for determining which covariates are most important. In order\nfor conﬁdence intervals to be valid, the true model must be assumed to be sparse. It may be\nprudent in a particular datset to perform some supplementary analysis to verify that the method\nis not over-ﬁtting; for example, one could test the approach by using only half of the data to es-\ntimate the LASSO, and then comparing the results to an ordinary least squares regression with\nthe variables selected by LASSO in the other half of the data. If the results are inconsistent,\nit could simply indicate that using half the data is not good enough; but it also might indicate\nthat sample splitting is warranted to protect against over-ﬁtting or other sources of bias that\narise when data-driven model selection is used. A natural application of personalized treatment eﬀect estimation is to estimating optimal pol-\nicy functions.",
    "content_hash": "bc6e10f325fbcc4833c11f467b646e933d272c19310960c22a4040633dd80d33",
    "location": null,
    "page_start": 59,
    "page_end": 59,
    "metadata": {
      "section": "Machine Learning Methods with Instrumental Variables",
      "heading_level": 2
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0720113b-859f-4165-9072-8edc09feb6e9",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "A literature in machine learning considers this problem ([Beygelzimer and Langford,\n2009]; [Dud´ık et al., 2011]); some open questions include the ability to obtain conﬁdence intervals\non diﬀerences between policies obtained from these methods. The machine learning literature\ntends to focus more on worst-case risk analysis rather than conﬁdence intervals. 4.4\nMachine Learning Methods with Instrumental Variables\nAnother setting where high-dimensional predictive methods can be useful is in settings with\ninstrumental variables. The ﬁrst stage in instrumental variables is typically purely a predic-\ntive exercise, where the conditional expectation of the endogenous variables is estimated using\nall the exogenous variables and excluded instruments. If there are many instruments, and\nthese can arise from a few instruments interacted with indicators for subpopulations, or from\nother ﬂexible transformations of the basic instrument, standard methods are known to have\npoor properties (Staiger and Stock [1997]). Alternative methods have focused on asymptotics\nbased on many instruments (Bekker [1994]), or hierarchical Bayes or random eﬀects methods\n[58]",
    "content_hash": "cd144843ddac739308bc5e5e017e4638abddd224052ccc9140251b131887d62e",
    "location": null,
    "page_start": 59,
    "page_end": 59,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "b0f99b7d-dc63-4ee8-819e-63567c5911b5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "(Chamberlain and Imbens [2004]). It is possible to interpret the latter approach as instituting\na form of “shrinkage” similar to ridge. Belloni et al. [2013] develop LASSO methods to estimate the ﬁrst (as well as second) stage\nin such settings, providing conditions under which valid conﬁdence intervals can be obtained. In a diﬀerent setting Eckles and Bakshy [forthcoming] study the use of instrumental variables\nin network settings. Encouragement to take particular actions that aﬀects friends an individual\nis connnected to is randomized at the individual level. These then generate many instruments\nthat each only weakly aﬀect a particular individual. 5\nConclusion\nThis review has covered selected topics in the area of causality and policy evaluation. We have\nattempted to highlight recently developed approaches for estimating the impact of policies. Relative to the previous literature, we have tried to place more emphasis on supplementary\nanalyses that help the analyst assess the credibility of estimation and identiﬁcation strategies. We further review recent developments in the use of machine learning for causal inference;\nalthough in some cases, new estimation methods have been proposed, we also believe that the\nuse of machine learning can help buttress the credibility of policy evaluation, since in many\ncases it is important to ﬂexibly control for a large number of covariates as part of an estimation\nstrategy for drawing causal inferences from observational data. We believe that in the coming\nyears, this literature will develop further, helping researchers avoid unnecessary functional form\nand other modeling assumptions, and increasing the credibility of policy analysis. References\nAlberto Abadie and Javier Gardeazabal. The economic costs of conﬂict: A case study of the\nbasque country.",
    "content_hash": "b12a87b94907a2153f6617b2f3a95f704eaaa4df36a16e9a3fab981a70cad576",
    "location": null,
    "page_start": 60,
    "page_end": 60,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "7417d166-94ef-41c5-8543-fefc7423cac9",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Using maimonides’ rule to estimate the eﬀect of class size\non scholastic achievement. The Quarterly Journal of Economics, 114(2):533–575, 1999. Joshua D Angrist and Miikka Rokkanen. Wanna get away? regression discontinuity estimation\nof exam school eﬀects away from the cutoﬀ. Journal of the American Statistical Association,\n110(512):1331–1344, 2015. [60]",
    "content_hash": "ab889e18e37503b89dd87d78770fe566e740a312f90516d0f81f26dd94b0ecc4",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "268c36f6-7c18-4fa6-9b41-b663252cffab",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "parative case studies: Estimating the eﬀect of californias tobacco control program. Journal\nof the American Statistical Association, 105(-):493–505, 2010. Alberto Abadie, Susan Athey, Guido W Imbens, and Jeﬀrey M Wooldridge. Finite population\ncausal standard errors. Technical report, National Bureau of Economic Research, 2014a. Alberto Abadie, Alexis Diamond, and Jens Hainmueller. Comparative politics and the synthetic\ncontrol method. American Journal of Political Science, pages 2011–25, 2014b. Alberto Abadie, Susan Athey, Guido Imbens, and Jeﬀrey Wooldrige. Clustering as a design\nproblem. 2016. Hunt Allcott. Site selection bias in program evaluation. Quarterly Journal of Economics, pages\n1117–1165, 2015. Joseph G Altonji, Todd E Elder, and Christopher R Taber. Using selection on observed variables\nto assess bias from unobservables when evaluating swan-ganz catheterization. The American\nEconomic Review, 98(2):345–350, 2008. Donald Andrews and James H. Stock. Inference with weak instruments. 2006. Joshua Angrist and Ivan Fernandez-Val. Extrapolate-ing: External validity and overidentiﬁca-\ntion in the late framework. Technical report, National Bureau of Economic Research, 2010. Joshua Angrist and Alan Krueger. Empirical strategies in labor economics. Handbook of Labor\nEconomics, 3, 2000. Joshua D Angrist. Treatment eﬀect heterogeneity in theory and practice. The Economic Journal,\n114(494):C52–C83, 2004. Joshua D Angrist and Victor Lavy.",
    "content_hash": "a0ad1669969b306b77b3685260d29c442dee5afa82caee6281d35f8ed6efbf2c",
    "location": null,
    "page_start": 61,
    "page_end": 61,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "3887c6dc-c887-428f-a093-a67f36f5b7a3",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Joshua D Angrist, Guido W Imbens, and Donald B. Rubin. Identiﬁcation of causal eﬀects using\ninstrumental variables. Journal of the American Statistical Association, 91:444–472, 1996. Dmitry Arkhangelskiy and Evgeni Drynkin. Sensitivity to model speciﬁcation. 2016. Peter Aronow. A general method for detecting interference between units in randomized exper-\niments. Sociological Methods & Research, 41(1):3–16, 2012. Peter M. Aronow and Cyrus Samii. Estimating average causal eﬀects under interference between\nunits, 2013. Susan F Assmann, Stuart J Pocock, Laura E Enos, and Linda E Kasten. Subgroup analysis and\nother (mis) uses of baseline data in clinical trials. The Lancet, 355(9209):1064–1069, 2000. Susan Athey and Guido Imbens. Identiﬁcation and inference in nonlinear diﬀerence-in-diﬀerences\nmodels. Econometrica, 74(2):431–497, 2006. Susan Athey and Guido Imbens. A measure of robustness to misspeciﬁcation. The American\nEconomic Review, 105(5):476–480, 2015. Susan Athey and Guido Imbens. The econometrics of randomized experiments. arXiv preprint,\n2016. Susan Athey and Guido Imbens. Recursive partitioning for estimating heterogeneous causal\neﬀects. Proceedings of the National Academy of Science, forthcoming. Susan Athey, Dean Eckles, and Guido Imbens.",
    "content_hash": "c5637dd61ef8bde753042048049d7caecae90bccd8f12e3da43ad170756a90d2",
    "location": null,
    "page_start": 62,
    "page_end": 62,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d5e9994d-b2a2-4998-b106-b4696939e6d5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "In\nProceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and\ndata mining, pages 129–138. ACM, 2009. Sandra Black. Do better schools matter? parental valuation of elementary education. Quarterly\nJournal of Economics, 114, 1999. A Bloniarz, H Liu, Zhang C, Sekhon Jasjeet, and Bin Yu. Lasso adjustments of treatment eﬀect\nestimates in randomized experiments. To Appear: Proceedings of the National Academy of\nSciences, 2016. [62]",
    "content_hash": "b7f5924e8027fd1b2a0b3628029e592f88011ab056be48a8c480ba9cd9898975",
    "location": null,
    "page_start": 63,
    "page_end": 63,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0693dbae-b891-4420-a92d-de350d0748bd",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Eﬃcient semiparametric estimation of multi-valued treatment eﬀects under\nignorability. Journal of Econometrics, 155(2):138–154, 2010. Gary Chamberlain and Guido Imbens. Random eﬀects estimators with many instrumental\nvariables. Econometrica, 72(1):295–306, 2004. Arun Chandrasekhar. Arun Chandrasekhar and Matthew Jackson. Technical report. Raj Chetty. Suﬃcient statistics for welfare analysis: A bridge between structural and reduced-\nform methods. Annual Review of Economics, 2009. [63]",
    "content_hash": "d1e2107abedd0a8971a0ca986af1d9acfcd3a2a6ecf2e7b83724862ad3052413",
    "location": null,
    "page_start": 64,
    "page_end": 64,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a057ab3e-1d3a-4403-8b90-e9b61936df82",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Ying Dong and Arthur Lewbel. Identifying the eﬀect of changing the policy threshold in regres-\nsion discontinuity models. Review of Economics and Statistics, 2015. Yingying Dong. Jump or kink? identiﬁcation of binary treatment regression discontinuity design\nwithout the discontinuity. Unpublished manuscript, 2014. Nikolay Doudchenko and Guido Imbens. Balancing, regression, diﬀerence-in-diﬀerences and\nsynthetic control methods: A synthesis. 2016. [64]",
    "content_hash": "17021311b84da934392812b2a780eb52b70ced63170cbc4e94a14d08b3cd9d9c",
    "location": null,
    "page_start": 65,
    "page_end": 65,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "fd1ac186-6dd1-492f-9763-7f81fc7f82bc",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Statistical models for causality: What leverage do they provide. Evaluation\nReview, 30(0):691–713, 2006. David Freedman. On regression adjustmens to experimental data. Advances in Applied\nMathematics, 30(6):180–193, 2008. Andrew Gelman and Guido Imbens. Why high-order polynomials should not be used in regres-\nsion discontinuity designs. 2014. [65]",
    "content_hash": "cf0300536122e07c264392d20edc3134428a395ba22b3a988edc02dc21b453d3",
    "location": null,
    "page_start": 66,
    "page_end": 66,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "903ad0a3-5f79-418e-92ab-6ba567b68d3c",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Wenfei Du, Jonathan Taylor, Robert Tibshirani, and Wager Stefan. High-dimensional regression\nadjustments in randomized experiments. arXiv preprint, 2016. Miroslav Dud´ık, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In Proceedings of the 28th International Conference on Machine Learning, pages 1097–1104,\n2011. Kizilcec R. Eckles, D. and E. Bakshy. Estimating peer eﬀects in networks with peer encourage-\nment designs. Proceedings of the National Academy of Sciences, forthcoming. Avraham Edenstein, Maoyong Fan, Michael Greenstone, Guojun He, and Maigeng Zhou. The\nimpact of sustained exposure to particulate matter on life expectancy: New evidence from\nchina’s huai river policy. 2016. Friedhelm Eicker. Limit theorems for regressions with unequal and dependent errors. In\nProceedings of the ﬁfth Berkeley symposium on mathematical statistics and probability, vol-\nume 1, pages 59–82, 1967. Ronald Fisher. Statistical Methods for Research Workers. Oliver and Boyd, London, 1925. Ronald Fisher. Design of Experiments. Oliver and Boyd, London, 1935. Jared C Foster, Jeremy MG Taylor, and Stephen J Ruberg. Subgroup identiﬁcation from\nrandomized clinical trial data. Statistics in medicine, 30(24):2867–2880, 2011. Constantine E Frangakis and Donald B Rubin. Principal stratiﬁcation in causal inference. Biometrics, 58(1):21–29, 2002. David Freedman.",
    "content_hash": "935d4228335c32aa0afaaaa936e0f1bd123bbb328a035239bfe245a95fcfe499",
    "location": null,
    "page_start": 66,
    "page_end": 66,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d9a36a26-6601-4d70-ba29-09aacb807264",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Matthew Gentzkow and Jesse Shapiro. Measuring the sensitivity of parameter estimates to\nsample statistics. 2015. Edward L Glaeser, Andrew Hillis, Scott Duke Kominers, and Michael Luca. Predictive cities\ncrowdsourcing city government: Using tournaments to improve inspection accuracy. The\nAmerican Economic Review, 106(5):114–118, 2016. Arthur Goldberger. Selection bias in evaluating treatment eﬀects: Some formal illustrations. Discussion Paper 129-72, 1972. Arthur Goldberger. Selection bias in evaluating treatment eﬀects: Some formal illustrations. Advances in Econometrics, 2008. Paul Goldsmith-Pinkham and Guido W Imbens. Social networks and the identiﬁcation of peer\neﬀects. Journal of Business & Economic Statistics, 31(3):253–264, 2013. Bryan Graham, Christine Pinto, and Daniel Egel. Inverse probability tilting for moment condi-\ntion models with missing data. Review of Economic Studies, pages 1053–1079, 2012. Bryan Graham, Christine Pinto, and Daniel Egel. Eﬃcient estimation of data combination\nmodels by the method of auxiliary-to-study tilting (ast). Journal of Business and Economic\nStatistics, pages –, 2016. Bryan S Graham. Identifying social interactions through conditional variance restrictions. Econometrica, 76(3):643–660, 2008. Donald P Green and Holger L Kern. Modeling heterogeneous treatment eﬀects in survey ex-\nperiments with bayesian additive regression trees. Public opinion quarterly, 76(3):491–511,\n2012. Jinyong Hahn. On the role of the propensity score in eﬃcient semiparametric estimation of\naverage treatment eﬀects.",
    "content_hash": "d51210cd6c1f0d09293e27fd20562ebacc1daf0da975255b2754865218443728",
    "location": null,
    "page_start": 67,
    "page_end": 67,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "c7cc51bf-c894-4998-8f77-b68f98650a66",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Jens Hainmueller. Entropy balancing for causal eﬀects: A multivariate reweighting method to\nproduce balanced samples in observational studies. Political Analysis, 20(1):25–46, 2012. Wolfgang H¨ardle. Applied nonparametric regression. Cambridge University Press, 1990. Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. New York: Springer, 2009. Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning with Sparsity:\nThe Lasso and Generalizations. CRC Press, 2015. Jerry A Hausman. Speciﬁcation tests in econometrics. Econometrica:\nJournal of the\nEconometric Society, pages 1251–1271, 1978. James J Heckman and V Joseph Hotz. Choosing among alternative nonexperimental methods\nfor estimating the impact of social programs: The case of manpower training. Journal of the\nAmerican statistical Association, 84(408):862–874, 1989. James J. Heckman and Edward Vytlacil. Econometric evaluation of social programs, causal\nmodels, structural models and econometric policy evaluation. Handbook of Econometrics,\n2007a. James J. Heckman and Edward Vytlacil. Econometric evaluation of social programs, part ii:\nUsing the marginal treatment eﬀct to organize alternative econometric estimators to evaluate\nsocial programs, and to forecast their eﬀects in new environments. Handbook of Econometrics,\n2007b. Miguel Hern´an and James Robins. Estimating causal eﬀects from epidemiology.",
    "content_hash": "e04d4bb7f0f7ff300bcbe3854ec79bf23b7b815a9c31ce9e0e054cedc6b17434",
    "location": null,
    "page_start": 68,
    "page_end": 68,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "39d0e020-0d5f-4270-8d36-699cdcb85d98",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Agnostic notes on regression adjustments for experimental data: Reexamining\nfreedman’s critique. The Annals of Applied Statistics, 7(1), 2013. John A List, Azeem M Shaikh, and Yang Xu. Multiple hypothesis testing in experimental\neconomics. Technical report, National Bureau of Economic Research, 2016. [70]",
    "content_hash": "933c22683f030cb831340e2d08f8501ee1b09c468d9aa496c7084d36bc433305",
    "location": null,
    "page_start": 71,
    "page_end": 71,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "5e1bb1da-25f7-431f-80c4-e93133b38e6b",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "B Jacob and L Lefgren. Remedial education and student achievement: A regression-discontinuity\nanalysis. Review of Economics and Statistics, 68, 2004. Joseph Kang and Joseph Schafer. Demystifying double robustness: A comparison of alternative\nstrategies for estimating a population mean from incomplete data. Statistical Science, 22(4):\n523–529, 2007. Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, and Ziad Obermeyer. Prediction policy\nproblems. The American economic review, 105(5):491–495, 2015. Amanda Kowalski. Doing more when you’re running late: Applying marginal treatment eﬀect\nmethods to examine treatment eﬀect heterogeneity in experiments. 2015. Robert J LaLonde. Evaluating the econometric evaluations of training programs with experi-\nmental data. The American economic review, pages 604–620, 1986. Edward Leamer. Speciﬁcation Searches. Wiley, 1978. Edward E Leamer. Let’s take the con out of econometrics. The American Economic Review,\n73(1):31–43, 1983. Michael Lechner. Identiﬁcation and estimation of causal eﬀects of multiple treatments under\nthe conditional independence assumption. Econometric Evaluations of Active Labor Market\nPolicies in Europe, 2001. David Lee. Randomized experiments from non-random selection in u.s. house elections. Journal\nof Econometrics, 142(2), 2008. David Lee and Thomas Lemieux. Regression discontinuity designs in economics. Journal of\nEconomic Literature, 48, 2010. Winston Lin.",
    "content_hash": "b3dc5d7582430eff710362e99187c64ec1afc4bf912a627af629c8d18daaeb34",
    "location": null,
    "page_start": 71,
    "page_end": 71,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "a324b646-e474-46cd-a863-c53f2fe46382",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Estimating the eﬀect of stu-\ndent aid on college enrollment: Evidence from a government grant policy reform,. American\nEconomic Journal: Economic Policy, 2(2):185215, 2010. Emily Oster. Diabetes and diet: Behavioral response and the value of health. Technical report,\nNational Bureau of Economic Research, 2015. [71]",
    "content_hash": "c38ec656791c38f881797d94ba42bec39d3bcb752f0d86ca448b0074c4380978",
    "location": null,
    "page_start": 72,
    "page_end": 72,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "0486a4e5-3f1f-4de2-b5d1-eb373ee22250",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Charles Manski. Identiﬁcation of endogenous social eﬀects: The reﬂection problem. Review of\nEconomic Studies, 60(3), 1993. Charles F Manski. Nonparametric bounds on treatment eﬀects. The American Economic Review,\n80(2):319–323, 1990. Charles F Manski. Public policy in an uncertain world: analysis and decisions. Harvard Uni-\nversity Press, 2013. J Matsudaira. Mandatory summer school and student achievement. Journal of Econometrics,\n142(2), 2008. Daniel F McCaﬀrey, Greg Ridgeway, and Andrew R Morral. Propensity score estimation\nwith boosted regression for evaluating causal eﬀects in observational studies. Psychological\nMethods, 9(4):403, 2004. Justin McCrary. Testing for manipulation of the running variable in the regression discontinuity\ndesign. Journal of Econometrics, 142(2), 2008. Angelo Mele. A structural model of segregation in social networks. Available at SSRN 2294957,\n2013. Whitney K Newey and Daniel McFadden. Large sample estimation and hypothesis testing. Handbook of econometrics, 4:2111–2245, 1994. Jerzey Neyman. On the application of probability theory to agricultural experiments. essay on\nprinciples. section 9. Statistical Science, pages –, 1923/1990. Jerzey Neyman. Statistical problems in agricultural experimentation ”(with discussion). Journal\nof the Royal Statistal Society, Series B, 0(2):107–180, 1935. Helena Skyt Nielsen, Torben Sorensen, and Christopher Taber.",
    "content_hash": "ca080adbf0e5620537b919a8d408aab3a458694a6d1dcc8a3f716fe1f365d614",
    "location": null,
    "page_start": 72,
    "page_end": 72,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "f8d2e183-d99c-4225-a9a5-d9386d7cc70e",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Taisuke Otsu, Xu Ke-Li, and Yukitoshi Matsushita. Estimation and inference of discontinuity\nin density. Journal of Business and Eonomic Statistics, 2013. Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New\nYork, NY, USA, 2000. ISBN 0-521-77362-8. Giovanni Peri and Vasil Yasenov. The labor market eﬀects of a refugee wave: Applying the syn-\nthetic control method to the mariel boatlift. Technical report, National Bureau of Economic\nResearch, 2015. Jack Porter. Estimation in the regression discontinuity model. 2003. Ross L Prentice. Surrogate endpoints in clinical trials: deﬁnition and operational criteria. Statistics in medicine, 8(4):431–440, 1989. James Robins and Andrea Rotnitzky. Semiparametric eﬃciency in multivariate regression mod-\nels with missing data. Journal of the American Statistical Association, 90(1):122–129, 1995. James Robins, Andrea Rotnitzky, and L.P. Zhao. Analysis of semiparametric regression models\nfor repeated outcomes in the presence of missing data. Journal of the American Statistical\nAssociation, 90(1):106–121, 1995. Paul R Rosenbaum. Observational studies. In Observational Studies. Springer, 2002. Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational\nstudies for causal eﬀects. Biometrika, 70(1):41–55, 1983a. Paul R Rosenbaum and Donald B Rubin.",
    "content_hash": "85b438cfe3db6e8fc4065aa863d8662f5b2b33a9d22b26b745f680c88fce4bb8",
    "location": null,
    "page_start": 73,
    "page_end": 73,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d48e71a5-ca4e-4a73-88d5-ba341238f8c5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Assessing sensitivity to an unobserved binary covariate\nin an observational study with binary outcome. Journal of the Royal Statistical Society. Series\nB (Methodological), pages 212–218, 1983b. Paul R Rosenbaum et al. The role of a second control group in an observational study. Statistical\nScience, 2(3):292–306, 1987. Bruce Sacerdote. Peer eﬀects with random assignment: results for dartmouth roommates. Quarterly Journal of Economics, 116(2):681–704, 2001. [72]",
    "content_hash": "0d3063ea45db90980fd6aa07abaaf3c2486132fd0e6f80e17a4d87ba48c72f62",
    "location": null,
    "page_start": 73,
    "page_end": 73,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "4f74e413-2438-4e43-9968-98ed7228d7d5",
    "source_id": "d8c1e0bc-0e7d-44e5-a883-785efff197e5",
    "content": "Regression-discontinuity analysis: A survey of recent developments\nin economics. Labour, 22(2):219–245, 2008. Mark J Van der Laan, Eric C Polley, and Alan E Hubbard. Super learner. Statistical applications\nin genetics and molecular biology, 6(1), 2007. Stefan Wager and Susan Athey. Causal random forests. arXiv preprint, 2015. [73]",
    "content_hash": "19f8512615173d60a6d9ce05ec59f74960871a4701fd657832f9a0328018fddf",
    "location": null,
    "page_start": 74,
    "page_end": 74,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "econometrics"
  },
  {
    "id": "d1d58ca3-89c1-46b2-874b-badeb20f1a1f",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "ods for doing so, e.g., [14]. For example, as in [10] we can do so by propensity score weighting. Eﬃciency will improve if we renormalize the weights within each leaf and and within the treat-\nment and control group when estimating treatment eﬀects. [5] propose approaches to trimming\nobservations with extreme values for the propensity score to improve robustnesses. Note that\nthere are some additional conditions required to establish asymptotic normality of treatment\neﬀect estimates when propensity score weighting is used (see, e.g., [10]); these results apply\nwithout modiﬁcation to the estimation phase of honest partitioning algorithms. 8\nThe Literature\nA small but growing literature seeks to apply supervised machine learning techniques to the\nproblem of estimating heterogeneous treatment eﬀects. Beyond those previously discussed, [23]\ntransform the features rather than the outcomes and then apply LASSO to the model with\nthe original outcome and the transformed features. [7] estimate µ(w, x) = E[Yi(w)|Xi = x] for\nw = 0, 1 using random forests, then calculate ˆτi = ˆµ(1, Xi) −ˆµ(0, Xi). They then use machine\nlearning algorithms to estimate ˆτi as a function of the units’ attributes, Xi. Our approach diﬀers\nin that we apply machine learning methods directly to the treatment eﬀect in a single stage\nprocedure. [13] use LASSO to estimate the eﬀects of both treatments and attributes, but with\ndiﬀerent penalty terms for the two types of features to allow for the possibility that the treatment\neﬀects are present but the magnitudes of the interactions are small.",
    "content_hash": "3cc8876dfc0cee6e7182fe83a21e69f6087770dc3b96768fbec61c7d103c8ac5",
    "location": null,
    "page_start": 1,
    "page_end": 16,
    "metadata": {
      "section": "The Literature",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ecbff217-7e90-4040-ab25-d910043e9531",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "arXiv:1504.01132v3  [stat.ML]  30 Dec 2015\nRecursive Partitioning for Heterogeneous Causal Eﬀects∗\nSusan Athey†\nGuido W. Imbens‡\nFirst Draft: October 2013\nThis Draft: December 2015\nAbstract\nIn this paper we propose methods for estimating heterogeneity in causal eﬀects in exper-\nimental and observational studies, and for conducting hypothesis tests about the magnitude\nof the diﬀerences in treatment eﬀects across subsets of the population. We provide a data-\ndriven approach to partition the data into subpopulations which diﬀer in the magnitude of\ntheir treatment eﬀects. The approach enables the construction of valid conﬁdence intervals\nfor treatment eﬀects, even in samples with many covariates relative to the sample size, and\nwithout “sparsity” assumptions. To accomplish this, we propose an “honest” approach to\nestimation, whereby one sample is used to construct the partition and another to estimate\ntreatment eﬀects for each subpopulation. Our approach builds on regression tree methods,\nmodiﬁed to optimize for goodness of ﬁt in treatment eﬀects and to account for honest esti-\nmation. Our model selection criteria focus on improving the prediction of treatment eﬀects\nconditional on covariates, anticipating that bias will be eliminated by honest estimation,\nbut also accounting for the change in the variance of treatment eﬀect estimates within each\nsubpopulation as a result of the split. We also address the challenge that the “ground truth”\nfor a causal eﬀect is not observed for any individual unit, so that standard approaches to\ncross-validation must be modiﬁed.",
    "content_hash": "858194b9d35486fc644c460860493898c6eac71f1846344b957d40e5745a5a04",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "arXiv:1504.01132v3  [stat.ML]  30 Dec 2015",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "26d2ebca-3bd1-4e26-b1f9-5df41ce1f74d",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Through a simulation study, we show that honest esti-\nmation can result in substantial improvements in coverage of conﬁdence intervals, where our\nmethod attains nominal coverage rates, without much sacriﬁce in terms of ﬁtting treatment\neﬀects. In this paper we study two closely related problems: ﬁrst, estimating heterogeneity by\ncovariates or features in causal eﬀects in experimental or observational studies, and second,\nconducting inference about the magnitude of the diﬀerences in treatment eﬀects across subsets\nof the population. Causal eﬀects, in the Rubin Causal Model or potential outcome framework\nwe use here ([19], [11], [14]), are comparisons between outcomes we observe and counterfactual\noutcomes we would have observed under a diﬀerent regime or treatment. We introduce data-\ndriven methods that select subpopulations to estimate more precisely average treatment eﬀects\n∗We are grateful for comments provided at seminars at the National Academy of Science Sackler\nColloquium, the Southern Economics Association, the Stanford Conference on Causality in the Social\nSciences, the MIT Conference in Digital Experimentation, Harvard, University of Washington, Cornell,\nMicrosoft Research, Facebook, KDD, the AAAI Embedded Machine Learning Conference, the University\nof Pennsylvania, the University of Arizona. Part of this research was conducted while the authors were\nvisiting Microsoft Research. †Graduate School of Business,\nStanford University,\nand NBER. Electronic correspondence:\nathey@stanford.edu\n‡Graduate School of Business, Stanford University, and NBER. Electronic correspondence:\nim-\nbens@stanford.edu\n[1]",
    "content_hash": "5470f7fd4f2a4e15744cb6a2f27432a3b96df656d45e920cc55a931263405a82",
    "location": null,
    "page_start": 1,
    "page_end": 1,
    "metadata": {
      "section": "This Draft: December 2015",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e725ed58-66fb-4014-931f-b5cd2990de4e",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "and to test hypotheses about the diﬀerences between the eﬀects in diﬀerent subpopulations. For experiments, our method allows researchers to identify heterogeneity in treatment eﬀects\nthat was not speciﬁed in a pre-analysis plan, without concern about invalidating inference due\nto concerns about multiple testing. Our approach is tailored for applications where there may be many attributes of a unit rela-\ntive to the number of units observed, and where the functional form of the relationship between\ntreatment eﬀects and the attributes of units is not known. The supervised machine learning\nliterature (e.g. [9]) has developed a variety of eﬀective methods for a closely related problem,\nthe problem of predicting outcomes as a function of covariates in similar environments. The\nmost popular approaches (e.g. regression trees ([4]), random forests ([3]), LASSO ([24]), sup-\nport vector machines ([26]), etc.) entail building a model of the relationship between attributes\nand outcomes, with a penalty parameter that penalizes model complexity. Cross-validation is\noften used to select the optimal level of complexity (the one that maximizes predictive power\nwithout “overﬁtting”). Within the prediction-based machine learning literature, regression trees diﬀer from most\nother methods in that they produce a partition of the population according to covariates,\nwhereby all units in a partition receive the same prediction. In this paper, we focus on the\nanalogous goal of deriving a partition of the population according to treatment eﬀect het-\nerogeneity, building on standard regression trees ([4], [3]).",
    "content_hash": "03a682e58d973b0673edf06d91a08102a26acef1b798ec27807a3c4f0b05e00b",
    "location": null,
    "page_start": 1,
    "page_end": 2,
    "metadata": {
      "section": "This Draft: December 2015",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b63cc355-4cb4-4927-b04a-93265b5ccbdf",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "The\noutput of our method is a set of treatment eﬀects and conﬁdence intervals for each subspace. A potentially important application of the techniques is to “data-mining” in randomized ex-\nperiments. Our method can be used to explore any previously conducted randomized controlled\ntrial, for example, medical studies or ﬁeld experiments in developed economics. A researcher can\napply our methods and discover subpopulations with lower-than-average or higher-than-average\n[16]",
    "content_hash": "a5fdeb47d4a1cae679ae4fe7b53d94b767b6d62d45ca608e1c632768bb7318b3",
    "location": null,
    "page_start": 1,
    "page_end": 16,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c914bf2d-a839-4ec3-b01b-04b6e7be0769",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Whether the ultimate goal in an\napplication is to derive a partition or fully personalized treatment eﬀect estimates depends on\nthe setting; settings where partitions may be desirable include those where decision rules must\nbe remembered, applied or interpreted by human beings or computers with limited processing\npower or memory. Examples include treatment guidelines to be used by physicians or even\nonline personalization applications where having a simple lookup table reduces latency for the\nuser. We show that an attractive feature of focusing on partitions is that we can achieve nominal\nconverage of conﬁdence intervals for estimated treatment eﬀects even in settings with a modest\nnumber of observations and many covariates. Our approach has applicability even for settings\nsuch as clinical trials of drugs with only a few hundred patients, where the number of patient\ncharacteristics is potentially quite large. Our method may also be viewed as a complement to\nthe use of “pre-analysis plans” where the researcher must commit in advance to the subgroups\nthat will be considered for analysis. It enables researchers to let the data discover relevant\nsubgroups without falling prey to concerns of multiple hypothesis testing that would invalidate\np-values. A ﬁrst challenge for our goal of ﬁnding a partition and then testing hypotheses about\ntreatment eﬀects is that many existing machine learning methods cannot be used directly for\nconstructing conﬁdence intervals. This is because the methods are “adaptive”: they use the\ntraining data for model selection, so that spurious correlations between covariates and outcomes\naﬀect the selected model, leading to biases that disappear only slowly as the sample size grows.",
    "content_hash": "4e13da90a62d336d315d4677c8eb9088570a7393add1705192d5b90437f989e9",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "This Draft: December 2015",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f4636e6a-d2e4-4956-b24d-7829486a1627",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "In some contexts, additional assumptions such as “sparsity” (only a few covariates aﬀect the\noutcomes) can be applied to guarantee consistency or asymptotic normality of predictions ([28]). In this paper, we use an alternative approach that places no restrictions on model complexity,\nwhich we refer to as “honesty.” We say that a model is “honest” if it does not use the same\ninformation for selecting the model structure (in our case, the partition of the covariate space)\nand for estimation given a model structure. We accomplish this by splitting the training\nsample into two parts, one for constructing the tree (including the cross-validation step), and a\nsecond for estimating treatment eﬀects within leaves of the tree, implying that the asymptotic\n[2]",
    "content_hash": "9074f18cd15c00a49ec6bdd1226850a594484a53bf80efd4aaf302c83dff50f8",
    "location": null,
    "page_start": 2,
    "page_end": 2,
    "metadata": {
      "section": "This Draft: December 2015",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7a24befe-82d6-422f-bcc3-f9bced1b68a9",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": ", N,\nwhich are regarded as an i.i.d sample drawn from a large population. Expectations and prob-\nabilities will refer to the distribution induced by the random sampling, or by the (conditional)\nrandom assignment of the treatment. We assume that observations are exchangeable, and that\nthere is no interference (the stable unit treatment value assumption, or sutva [20]). This as-\nsumption may be violated in settings where some units are connected through networks. Let\n[3]",
    "content_hash": "31ff2f161abcb897f6d12b426b7c8ac57d78b930f84e73984e38961d92d0ddcd",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "The Set Up",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "d4a0acee-078a-424f-8b81-5d0f2d8355df",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "properties of treatment eﬀect estimates within the partitions are the same as if the partition\nhad been exogenously given. Although there is a loss of precision due to sample splitting (which\nreduces sample size in each step of estimation), there is a beneﬁt for ﬁt in terms of eliminating\nbias that oﬀsets at least part of the cost. A novel contribution of this paper is to show that criteria for both constructing the par-\ntition and cross-validation change when we anticipate honest estimation. In the ﬁrst stage of\nestimation, the criteria is the expectation of the mean-squared error when treatment eﬀects\nare re-estimated in the second stage. Crucially, we anticipate that second-stage estimates of\ntreatment eﬀects will be unbiased in each leaf, since they will be performed on a separate (and\nindependent) sample. In that case, splitting and cross-validation criteria are adjusted to ignore\nsystematic bias in estimation, and focus instead on the tradeoﬀbetween more tailored predic-\ntion (smaller leaf size) and the variance that will arise in the second (honest estimation) stage\ndue to noisy estimation within small leaves. A second and perhaps more fundamental challenge to applying machine learning methods\nsuch as regression trees [4] oﬀ-the-shelf to the problem of causal inference is that regularization\napproaches based on cross-validation typically rely on observing the “ground truth,” that is,\nactual outcomes in a cross-validation sample. However, if our goal is to minimize the mean-\nsquared error of treatment eﬀects, we encounter what [11] calls the “fundamental problem\nof causal inference”: the causal eﬀect is not observed for any individual unit, and so we don’t\ndirectly have a ground truth.",
    "content_hash": "892a3d1a13f474ece9f73ffd84972634a7d50667448ffd47eab1b6f12f529f4f",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "This Draft: December 2015",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f454aeed-d820-4931-8e06-b84d2ef91910",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "We address this by proposing approaches for constructing unbiased\nestimates of the mean-squared error of the causal eﬀect of the treatment. Using theoretical arguments and a simulation exercise we evaluate the costs and beneﬁts of\nhonest estimation and compare our approach with previously proposed ones. We ﬁnd in the\nsettings we consider that honest estimation results in improvements in ﬁt, in some cases very\nlarge improvements, over a more traditional “adaptive” estimation approach. 1\nThe Problem\n1.1\nThe Set Up\nWe consider a setup where there are N units, indexed by i = 1, . . . , N. We postulate the\nexistence of a pair of potential outcomes for each unit, (Yi(0), Yi(1)) (following the potential\noutcome or Rubin Causal Model [19], [11], [14], with the unit-level causal eﬀect deﬁned as the\ndiﬀerence in potential outcomes, τi = Yi(1) −Yi(0). Let Wi ∈{0, 1} be the binary indicator for\nthe treatment, with Wi = 0 indicating that unit i received the control treatment, and Wi = 1\nindicating that unit i received the active treatment. The realized outcome for unit i is the\npotential outcome corresponding to the treatment received:\nY obs\ni\n= Yi(Wi) =\n\u001a Yi(0)\nif Wi = 0,\nYi(1)\nif Wi = 1. Let Xi be a K-component vector of features, covariates or pretreatment variables, known not\nto be aﬀected by the treatment. Our data consist of the triple (Y obs\ni\n, Wi, Xi), for i = 1, . . .",
    "content_hash": "8adcdf25bcc4fcd2304159f6632c90b722ad07aee4648a728f42b5bc5f111f39",
    "location": null,
    "page_start": 3,
    "page_end": 3,
    "metadata": {
      "section": "This Draft: December 2015",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ba6adbb9-b383-4045-a141-db36ead06c9b",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "[14], [15]) is focused on estimating the\npopulation (marginal) average treatment eﬀect E[Yi(1) −Yi(0)]. The main focus of the current\npaper is on obtaining accurate estimates of and inferences for the conditional average treatment\neﬀect τ(x). We are interested in estimators ˆτ(·) that are based on partitioning the feature space,\nand do not vary within the partitions. 2\nHonest Inference for Population Averages\nOur approach departs from conventional classiﬁcation and regression trees (CART) in two\nfundamental ways. First, we focus on estimating conditional average treatment eﬀects rather\nthan predicting outcomes. This creates complications for conventional methods because we\ndo not observe unit level causal eﬀects for any unit. Second, we impose a separation between\nconstructing the partition and estimating eﬀects within leaves of the partition, using separate\nsamples for the two tasks, in what we refer to as “honest” estimation. We contrast ‘’honest”\nestimation with “adaptive” estimation used in conventional CART, where the same data is\nused to build the partition and estimate leaf eﬀects. In this section we introduce the changes\ninduced by honest estimation in the context of the conventional prediction setting; in the next\nsection we consider causal eﬀects. In the discussion in this section we observe for each unit i a\npair of variables (Yi, Xi), with the interest in the conditional expectation µ(x) ≡E[Yi|Xi = x]. [4]",
    "content_hash": "1ba587e53ce824387b2b405e2126b21cccd3f6892cc6f99a6b6d9350c4008753",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "Conditional Average Treatment Eﬀects and Partitioning",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c41f16fe-d743-4132-8d35-f4d5eae8abcd",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "p = pr(Wi = 1) be the marginal treatment probability, and let e(x) = pr(Wi = 1|Xi = x) be the\nconditional treatment probability (the “propensity score” as deﬁned by [17]). In a randomized\nexperiment with constant treatment assignment probabilities e(x) = p for all values of x. 1.2\nUnconfoundedness\nThroughout the paper, we maintain the assumption of randomization conditional on the co-\nvariates, or “unconfoundedness” ([17]), formalized as:\nAssumption 1. (Unconfoundedness)\nWi ⊥⊥\n\u0010\nYi(0), Yi(1)\n\u0011 \f\f\f Xi. This assumption, sometimes referred to as “selection on observables” in the econometrics\nliterature, is satisﬁed in a randomized experiment without conditioning on covariates, but also\nmay be justiﬁed in observational studies if the researcher is able to observe all the variables\nthat aﬀect the unit’s receipt of treatment and are associated with the potential outcomes. To simplify exposition, in the main body of the paper we maintain the stronger assumption\nof complete randomization, whereby Wi ⊥⊥(Yi(0), Yi(1), Xi). Later we show that by using\npropensity score weighting [19], we can adapt all of the methods to that case. 1.3\nConditional Average Treatment Eﬀects and Partitioning\nDeﬁne the conditional average treatment eﬀect (CATE)\nτ(x) ≡E[Yi(1) −Yi(0)|Xi = x]. A large part of the causal inference literature (e.g.",
    "content_hash": "a1a72980b3e617566d1e364597bb038f40a90d8fd0f06e60ce73e1d94def921b",
    "location": null,
    "page_start": 4,
    "page_end": 4,
    "metadata": {
      "section": "The Set Up",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "564148e7-d39d-4e95-be30-52cc6b951926",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "While Y L −Y R is in general an unbiased estimator for the diﬀerence in the population\nconditional means µ(L) −µ(R), if we condition on ﬁnding that Y L −Y R ≥c in a particular\nsample, we expect that Y L −Y R is larger than the population analog. Given a partition Π, deﬁne the conditional mean function µ(x; Π) as\nµ(x; Π) ≡E[Yi|Xi ∈ℓ(x; Π)] = E[µ(Xi)|Xi ∈ℓ(x; Π)],\nwhich can be viewed as a step-function approximation to µ(x). Given a sample S the estimated\ncounterpart is\nˆµ(x; S, Π) ≡\n1\n#(i ∈S : Xi ∈ℓ(x; Π))\nX\ni∈S:Xi∈ℓ(x;Π)\nYi,\nwhich is unbiased for µ(x; Π). We index this estimator by the sample because we need to be\nprecise which sample is used for estimation of the regression function. 2.2\nThe Honest Target\nA central concern in this paper is the criterion used to compare alternative estimators; following\nmuch of the literature, we focus on Mean-squared error (MSE) criteria, but we will modify these\ncriteria in a variety of ways. For the prediction case, we adjust the MSE by E[Y 2\ni ]; since this does\nnot depend on an estimator, subtracting it does not aﬀect how the criterion ranks estimators.",
    "content_hash": "239365e945a9d603abf0b30082b90ff893e0a36048eef446bdbcc812c635f19f",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "Set Up",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ba07f39e-ac5d-4445-a0a2-4d4a0e3a59ff",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "2.1\nSet Up\nWe begin by deﬁning key concepts and functions. First, a tree or partitioning Π corresponds\nto a partitioning of the feature space X, with #(Π) the number of elements in the partition. We write\nΠ = {ℓ1, . . . , ℓ#(T )},\nwith ∪#(Π)\nj=1 ℓj = X. Let P denote the space of partitions. Let ℓ(x; Π) denote the leaf ℓ∈Π such that x ∈ℓ. Let S be\nthe space of data samples from a population. Let π : S 7→P be an algorithm that on the basis\nof a sample S ∈S constructs a partition. As a very simple example, suppose the feature space\nis X = {L, R}. In this case there are two possible partitions, ΠN = {L, R} (no split), or ΠS =\n{{L}, {R}} (full split), and so the space of trees is P = {ΠN, ΠS} = {{L, R}, {{L}, {R, }}}. Given a sample S, the average outcomes in the two subsamples are Y L and Y R. A simple\nexample of an algorithm is one that splits if the diﬀerence in average outcomes exceeds a\nthreshold c:\nπ(S) =\n\u001a {{L, R}}\nif Y L −Y R ≤c,\n{{L}, {R}}\nif Y L −Y R > c. The potential bias in leaf estimates from adaptive estimation can be seen in this simple exam-\nple.",
    "content_hash": "73c205d4678c11b29dcc5f2a8091e7d70e52c6d145e899f56deb859ca8d1cf5a",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "Set Up",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2a0f8cf4-71ad-4d32-9715-7ec70de5a816",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Given a partition Π, deﬁne the mean squared error, where we average over a test sample Ste\nand the conditional mean is estimated on an estimation sample Sest, as\nMSE(Ste, Sest, Π) ≡\n1\n#(Ste)\nX\ni∈Ste\nnYi −ˆµ(Xi; Sest, Π)\n\u00012 −Y 2\ni\no\n. [5]",
    "content_hash": "e4f03405ea49021b3c052f2f017f56775a78883a87529cfc76146b23210e73fa",
    "location": null,
    "page_start": 5,
    "page_end": 5,
    "metadata": {
      "section": "The Honest Target",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "37df5540-62b3-40fd-b421-adadc2d5a085",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "We refer to the\nconventional CART approach as “adaptive,” and our approach as “honest.”\nIn practice there will be costs and beneﬁts of the honest approach relative to the adaptive\napproach. The cost is sample size; given a data set, putting some data in the estimation sample\nleaves fewer units for the training data set. The advantage of honest estimation is that it avoids\na problem of adaptive estimation, which is that spurious extreme values of Yi are likely to be\nplaced into the same leaf as other extreme values by the algorithm π(·), and thus the sample\nmeans (in sample Str) of the elements of π(Str) are more extreme than they would be in an\nindependent sample. 2.4\nThe Implementation of CART\nThere are two distinct parts of the conventional CART algorithm, initial tree building and\ncross-validation to select a complexity parameter used for pruing. Each part of the algorithm\nrelies on a criterion function based on mean-squared error. In this paper we will take as given\nthe overall structure of the CART algorithm (e.g., [4], [9]), and our focus will be on modifying\nthe criteria. In the tree-building phase, CART recursively partitions the observations of the training\nsample. For each leaf, the algorithm evaluates all candidate splits of that leaf (which induce al-\nternative partitions Π) using a “splitting” criterion that we refer to as the “in-sample” goodness\nof ﬁt criterion −MSE(Str, Str, Π).",
    "content_hash": "ddd6154c4493e37b0ffecf518cfed8d3d1604bdb5ae867bc60b494933a45da41",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "The Adaptive Target",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9ddfa6de-1679-4ae6-b213-eeeb8b0474f5",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "The (adjusted) expected mean squared error is the expectation of MSE(Ste, Sest, Π) over the\ntest sample and the estimation sample:\nEMSE(Π) ≡ESte,Sest\n\u0002\nMSE(Ste, Sest, Π)\n\u0003\n,\nwhere the test and estimation samples are independent. In the algorithms we consider, we will\nconsider a variety of estimators for the (adjusted) EMSE, all of which take the form of MSE\nestimators MSE(Ste, Sest, Π), evaluated at the units in sample Ste, with the estimates based\non sample Sest and the tree Π. For brevity in this paper we will henceforth omit the term\n“adjusted” and abuse terminology slightly by referring to these objects as MSE functions. Our ultimate goal is to construct and assess algorithms π(·) that maximize the “honest”\ncriterion\nQH(π) ≡−ESest,Sest,Str\n\u0002\nMSE(Ste, Sest, π(Str))\n\u0003\n. Note that throughout the paper we focus on maximixing criterion functions, which typically\ninvolve the negative of mean-squared-error expressions. 2.3\nThe Adaptive Target\nIn the conventional CART approach the target is slightly diﬀerent:\nQC(π) ≡−ESte,Str\n\u0002\nMSE(Ste, Str, π(Str))\n\u0003\n,\nwhere the same training sample is used to construct and estimate the tree. Compared to our\ntarget QH(π) the diﬀerence is that in our approach diﬀerent samples Str and Sest are used for\nconstruction of the tree and estimation of the conditional means respectively.",
    "content_hash": "7183ebe7799b589ae273d36ec4a785925afa15fda8982ba3f434426f88ea0722",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "The Honest Target",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "85084533-5a87-4f9c-859e-b28148636ac3",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "It is well-understood that the conventional criterion leads to\n“over-ﬁtting,” a problem that is solved by cross-validation to select a penalty on tree depth. The in-sample goodness of ﬁt criterion will always improve with additional splits, even though\n[6]",
    "content_hash": "741b4eeca0468ffcc1da5d6419ac2f9693983fc1b620e9c52fb10dd6b75a746b",
    "location": null,
    "page_start": 6,
    "page_end": 6,
    "metadata": {
      "section": "The Implementation of CART",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "9d219ff1-b5c8-4ef3-b6be-08794737b640",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "To construct an estimator for the second\nterm, observe that within each leaf of the tree there is an unbiased estimator for the variance\nof the estimated mean in that leaf. Speciﬁcally, to estimate the variance of ˆµ(x; Sest, Π) on the\ntraining sample we can use\nbV(ˆµ(x; Sest, Π)) ≡S2\nStr(ℓ(x; Π))\nN est(ℓ(x; Π)),\nwhere S2\nStr(ℓ) is the within-leaf variance, to estimate the variance. We then weight this by the\nleaf shares pℓto estimate the expected variance. Assuming the leaf shares are approximately\n[7]",
    "content_hash": "27d8a052eca23fc0fbf917f96c1bc866551b40a48f6dd9e31221255cef69b654",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Honest Splitting",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b9e770d1-36dc-4d8e-90d8-dd143e20c715",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "The issue that smaller leaves lead to noisier estimates of leaf\nmeans is implicitly incorporated by the fact that a smaller leaf penalty will lead to deeper trees\nand thus smaller leaves, and the noisier estimates will lead to larger average MSE across the\ncross-validation samples. 2.5\nHonest Splitting\nIn our honest estimation algorithm, we modify CART in two ways. First, we use an independent\nsample Sest instead of Str to estimate leaf means. Second (and closely related), we modify our\nsplitting and cross-validation criteria to incorporate the fact that we will generate unbiased\nestimates using Sest for leaf estimation (eliminating one aspect of over-ﬁtting), where Sest is\ntreated as a random variable in the tree building phase. In addition, we explicitly incorporate\nthe fact that ﬁner partitions generate greater variance in leaf estimates. To begin developing our criteria, let us expand EMSE(Π):\n−EMSE(Π) = −E(Yi,Xi),Sest[(Yi −µ(Xi; Π))2 −Y 2\ni ]\n−EXi,Sest\nhˆµ(Xi; Sest, Π) −µ(Xi; Π)\n\u00012i\n=\nEXi[µ2(Xi; Π)] −ESest,Xi\n\u0002\nV(ˆµ2(Xi; Sest, Π)\n\u0003\n,\nwhere we exploit the equality ES[ˆµ(x; S, Π)] = µ(x; Π). We wish to estimate −EMSE(Π) on the basis of the training sample Str and knowledge\nof the sample size of the estimation sample N est.",
    "content_hash": "30b046b6936defa1dea3da976a4cb2aef7ac2f8c03d3897fa3e9e5164558b25c",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "Honest Splitting",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "0cdbcc2b-32e0-4274-a0bc-c53cd401072e",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "additional reﬁnements of a partition Π might in fact increase the expected mean squared error,\nespecially when the leaf sizes become small. The reason is that the criterion ignores the fact\nthat smaller leaves lead to higher-variance estimates of leaf means. To account for this factor, the conventional approach to avoiding “overﬁtting” is to add a\npenalty term to the criterion that is equal to a constant times the number of splits, so that\nessentially we only consider splits where the improvement in a goodness-of-ﬁt criterion is above\nsome threshold. The penalty term is choosen to maximize a goodness of ﬁt criterion in cross-\nvalidation samples. In the conventional cross-validation the training sample is repeatedly split\ninto two subsamples, the Str,tr sample that is used to build a new tree as well as estimate the\nconditional means and the Str,cv sample that is used to evaluate the estimates. We “prune”\nthe tree using a penalty parameter that represents the cost of a leaf. We choose the optimal\npenalty parameter by evaluating the trees associated with each value of the penalty parameter. The goodness of ﬁt criterion for cross-validation can be written as −MSE(Str,cv, Str,tr, Π). Note\nthat the cross-validation criterion directly addresses the issue we highlighted with the in-sample\ngoodness of ﬁt criterion, since Str,cv is independent of Str,tr, and thus too-extreme estimates\nof leaf means will be penalized.",
    "content_hash": "46bc4c7cff36d571ddaa4fdd8231bbc625f4a2bb06589d11265bdec867825cfd",
    "location": null,
    "page_start": 7,
    "page_end": 7,
    "metadata": {
      "section": "The Implementation of CART",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "74da17dc-7c95-4897-8477-72e8683fe8ee",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Thus,\n\\\nEMSE(Str, Π) is likely to overstate goodness of ﬁt as we grow a deeper and deeper tree, implying\nthat cross-validation can still play an important role with our honest estimation approach,\nthough perhaps less so than in the conventional CART. [8]",
    "content_hash": "6c39fce873e43bc551937b55e249175ddf3272b273113632fa2c04f75c5f69d8",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "Honest Crossvalidation",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "79a57943-865c-466c-952b-d49756f81bda",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "the same in the estimation and training sample, we can approximate this variance estimator by\nbE\n\u0002\nV(ˆµ2(Xi; Sest, Π)|i ∈Ste\u0003\n≡\n1\nN est ·\nX\nℓ∈Π\nS2\nStr(ℓ). To estimate the average of the squared outcome µ2(x; Π) (the ﬁrst term of the target crite-\nrion), we can use the square of the estimated means in the training sample ˆµ2(x; Π), minus an\nestimate of its variance,\nbE[µ2(x; Π)] = ˆµ2(x; Str, Π) −S2\nStr(ℓ(x; Π))\nN tr(ℓ(x; Π)) . Combining these estimators leads to the following unbiased estimator for EMSE(Π), denoted\n\\\nEMSE(Str, N est, Π):\n1\nN tr\nX\ni∈Str\nˆµ2(Xi; Str, Π) −\n\u0012 1\nN tr +\n1\nN est\n\u0013\n·\nX\nℓ∈Π\nS2\nStr(ℓ). In practice we use the same sample size for the estimation sample and the training sample, so\nwe use as the estimator\n\\\nEMSE(Str, Π) ≡\n1\nN tr\nX\ni∈Str\nˆµ2(Xi; Str, Π) −\n2\nN tr ·\nX\nℓ∈Π\nS2\nStr(ℓ).",
    "content_hash": "fa7b464ea21e5996da5c7ba3bfd19ca9f88aa84c4365e30b3751ba16c545ee91",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "Honest Splitting",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "6c0e074e-d30c-4749-b379-924658ced550",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Comparing this to the criterion used in the conventional CART algorithm, which can be written\nas\nMSE(Str, Str, Π) =\n1\nN tr\nX\ni∈Str\nˆµ2(Xi; Str, Π),\nthe diﬀerence comes from the terms involving the variance. In the prediction setting the adjust-\nment makes very little diﬀerence. Because of the form of the within-leaf sample variances, it fol-\nlows that the gain from a particular split according to the unadjusted criterion MSE(Str, Str, Π)\nis proportional to the gain based on \\\nEMSE(Str, Π), with the constant of proportionality a func-\ntion of the leaf size. Thus, in contrast to the treatment eﬀect case discussed below, the variance\nadjustment does matter much here. 2.6\nHonest Crossvalidation\nEven though \\\nEMSE(Str, Π) is approximately unbiased as an estimator of our ideal criterion\nEMSE(Π) for a ﬁxed Π, it is not unbiased when we use it repeatedly to evaluate splits using\nrecursive partitioning on the training data Str. The reason is that initial splits tend to group\ntogether observations with similar, extreme outcomes. So after the training data has been\ndivided once, the sample variance of observations in the training data within a given leaf\nis on average lower than the sample variance would be in a new, independent sample.",
    "content_hash": "590e599528b5bfa44da9fcd5d9c939a7de8ce5437e16425c155992bbe896e58c",
    "location": null,
    "page_start": 8,
    "page_end": 8,
    "metadata": {
      "section": "Honest Crossvalidation",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "17e97506-a17b-4752-89ec-5c29e09d3537",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Given a tree Π, deﬁne for all x and both\ntreatment levels w the population average outcome\nµ(w, x; Π) ≡E [Yi(w)| Xi ∈ℓ(x; Π)] ,\nand the average causal eﬀect\nτ(x; Π) ≡E [Yi(1) −Yi(0)| Xi ∈ℓ(x; Π)] . The estimated counter parts are\nˆµ(w, x; S, Π) ≡\n1\n#({i ∈Sw : Xi ∈ℓ(x; Π)})\nX\ni∈Sw:Xi∈ℓ(x;Π)\nY obs\ni\n,\nand\nˆτ(x; S, Π) ≡ˆµ(1, x; S, Π) −ˆµ(0, x; S, Π). Deﬁne the mean-squared error for treatment eﬀects as\nMSEτ(Ste, Sest, Π) ≡\n1\n#(Ste)\nX\ni∈Ste\nnτi −ˆτ(Xi; Sest, Π)\n\u00012 −τ 2\ni\no\n,\nand deﬁne EMSEτ(Π) to be its expectation over the estimation and test samples,\nEMSEτ(Π) ≡ESte,Sest\n\u0002\nMSEτ(Ste, Sest, Π)\n\u0003\n. A key challenge is that the workhorse mean-squared error function MSEτ(Ste, Sest, Π) is\ninfeasible, because we do not observe the τi. However, we show below that we can estimate it. [9]",
    "content_hash": "81f02d45ec8b587daac5616ea044812fa40403ed4f53cfb22b1ca26f90fe7ce3",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Modifying Conventional CART for Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "f0261075-e17b-4163-80a8-f64dbe3e6091",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Because the conventional CART cross-validation criterion does not account for honest esti-\nmation we consider the analogue of our unbiased estimate of the criterion, which accounts for\nhonest estimation by evaluating a partition Π using only outcomes for units from the cross-\nvalidation sample Str,cv:\n−\\\nEMSE(Str,cv, Π)\nThis estimator for the honest criterion is unbiased, although it may have higher variance than\nMSE(Str,cv, Str,tr, Π) due to the small sample size of the cross-validation sample. 3\nHonest Inference for Treatment Eﬀects\nIn this section we change the focus to estimating conditional average treatment eﬀects instead of\nestimating conditional population means. We refer to the estimators developed in this section\nas “Causal Tree” (CT) estimators. The setting with treatment eﬀects creates some speciﬁc\nproblems because we do not observe the value of the treatment eﬀect whose conditional mean\nwe wish to estimate. This complicates the calculation of the criteria we introduced in the\nprevious section. However, a key point of this paper is that we can estimate these criteria and\nuse those estimates for splitting and cross-validation. We now observe in each sample the triple (Y obs\ni\n, Xi, Wi). For a sample S let Streat and\nScontrol denote the subsamples of treated and control units respectively, with cardinality Ntreat\nand Ncontrol respectively, and let p = Ntreat/N be the share of treated units. The concept of\na tree remains the same as in the previous section.",
    "content_hash": "f13ee30924f3952a17500919fdfa85eedef833f426a6ccf149be92f049e55c2c",
    "location": null,
    "page_start": 9,
    "page_end": 9,
    "metadata": {
      "section": "Honest Inference for Treatment Eﬀects",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b2140df6-27b4-4cea-92ec-c1388ebd3411",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Again, the\ntreatment eﬀect analog is infeasible, but we can use an unbiased estimate of it, which leads to\n−[\nMSEτ(Str,cv, Str,tr, Π). 3.2\nModifying the Honest Approach\nThe honest approach described in the previous section for prediction problems also needs to be\nmodiﬁed for the treatment eﬀect setting. Using the same expansion as before, now applied to\nthe treatment eﬀect setting, we ﬁnd\n−EMSEτ(Π) = EXi[τ 2(Xi; Π)] −ESest,Xi\n\u0002\nV(ˆτ 2(Xi; Sest, Π)\n\u0003\n. For splitting we can estimate both components of this expectation using only the training\nsample. This leads to an estimator for the infeasible criterion that depends only on Str:\n\\\nEMSEτ(Str, Π) ≡\n1\nN tr\nX\ni∈Str\nˆτ 2(Xi; Str, Π)\n−2\nN tr ·\nX\nℓ∈Π\nS2\nStr\ntreat(ℓ)\np\n+\nS2\nStr\ncontrol(ℓ)\n1 −p\n! . [10]",
    "content_hash": "9be0e9463a8026998bd033637f3fa63fffc0193b86447486d39ede8592a20050",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Modifying the Honest Approach",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1ee23af9-6256-46cc-b847-2db866e4ad13",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "3.1\nModifying Conventional CART for Treatment Eﬀects\nConsider ﬁrst modifying conventional (adaptive) CART to estimate heterogeneous treatment\neﬀects. Note that in the prediction case, using the fact that ˆµ is constant within each leaf, we\ncan write\nMSEµ(Ste, Str, Π) = −2\nN tr\nX\ni∈Ste\nˆµ(Xi; Ste, Π) · ˆµ(Xi; Str, Π)\n+ 1\nN tr\nX\ni∈Ste\nˆµ2(Xi; Str, Π). In the treatment eﬀect case we can use the fact that\nESte\n\u0002\nτi|i ∈Ste : i ∈ℓ(x, Π)\n\u0003\n= ESte\n\u0002\nˆτ(x; Ste, Π)\n\u0003\nto construct an unbiased estimator of MSEτ(Ste, Str, Π):\n[\nMSEτ(Ste, Str, Π) ≡−2\nN tr\nX\ni∈Ste\nˆτ(Xi; Ste, Π) · ˆτ(Xi; Str, Π)\n+ 1\nN tr\nX\ni∈Ste\nˆτ 2(Xi; Str, Π). This leads us to propose, by analogy to CART’s in-sample mean-squared error criterion −MSEµ(Str, Str, Π),\n−[\nMSEτ(Str, Str, Π) =\n1\nN tr\nX\ni∈Str\nˆτ 2(Xi; Str, Π),\nas an estimator for the infeasible in-sample goodness of ﬁt criterion. For cross-validation we used in the prediction case −MSEµ(Str,cv, Str,tr, Π).",
    "content_hash": "a5b8f4c0e89697cab37714d5612fd4b1d6b24dce7a642baade2357a8da55bdfc",
    "location": null,
    "page_start": 10,
    "page_end": 10,
    "metadata": {
      "section": "Modifying Conventional CART for Treatment Eﬀects",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1fd4a088-9fec-4523-93ca-63e7e254a792",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Similar approaches are used in [2], [6], [22], and [29]. Because E[Y ∗\ni |Xi = x] = τ(x), oﬀ-the-\nshelf CART methods can be used directly, where estimates of the sample average of Y ∗\ni within\neach leaf can be interpreted as estimates of treatment eﬀects. This ease of application is the\nkey attraction of this method. The main drawback (relative to CT-A) is that in general it\nis not eﬃcient because it does not use the information in the treatment indicator beyond the\nconstruction of the transformed outcome. For example, the sample average in S of Y ∗\ni within a\n[11]",
    "content_hash": "47f06957e71969f5745956d1a7641b226ed505040bde68b14a809d9ce30010bf",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Transformed Outcome Trees (TOT)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "582d2d47-bc6c-4734-a9b0-0f375e3a9619",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "For each of the four types there is an adaptive version and an honest version, where the latter\ntakes into account that estimation will be done on a sample separate from the sample used for\nconstructing the partition, leading to a total of eight estimators. Note that further variations\nare possible; for example, one could use adaptive splitting and cross-validation methods to\nconstruct a tree, but still perform honest estimation on a separate sample. We do not consider\nthose variations in this paper. 4.1\nCausal Trees (CT)\nThe discussion above developed our preferred estimator, Causal Trees. To summarize, for the\nadaptive version of causal trees, denoted CT-A, we use for splitting the objective −[\nMSE(Str, Str, Π). For cross-validation we use the same objective function, but evaluated at the samples Str,cv and\nStr,tr, namely −[\nMSE(Str,cv, Str,tr, Π). For the honest version, CT-H, the splitting objective\nfunction is −\\\nEMSE(Str, Π). For cross-validation we use the same objective function, but now\nevaluated at the cross validation sample, −\\\nEMSE(Str,cv, Π). 4.2\nTransformed Outcome Trees (TOT)\nOur ﬁrst alternative method is based on the insight that by using a transformed version of\nthe outcome Y ∗\ni\n= (Yi −Wi)/(p · (1 −p)), it is possible to use oﬀ-the-shelf regression tree\nmethods to focus splitting and cross-validation on treatment eﬀects rather than outcomes.",
    "content_hash": "f9b4ffa91739556815115fcc4c1c05c27111a6964cf01c98f41d41bcb3c0b38a",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Causal Trees (CT)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "03491adf-33ae-4472-9b18-be3f5227e4c2",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "For cross-validation we use the same expression, now with the cross-validation sample: \\\nEMSEτ(Str,cv, Π). These expressions are directly analogous to the criteria we proposed for the honest version of\nCART in the prediction case. The criteria reward a partition for ﬁnding strong heterogeneity\nin treatment eﬀects, and penalize a partition that creates variance in leaf estimates. One\ndiﬀerence with the prediction case, however, is that in the prediction case, the two terms are\nproportional; whereas for the treatment eﬀect case they are not. It is possible to reduce the\nvariance of a treatment eﬀect estimator by introducing a split, even if both child leaves have the\nsame average treatment eﬀect, if a covariate aﬀects the mean outcome but not treatment eﬀects. In such a case, the split results in more homogenous leaves, and thus lower-variance estimates of\nthe means of the treatment group and control group outcomes. Thus, the distinction between\nadaptive and honest splitting criterion will be more pronounced in this case. The cross-validation criterion estimates treatment eﬀects within leaves using the Str,cv sam-\nple rather than Str,tr, to account for the fact that leaf estimates will subsequently be constructed\nusing an estimation sample that is independent of the training sample. 4\nFour Partitioning Estimators for Causal Eﬀects\nIn this section we brieﬂy summarize our CT estimator, and then describe three alternative\ntypes of estimators. We compare CT to the alternatives theoretically and through simulations.",
    "content_hash": "4056bd93c7e3133a45f72013b0ea8c3924d4e54f4ecf574860fc1177ef556d83",
    "location": null,
    "page_start": 11,
    "page_end": 11,
    "metadata": {
      "section": "Modifying the Honest Approach",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "675c9565-631b-4f69-bd46-733b282ca950",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "given leaf ℓ(x; Π) will only be equal to ˆτ(x; Π, S) if the fraction of treated observations within\nthe leaf is exactly equal to p. Since this method is primarily considered as a benchmark, in\nsimulations we focus only on an adaptive version that can use existing learning methods entirely\noﬀ-the-shelf. The adaptive version of the transformed outcome tree estimator we consider, TOT-\nA, uses the conventional CART algorithm with the transformed outcome replacing the original\noutcome. The honest version, TOT-H, uses the same splitting and cross-validation criteria, so\nthat it builds the same trees; it diﬀers only in that a separate estimation sample is used to\nconstruct the leaf estimates. The treatment eﬀect estimator within a leaf is the same as the\nadaptive method, that is, the sample mean of Y ∗\ni within the leaf. 4.3\nFit-based Trees (F)\nWe consider two additional alternative methods for constructing trees, based on suggestions in\nthe literature. In the ﬁrst of these alternatives the choice of which feature to split on, and at\nwhat value of the feature to split, is based on comparisons of the goodness–of–ﬁt of the outcome\nrather than the treatment eﬀect. In standard CART of course goodness–of–ﬁt of outcomes is\nalso the split criterion, but here we estimate a model for treatment eﬀects within each leaf. Speciﬁcally, we have a linear model with an intercept and an indicator for the treatment as\nthe regressors, rather only an intercept as in standard CART. This approach is used in [30],\nwho consider building general models at the leaves of the trees. Treatment eﬀect estimation is\na special case of their framework.",
    "content_hash": "48484acde2ba9518004779126eb7fea37b6093943e86656fbe68d53c9f2caf09",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Fit-based Trees (F)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7a90fb51-12c7-4259-9d6b-9fe571a3f3a8",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "The ﬁrst split\nwould be better from the perspective of estimating heterogeneous treatment eﬀects, but the ﬁt\ncriterion would view the two splits as equally attractive. 4.4\nSquared T-statistic Trees (TS)\nFor the last estimator we look for splits with the largest value for the square of the t-statistic\nfor testing the null hypothesis that the average treatment eﬀect is the same in the two potential\nleaves. This estimator was proposed by [21]. If the two leaves are denoted L (Left) and R\n[12]",
    "content_hash": "797caf351acebd9f3f952bd717dd04cc6175e74c74a5de2a1d103b7ad5bcaa03",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Squared T-statistic Trees (TS)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5e74b379-a999-4f12-993a-c56c6f7c22ee",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "[30] propose using statistical tests based on improvements\nin goodness-of-ﬁt to determine when to stop growing the tree, rather than relying on cross-\nvalidation, but for ease of comparison to CART, in this paper we will stay closer to traditional\nCART in terms of growing deep trees and pruning them. We modify the mean-squared error\nfunction:\nMSEµ,W (Ste, Sest, Π) ≡\nX\ni∈Ste\n((Y obs\ni\n−ˆµw(Wi, Xi; Sest, Π))2 −Y 2\ni ). For the adaptive version F-A we follow conventional CART, using the criterion −MSEµ,W in\nplace of −MSE for splitting, and the analog of −[\nMSE(Str,cv, Str,tr, Π) with with ˆµw in place\nof ˆµ for cross-validation. For the honest version we use the analogs of −\\\nEMSE(Str, Π) and\n−\\\nEMSE(Str,cv, Π), with ˆµw in place of ˆµ, for splitting and cross-validation. Similar to the\nprediction case, the variance term in the honest splitting criterion does not make much of a\ndiﬀerence for the choice of splits. An advantage of the ﬁt-based tree approach is that it is\na straightforward extension of conventional CART methods. In particular, the mean-squared\nerror criterion is feasible, since Yi is observed. To highlight the disadvantages of the F approach,\nconsider a case where two splits improve the ﬁt to an equal degree. In one case, the split leads\nto variation in average treatment eﬀects, and in the other case it does not.",
    "content_hash": "0bdcf09206660717a2b63ef060a17ec04ddc17420b8920ba53698a49f2aed10b",
    "location": null,
    "page_start": 12,
    "page_end": 12,
    "metadata": {
      "section": "Fit-based Trees (F)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "468d59e8-c99a-4b3c-9ee4-8786f8d44d07",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "(Right), the square of the t-statistic is\nT 2 ≡N ·\n(Y L −Y R)2\nS2/NL + S2/NR\n,\nwhere S2 is the conditional sample variance given the split. At each leaf, successive splits are\ndetermined by selecting the split that maximizes T 2. The concern with this criterion is that it\nplaces no value on splits that improve the ﬁt. While such splits do not deserve as much weight\nas the ﬁt criterion puts on them, they do have some value. Both the adaptive and honest versions of the TS approach use T 2 as the splitting criterion. For cross-validation and pruning, it is less obvious how to proceed. [30] suggests that when\nusing a statistical test for splitting, if it is desirable in an application to grow deep trees and\nthen cross-validate to determine depth, then one can use a standard goodness of ﬁt measure\nfor pruning and cross-validation. However, this could undermine the key advantage of TS, to\nfocus on heterogeneous treatment eﬀects. For this reason, we instead propose to use the CT-A\nand CT-H criteria for cross-validation for TS-A and TS-H, respectively. 4.5\nComparison of the Causal Trees, the Fit Criterion, and the Squared\nt-statistic Criterion\nIt is useful to compare our proposed criterion to the F and TS criteria in a simple setting to gain\ninsight into the relative merits of the three approaches. We do so here focusing on a decision\nwhether to proceed with a single possible split, based on a binary covariate Xi ∈{L, R}.",
    "content_hash": "9808aee018da1581572af3b89dec9378e685456006dda13c4a916f5dbeacd574",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "t-statistic Criterion",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "fda5000c-27b6-44ac-94fd-a8c33050e1de",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Let\nΠN and ΠS denote the trees without and with the split, and let Y w, Y Lw and Y Rw denote\nthe average outcomes for units with treatment status Wi = w. Let Nw, NLw, and NRw be the\nsample sizes for the corresponding subsamples. Let S2 be the sample variance of the outcomes\ngiven a split, and let ˜S2 be the sample variance without a split. Deﬁne the squared t-statistics\nfor testing that the average outcomes for control (treated) units in both leaves are identical,\nT 2\n0 ≡\n(Y L0 −Y R0)2\nS2/NL0 + S2/NR0\n,\nT 2\n1 ≡\n(Y L1 −Y R1)2\nS2/NL1 + S2/NR1\n. Then we can write the improvement in goodness of ﬁt from splitting the single leaf into two\nleaves as\nF = ˜S2 ·\n2 · (T 2\n0 + T 2\n1 )\n1 + 2 · (T 2\n0 + T 2\n1 )/N . Ignoring degrees-of-freedom correctcions, the change in our proposed criterion for the honest\nversion of the causal tree in this simple setting can be written as a combination of the F and\nTS criteria:\n\\\nEMSEτ(S, ΠN) −\\\nEMSEτ(S, ΠS) = (T 2 −4)( ˜S2 −F/N) + 2 ˜S2\np · (1 −p)\n. Our criterion focuses primarily on T 2. Unlike the TS approach, however, it incorporates the\nbeneﬁts of splits due to improvement in the ﬁt. [13]",
    "content_hash": "a2ae5ffd262582ef0afee64d4b9a05590768e6e6cf2fc34edde348ebeee2df20",
    "location": null,
    "page_start": 13,
    "page_end": 13,
    "metadata": {
      "section": "Inference",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "27853418-1d4c-4de9-97bf-fb8a6dbe1599",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "The ﬁrst panel of Table 1 compares the number of leaves in diﬀerent designs and diﬀerent\nvalues of N tr = N est. Recalling that TOT-A and TOT-H have the same splitting method,\n[14]",
    "content_hash": "31b2242f18797543b28ff1eec097625b1047074994158655517f88f5a762479d",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "A Simulation Study",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "ae144a8f-695d-4852-aac6-2956d63c6713",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "5\nInference\nGiven the estimated conditional average treatment eﬀect we also would like to do inference. Once constructed, the tree is a function of covariates, and if we use a distinct sample to conduct\ninference, then the problem reduces to that of estimating treatment eﬀects in each member of\na partition of the covariate space. For this problem, standard approaches are therefore valid\nfor the estimates obtained via honest estimation, and in particular, no assumptions about\nmodel complexity are required. For the adaptive methods standard approaches to conﬁdence\nintervals are not generally valid for the reasons discussed above, and below we document through\nsimulations that this can be important in practice. 6\nA Simulation Study\nTo assess the relative performance of the proposed algorithms we carried out a small simulation\nstudy with three distinct designs. In Table 1 we report a number of summary statistics from the\nsimulations. We report averages; results for medians are similar. We report results for N tr =\nN est with either 500 or 1000 observations. When comparing adaptive to honest approaches,\nwe report the ratio of the MSEτ for adaptive estimation with N tr = 1000 to MSEτ for honest\nestimation with N tr = N est = 500, in order to highlight the tradeoﬀbetween sample size and\nbias reduction that arises with honest estimation. We evaluate MSEτ using a test sample with\nN te = 6000 observations to test the methods in order to minimize the sampling variance in our\nsimulation results. In all designs, the marginal treatment probability is p = 0.5. K denotes the number of\nfeatures. In each design, we have a model η(x) for the mean eﬀect and κsim(x) for the treatment\neﬀect.",
    "content_hash": "11f948d473ce7e11d06640a9037a730df85f4cc372b2fa72b3d31dbdd936c51c",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "A Simulation Study",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "8dceab8e-8046-4a0e-af71-9aa3b5f44327",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Then, the potential outcomes are written\nYi(w) = ηsim(Xi) + 1\n2 · (2w −1) · κ(Xi) + ǫi,\nwhere ǫi ∼N(0, .01), and the Xi are independent of ǫi and one another, and Xi ∼N(0, 1). The designs are summarized as follows:\n1: K = 2; η(x) = 1\n2x1 + x2; κ(x) = 1\n2x1. 2: K = 10; η(x) = 1\n2\n2\nX\nk=1\nxk +\n6\nX\nk=3\nxk; κ(x) =\n2\nX\nk=1\n1{xk > 0} · xk\n3: K = 20; η(x) = 1\n2\n4\nX\nk=1\nxk +\n8\nX\nk=5\nxk; κ(x) =\n4\nX\nk=1\n1{xk > 0} · xk\nIn each design, there are some covariates that aﬀect treatment eﬀects (κ) and mean outcomes\n(η); some covariates that enter η but not κ; and some covariates that do not aﬀect outcomes at\nall (“noise” covariates). Design 1 does not have noise covariates. In Designs 2 and 3, the ﬁrst\nfew covariates enter κ, but only when their signs are positive, while they aﬀect η throughout\ntheir range. Diﬀerent criterion will thus lead to diﬀerent optimal splits, even within a covariate;\nF will focus more on splits when the covariates are negative.",
    "content_hash": "c68fa70b8fcd92c27269f363349aea39bfb1f944666c47c8d5cf75db638aa2ab",
    "location": null,
    "page_start": 14,
    "page_end": 14,
    "metadata": {
      "section": "A Simulation Study",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "773f2ac0-45ab-4c05-9a4b-038abb452820",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Thus it has double the sample size (1000\nobservations) at each step, while the honest version uses 500 of the observations in training and\ncross-validation, with the complement used for estimating treatment eﬀects within leaves. The\nresults show that there is a cost to honest estimation in terms of MSEτ, varying by design and\nestimator. The ﬁnal two panels of Table 1 show the coverage rate for 90% conﬁdence intervals. We\nachieve nominal coverage rates for honest methods in all designs, where, in contrast, the adap-\ntive methods have coverage rates substantially below nominal rates. Thus, our simulations bear\nout the tradeoﬀthat honest estimation sacriﬁces some goodness of ﬁt (of treatment eﬀects) in\nexchange for valid conﬁdence intervals. 7\nObservational Studies with Unconfoundedness\nThe discussion so far has focused on the setting where the assignment to treatment is random-\nized. The proposed methods can be adapted to observational studies under the assumption of\nunconfoundedness. In that case we need to modify the estimates within leaves to remove the\nbias from simple comparisons of treated and control units. There is a large literature on meth-\n[15]",
    "content_hash": "57e72db816b0af6f6de95445cdcd67a58ef8e8ecdfc67e92046de655be758e06",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "Observational Studies with Unconfoundedness",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "07d877f9-ec14-4436-a0ab-a9594cbef348",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "we see that it tends to build shallow trees. The failure to control for the realized value of Wi\nleads to additional noise in estimates, which tends to lead to aggressive pruning. For the other\nestimators, the adaptive versions lead to shallower trees than the honest versions, as the honest\nversions correct for overﬁtting, and the main cost of small leaf size is high variance in leaf\nestimates. F-A and F-H are very similar; as discussed above, the splitting criterion are very\nsimilar, and further, the F estimators are less prone to overﬁtting treatment eﬀects, because\nthey split based upon overall model ﬁt. We also observe that the F estimators build the deepest\ntrees; they reward splitting on covariates that aﬀect mean outcomes as well as treatment eﬀects. The second panel of Table 1 examines the performance of the alternative honest estimators,\nas evaluated by the infeasible criterion MSEτ. We report the average of the ratio of MSEτ for\na given estimator to MSEτ for our preferred estimtor, CT-H. The TOT-H estimator performs\nwell in Designs 2 and 3, but suﬀers in Design 1. In Design 1, the variance of Yi conditional\non (Wi, Xi) is very low at .01, and so the failure of TOT to account for the realization of Wi\nresults in a noticeable loss of performance. The F-H estimator suﬀers in all 3 designs; all designs\ngive the F-H criterion attractive opportunities to split based on covariates that do not enter\nκ.",
    "content_hash": "7d5fb50f65812765f93bae2732fd4ffae94d6df8ab65f38dcc2c767ef81c7176",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "A Simulation Study",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a52a101a-0870-48a6-991d-947fff7c0c4d",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "F-H would perform better in alternative designs where η(x) = κ(x); F-H also does well at\navoiding splits on noise covariates. The TS-H estimator performs well in Design 1, where x1\naﬀects η and κ the same way, so that the CT-H criterion is aligned with TS-H. Design 3 is more\ncomplex, and the ideal splits from the perspective of balancing overall mean-squared error of\ntreatment eﬀects (including variance reduction) are diﬀerent from those favored by TS-H. Thus,\nTS performs worse, and the diﬀerence is exacerbated with larger sample size, where there are\nmore opportunities for the estimators to build deeper trees and thus to make diﬀerent choices. We also calculate comparisons based on a feasible criterion, the average squared diﬀerence\nbetween the transformed outcome Y ∗\ni\nand the estimated treatment eﬀect ˆτi. For details for\nthis comparison see the SI Appendix. In general the results are consistent with those from the\ninfeasible criterion. The third panel of Table 1 explores the costs and beneﬁts to honest estimation. The Table re-\nports the ratio of MSEτ(Ste, Sest∪Str, πEstimator-A(Sest∪Str)) to MSEτ(Ste, Sest, πEstimator-H(Str)\nfor each estimator. The adaptive version uses the union of the training and estimation samples\nfor tree-building, cross-validation, and leaf estimation.",
    "content_hash": "0c6a306ba5813ddd5935f5a1f52b3c1412b49a865e63c608702874e5665cae93",
    "location": null,
    "page_start": 15,
    "page_end": 15,
    "metadata": {
      "section": "A Simulation Study",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "22e5ac11-a20f-4a80-ad64-2b8a78f95d2c",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Their approach is similar\nto ours in that they distinguish between the estimation of treatment eﬀects and the estimation\nof the impact of other attributes of units. [25] consider a model with the outcome linear in\nthe covariates and the interaction with the treatment variable. Using Bayesian nonparametric\nmethods with Dirichlet priors, they project their estimates of heterogeneous treatment eﬀects\ndown onto the feature space using LASSO-type regularization methods to get low-dimensional\nsummaries of the heterogeneity. [6] and [2] propose a related appoach for ﬁnding the optimal\ntreatment policy that combines inverse propensity score methods with “direct methods” (e.g. the “single tree” approach considered above) that predict the outcome as a function of the\ntreatment and the unit attributes. The methods can be used to evaluate the average diﬀerence\nin outcomes from any two policies that map attributes to treatments, as well as to select the\noptimal policy function. They do not focus on hypothesis testing for heterogeneous treatment\neﬀects, and they use conventional approaches for cross-validation. Also related is the work on\nTargeted Learning [27], which modiﬁes the loss function to increase the weight on the parts of\nthe likelihood that concern the parameters of interest. 9\nConclusion\nIn this paper we introduce new methods for constructing trees for causal eﬀects that allow us to\ndo valid inference for the causal eﬀects in randomized experiments and in observational studies\nsatisfying unconfoundedness, without restrictions on the number of covariates or the complexity\nof the data generating process. Our methods partition the feature space into subspaces.",
    "content_hash": "90b8d19fe48b00aacf28dd70461053f7a097eaeb09a87b6756fcd352179484cd",
    "location": null,
    "page_start": 16,
    "page_end": 16,
    "metadata": {
      "section": "Conclusion",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "7ea86389-99c4-435c-86eb-133fc93c0ecc",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Kern, (2010), Detecting Heterogeneous Treatment Eﬀects in Large-Scale\nExperiments Using Bayesian Additive Regression Trees, Unpublished Manuscript, Yale\nUniversity. [9] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data\nMining, Inference, and Prediction, Second Edition, (2011), Springer. [10] K. Hirano, G. Imbens and G. Ridder, Eﬃcient Estimation of Average Treatment Eﬀects\nUsing the Estimated Propensity Score, Econometrica, 71 (4), (2003), 1161-1189. [11] P. Holland, Statistics and Causal Inference (with discussion), Journal of the American\nStatistical Association, 81, (1986), 945-970. [12] D. Horvitz, and D. Thompson, A generalization of sampling without replacement from a\nﬁnite universe, Journal of the American Statistical Association, Vol. 47, (1952), 663685. [13] K. Imai and M. Ratkovic, Estimating Treatment Eﬀect Heterogeneity in Randomized\nProgram Evaluation, Annals of Applied Statistics, 7(1), (2013), 443-470. [14] G. Imbens and D. Rubin, Causal Inference for Statistics, Social, and Biomedical Sciences:\nAn Introduction, Cambridge University Press, (2015). [15] J. Pearl, Causality: Models, Reasoning and Inference, Cambridge University Press, (2000). [16] P. Rosenbaum, Observational Studies, (2002), Springer. [17]",
    "content_hash": "0eedae9180c9b120a1d19fbe1c4eda44407b62c32d26ac78e58b6f3b685f74e3",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1ae11764-e219-43fc-b103-bc6eb4a520a7",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "treatment eﬀects, and can report conﬁdence intervals for these estimates without concern about\nmultiple testing. References\n[1] A. Abadie and G. Imbens, Large Sample Properties of Matching Estimators for Average\nTreatment Eﬀects, Econometrica, 74(1), 235-267. [2] A. Beygelzimer and J. Langford, The Oﬀset Tree for Learning with Partial Labels,\nhttp://arxiv.org/pdf/0812.4044v2.pdf, (2009). [3] L. Breiman, Random forests, Machine Learning, 45, (2001), 5-32. [4] L. Breiman, J. Friedman, R. Olshen, and C. Stone, Classiﬁcation and Regression Trees,\n(1984), Wadsworth. [5] R. Crump, R., J. Hotz, G. Imbens, and O. Mitnik, Nonparametric Tests for Treatment\nEﬀect Heterogeneity, Review of Economics and Statistics, 90(3), (2008), 389-405. [6] M. Dudik, J. Langford and L. Li, Doubly Robust Policy Evaluation and Learning , Pro-\nceedings of the 28th International Conference on Machine Learning (ICML-11), (2011). [7] J. Foster, J. Taylor and S. Ruberg, Subgroup Identiﬁcation from Randomized Clinical\nData, Statistics in Medicine, 30, (2010), 2867-2880. [8] Green, D., and H.",
    "content_hash": "d3becc9030b76045012f08bb5db989a9e234a569f1f08e29bc96c80002a6ab99",
    "location": null,
    "page_start": 17,
    "page_end": 17,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "df3ffbcc-d940-46cb-8b0a-11ce2c086cb1",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "[17] P. Rosenbaum and D. Rubin, The Central Role of the Propensity Score in Observational\nStudies for Causal Eﬀects, Biometrika, 70, (1983), 41-55. [18] M. Rosenblum and M. Van Der Laan., Optimizing Randomized Trial Designs to Distinguish\nwhich Subpopulations Beneﬁt from Treatment , Biometrika, 98(4), (2011), 845-860. [19] D. Rubin, Estimating Causal Eﬀects of Treatments in Randomized and Non-randomized\nStudies Journal of Educational Psychology, 66, (1974), 688-701. [20] D. Rubin, Bayesian inference for causaleﬀects: The Role of Randomization, Annals of\nStatistics, 6, (1978), 34-58. [21] X. Su, C. Tsai, H. Wang, D. Nickerson, and B. Li, Subgroup Analysis via Recursive\nPartitioning, Journal of Machine Learning Research, 10, (2009), 141-158. [22] J. Signovitch, J., Identifying informative biological markers in high-dimensional genomic\ndata and clinical trials, PhD Thesis, Department of Biostatistics, Harvard University,\n(2007). [23] L. Tian, A. Alizadeh, A. Gentles, and R. Tibshirani, A Simple Method for Estimating\nInteractions Between a Treatment and a Large Number of Covariates, Journal of the Amer-\nican Statistical Association, 109(508), (2014) 1517-1532. [24] R.",
    "content_hash": "481260a4c28b2fb40c8fd26cd2d02c28f0adc9096183f8c95f68cfac28397a4e",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "b2b9b825-3836-46d5-976c-75c9e9e8b97c",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Tibshirani, Regression shrinkage and selection via the lasso, Journal of the Royal Sta-\ntistical Society. Series B (Methodological), Volume 58, Issue 1. (1996), 267-288. [25] M. Taddy, M. Gardner, L. Chen, and D. Draper,, Heterogeneous Treatment Eﬀects in\nDigital Experimentation, Unpublished Manuscript, (2015), arXiv:1412.8563. [26] V. Vapnik, Statistical Learning Theory, Wiley, (1998). [27] M. Van Der Laan, and S. Rose, Targeted Learning: Causal Inference for Observational\nand Experimental Data, Springer, (2011). [28] S. Wager, and S. Athey, Estimation and Inference of Heterogeneous Treatment Eﬀects\nusing Random Forests, http://arxiv.org/pdf/1510.04342v2.pdf, (2015). [29] H. Weisburg, H. and V. Pontes, Post hoc subgroups in Clinical Trials: Anathema or\nAnalytics? Clinical Trials, June, 2015. [30] A. Zeileis, T. Hothorn, and K. Hornik, Model-based recursive partitioning. Journal of\nComputational and Graphical Statistics, 17(2), (2008), 492-514. [18]",
    "content_hash": "f66ec5b77cdccdca3ccc1196f012bd5c7e45f913a86eea7ea5e69d67658fc444",
    "location": null,
    "page_start": 18,
    "page_end": 18,
    "metadata": {
      "section": "References",
      "heading_level": 1
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "5d4b9f03-60bb-4c8f-8263-5c781d6a7026",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Table 1: Simulation Study\nDesign\n1\n2\n3\nN tr = N est\n500\n1000\n500\n1000\n500\n1000\nEstimator\nNumber of Leaves\nTOT\n2.8\n3.4\n2.1\n2.7\n4.7\n6.1\nF-A\n6.1\n13.2\n6.3\n13.1\n6.1\n13.2\nTS-A\n4.0\n5.6\n2.5\n3.3\n4.4\n8.9\nCT-A\n4.0\n5.7\n2.3\n2.5\n4.5\n6.2\nF-H\n6.1\n13.2\n6.4\n13.3\n6.3\n13.4\nTS-H\n4.4\n7.7\n5.3\n11.0\n6.0\n12.3\nCT-H\n4.2\n7.5\n5.3\n11.2\n6.2\n12.3\nInfeasible MSE Divided by Infeasible MSE for CT-H∗\nTOT-H\n1.77\n2.12\n1.03\n1.04\n1.03\n1.05\nF-A\n1.93\n1.54\n1.69\n2.07\n1.63\n2.08\nTS-H\n1.01\n1.02\n1.06\n0.99\n1.24\n1.38\nCT-H\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nRatio of Infeasible MSE: Honest to Adaptive∗∗\nTOT-H/TOT-A\n0.99\n0.86\n0.76\nF-H/F-A\n0.50\n0.98\n0.91\nTS-H/TS-A\n0.92\n0.90\n0.85\nCT-H/CT-A\n0.91\n0.93\n0.76\nCoverage of 90% Conﬁdence Intervals - Adaptive\nTOT-A\n0.83\n0.86\n0.83\n0.83\n0.74\n0.79\nF-A\n0.89\n0.89\n0.86\n0.86\n0.82\n0.82\nTS-A\n0.85\n0.85\n0.80\n0.83\n0.77\n0.80\nCT-A\n0.85\n0.85\n0.81\n0.83\n0.80\n0.81\nCoverage of 90% Conﬁdence Intervals - Honest\nTOT-H\n0.90\n0.89\n0.90\n0.92\n0.89\n0.89\nF-H\n0.91\n0.90\n0.90\n0.90\n0.90\n0.89\nTS-H\n0.89\n0.90\n0.90\n0.90\n0.90\n0.90\nCT-H\n0.90\n0.90\n0.90\n0.89\n0.90\n0.90\n∗MSEτ(Ste, Sest, πEstimator(Str))/MSEτ(Ste, Sest, πCT-H(Str))\n∗∗MSEτ(Ste, Sest ∪Str, πEstimator-A(Sest ∪Str))/\nMSEτ(Ste, Sest, πEstimator-H(Str)\n[19]",
    "content_hash": "71d8ab37f6edcae03eaa764f608aa8bc2d1ddadff9034b7f60b0e9563b4d42dc",
    "location": null,
    "page_start": 19,
    "page_end": 19,
    "metadata": {
      "section": "Additional Simulation Details (Online Appendix)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "01ad0d28-3caa-4e0e-b130-a7c78f6d8246",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "In the simulations\nreported in Table 1 of the paper and in this Appendix, we use nm = 25 for all models except\nTOT, while for the TOT model the minimum leaf size is 50 (without restrictions on treated\nand control observations). We make one additional modiﬁcation to the way the standard rpart splitting function works. We restrict the set of potential split points considered, and further, in the splitting process\nwe rescale the covariate values within each leaf and each treatment group in order to ensure\nthat when moving from one potential split point to the next, we move the same number of\ntreatment and control observations from the right leaf to the left leaf. We begin by describing\nthe motivation for these modiﬁcations, and then we give details. The rpart algorithm considers every value of Xi,k in Str as a possible split point for covariate\nXk. An obvious disadvantage of this approach is that computation time can grow prohibitively\nlarge in models with many observations and covariates. But there are some more subtle disad-\nvantages as well. The ﬁrst is that there will naturally be sampling variation in estimates of ˆτ\nas we vary the possible split points. A problem akin to a multiple hypothesis testing problem\narises: since we are looking for the maximum value of an estimated criterion across a large\nnumber of possible split points, as the number of split points tested grows, it becomes more\nand more likely that one of the splits for a given covariate appears to improve the ﬁt criterion\neven if the true value of the criterion would indicate that it is better not to split.",
    "content_hash": "bcc672ae7af757c5619770529588dd7a6fc4814f3e7947eb07aeddc5af5eb206",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "Additional Simulation Details (Online Appendix)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "c2a2cf75-3526-477a-89ab-7c44e9461af0",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Additional Simulation Details (Online Appendix)\nThis Appendix describes some additional details of the simulation study, and also presents\nadditional simulation results in Appendix Table A1. The code for our simulations was written as an R software package that is in prepara-\ntion for public release. It is based on the ‘rpart’ R package, available at https://cran.r-\nproject.org/web/packages/rpart/index.html. For TOT, we directly use rpart applied to the\ntransformed outcome Y ∗\ni , and we use 10-fold cross-validation for pruning the tree. For each of\nour other estimators, we modiﬁed several components of the package. In the remainder of this\nAppendix, discussions of modiﬁcations apply to F, CT, and TS estimators. For these estimators,\nwe create new versions of the “anova” functions, functions that in the standard rpart package\nare used for calculating the total goodness of ﬁt for a node of the tree, evaluating the quality\nof alternative splits, and estimating the goodness of ﬁt for pruned trees using cross-validation\nsamples. We maintain the overall structure of the rpart package. The rpart package has an\nimportant tuning parameter, which is the minimum number of observations per leaf, denoted\nnm. We modify the rpart routine to insist on at least nm treated and nm control observations\nper leaf, to ensure that we can calculate a treatment eﬀect within each leaf.",
    "content_hash": "6a6a1bb2f019589f4b07d266a40b56308016ed5545a51950bb46831ff945295a",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "Additional Simulation Details (Online Appendix)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "a6f28078-ac7a-4799-b97f-b8f69fc5cdea",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "One way to\nmitigate both the computation time problem and the multiple-testing problem is to consider\nonly a limited number of split points. A third problem is speciﬁc to considering treatment eﬀect heterogeneity. To see the problem,\nsuppose that a covariate strongly aﬀects the mean of outcomes, but not treatment eﬀects. Within a leaf, some observations are treated and some are control. If we consider every level\nof the covariate in the leaf as a possible split point, then shifting from one split point to the\nnext shifts a single observation from the right leaf to the left leaf. This observation is in the\ntreatment or the control group, but not both; suppose it is in the treatment group. If the\ncovariate has a strong eﬀect on the level of outcomes, the observation that is shifted will be\nlikely have an outcome more extreme than average. It will change the sample average of the\ntreatment group, but not the control group, leading to a large change in the estimated treatment\n[20]",
    "content_hash": "c8a4045c6e538d3dbe97151808c90df5156c343117d09e5c73d07c3ada5768af",
    "location": null,
    "page_start": 20,
    "page_end": 20,
    "metadata": {
      "section": "Additional Simulation Details (Online Appendix)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "57b29cdd-4071-4410-a02d-93fe9c52b05e",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "eﬀect diﬀerence. We expect the estimated diﬀerence in treatment eﬀects across the left and\nright leaves to ﬂuctuate greatly with the split point in this scenario. This variability around\nthe true mean diﬀerence in treatment eﬀects occurs more often when covariates aﬀect mean\noutcomes, and thus it can lead the estimators to split too much on such covariates, and also to\nﬁnd spurious opportunities to split. To address this problem, we propose the following modiﬁcations to the splitting rule. We\ninclude a parameter b, the target number of observations per “bucket.” For each leaf, before\ntesting possible splits for a particular covariate, we order the observations by the covariate value\nin the treatment and control group separately. Within each group, we place the observations\ninto buckets with b observations per bucket. If this results in less than nm buckets, then we use\nfewer observations per bucket (to attain nm buckets). We number the buckets, and considering\nsplitting by bucket number rather than the raw values of the covariates. This guarantees that\nwhen we shift from one split point to the next, we add both treatment and control observations,\nleading to a smoother estimate of the goodness of ﬁt function as a function of the split point. After the best bucket number to split on is selected, we translate that into a split point by\naveraging the maximum covariate value in the corresponding treatment and control buckets. In\nthe simulations presented in this paper, we do not constrain the maximum number of buckets,\nand we let b = 4.",
    "content_hash": "477c8b34fa4864d8ad48c3f2ac1c52a7a833a85d2207b6e0e28063cefc9f086a",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "Additional Simulation Details (Online Appendix)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "e091c41b-a773-4fd2-b209-206a1751563a",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "We found that this discretization approach improved goodness of ﬁt on\naverage for the simulations we considered, although it can in principle make things worse. In the simulations reported in Table 1 of ths paper, we used the infeasible MSEτ to evalute\nalternative estimators. In practice, we must estimate the infeasible criterion. In the paper, we\npropose estimators that rely on the tree structure of our estimator, but we may also wish to\ncompare our performance to estimators that don’t rely on partitions. One alternative is the\nMSETOT criterion. Given an estimator ˆτi, it is equal to\nMSETOT =\n1\nN te\nNte\nX\ni=1\n(Y ∗\ni −ˆτi)2. Because E[Y ∗\ni |Xi] = τ(Xi), this is an unbiased (but noisy) estimator for MSEτ. In Appendix\nTable A1, we present rankings of estimators using this criterion. We see that it ranks estimators\nin the same way as MSEτ except in one case (Design 3 with 500 observations), but the percentage\ndiﬀerences between estimators are smaller than with the infeasible criterion. Another tuning parameter for standard CART as well as the methods proposed here is the\nnumber of cross-validation samples. A common convention is to use 10 samples. We deviate\nfrom that convention and use 5 cross-validation samples. The reason is that our methods\nrequire various quantities to be estimated within leaves.",
    "content_hash": "1601546c7b1aa267f82f666d627474c79db0b46ccad266f469a094d26a452154",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "Additional Simulation Details (Online Appendix)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "1ddba9e4-29e7-4c3a-8e31-2de87178a683",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Given a minimum leaf restriction of\n25 treated and control units, if we take a cross-validation sample of one-tenth of the original\ntraining sample, we might end up with no treated or no control observations in a leaf in a\ncross-validation sample. In addition, it may be diﬃcult to estimate a sample variance within a\nleaf. Rather than require larger leaf sizes, we simply use fewer cross-validation samples. Appendix Table A1 also includes the full set of estimates for the infeasible criterion MSEτ,\nto illustrate how sample size and honest versus adaptive estimation aﬀects the criterion. Note\nthat for purposes of comparison to the simulation results from Table 1, Table 1 reports the\naverage over simulations of the ratio of the goodness of ﬁt measures; the second panel of this\ntable shows the average of goodness of ﬁt measures, but the ratio of the averages shown here is\nnot exactly equal to the average of the ratios shown in Table 1. [21]",
    "content_hash": "5b46c13582e127e525b3929673ab06724ca7531472330c9515ccfbbb5cb9e787",
    "location": null,
    "page_start": 21,
    "page_end": 21,
    "metadata": {
      "section": "Additional Simulation Details (Online Appendix)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  },
  {
    "id": "2e4554ba-4ee1-4930-aac0-541b8abbe60b",
    "source_id": "fbb14822-a241-4e86-8cf5-fcdaa73988d4",
    "content": "Appendix Table A1: Infeasible and Feasible MSE Estimates for Simulation Study\nDesign\n1\n2\n3\nN tr = N est\n500\n1000\n500\n1000\n500\n1000\nEstimator\nMSETOT\nτ\nDivided by MSETOT\nτ\nfor CT-H∗\nTOT-H\n1.010\n1.009\n1.001\n1.001\n1.004\n1.008\nF-H\n1.013\n1.004\n1.039\n1.049\n1.121\n1.161\nTS-H\n0.999\n1.000\n1.003\n0.999\n1.046\n1.056\nCT-H\n1.000\n1.000\n1.000\n1.000\n1.000\n1.000\nInfeasible MSEτ\nTOT-A\n0.171\n0.139\n1.267\n1.010\n3.235\n2.344\nF-A\n0.150\n0.075\n1.914\n1.861\n4.914\n4.443\nTS-A\n0.103\n0.077\n1.509\n1.080\n4.068\n3.169\nCT-A\n0.104\n0.079\n1.418\n1.071\n3.324\n2.314\nTOT-H\n0.140\n0.104\n1.176\n0.932\n3.102\n2.252\nF-H\n0.151\n0.075\n1.898\n1.850\n4.860\n4.420\nTS-H\n0.083\n0.053\n1.201\n0.887\n3.732\n2.935\nCT-H\n0.087\n0.054\n1.149\n0.910\n3.033\n2.143\n∗MSETOT\nτ\n(Ste, Sest, πEstimator(Str))/MSETOT\nτ\n(Ste, Sest, πCT-H(Str))\n[22]",
    "content_hash": "88c3cf143e755fd3200a18cefad64204a609b08468c4cda9c5c64f8f829794f9",
    "location": null,
    "page_start": 22,
    "page_end": 22,
    "metadata": {
      "section": "Additional Simulation Details (Online Appendix)",
      "heading_level": 3
    },
    "domain_id": "causal_inference"
  }
]
